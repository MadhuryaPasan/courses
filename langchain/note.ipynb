{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50646e10",
   "metadata": {},
   "source": [
    "langchain-core: The base abstractions (like runnable, messages, and standard interfaces).\n",
    "\n",
    "langchain: The chain, agent, and retrieval logic.\n",
    "\n",
    "langchain-community: The massive collection of specific tools, document loaders, vector stores, and chat models provided by the community and third parties (e.g., Wikipedia tools, FAISS vector store, specific PDF loaders)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca165fe",
   "metadata": {},
   "source": [
    "We will be taking an article draft and using LangChain to generate various useful items around this article. We'll be creating:\n",
    "\n",
    "01. An article title\n",
    "02. An article description\n",
    "03. Editor advice where we will insert an additional paragraph in the article\n",
    "04. A thumbnail / hero image for our article.\n",
    "\n",
    "Here we input our article to start with. Currently this is using an article from the Aurelio AI learning page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e1c6f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "import textwrap\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\" # still no good\n",
    "MODEL = \"qwen3:0.6b\" # okay\n",
    "# MODEL = \"gemma3:4b\" # okay\n",
    "BASE_URL = \"http://localhost:11434/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af265f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL, temperature=0.0)\n",
    "\n",
    "creative_llm = ChatOpenAI(model=MODEL, base_url=BASE_URL, temperature=0.9)\n",
    "\n",
    "# response = model.invoke(\"Why do parrots talk?\")\n",
    "# response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "352a8450",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"\n",
    "\\\n",
    "We believe AI's shortâ€”to mid-term future belongs to agents and that the long-term future of *AGI* may evolve from agentic systems. Our definition of agents covers any neuro-symbolic system in which we merge neural AI (such as an LLM) with semi-traditional software.\n",
    "\n",
    "With agents, we allow LLMs to integrate with code â€” allowing AI to search the web, perform math, and essentially integrate into anything we can build with code. It should be clear the scope of use cases is phenomenal where AI can integrate with the broader world of software.\n",
    "\n",
    "In this introduction to AI agents, we will cover the essential concepts that make them what they are and why that will make them the core of real-world AI in the years to come.\n",
    "\n",
    "---\n",
    "\n",
    "## Neuro-Symbolic Systems\n",
    "\n",
    "Neuro-symbolic systems consist of both neural and symbolic computation, where:\n",
    "\n",
    "- Neural refers to LLMs, embedding models, or other neural network-based models.\n",
    "- Symbolic refers to logic containing symbolic logic, such as code.\n",
    "\n",
    "Both neural and symbolic AI originate from the early philosophical approaches to AI: connectionism (now neural) and symbolism. Symbolic AI is the more traditional AI. Diehard symbolists believed they could achieve true AGI via written rules, ontologies, and other logical functions.\n",
    "\n",
    "The other camp were the connectionists. Connectionism emerged in 1943 with a theoretical neural circuit but truly kicked off with Rosenblatt's perceptron paper in 1958 [1][2]. Both of these approaches to AI are fascinating but deserve more time than we can give them here, so we will leave further exploration of these concepts for a future chapter.\n",
    "\n",
    "Most important to us is understanding where symbolic logic outperforms neural-based compute and vice-versa.\n",
    "\n",
    "| Neural | Symbolic |\n",
    "| --- | --- |\n",
    "| Flexible, learned logic that can cover a huge range of potential scenarios. | Mostly hand-written rules which can be very granular and fine-tuned but hard to scale. |\n",
    "| Hard to interpret why a neural system does what it does. Very difficult or even impossible to predict behavior. | Rules are written and can be understood. When unsure why a particular ouput was produced we can look at the rules / logic to understand. |\n",
    "| Requires huge amount of data and compute to train state-of-the-art neural models, making it hard to add new abilities or update with new information. | Code is relatively cheap to write, it can be updated with new features easily, and latest information can often be added often instantaneously. |\n",
    "| When trained on broad datasets can often lack performance when exposed to unique scenarios that are not well represented in the training data. | Easily customized to unique scenarios. |\n",
    "| Struggles with complex computations such as mathematical operations. | Perform complex computations very quickly and accurately. |\n",
    "\n",
    "Pure neural architectures struggle with many seemingly simple tasks. For example, an LLM *cannot* provide an accurate answer if we ask it for today's date.\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is commonly used to provide LLMs with up-to-date knowledge on a particular subject or access to proprietary knowledge.\n",
    "\n",
    "### Giving LLMs Superpowers\n",
    "\n",
    "By 2020, it was becoming clear that neural AI systems could not perform tasks symbolic systems typically excelled in, such as arithmetic, accessing structured DB data, or making API calls. These tasks require discrete input parameters that allow us to process them reliably according to strict written logic.\n",
    "\n",
    "In 2022, researchers at AI21 developed Jurassic-X, an LLM-based \"neuro-symbolic architecture.\" Neuro-symbolic refers to merging the \"neural computation\" of large language models (LLMs) with more traditional (i.e. symbolic) computation of code.\n",
    "\n",
    "Jurassic-X used the Modular Reasoning, Knowledge, and Language (MRKL) system [3]. The researchers developed MRKL to solve the limitations of LLMs, namely:\n",
    "\n",
    "- Lack of up-to-date knowledge, whether that is the latest in AI or something as simple as today's date.\n",
    "- Lack of proprietary knowledge, such as internal company docs or your calendar bookings.\n",
    "- Lack of reasoning, i.e. the inability to perform operations that traditional software is good at, like running complex mathematical operations.\n",
    "- Lack of ability to generalize. Back in 2022, most LLMs had to be fine-tuned to perform well in a specific domain. This problem is still present today but far less prominent as the SotA models generalize much better and, in the case of MRKL, are able to use tools relatively well (although we could certainly take the MRKL solution to improve tool use performance even today).\n",
    "\n",
    "MRKL represents one of the earliest forms of what we would now call an agent; it is an LLM (neural computation) paired with executable code (symbolic computation).\n",
    "\n",
    "## ReAct and Tools\n",
    "\n",
    "There is a misconception in the broader industry that an AI agent is an LLM contained within some looping logic that can generate inputs for and execute code functions. This definition of agents originates from the huge popularity of the ReAct agent framework and the adoption of a similar structure with function/tool calling by LLM providers such as OpenAI, Anthropic, and Ollama.\n",
    "\n",
    "![ReAct agent flow with the Reasoning-Action loop [4]. When the action chosen specifies to use a normal tool, the tool is used and the observation returned for another iteration through the Reasoning-Action loop. To return a final answer to the user the LLM must choose action \"answer\" and provide the natural language response, finishing the loop.](/images/posts/ai-agents/ai-agents-00.png)\n",
    "\n",
    "<small>ReAct agent flow with the Reasoning-Action loop [4]. When the action chosen specifies to use a normal tool, the tool is used and the observation returned for another iteration through the Reasoning-Action loop. To return a final answer to the user the LLM must choose action \"answer\" and provide the natural language response, finishing the loop.</small>\n",
    "\n",
    "Our \"neuro-symbolic\" definition is much broader but certainly does include ReAct agents and LLMs paired with tools. This agent type is the most common for now, so it's worth understanding the basic concept behind it.\n",
    "\n",
    "The **Re**ason **Act**ion (ReAct) method encourages LLMs to generate iterative *reasoning* and *action* steps. During *reasoning,* the LLM describes what steps are to be taken to answer the user's query. Then, the LLM generates an *action,* which we parse into an input to some executable code, which we typically describe as a tool/function call.\n",
    "\n",
    "![ReAct method. Each iteration includes a Reasoning step followed by an Action (tool call) step. The Observation is the output from the previous tool call. During the final iteration the agent calls the answer tool, meaning we generate the final answer for the user.](/images/posts/ai-agents/ai-agents-01.png)\n",
    "\n",
    "<small>ReAct method. Each iteration includes a Reasoning step followed by an Action (tool call) step. The Observation is the output from the previous tool call. During the final iteration the agent calls the answer tool, meaning we generate the final answer for the user.</small>\n",
    "\n",
    "Following the reason and action steps, our action tool call returns an observation. The logic returns the observation to the LLM, which is then used to generate subsequent reasoning and action steps.\n",
    "\n",
    "The ReAct loop continues until the LLM has enough information to answer the original input. Once the LLM reaches this state, it calls a special *answer* action with the generated answer for the user.\n",
    "\n",
    "## Not only LLMs and Tool Calls\n",
    "\n",
    "LLMs paired with tool calling are powerful but far from the only approach to building agents. Using the definition of neuro-symbolic, we cover architectures such as:\n",
    "\n",
    "- Multi-agent workflows that involve multiple LLM-tool (or other agent structure) combinations.\n",
    "- More deterministic workflows where we may have set neural model-tool paths that may fork or merge as the use case requires.\n",
    "- Embedding models that can detect user intents and decide tool-use or LLM selection-based selection in vector space.\n",
    "\n",
    "These are just a few high-level examples of alternative agent structures. Far from being designed for niche use cases, we find these alternative options to frequently perform better than the more common ReAct or Tool agents. We will cover all of these examples and more in future chapters.\n",
    "\n",
    "---\n",
    "\n",
    "Agents are fundamental to the future of AI, but that doesn't mean we should expect that future to come from agents in their most popular form today. ReAct and Tool agents are great and handle many simple use cases well, but the scope of agents is much broader, and we believe thinking beyond ReAct and Tools is key to building future AI.\n",
    "\n",
    "---\n",
    "\n",
    "You can sign up for the [Aurelio AI newsletter](https://b0fcw9ec53w.typeform.com/to/w2BDHVK7) to stay updated on future releases in our comprehensive course on agents.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "[1] The curious case of Connectionism (2019) [https://www.degruyter.com/document/doi/10.1515/opphil-2019-0018/html](https://www.degruyter.com/document/doi/10.1515/opphil-2019-0018/html)\n",
    "\n",
    "[2] F. Rosenblatt, [The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf) (1958), Psychological Review\n",
    "\n",
    "[3] E. Karpas et al. [MRKL Systems: A Modular, Neuro-Symbolic Architecture That Combines Large Language Models, External Knowledge Sources and Discrete Reasoning](https://arxiv.org/abs/2205.00445) (2022), AI21 Labs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9284bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI assistant called {name} that helps generate article titles.\",\n",
    "    input_variable=[\"name\"],\n",
    ")\n",
    "\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are tasked with creating a name for a article.\n",
    "The article is here for you to examine:\n",
    "\n",
    "---\n",
    "\n",
    " {article}\n",
    " \n",
    "---\n",
    "The name should be based of the context of the article.\n",
    "Be creative, but make sure the names are clear, catchy,\n",
    "and relevant to the theme of the article.\n",
    "\n",
    "Only output the article name, no other explanation or\n",
    "text can be provided and need it simple and short.\"\"\",\n",
    "    input_variables=[\"article\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3676e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_prompt.format(article=\"TEST STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd42664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview of the user prompt output. Later for the variable `article` we can give a real value.\n",
    "# print(user_prompt.format(article=\"TEST STRING\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1cc339da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ce70286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: You are an AI assistant called TEST NAME that helps generate article titles.\\nHuman: You are tasked with creating a name for a article.\\nThe article is here for you to examine:\\n\\n---\\n\\n TEST STRING\\n\\n---\\nThe name should be based of the context of the article.\\nBe creative, but make sure the names are clear, catchy,\\nand relevant to the theme of the article.\\n\\nOnly output the article name, no other explanation or\\ntext can be provided and need it simple and short.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format `first_prompt` also\n",
    "\n",
    "first_prompt.format(article=\"TEST STRING\", name=\"TEST NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3194c727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an AI assistant called TEST NAME that helps generate article titles.\n",
      "Human: You are tasked with creating a name for a article.\n",
      "The article is here for you to examine:\n",
      "\n",
      "---\n",
      "\n",
      " TEST STRING\n",
      "\n",
      "---\n",
      "The name should be based of the context of the article.\n",
      "Be creative, but make sure the names are clear, catchy,\n",
      "and relevant to the theme of the article.\n",
      "\n",
      "Only output the article name, no other explanation or\n",
      "text can be provided and need it simple and short.\n"
     ]
    }
   ],
   "source": [
    "print(first_prompt.format(article=\"TEST STRING\", name=\"TEST NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10fc4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_one = (\n",
    "    {\n",
    "        \"article\": lambda x: x[\"article\"],\n",
    "        \"name\": lambda x: x[\"name\"],\n",
    "    }\n",
    "    | first_prompt\n",
    "    | creative_llm\n",
    "    | {\"article_title\": lambda x: x.content} # extract the content field of llm output\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# {left side} | {right side} => left is going to go into the right side\n",
    "# {...} go into `first_prompt`, first prompt go into `creative_llm`, creative llm go into {...}\n",
    "\n",
    "article_title_msg = chain_one.invoke({\n",
    "    \"article\" : article,\n",
    "    \"name\" : \"Title Finder\"\n",
    "})\n",
    "\n",
    "article_title_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea32193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summery': 'Neuro-symbolic agents for the future of AI: explore how combining neural and symbolic systems will shape the next generation of AI. With applications in search, computation, and integration, these agents will redefine the boundaries of real-world AI. The MRKL architecture, ReAct method, and future use cases are highlighted. [1-3]'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI assistant that helps build good articles.\",\n",
    ")\n",
    "\n",
    "second_user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are tasked with creating a description for\n",
    "the article. The article is here for you to examine:\n",
    "\n",
    "---\n",
    "\n",
    "{article}\n",
    "\n",
    "---\n",
    "\n",
    "Here is the article title '{article_title}'.\n",
    "\n",
    "Output the SEO friendly article description. \n",
    "make sure we don't exceed 120 characters.\n",
    "Do not output anything other than the description.\"\"\",\n",
    "    input_variables=[\"article\", \"article_title\"],\n",
    ")\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_messages([system_prompt, second_user_prompt])\n",
    "\n",
    "\n",
    "chain_two = (\n",
    "    {\n",
    "        \"article\": lambda x: x[\"article\"], \n",
    "        \"article_title\": lambda x: x[\"article_title\"]\n",
    "    }\n",
    "    | second_prompt\n",
    "    | llm\n",
    "    | {\"summery\": lambda x: x.content}\n",
    ")\n",
    "\n",
    "article_description_msg = chain_two.invoke (\n",
    "    {\n",
    "        \"article\": article,\n",
    "        \"article_title\": article_title_msg[\"article_title\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "article_description_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5989d39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_paragraph': \"We believe AI's shortâ€”to mid-term future belongs to agents and that the long-term future of *AGI* may evolve from agentic systems. Our definition of agents covers any neuro-symbolic system in which we merge neural AI (such as an LLM) with semi-traditional software.\",\n",
       " 'edited_paragraph': \"We believe AI's short-term to the mid-term future belongs to agents, and that the long-term future of *AGI* may evolve through agentic systems. Our definition of agents includes any neuro-symbolic systems that integrate neural AI (like large language models) with traditional software. These systems enable AI to perform a wide array of tasks, from conducting complex computations to interacting with external knowledge bases.\",\n",
       " 'feedback': \"The paragraph highlights the importance of agents for the future, making it a strong start. You can expand on the term 'short-term' to better emphasize the focus on current and future systems. The definition could be more precise, emphasizing how agents are key to AI's development. Adding details about the role of neuro-symbolic systems would enhance the overall scope.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are tasked with creating a new paragraph for the\n",
    "article. The article is here for you to examine:\n",
    "\n",
    "---\n",
    "\n",
    "{article}\n",
    "\n",
    "---\n",
    "\n",
    "Choose one paragraph to review and edit. During your edit\n",
    "ensure you provide constructive feedback to the user so they\n",
    "can learn where to improve their own writing.\"\"\",\n",
    "    input_variables=[\"article\"]\n",
    ")\n",
    "\n",
    "# prompt template 3: creating a new paragraph for the article\n",
    "third_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_prompt,\n",
    "    third_user_prompt\n",
    "])\n",
    "\n",
    "\n",
    "#* force a Large Language Model (LLM) to output data in a strict, structured format rather than free text.\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Paragraph(BaseModel):\n",
    "    original_paragraph: str = Field(description=\"The original paragraph\")\n",
    "    edited_paragraph: str = Field(description=\"The improved edited paragraph\")\n",
    "    feedback: str = Field(\n",
    "        description = \"Constructive feedback on the original paragraph.\"\n",
    "    )\n",
    "\n",
    "structured_llm = creative_llm.with_structured_output(Paragraph)\n",
    "\n",
    "\n",
    "chain_tree = (\n",
    "    {\"article\": lambda x: x[\"article\"]}\n",
    "    | third_prompt\n",
    "    | structured_llm\n",
    "    | {\n",
    "        \"original_paragraph\": lambda x: x.original_paragraph,\n",
    "        \"edited_paragraph\": lambda x: x.edited_paragraph,\n",
    "        \"feedback\": lambda x: x.feedback\n",
    "    }\n",
    ")\n",
    "\n",
    "out = chain_tree.invoke({\"article\": article})\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1df7a8",
   "metadata": {},
   "source": [
    "## image generation\n",
    "\n",
    "use a model that can generate images (like open ai dall-e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fd5bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# image_prompt = PromptTemplate(\n",
    "#     input_variables=[\"article\"],\n",
    "#     template=(\n",
    "#         \"Generate a prompt with less then 500 characters to generate an image \"\n",
    "#         \"based on the following article: {article}\"\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "\n",
    "# from skimage import io\n",
    "# import matplotlib.pyplot as plt\n",
    "# from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "# def generate_and_display_image(image_prompt):\n",
    "#     image_url = DallEAPIWrapper(model=\"dall-e-3\").run(image_prompt)\n",
    "#     image_data = io.imread(image_url)\n",
    "\n",
    "#     # And update the display code to:\n",
    "#     plt.imshow(image_data)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# # we wrap this in a RunnableLambda for use with LCEL\n",
    "# image_gen_runnable = RunnableLambda(generate_and_display_image)\n",
    "\n",
    "\n",
    "# # chain 4: inputs: article, article_para / outputs: new_suggestion_article\n",
    "# chain_four = (\n",
    "#     {\"article\": lambda x: x[\"article\"]}\n",
    "#     | image_prompt\n",
    "#     | llm\n",
    "#     | (lambda x: x.content)\n",
    "#     | image_gen_runnable\n",
    "# )\n",
    "\n",
    "\n",
    "# chain_four.invoke({\"article\": article})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b640371c",
   "metadata": {},
   "source": [
    "# Langsmith\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3053ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# must enter API key\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or getpass(\n",
    "    \"Enter LangSmith API Key: \"\n",
    ")\n",
    "\n",
    "# below should not be changed\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# you can change this as preferred\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain_course_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8669e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\" # still no good\n",
    "MODEL = \"qwen3:0.6b\"  # okay\n",
    "# MODEL = \"gemma3:4b\" # okay\n",
    "BASE_URL = \"http://localhost:11434/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40dd2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today? ðŸ˜Š', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 93, 'prompt_tokens': 11, 'total_tokens': 104, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'qwen3:0.6b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-150', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b5b26-b02c-76f3-8324-fae4de46a2ba-0', usage_metadata={'input_tokens': 11, 'output_tokens': 93, 'total_tokens': 104, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\n",
    "    \"Enter OpenAI API Key: \"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0, model=MODEL, base_url=BASE_URL)\n",
    "\n",
    "llm.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45026fba",
   "metadata": {},
   "source": [
    "### Tracing Non-LangChain Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d926e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "@traceable\n",
    "def generate_random_number():\n",
    "    return random.randint(0, 100)\n",
    "\n",
    "\n",
    "@traceable\n",
    "def generate_string_delay(input_str: str):\n",
    "    number = random.randint(1, 5)\n",
    "    time.sleep(number)\n",
    "    return f\"{input_str} ({number})\"\n",
    "\n",
    "\n",
    "@traceable\n",
    "def random_error():\n",
    "    number = random.randint(0, 1)\n",
    "    if number == 0:\n",
    "        raise ValueError(\"Random error\")\n",
    "    else:\n",
    "        return \"No error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed371c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbdef04ccba424c84a8028e16ac12aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for _ in tqdm(range(10)):\n",
    "    generate_random_number()\n",
    "    generate_string_delay(\"Hello\")\n",
    "    try:\n",
    "        random_error()\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f206749",
   "metadata": {},
   "source": [
    "#### with specific names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8cb669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "\n",
    "@traceable(name=\"Chitchat Maker\")\n",
    "def error_generation_function(question: str):\n",
    "    delay = random.randint(0, 3)\n",
    "    time.sleep(delay)\n",
    "    number = random.randint(0, 1)\n",
    "    if number == 0:\n",
    "        raise ValueError(\"Random error\")\n",
    "    else:\n",
    "        return \"I'm great how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9d9f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc5f80e89224e3382b186a201a16a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in tqdm(range(5)):\n",
    "    try:\n",
    "        error_generation_function(\"How are you today?\")\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ec69b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d691369",
   "metadata": {},
   "source": [
    "# Prompting technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee67ac",
   "metadata": {},
   "source": [
    "We'll start by looking at the various parts of our prompt. For RAG use-cases we'll typically have three core components however this is _very_ use-cases dependant and can vary significantly. Nonetheless, for RAG we will typically see:\n",
    "\n",
    "* **Rules for our LLM**: this part of the prompt sets up the behavior of our LLM, how it should approach responding to user queries, and simply providing as much information as possible about what we're wanting to do as possible. We typically place this within the _system prompt_ of an chat LLM.\n",
    "\n",
    "* **Context**: this part is RAG-specific. The context refers to some _external information_ that we may have retrieved from a web search, database query, or often a _vector database_. This external information is the **R**etrieval **A**ugmentation part of **RA**G. For chat LLMs we'll typically place this inside the chat messages between the assistant and user.\n",
    "\n",
    "* **Question**: this is the input from our user. In the vast majority of cases the question/query/user input will always be provided to the LLM (and typically through a _user message_). However, the format and location of this being provided often changes.\n",
    "\n",
    "* **Answer**: this is the answer from our assistant, again this is _very_ typical and we'd expect this with every use-case.\n",
    "\n",
    "The below is an example of how a RAG prompt may look:\n",
    "\n",
    "```\n",
    "Answer the question based on the context below,                 }\n",
    "if you cannot answer the question using the                     }--->  (Rules) For Our Prompt\n",
    "provided information answer with \"I don't know\"                 }\n",
    "\n",
    "Context: Aurelio AI is an AI development studio                 }\n",
    "focused on the fields of Natural Language Processing (NLP)      }\n",
    "and information retrieval using modern tooling                  }--->   Context AI has\n",
    "such as Large Language Models (LLMs),                           }\n",
    "vector databases, and LangChain.                                }\n",
    "\n",
    "Question: Does Aurelio AI do anything related to LangChain?     }--->   User Question\n",
    "\n",
    "Answer:                                                         }--->   AI Answer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf107c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\" # still no good\n",
    "MODEL = \"qwen3:0.6b\"  # okay\n",
    "# MODEL = \"gemma3:4b\" # okay\n",
    "BASE_URL = \"http://localhost:11434/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a90bb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = \"\"\"\n",
    "Answer the user's query based on the context below.\n",
    "If you cannot answer the question using the\n",
    "provided information answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# from langchain.prompts import (\n",
    "#     SystemMessagePromptTemplate,\n",
    "#     HumanMessagePromptTemplate\n",
    "# )\n",
    "\n",
    "# prompt_template = ChatPromptTemplate.from_messages([\n",
    "#     SystemMessagePromptTemplate.from_template(prompt),\n",
    "#     HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "# ])\n",
    "\n",
    "# short way\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", prompt),\n",
    "        (\"user\", \"{query}\"), # now langchain can automatically detects inputs. so no need to specify input_variables.\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4de7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['context', 'query']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86008903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: {context}\\n'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfedbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL, temperature=0.0)\n",
    "\n",
    "# pipeline = (\n",
    "#     {\n",
    "#         \"query\": lambda x: x[\"query\"], # to these can add (x[\"query\"].lower())\n",
    "#         \"context\": lambda x: x[\"context\"]\n",
    "#     }\n",
    "#     | prompt_template\n",
    "#     | llm\n",
    "# )\n",
    "\n",
    "pipeline = prompt_template | llm #can use like this also\n",
    "\n",
    "context = \"\"\"Aurelio AI is an AI company developing tooling for AI\n",
    "engineers. Their focus is on language AI with the team having strong\n",
    "expertise in building AI agents and a strong background in\n",
    "information retrieval.\n",
    "\n",
    "The company is behind several open source frameworks, most notably\n",
    "Semantic Router and Semantic Chunkers. They also have an AI\n",
    "Platform providing engineers with tooling to help them build with\n",
    "AI. Finally, the team also provides development services to other\n",
    "organizations to help them bring their AI tech to market.\n",
    "\n",
    "Aurelio AI became LangChain Experts in September 2024 after a long\n",
    "track record of delivering AI solutions built with the LangChain\n",
    "ecosystem.\"\"\"\n",
    "\n",
    "query = \"what does Aurelio AI do?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b085423a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: \\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: ___CONTEXT___\\n\\nHuman: ___QUERY___'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.format( query = \"___QUERY___\", context = \"___CONTEXT___\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc8eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aurelio AI develops tooling for AI engineers, focuses on language AI with expertise in building AI agents and information retrieval, provides open-source frameworks like Semantic Router and Semantic Chunkers, an AI Platform for building AI tools, and offers development services to other organizations. They became LangChain Experts in September 2024 after a long track record of delivering AI solutions built with the LangChain ecosystem.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = pipeline.invoke({\"query\": query, \"context\": context})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f829c63e",
   "metadata": {},
   "source": [
    "`video time : 1:12:40`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43529ad",
   "metadata": {},
   "source": [
    "### Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d3edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Here is query #1\\nAI: Here is the answer #1\\nHuman: Here is query #2\\nAI: Here is the answer #2\\nHuman: Here is query #3\\nAI: Here is the answer #3'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# template\n",
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"), \n",
    "        (\"ai\", \"{output}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"Here is query #1\", \"output\": \"Here is the answer #1\"},\n",
    "    {\"input\": \"Here is query #2\", \"output\": \"Here is the answer #2\"},\n",
    "    {\"input\": \"Here is query #3\", \"output\": \"Here is the answer #3\"},\n",
    "]\n",
    "\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt= example_prompt,\n",
    "    examples = examples\n",
    ")\n",
    "\n",
    "few_shot_prompt.format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c511bdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Develop tooling for AI engineers focused on language AI  \n",
      "- Focus on building AI agents and information retrieval systems  \n",
      "- Create open-source frameworks like Semantic Router and Semantic Chunkers  \n",
      "- Offer an AI Platform for engineers to build with AI  \n",
      "- Provide development services to other organizations  \n",
      "\n",
      "Aurelio AI has a strong focus on AI engineering, open-source frameworks, and AI platform development, with a track record of delivering solutions built on the LangChain ecosystem. They became LangChain Experts in September 2024.\n"
     ]
    }
   ],
   "source": [
    "# Few-Shot Example\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "new_system_prompt = \"\"\"\n",
    "Answer the user's query based on the context below.\n",
    "If you cannot answer the question using the\n",
    "provided information answer with \"I don't know\".\n",
    "\n",
    "Always answer in markdown format. When doing so please\n",
    "provide headers, short summaries, follow with bullet\n",
    "points, then conclude.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", new_system_prompt),\n",
    "        (\"user\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL, temperature=0.5)\n",
    "\n",
    "\n",
    "pipeline = prompt_template | llm\n",
    "\n",
    "context = \"\"\"Aurelio AI is an AI company developing tooling for AI\n",
    "engineers. Their focus is on language AI with the team having strong\n",
    "expertise in building AI agents and a strong background in\n",
    "information retrieval.\n",
    "\n",
    "The company is behind several open source frameworks, most notably\n",
    "Semantic Router and Semantic Chunkers. They also have an AI\n",
    "Platform providing engineers with tooling to help them build with\n",
    "AI. Finally, the team also provides development services to other\n",
    "organizations to help them bring their AI tech to market.\n",
    "\n",
    "Aurelio AI became LangChain Experts in September 2024 after a long\n",
    "track record of delivering AI solutions built with the LangChain\n",
    "ecosystem.\"\"\"\n",
    "\n",
    "query = \"what does Aurelio AI do?\"\n",
    "\n",
    "out = pipeline.invoke({\"query\": query, \"context\": context}).content\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4798b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- Develop tooling for AI engineers focused on language AI  \n",
       "- Focus on building AI agents and information retrieval systems  \n",
       "- Create open-source frameworks like Semantic Router and Semantic Chunkers  \n",
       "- Offer an AI Platform for engineers to build with AI  \n",
       "- Provide development services to other organizations  \n",
       "\n",
       "Aurelio AI has a strong focus on AI engineering, open-source frameworks, and AI platform development, with a track record of delivering solutions built on the LangChain ecosystem. They became LangChain Experts in September 2024."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5942ba07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Human: Can you explain gravity?\n",
       "AI: \n",
       "## Gravity\n",
       "\n",
       "Gravity is one of the fundamental forces in the universe.\n",
       "\n",
       "### Discovery\n",
       "\n",
       "* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\n",
       "* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\n",
       "\n",
       "### In General Relativity\n",
       "\n",
       "* Gravity is described as the curvature of spacetime.\n",
       "* The more massive an object is, the more it curves spacetime.\n",
       "* This curvature is what causes objects to fall towards each other.\n",
       "\n",
       "### Gravitons\n",
       "\n",
       "* Gravitons are hypothetical particles that mediate the force of gravity.\n",
       "* They have not yet been detected.\n",
       "\n",
       "**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\n",
       "\n",
       "\n",
       "Human: What is the capital of France?\n",
       "AI: \n",
       "## France\n",
       "\n",
       "The capital of France is Paris.\n",
       "\n",
       "### Origins\n",
       "\n",
       "* The name Paris comes from the Latin word \"Parisini\" which referred to a Celtic people living in the area.\n",
       "* The Romans named the city Lutetia, which means \"the place where the river turns\".\n",
       "* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\n",
       "\n",
       "**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world's greatest cultural and economic centres.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "from IPython.display import display, Markdown\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Can you explain gravity?\",\n",
    "        \"output\": (\n",
    "            \"\\n## Gravity\\n\\n\"\n",
    "            \"Gravity is one of the fundamental forces in the universe.\\n\\n\"\n",
    "            \"### Discovery\\n\\n\"\n",
    "            \"* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\\n\"\n",
    "            \"* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\\n\\n\"\n",
    "            \"### In General Relativity\\n\\n\"\n",
    "            \"* Gravity is described as the curvature of spacetime.\\n\"\n",
    "            \"* The more massive an object is, the more it curves spacetime.\\n\"\n",
    "            \"* This curvature is what causes objects to fall towards each other.\\n\\n\"\n",
    "            \"### Gravitons\\n\\n\"\n",
    "            \"* Gravitons are hypothetical particles that mediate the force of gravity.\\n\"\n",
    "            \"* They have not yet been detected.\\n\\n\"\n",
    "            \"**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\\n\\n\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is the capital of France?\",\n",
    "        \"output\": (\n",
    "            \"\\n## France\\n\\n\"\n",
    "            \"The capital of France is Paris.\\n\\n\"\n",
    "            \"### Origins\\n\\n\"\n",
    "            \"* The name Paris comes from the Latin word \\\"Parisini\\\" which referred to a Celtic people living in the area.\\n\"\n",
    "            \"* The Romans named the city Lutetia, which means \\\"the place where the river turns\\\".\\n\"\n",
    "            \"* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\\n\\n\"\n",
    "            \"**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world's greatest cultural and economic centres.\\n\\n\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"), \n",
    "        (\"ai\", \"{output}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "out = few_shot_prompt.format()\n",
    "\n",
    "display(Markdown(out))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c917cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotChatMessagePromptTemplate(examples=[{'input': 'Can you explain gravity?', 'output': '\\n## Gravity\\n\\nGravity is one of the fundamental forces in the universe.\\n\\n### Discovery\\n\\n* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\\n* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\\n\\n### In General Relativity\\n\\n* Gravity is described as the curvature of spacetime.\\n* The more massive an object is, the more it curves spacetime.\\n* This curvature is what causes objects to fall towards each other.\\n\\n### Gravitons\\n\\n* Gravitons are hypothetical particles that mediate the force of gravity.\\n* They have not yet been detected.\\n\\n**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\\n\\n'}, {'input': 'What is the capital of France?', 'output': '\\n## France\\n\\nThe capital of France is Paris.\\n\\n### Origins\\n\\n* The name Paris comes from the Latin word \"Parisini\" which referred to a Celtic people living in the area.\\n* The Romans named the city Lutetia, which means \"the place where the river turns\".\\n* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\\n\\n**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world\\'s greatest cultural and economic centres.\\n\\n'}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1b8818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Aurelio AI: AI Tooling for Engineers\n",
       "\n",
       "### Summary\n",
       "\n",
       "Aurelio AI is an AI company focused on providing tools and services to AI engineers. They specialize in language AI, with a strong emphasis on building AI agents and leveraging information retrieval techniques.\n",
       "\n",
       "### Key Activities\n",
       "\n",
       "*   **Open Source Frameworks:** They develop and maintain open-source frameworks like Semantic Router and Semantic Chunkers, primarily for language AI applications.\n",
       "*   **AI Platform:** They offer an AI Platform that equips engineers with tools to build AI solutions.\n",
       "*   **Development Services:** Aurelio AI also provides development services to other organizations, assisting them in bringing their AI technology to market.\n",
       "*   **LangChain Expertise:** Since September 2024, they've become LangChain Experts, signifying their significant experience in utilizing the LangChain ecosystem.\n",
       "\n",
       "**To conclude**, Aurelio AIâ€™s core business is to empower AI engineers with tools and expertise for developing advanced language AI solutions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", new_system_prompt),\n",
    "    few_shot_prompt,\n",
    "    (\"user\", \"{query}\"),\n",
    "])\n",
    "\n",
    "pipeline = prompt_template | llm\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL)\n",
    "\n",
    "out = pipeline.invoke({\"query\": query, \"context\": context}).content\n",
    "display(Markdown(out))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb15cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "## What does Aurelio AI do?  \n",
      "\n",
      "- **Focus on Language AI**: Develops AI agents and tools for language processing.  \n",
      "- **Open-Source Frameworks**: Creates Semantic Router and Semantic Chunkers, which help with AI engineering.  \n",
      "- **AI Platform**: Offers tooling for building AI systems.  \n",
      "- **Development Services**: Provides support to other organizations bringing AI technology to market.  \n",
      "\n",
      "**Conclusion**: Aurelio AI specializes in language AI innovation, framework development, and AI platform support."
     ]
    }
   ],
   "source": [
    "# this is an extra code i just wanted to see the stream \n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", new_system_prompt),\n",
    "        few_shot_prompt,\n",
    "        (\"user\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = prompt_template | llm\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL)\n",
    "\n",
    "# Create the stream generator\n",
    "stream = pipeline.stream({\"query\": query, \"context\": context})\n",
    "\n",
    "# You must iterate over the stream to get chunks\n",
    "print(\"Streaming response:\")\n",
    "for chunk in stream:\n",
    "    # In LangChain, 'chunk' is usually an AIMessageChunk object\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dac5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Aurelio AI is an AI-driven company focused on language AI tools. Here are their key activities:  \n",
       "- **Language and AI Agents**: Building AI agents and expertise in information retrieval.  \n",
       "- **Open Source Tools**: Develops frameworks like Semantic Router and Semantic Chunkers.  \n",
       "- **AI Platform**: Provides engineers with tooling for AI development.  \n",
       "- **Services**: Offers development services to other organizations to integrate AI technologies.  \n",
       "\n",
       "**To conclude**, Aurelio AI specializes in building AI systems for engineers and developers."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with markdown\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", new_system_prompt),\n",
    "        few_shot_prompt,\n",
    "        (\"user\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = prompt_template | llm\n",
    "\n",
    "# --- The Streaming Logic ---\n",
    "\n",
    "# 1. Create the stream generator\n",
    "stream = pipeline.stream({\"query\": query, \"context\": context})\n",
    "\n",
    "# 2. Create a placeholder in the output area\n",
    "#    display_id=True allows us to reference this specific output later\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "# 3. Initialize a variable to store the full text as it builds up\n",
    "full_response = \"\"\n",
    "\n",
    "for chunk in stream:\n",
    "    # Check if content exists (sometimes chunks are empty)\n",
    "    if chunk.content:\n",
    "        # A. Append the new piece to our full text\n",
    "        full_response += chunk.content\n",
    "        \n",
    "        # B. \"Update\" the specific display handle with the new full text\n",
    "        #    This re-renders the Markdown every time a chunk arrives\n",
    "        display_handle.update(Markdown(full_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b97cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1392\n"
     ]
    }
   ],
   "source": [
    "no_cot_system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\n",
    "You MUST answer the question directly without any other\n",
    "text or explanation.\n",
    "\"\"\"\n",
    "\n",
    "no_cot_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", no_cot_system_prompt),\n",
    "        (\"user\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "query = \"How many keystrokes are needed to type the numbers from 1 to 500?\"\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL)\n",
    "no_cot_pipeline = no_cot_prompt_template | llm\n",
    "no_cot_result = no_cot_pipeline.invoke({\"query\": query}).content\n",
    "print(no_cot_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5611ebce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To determine the total number of keystrokes needed to type numbers from 1 to 500, we break it down into separate digit ranges:\n",
       "\n",
       "1. **Single-digit numbers (1 to 9):**  \n",
       "   - 9 numbers (1â€“9) each have 1 digit.  \n",
       "   - Keystrokes: $9 \\times 1 = 9$.\n",
       "\n",
       "2. **Two-digit numbers (10 to 99):**  \n",
       "   - 90 numbers (10â€“99) each have 2 digits.  \n",
       "   - Keystrokes: $90 \\times 2 = 180$.\n",
       "\n",
       "3. **Three-digit numbers (100 to 500):**  \n",
       "   - 400 numbers (100â€“500) each have 3 digits.  \n",
       "   - Keystrokes: $400 \\times 3 = 1200$.\n",
       "\n",
       "**Summing all:**  \n",
       "$9 + 180 + 1200 = 1389$ keystrokes.\n",
       "\n",
       "**Final Answer:** 1389."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the chain-of-thought prompt template\n",
    "cot_system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\n",
    "To answer the question, you must:\n",
    "\n",
    "- List systematically and in precise detail all\n",
    "  sub problems that need to be solved to answer the\n",
    "  question.\n",
    "- Solve each sub problem INDIVIDUALLY and in sequence.\n",
    "- Finally, use everything you have worked through to\n",
    "  provide the final answer.\n",
    "\"\"\"\n",
    "\n",
    "cot_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", cot_system_prompt),\n",
    "        (\"user\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL)\n",
    "cot_pipeline = cot_prompt_template | llm\n",
    "\n",
    "cot_result = cot_pipeline.invoke({\"query\": query}).content\n",
    "display(Markdown(cot_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf13aaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To calculate the total number of keystrokes needed to type numbers from 1 to 500, we break down the problem into digits in each position (units, tens, hundreds):\n",
       "\n",
       "1. **Units place**:  \n",
       "   - Each digit (0-9) appears for groups of 90 numbers (100-499).  \n",
       "   - Total digits in units place: $90 + 90 + 1 = 181$.  \n",
       "\n",
       "2. **Tens place**:  \n",
       "   - Each digit (0-9) appears for groups of 90 numbers (10-99).  \n",
       "   - Total digits in tens place: $90 + 90 + 90 = 270$.  \n",
       "\n",
       "3. **Hundreds place**:  \n",
       "   - Each digit (0-9) appears in hundreds place for numbers 100-500.  \n",
       "   - Total digits in hundreds place: $100 + 100 + 100 = 300$.  \n",
       "\n",
       "**Total keystrokes**: $181 + 270 + 300 = 751$.  \n",
       "\n",
       "**Answer**: 751 keystrokes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL )\n",
    "pipeline = prompt_template | llm\n",
    "\n",
    "\n",
    "result = pipeline.invoke({\"query\": query}).content\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350d34cd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52143f08",
   "metadata": {},
   "source": [
    "## Chat Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c6797",
   "metadata": {},
   "source": [
    "### 01. ConversationBufferMemory\n",
    "\n",
    "\n",
    "ConversationBufferMemory is the simplest form of conversational memory, it is literally just a place that we store messages, and then use to feed messages into our LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb52713",
   "metadata": {},
   "source": [
    "#### `Old Method`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67198860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\" # still no good\n",
    "# MODEL = \"qwen3:0.6b\"  # okay\n",
    "MODEL = \"gemma3:4b\" # okay\n",
    "BASE_URL = \"http://localhost:11434/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af4e43a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkmpp\\AppData\\Local\\Temp\\ipykernel_10684\\1671511597.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages = True)\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "662c657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are several ways that we can add messages to our memory, using the save_context method we can add a user query (via the input key) and the AI's response (via the output key). So, to create the following conversation:\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hi, my name is James\"},  # user message\n",
    "    {\"output\": \"Hey James, what's up? I'm an AI model called Zeta.\"},  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"I'm researching the different types of conversational memory.\"\n",
    "    },  # user message\n",
    "    {\"output\": \"That's interesting, what are some examples?\"},  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"\n",
    "    },  # user message\n",
    "    {\"output\": \"That's interesting, what's the difference?\"},  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"Buffer memory just stores the entire conversation, right?\"\n",
    "    },  # user message\n",
    "    {\n",
    "        \"output\": \"That makes sense, what about ConversationBufferWindowMemory?\"\n",
    "    },  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "    },  # user message\n",
    "    {\"output\": \"Very cool!\"},  # AI response\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f482e80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({}) # Load history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c4241d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alternative method to add messages to out memory.\n",
    "# With this other method, we pass individual user and AI messages via the add_user_message and add_ai_message methods. To reproduce what we did above, we do\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
    "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
    "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
    "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "591bea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = MODEL, base_url = BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e313151d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkmpp\\AppData\\Local\\Temp\\ipykernel_10684\\1592351593.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use `langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3238b8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
      "Human: What is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name again?',\n",
       " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name again?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='You told me your name is James! Iâ€™m really good with remembering details from our conversation. Itâ€™s nice to meet you, James. Iâ€™m Zeta, by the way. Iâ€™m currently tracking that you were researching conversational memory and you were discussing ConversationBufferMemory and ConversationBufferWindowMemory. Would you like to delve deeper into those?', additional_kwargs={}, response_metadata={})],\n",
       " 'response': 'You told me your name is James! Iâ€™m really good with remembering details from our conversation. Itâ€™s nice to meet you, James. Iâ€™m Zeta, by the way. Iâ€™m currently tracking that you were researching conversational memory and you were discussing ConversationBufferMemory and ConversationBufferWindowMemory. Would you like to delve deeper into those?'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\":\"What is my name again?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b483286f",
   "metadata": {},
   "source": [
    "#### `New Method`\n",
    "\n",
    "ConversationBufferMemory with RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0b9c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Zeta\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "949598cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_map = {}\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99c7d74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",  # \"query\" need to match to the prompt_template\n",
    "    history_messages_key=\"history\",  # \"history\" need to match to the prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5109e49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi James! Itâ€™s really nice to meet you. Iâ€™m Zeta, your helpful assistant. \\n\\nWhat can I do for you today? Do you have a question, need help with something, or just want to chat?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 27, 'total_tokens': 76, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-539', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b60f2-3b55-76d3-b68f-af3aee460c75-0', usage_metadata={'input_tokens': 27, 'output_tokens': 49, 'total_tokens': 76, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"}, config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a545e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is James! ðŸ˜Š I just wanted to confirm â€“ itâ€™s always good to be sure. \\n\\nIs there anything youâ€™d like to talk about, James?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 91, 'total_tokens': 128, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-976', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b60f2-7cc3-72e2-87d0-9ad8794e96d5-0', usage_metadata={'input_tokens': 91, 'output_tokens': 37, 'total_tokens': 128, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"}, config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8e77773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Thatâ€™s fantastic, James! Python is a really popular and versatile language. Itâ€™s known for being relatively easy to learn and itâ€™s used in so many different areas â€“ web development, data science, machine learning, scriptingâ€¦ \\n\\nWhat specifically do you enjoy about Python coding? Are you working on a particular project, or are you just interested in learning more?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 77, 'prompt_tokens': 142, 'total_tokens': 219, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-287', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b60f2-b87d-75c2-8d52-ac8c8c6c28b9-0', usage_metadata={'input_tokens': 142, 'output_tokens': 77, 'total_tokens': 219, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"I like Python coding language\"}, config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4594ff60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Okay, James, just to be sure Iâ€™ve got it right â€“ you enjoy coding languages! Specifically, you mentioned you like Python. \\n\\nIs that accurate? ðŸ˜Š', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 253, 'total_tokens': 289, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-607', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b60f3-525c-7770-ac71-5536f18dd166-0', usage_metadata={'input_tokens': 253, 'output_tokens': 36, 'total_tokens': 289, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"so i tell you that i like some coding language. can you repeat that to me what I like as a coding language.\"}, config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f6984db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Great! Itâ€™s really cool that youâ€™re into coding languages. Python is a fantastic choice. \\n\\nDo you have any specific questions about Python that youâ€™d like to ask me, or would you like to talk about it a little more?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 303, 'total_tokens': 356, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-578', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b60f3-e142-7690-ac8a-c394148b0cf5-0', usage_metadata={'input_tokens': 303, 'output_tokens': 53, 'total_tokens': 356, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"yes that is correct.\"}, config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c681fa05",
   "metadata": {},
   "source": [
    "## 2. ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3455d47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\" # still no good\n",
    "# MODEL = \"qwen3:0.6b\"  # okay\n",
    "MODEL = \"gemma3:4b\" # okay\n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da02fbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkmpp\\AppData\\Local\\Temp\\ipykernel_10172\\392160499.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=4, return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=4, return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439e4edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
    "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
    "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
    "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c91ee61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkmpp\\AppData\\Local\\Temp\\ipykernel_10172\\1588115874.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use `langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95653e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
      "Human: What is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name again?',\n",
       " 'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})],\n",
       " 'response': \"Let's seeâ€¦ based on our current conversation, you've introduced yourself as a researcher. Youâ€™ve been looking into conversational memory types like ConversationBufferMemory and ConversationBufferWindowMemory. And you recently clarified that Buffer Window Memory stores the last *k* messages, discarding the older ones. Is that correct? I'm trying to recall the specific details weâ€™ve discussed to give you the most accurate answer. I donâ€™t have a name for you beyond â€œresearcherâ€ based on our interaction!\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is my name again?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c042489",
   "metadata": {},
   "source": [
    "#### `New Method`\n",
    "\n",
    "ConversationBufferWindowMemory with RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcd86519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Zeta\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87b5dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, k: int):\n",
    "        super().__init__(k=k)\n",
    "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages.\n",
    "        \"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        self.messages = self.messages[-self.k :]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be3a4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "\n",
    "\n",
    "def get_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
    "    print(f\"get_chat_history called with session_id={session_id} and k={k}\")\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
    "    # remove anything beyond the last\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "994c69ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"The number of messages to keep in the history\",\n",
    "            default=4,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6781531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k4 and k=4\n",
      "Initializing BufferWindowMessageHistory with k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi James, itâ€™s really nice to meet you! Iâ€™m Zeta, your helpful assistant. \\n\\nHow can I help you out today? Do you have a question, need some information, or just want to chat?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 27, 'total_tokens': 75, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-266', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b653c-d2b2-77b1-adc9-aeba3c157082-0', usage_metadata={'input_tokens': 27, 'output_tokens': 48, 'total_tokens': 75, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fbb6b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BufferWindowMessageHistory(messages=[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content='Hi James, itâ€™s really nice to meet you! Iâ€™m Zeta, your helpful assistant. \\n\\nHow can I help you out today? Do you have a question, need some information, or just want to chat?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 27, 'total_tokens': 75, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-266', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b653c-d2b2-77b1-adc9-aeba3c157082-0', usage_metadata={'input_tokens': 27, 'output_tokens': 48, 'total_tokens': 75, 'input_token_details': {}, 'output_token_details': {}})], k=4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46a74f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map[\"id_k4\"].clear()  # clear the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8a09de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BufferWindowMessageHistory(messages=[], k=4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6d4f6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manually insert history\n",
    "chat_map[\"id_k4\"].add_user_message(\"Hi, my name is James\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"I'm an AI model called Zeta.\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"Very cool!\")\n",
    "\n",
    "chat_map[\"id_k4\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45d371f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BufferWindowMessageHistory(messages=[HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})], k=4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3377c4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k4 and k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Youâ€™re right to ask! My memory is a bit like a buffer window â€“ it only holds the most recent information. \\n\\nYour name is Zeta. Iâ€™m still learning to keep track of things in the way humans do, so apologies for the confusion! ðŸ˜Š \\n\\nWould you like to talk about something specific, or would you like me to refresh my memory about you?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 109, 'total_tokens': 188, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-542', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b653f-9e21-75a0-a98f-14dae37651a0-0', usage_metadata={'input_tokens': 109, 'output_tokens': 79, 'total_tokens': 188, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"what is my name again?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "474d931f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k14 and k=14\n",
      "Initializing BufferWindowMessageHistory with k=14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi James, itâ€™s nice to meet you! Iâ€™m Zeta, your helpful assistant. \\n\\nHow can I assist you today? Do you have a question you'd like me to answer, or is there something youâ€™d like me to help you do?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 27, 'total_tokens': 84, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-550', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b6540-1ca4-76c2-8b9f-09b30785e5c4-0', usage_metadata={'input_tokens': 27, 'output_tokens': 57, 'total_tokens': 84, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f41554d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hi James, itâ€™s nice to meet you! Iâ€™m Zeta, your helpful assistant. \\n\\nHow can I assist you today? Do you have a question you'd like me to answer, or is there something youâ€™d like me to help you do?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 27, 'total_tokens': 84, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-550', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b6540-1ca4-76c2-8b9f-09b30785e5c4-0', usage_metadata={'input_tokens': 27, 'output_tokens': 57, 'total_tokens': 84, 'input_token_details': {}, 'output_token_details': {}}),\n",
       " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k14\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"Very cool!\")\n",
    "\n",
    "chat_map[\"id_k14\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd00ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k14 and k=14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Your name is James! ðŸ˜Š \\n\\nIt's great youâ€™re digging into conversational memory â€“ itâ€™s a really fascinating area. Do you want to delve a little deeper into the differences between these memory types, or would you like me to explain how they relate to building a more sophisticated chatbot?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 223, 'total_tokens': 285, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-937', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b6540-d114-7031-b58a-08bf5b9e7842-0', usage_metadata={'input_tokens': 223, 'output_tokens': 62, 'total_tokens': 285, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new session\n",
    "\n",
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"what is my name again?\"},\n",
    "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f73bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hi James, itâ€™s nice to meet you! Iâ€™m Zeta, your helpful assistant. \\n\\nHow can I assist you today? Do you have a question you'd like me to answer, or is there something youâ€™d like me to help you do?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 27, 'total_tokens': 84, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-550', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b6540-1ca4-76c2-8b9f-09b30785e5c4-0', usage_metadata={'input_tokens': 27, 'output_tokens': 57, 'total_tokens': 84, 'input_token_details': {}, 'output_token_details': {}}),\n",
       " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Your name is James! ðŸ˜Š \\n\\nIt's great youâ€™re digging into conversational memory â€“ itâ€™s a really fascinating area. Do you want to delve a little deeper into the differences between these memory types, or would you like me to explain how they relate to building a more sophisticated chatbot?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 223, 'total_tokens': 285, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-937', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b6540-d114-7031-b58a-08bf5b9e7842-0', usage_metadata={'input_tokens': 223, 'output_tokens': 62, 'total_tokens': 285, 'input_token_details': {}, 'output_token_details': {}})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "chat_map[\"id_k14\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae2a4ea",
   "metadata": {},
   "source": [
    "## 3. ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dab4556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\" # still no good\n",
    "# MODEL = \"qwen3:0.6b\"  # okay\n",
    "MODEL = \"gemma3:4b\" # okay\n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e00171ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkmpp\\AppData\\Local\\Temp\\ipykernel_6600\\3945359647.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm)\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30cc259c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkmpp\\AppData\\Local\\Temp\\ipykernel_6600\\3667692610.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use `langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "430e6a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: hello there my name is James\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "James introduces himself, and Codex responds with a friendly greeting and reveals its identity as a large language model created by OpenAI. Codex then provides interesting information about the name \"James,\" including its Hebrew origins and historical prevalence, demonstrating its extensive knowledge gained from analyzing text data. It then asks James how he is doing.\n",
      "Human: I am researching the different types of conversational memory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "James introduces Codex, a large language model from OpenAI, and asks how he is doing. Codex then explains its capabilities regarding conversational memory, detailing techniques like context windows (currently 8,192 tokens), memory networks, transformers with long-range attention, external memory modules, and dialogue state tracking. It highlights the complexity of these approaches and invites James to specify his area of interest within conversational memory types.\n",
      "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "James introduced Codex, a large language model from OpenAI, and asked about its capabilities regarding conversational memory, specifically ConversationBufferMemory and ConversationBufferWindowMemory. Codex explained these memory types as components of â€œwindowed memory,â€ relying on a sliding window to retain conversation context. ConversationBufferWindowMemory uses a fixed-size window, offering simplicity but limited context retention. ConversationBufferMemory is more advanced, employing decay functions to reduce the influence of older tokens within the window, allowing for longer context retention. Codex is open to delving deeper into decay functions or exploring the relationship between these memory types and the overall architecture of a conversational AI, like transformers.\n",
      "Human: Buffer memory just stores the entire conversation\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "James introduced Codex and its memory types, specifically ConversationBufferMemory and ConversationBufferWindowMemory, explaining them as part of â€œwindowed memoryâ€ utilizing a sliding window to retain conversation context. ConversationBufferWindowMemory uses a fixed-size window for simplicity but limited retention, while ConversationBufferMemory employs decay functions to reduce the influence of older tokens, enabling longer context retention. Codex is open to further discussion on these functions and their relationship to the overall conversational AI architecture.\n",
      "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Buffer window memory stores the last k messages, dropping the rest.',\n",
       " 'history': 'James introduced Codex and its memory types, specifically ConversationBufferMemory and ConversationBufferWindowMemory, explaining them as part of â€œwindowed memoryâ€ utilizing a sliding window to retain conversation context. ConversationBufferWindowMemory uses a fixed-size window for simplicity but limited retention, while ConversationBufferMemory employs decay functions to reduce the influence of older tokens, enabling longer context retention. Codex is open to further discussion on these functions and their relationship to the overall conversational AI architecture.',\n",
       " 'response': 'Youâ€™ve hit on a really key aspect of how ConversationBufferWindowMemory works! It absolutely does store the last *k* messages â€“ thatâ€™s the core of its design. The value of *k* determines the window size. Think of it like a sliding window that moves along the conversation as new messages arrive. When the window reaches the end, the oldest messages are discarded, and the newest message is added. Itâ€™s a really efficient way to limit the context a model considers, preventing it from being overwhelmed by extremely long conversations. \\n\\nTo give you a bit more detail, the size of *k* isnâ€™t just a number; itâ€™s configurable, and the optimal value depends heavily on the specific task and the modelâ€™s capabilities. A larger *k* means the model can retain more context, which can be beneficial for complex, multi-turn conversations, but it also increases computational cost and potentially introduces noise if the conversation drifts too far from the initial topic. \\n\\nWe could delve into how the window size is determined, or perhaps talk about some strategies for selecting the best *k* for a given scenario. Would you like to explore any of those aspects further?'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"hello there my name is James\"})\n",
    "chain.invoke({\"input\": \"I am researching the different types of conversational memory.\"})\n",
    "chain.invoke({\"input\": \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"})\n",
    "chain.invoke({\"input\": \"Buffer memory just stores the entire conversation\"})\n",
    "chain.invoke({\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2efe951e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "James introduced Codex and its memory types, focusing on â€œwindowed memoryâ€ using ConversationBufferWindowMemory and ConversationBufferMemory. ConversationBufferWindowMemory maintains a sliding window of the last *k* messages â€“ discarding older ones as new ones arrive â€“ offering efficiency but limited retention. ConversationBufferMemory utilizes decay functions for longer context retention. The human clarified that *k* represents the window size, which is configurable and tied to the task and model capabilities, balancing context retention with computational cost. The AI offered to delve deeper into window size determination or strategies for selecting the optimal *k* value.\n",
      "Human: What is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name again?',\n",
       " 'history': 'James introduced Codex and its memory types, focusing on â€œwindowed memoryâ€ using ConversationBufferWindowMemory and ConversationBufferMemory. ConversationBufferWindowMemory maintains a sliding window of the last *k* messages â€“ discarding older ones as new ones arrive â€“ offering efficiency but limited retention. ConversationBufferMemory utilizes decay functions for longer context retention. The human clarified that *k* represents the window size, which is configurable and tied to the task and model capabilities, balancing context retention with computational cost. The AI offered to delve deeper into window size determination or strategies for selecting the optimal *k* value.',\n",
       " 'response': 'Youâ€™re James! Iâ€™ve got that noted in my current conversation context. Specifically, I recorded that you initiated the discussion about Codex and its memory types, focusing on windowed memory â€“ particularly ConversationBufferWindowMemory and ConversationBufferMemory. I also have a record of you clarifying what *k* represents, the window size, and its connection to task and model capabilities. Itâ€™s all right here, readily accessible for reference. ðŸ˜Š'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is my name again?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca5de3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b8d8544",
   "metadata": {},
   "source": [
    "#### `New Method`\n",
    "\n",
    "ConversationSummaryMemory with RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932123ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage, SystemMessage\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\" # still no good\n",
    "# MODEL = \"qwen3:0.6b\"  # okay\n",
    "MODEL = \"gemma3:4b\"  # okay\n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL)\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Zeta\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = prompt_template | llm\n",
    "\n",
    "\n",
    "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
    "\n",
    "    def __init__(self, llm: ChatOpenAI):\n",
    "        super().__init__(llm=llm)\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages.\n",
    "        \"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        # construct the summary chat messages\n",
    "        summary_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                SystemMessagePromptTemplate.from_template(\n",
    "                    \"Given the existing conversation summary and the new messages, \"\n",
    "                    \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                    \"as much relevant information as possible.\"\n",
    "                ),\n",
    "                HumanMessagePromptTemplate.from_template(\n",
    "                    \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                    \"New messages:\\n{messages}\"\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        # format the messages and invoke the LLM\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=self.messages,\n",
    "                messages=[x.content for x in messages],\n",
    "            )\n",
    "        )\n",
    "        # replace the existing history with a single system summary message\n",
    "        self.messages = [SystemMessage(content=new_summary.content)]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []\n",
    "\n",
    "\n",
    "chat_map = {}\n",
    "\n",
    "\n",
    "def get_chat_history(\n",
    "    session_id: str, llm: ChatOpenAI\n",
    ") -> ConversationSummaryMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
    "    # return the chat history\n",
    "    return chat_map[session_id]\n",
    "\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=ChatOpenAI,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e6a550b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi James! Itâ€™s nice to meet you. Iâ€™m Zeta. Howâ€™s your day going so far? Is there anything I can help you with today, or were you just saying hello?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 27, 'total_tokens': 70, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-259', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b6b50-d59c-7fb3-a414-d00633a2455f-0', usage_metadata={'input_tokens': 27, 'output_tokens': 43, 'total_tokens': 70, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74a63ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Hereâ€™s a new summary of the conversation, incorporating the new messages:\\n\\nJames introduced himself and stated his name is James. Zeta responded warmly, introducing herself as Zeta and asking how Jamesâ€™ day was going, inquiring if she could assist him or if he was simply saying hello.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a76c20d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Okay, hereâ€™s a revised summary of the conversation, incorporating the new messages and maintaining the key information:\\n\\n**Summary:**\\n\\nThe conversation began with James introducing himself and expressing interest in researching the different types of conversational memory. Zeta responded by providing a detailed explanation of four key categories of conversational memory:\\n\\n*   **Episodic Memory (Within the Conversation):** Recollection of specific details from the conversation, similar to remembering a past event.\\n*   **Priming (Implicit Conversation Memory):** Unconscious influence of previous conversations on responses.\\n*   **Contextual Memory (Situational Awareness):** Remembering details about the situation surrounding the conversation.\\n*   **Working Memory (Real-Time Processing):** Maintaining and manipulating information during the conversation.\\n\\nZeta then outlined the neural basis of conversational memory (prefrontal cortex, hippocampus, attention/language processing) and noted individual differences in abilities. She concluded by asking James what specifically he was interested in learning more about regarding conversational memory, and for what purpose (e.g., a paper, project, or simply curiosity). James confirmed his interest in researching these different types of conversational memory.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"I'm researching the different types of conversational memory.\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm},\n",
    ")\n",
    "\n",
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15a36567",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in [\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\",\n",
    "]:\n",
    "    pipeline_with_history.invoke(\n",
    "        {\"query\": msg}, config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7750bcf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='**Summary:**\\n\\nThe conversation continues to explore conversational memory, specifically focusing on â€œConversationBufferWindowMemory.â€ Zeta explains that this memory type operates by storing the last â€˜kâ€™ messages of a conversation, discarding older ones. This â€˜kâ€™ value represents the window size â€“ the amount of prior context the Large Language Model retains. Itâ€™s described as a â€œsliding windowâ€ that shifts forward as the conversation progresses. Zeta proposes discussing the implications of this approach, such as how the window size impacts coherence and its interaction with other conversational memory categories like Episodic, Priming, Contextual, and Working Memory.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f79e39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Youâ€™ve been calling yourself Zeta! Iâ€™m here to help you, Zeta. ðŸ˜Š \\n\\nDo you want me to remind you of anything else about our conversation so far?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 150, 'total_tokens': 188, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-543', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b6b5f-f447-7a70-9cc1-7abee00c11ea-0', usage_metadata={'input_tokens': 150, 'output_tokens': 38, 'total_tokens': 188, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"}, config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6304e1",
   "metadata": {},
   "source": [
    "`video time -> 1:57:34`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b60c51",
   "metadata": {},
   "source": [
    "## 4. ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2463dd3c",
   "metadata": {},
   "source": [
    "`not working. need a model of open ai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351accd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\" # still no good\n",
    "# MODEL = \"qwen3:0.6b\"  # okay\n",
    "MODEL = \"gemma3:4b\" # okay\n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3435056b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkmpp\\AppData\\Local\\Temp\\ipykernel_17220\\114919256.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=300,\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bd0b695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkmpp\\AppData\\Local\\Temp\\ipykernel_17220\\1592351593.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use `langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed7b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"hi, my name is James\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ed835",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in [\n",
    "    \"I'm researching the different types of conversational memory.\",\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]:\n",
    "    chain.invoke({\"input\": msg})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b8f6e1",
   "metadata": {},
   "source": [
    "`new method`\n",
    "\n",
    "ConversationSummaryBufferMemory with RunnableWithMessageHistory\n",
    "\n",
    "`this part is working`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2ff1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage, SystemMessage\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\" # still no good\n",
    "# MODEL = \"qwen3:0.6b\"  # okay\n",
    "MODEL = \"gemma3:4b\"  # okay\n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL)\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Zeta\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = prompt_template | llm\n",
    "\n",
    "\n",
    "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, llm: ChatOpenAI, k: int):\n",
    "        super().__init__(llm=llm, k=k)\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages and summarizing the messages that we\n",
    "        drop.\n",
    "        \"\"\"\n",
    "        existing_summary: SystemMessage | None = None\n",
    "        old_messages: list[BaseMessage] | None = None\n",
    "        # see if we already have a summary message\n",
    "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
    "            print(\">> Found existing summary\")\n",
    "            existing_summary = self.messages.pop(0)\n",
    "        # add the new messages to the history\n",
    "        self.messages.extend(messages)\n",
    "        # check if we have too many messages\n",
    "        if len(self.messages) > self.k:\n",
    "            print(\n",
    "                f\">> Found {len(self.messages)} messages, dropping \"\n",
    "                f\"oldest {len(self.messages) - self.k} messages.\"\n",
    "            )\n",
    "            # pull out the oldest messages...\n",
    "            old_messages = self.messages[: -self.k]\n",
    "            # ...and keep only the most recent messages\n",
    "            self.messages = self.messages[-self.k :]\n",
    "        if old_messages is None:\n",
    "            print(\">> No old messages to update summary with\")\n",
    "            # if we have no old_messages, we have nothing to update in summary\n",
    "            return\n",
    "        # construct the summary chat messages\n",
    "        summary_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                SystemMessagePromptTemplate.from_template(\n",
    "                    \"Given the existing conversation summary and the new messages, \"\n",
    "                    \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                    \"as much relevant information as possible.\"\n",
    "                ),\n",
    "                HumanMessagePromptTemplate.from_template(\n",
    "                    \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                    \"New messages:\\n{old_messages}\"\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        # format the messages and invoke the LLM\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=existing_summary, old_messages=old_messages\n",
    "            )\n",
    "        )\n",
    "        print(f\">> New summary: {new_summary.content}\")\n",
    "        # prepend the new summary to the history\n",
    "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []\n",
    "\n",
    "\n",
    "chat_map = {}\n",
    "\n",
    "\n",
    "def get_chat_history(\n",
    "    session_id: str, llm: ChatOpenAI, k: int\n",
    ") -> ConversationSummaryBufferMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = ConversationSummaryBufferMessageHistory(llm=llm, k=k)\n",
    "    # return the chat history\n",
    "    return chat_map[session_id]\n",
    "\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=ChatOpenAI,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"The number of messages to keep in the history\",\n",
    "            default=4,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d2c27c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> No old messages to update summary with\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hi James, itâ€™s nice to meet you! Iâ€™m Zeta, your helpful assistant. \\n\\nHow can I assist you today? Do you have something specific you'd like to talk about, or were you just saying hello?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 27, 'total_tokens': 77, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-47', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b7023-5acc-7df3-bf3c-7ec939a9485d-0', usage_metadata={'input_tokens': 27, 'output_tokens': 50, 'total_tokens': 77, 'input_token_details': {}, 'output_token_details': {}})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
    ")\n",
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1050957a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Message 1\n",
      "---\n",
      "\n",
      ">> No old messages to update summary with\n",
      "---\n",
      "Message 2\n",
      "---\n",
      "\n",
      ">> Found 6 messages, dropping oldest 2 messages.\n",
      ">> New summary: James introduced himself and is now speaking with Zeta, a helpful assistant. Zeta greeted him and asked how she could assist him, opening the conversation for a specific request or simply a chat.\n",
      "---\n",
      "Message 3\n",
      "---\n",
      "\n",
      ">> Found existing summary\n",
      ">> Found 6 messages, dropping oldest 2 messages.\n",
      ">> New summary: James is researching the different types of conversational memory, a fascinating area of psychology. Zeta is assisting him by exploring his specific interests within this topic. Sheâ€™s offered several starting points for discussion, including models of conversational memory (like the Interactive Activation Model and Situation Model), types of information remembered in conversation (topic, speaker identity, context), and the cognitive processes involved (attention, working memory, semantic memory). Zeta is now asking James to indicate where heâ€™d like to begin his exploration.\n",
      "---\n",
      "Message 4\n",
      "---\n",
      "\n",
      ">> Found existing summary\n",
      ">> Found 6 messages, dropping oldest 2 messages.\n",
      ">> New summary: James is researching conversational memory, with Zeta assisting him. Theyâ€™ve already discussed models like the Interactive Activation Model and Situation Model, focusing on aspects such as topic, speaker identity, and context. James has now introduced ConversationBufferMemory (CBM) and ConversationBufferWindowMemory (CBW), core components of Allan Paiceâ€™s Situation Model. Zeta explains that CBM is the â€œworking memoryâ€ for the conversation, constantly updated with current details, while CBW acts as a short-term buffer for recently recalled information. Key differences between the two are that CBM is active and CBW is a temporary holding space. Zeta is now offering to elaborate further on these concepts, specifically regarding their interaction, comparison with other models, or experimental evidence, and is asking James where heâ€™d like to begin his exploration.\n"
     ]
    }
   ],
   "source": [
    "for i, msg in enumerate(\n",
    "    [\n",
    "        \"I'm researching the different types of conversational memory.\",\n",
    "        \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "        \"Buffer memory just stores the entire conversation\",\n",
    "        \"Buffer window memory stores the last k messages, dropping the rest.\",\n",
    "    ]\n",
    "):\n",
    "    print(f\"---\\nMessage {i+1}\\n---\\n\")\n",
    "    pipeline_with_history.invoke(\n",
    "        {\"query\": msg}, config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c023148",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655b750b",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b401b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269607bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    \"\"\"Add 'x' and 'y'.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "# or it can be like this `def add(x: float, y: float, z: float | None = 0.3)`\n",
    "\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply 'x' and 'y'.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "\n",
    "@tool\n",
    "def exponentiate(x: float, y: float) -> float:\n",
    "    \"\"\"Raise 'x' to the power of 'y'.\"\"\"\n",
    "    return x**y\n",
    "\n",
    "\n",
    "@tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    \"\"\"Subtract 'x' from 'y'.\"\"\"\n",
    "    return y - x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623121f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='add', description=\"Add 'x' and 'y'.\", args_schema=<class 'langchain_core.utils.pydantic.add'>, func=<function add at 0x000001E5FABDC900>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f254883c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add.name ='add'\n",
      "add.description =\"Add 'x' and 'y'.\"\n"
     ]
    }
   ],
   "source": [
    "print(f\"{add.name =}\\n{add.description =}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65a18901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': \"Add 'x' and 'y'.\",\n",
       " 'properties': {'x': {'title': 'X', 'type': 'number'},\n",
       "  'y': {'title': 'Y', 'type': 'number'}},\n",
       " 'required': ['x', 'y'],\n",
       " 'title': 'add',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add.args_schema.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "904e9b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': \"Raise 'x' to the power of 'y'.\",\n",
       " 'properties': {'x': {'title': 'X', 'type': 'number'},\n",
       "  'y': {'title': 'Y', 'type': 'number'}},\n",
       " 'required': ['x', 'y'],\n",
       " 'title': 'exponentiate',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exponentiate.args_schema.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "506eb027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 5, 'y': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When invoking the tool, a JSON string output by the LLM will be parsed into JSON and then consumed as kwargs, similar to the below:\n",
    "import json\n",
    "\n",
    "llm_output_string = '{\"x\": 5, \"y\": 2}'  # this is the output from the LLM\n",
    "llm_output_dict = json.loads(llm_output_string)  # load as dictionary\n",
    "llm_output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d27ae70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is then passed into the tool function as kwargs (keyword arguments) as indicated by the ** operator - the ** operator is used to unpack the dictionary into keyword arguments.\n",
    "exponentiate.func(**llm_output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2b04722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add.func(**llm_output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070e4640",
   "metadata": {},
   "source": [
    "`video time : 2:16:40`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f19e76b",
   "metadata": {},
   "source": [
    "### Creating an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461eb54c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
