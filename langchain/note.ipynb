{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50646e10",
   "metadata": {},
   "source": [
    "langchain-core: The base abstractions (like runnable, messages, and standard interfaces).\n",
    "\n",
    "langchain: The chain, agent, and retrieval logic.\n",
    "\n",
    "langchain-community: The massive collection of specific tools, document loaders, vector stores, and chat models provided by the community and third parties (e.g., Wikipedia tools, FAISS vector store, specific PDF loaders)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca165fe",
   "metadata": {},
   "source": [
    "We will be taking an article draft and using LangChain to generate various useful items around this article. We'll be creating:\n",
    "\n",
    "01. An article title\n",
    "02. An article description\n",
    "03. Editor advice where we will insert an additional paragraph in the article\n",
    "04. A thumbnail / hero image for our article.\n",
    "\n",
    "Here we input our article to start with. Currently this is using an article from the Aurelio AI learning page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1c6f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "import textwrap\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\" # still no good\n",
    "MODEL = \"qwen3:0.6b\" # okay\n",
    "# MODEL = \"gemma3:4b\" # okay\n",
    "BASE_URL = \"http://localhost:11434/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af265f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL, temperature=0.0)\n",
    "\n",
    "creative_llm = ChatOpenAI(model=MODEL, base_url=BASE_URL, temperature=0.9)\n",
    "\n",
    "# response = model.invoke(\"Why do parrots talk?\")\n",
    "# response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "352a8450",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"\n",
    "\\\n",
    "We believe AI's short—to mid-term future belongs to agents and that the long-term future of *AGI* may evolve from agentic systems. Our definition of agents covers any neuro-symbolic system in which we merge neural AI (such as an LLM) with semi-traditional software.\n",
    "\n",
    "With agents, we allow LLMs to integrate with code — allowing AI to search the web, perform math, and essentially integrate into anything we can build with code. It should be clear the scope of use cases is phenomenal where AI can integrate with the broader world of software.\n",
    "\n",
    "In this introduction to AI agents, we will cover the essential concepts that make them what they are and why that will make them the core of real-world AI in the years to come.\n",
    "\n",
    "---\n",
    "\n",
    "## Neuro-Symbolic Systems\n",
    "\n",
    "Neuro-symbolic systems consist of both neural and symbolic computation, where:\n",
    "\n",
    "- Neural refers to LLMs, embedding models, or other neural network-based models.\n",
    "- Symbolic refers to logic containing symbolic logic, such as code.\n",
    "\n",
    "Both neural and symbolic AI originate from the early philosophical approaches to AI: connectionism (now neural) and symbolism. Symbolic AI is the more traditional AI. Diehard symbolists believed they could achieve true AGI via written rules, ontologies, and other logical functions.\n",
    "\n",
    "The other camp were the connectionists. Connectionism emerged in 1943 with a theoretical neural circuit but truly kicked off with Rosenblatt's perceptron paper in 1958 [1][2]. Both of these approaches to AI are fascinating but deserve more time than we can give them here, so we will leave further exploration of these concepts for a future chapter.\n",
    "\n",
    "Most important to us is understanding where symbolic logic outperforms neural-based compute and vice-versa.\n",
    "\n",
    "| Neural | Symbolic |\n",
    "| --- | --- |\n",
    "| Flexible, learned logic that can cover a huge range of potential scenarios. | Mostly hand-written rules which can be very granular and fine-tuned but hard to scale. |\n",
    "| Hard to interpret why a neural system does what it does. Very difficult or even impossible to predict behavior. | Rules are written and can be understood. When unsure why a particular ouput was produced we can look at the rules / logic to understand. |\n",
    "| Requires huge amount of data and compute to train state-of-the-art neural models, making it hard to add new abilities or update with new information. | Code is relatively cheap to write, it can be updated with new features easily, and latest information can often be added often instantaneously. |\n",
    "| When trained on broad datasets can often lack performance when exposed to unique scenarios that are not well represented in the training data. | Easily customized to unique scenarios. |\n",
    "| Struggles with complex computations such as mathematical operations. | Perform complex computations very quickly and accurately. |\n",
    "\n",
    "Pure neural architectures struggle with many seemingly simple tasks. For example, an LLM *cannot* provide an accurate answer if we ask it for today's date.\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is commonly used to provide LLMs with up-to-date knowledge on a particular subject or access to proprietary knowledge.\n",
    "\n",
    "### Giving LLMs Superpowers\n",
    "\n",
    "By 2020, it was becoming clear that neural AI systems could not perform tasks symbolic systems typically excelled in, such as arithmetic, accessing structured DB data, or making API calls. These tasks require discrete input parameters that allow us to process them reliably according to strict written logic.\n",
    "\n",
    "In 2022, researchers at AI21 developed Jurassic-X, an LLM-based \"neuro-symbolic architecture.\" Neuro-symbolic refers to merging the \"neural computation\" of large language models (LLMs) with more traditional (i.e. symbolic) computation of code.\n",
    "\n",
    "Jurassic-X used the Modular Reasoning, Knowledge, and Language (MRKL) system [3]. The researchers developed MRKL to solve the limitations of LLMs, namely:\n",
    "\n",
    "- Lack of up-to-date knowledge, whether that is the latest in AI or something as simple as today's date.\n",
    "- Lack of proprietary knowledge, such as internal company docs or your calendar bookings.\n",
    "- Lack of reasoning, i.e. the inability to perform operations that traditional software is good at, like running complex mathematical operations.\n",
    "- Lack of ability to generalize. Back in 2022, most LLMs had to be fine-tuned to perform well in a specific domain. This problem is still present today but far less prominent as the SotA models generalize much better and, in the case of MRKL, are able to use tools relatively well (although we could certainly take the MRKL solution to improve tool use performance even today).\n",
    "\n",
    "MRKL represents one of the earliest forms of what we would now call an agent; it is an LLM (neural computation) paired with executable code (symbolic computation).\n",
    "\n",
    "## ReAct and Tools\n",
    "\n",
    "There is a misconception in the broader industry that an AI agent is an LLM contained within some looping logic that can generate inputs for and execute code functions. This definition of agents originates from the huge popularity of the ReAct agent framework and the adoption of a similar structure with function/tool calling by LLM providers such as OpenAI, Anthropic, and Ollama.\n",
    "\n",
    "![ReAct agent flow with the Reasoning-Action loop [4]. When the action chosen specifies to use a normal tool, the tool is used and the observation returned for another iteration through the Reasoning-Action loop. To return a final answer to the user the LLM must choose action \"answer\" and provide the natural language response, finishing the loop.](/images/posts/ai-agents/ai-agents-00.png)\n",
    "\n",
    "<small>ReAct agent flow with the Reasoning-Action loop [4]. When the action chosen specifies to use a normal tool, the tool is used and the observation returned for another iteration through the Reasoning-Action loop. To return a final answer to the user the LLM must choose action \"answer\" and provide the natural language response, finishing the loop.</small>\n",
    "\n",
    "Our \"neuro-symbolic\" definition is much broader but certainly does include ReAct agents and LLMs paired with tools. This agent type is the most common for now, so it's worth understanding the basic concept behind it.\n",
    "\n",
    "The **Re**ason **Act**ion (ReAct) method encourages LLMs to generate iterative *reasoning* and *action* steps. During *reasoning,* the LLM describes what steps are to be taken to answer the user's query. Then, the LLM generates an *action,* which we parse into an input to some executable code, which we typically describe as a tool/function call.\n",
    "\n",
    "![ReAct method. Each iteration includes a Reasoning step followed by an Action (tool call) step. The Observation is the output from the previous tool call. During the final iteration the agent calls the answer tool, meaning we generate the final answer for the user.](/images/posts/ai-agents/ai-agents-01.png)\n",
    "\n",
    "<small>ReAct method. Each iteration includes a Reasoning step followed by an Action (tool call) step. The Observation is the output from the previous tool call. During the final iteration the agent calls the answer tool, meaning we generate the final answer for the user.</small>\n",
    "\n",
    "Following the reason and action steps, our action tool call returns an observation. The logic returns the observation to the LLM, which is then used to generate subsequent reasoning and action steps.\n",
    "\n",
    "The ReAct loop continues until the LLM has enough information to answer the original input. Once the LLM reaches this state, it calls a special *answer* action with the generated answer for the user.\n",
    "\n",
    "## Not only LLMs and Tool Calls\n",
    "\n",
    "LLMs paired with tool calling are powerful but far from the only approach to building agents. Using the definition of neuro-symbolic, we cover architectures such as:\n",
    "\n",
    "- Multi-agent workflows that involve multiple LLM-tool (or other agent structure) combinations.\n",
    "- More deterministic workflows where we may have set neural model-tool paths that may fork or merge as the use case requires.\n",
    "- Embedding models that can detect user intents and decide tool-use or LLM selection-based selection in vector space.\n",
    "\n",
    "These are just a few high-level examples of alternative agent structures. Far from being designed for niche use cases, we find these alternative options to frequently perform better than the more common ReAct or Tool agents. We will cover all of these examples and more in future chapters.\n",
    "\n",
    "---\n",
    "\n",
    "Agents are fundamental to the future of AI, but that doesn't mean we should expect that future to come from agents in their most popular form today. ReAct and Tool agents are great and handle many simple use cases well, but the scope of agents is much broader, and we believe thinking beyond ReAct and Tools is key to building future AI.\n",
    "\n",
    "---\n",
    "\n",
    "You can sign up for the [Aurelio AI newsletter](https://b0fcw9ec53w.typeform.com/to/w2BDHVK7) to stay updated on future releases in our comprehensive course on agents.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "[1] The curious case of Connectionism (2019) [https://www.degruyter.com/document/doi/10.1515/opphil-2019-0018/html](https://www.degruyter.com/document/doi/10.1515/opphil-2019-0018/html)\n",
    "\n",
    "[2] F. Rosenblatt, [The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf) (1958), Psychological Review\n",
    "\n",
    "[3] E. Karpas et al. [MRKL Systems: A Modular, Neuro-Symbolic Architecture That Combines Large Language Models, External Knowledge Sources and Discrete Reasoning](https://arxiv.org/abs/2205.00445) (2022), AI21 Labs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9284bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI assistant called {name} that helps generate article titles.\",\n",
    "    input_variable=[\"name\"],\n",
    ")\n",
    "\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are tasked with creating a name for a article.\n",
    "The article is here for you to examine:\n",
    "\n",
    "---\n",
    "\n",
    " {article}\n",
    " \n",
    "---\n",
    "The name should be based of the context of the article.\n",
    "Be creative, but make sure the names are clear, catchy,\n",
    "and relevant to the theme of the article.\n",
    "\n",
    "Only output the article name, no other explanation or\n",
    "text can be provided and need it simple and short.\"\"\",\n",
    "    input_variables=[\"article\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3676e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_prompt.format(article=\"TEST STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd42664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview of the user prompt output. Later for the variable `article` we can give a real value.\n",
    "# print(user_prompt.format(article=\"TEST STRING\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cc339da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ce70286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: You are an AI assistant called TEST NAME that helps generate article titles.\\nHuman: You are tasked with creating a name for a article.\\nThe article is here for you to examine:\\n\\n---\\n\\n TEST STRING\\n\\n---\\nThe name should be based of the context of the article.\\nBe creative, but make sure the names are clear, catchy,\\nand relevant to the theme of the article.\\n\\nOnly output the article name, no other explanation or\\ntext can be provided and need it simple and short.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format `first_prompt` also\n",
    "\n",
    "first_prompt.format(article=\"TEST STRING\", name=\"TEST NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3194c727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an AI assistant called TEST NAME that helps generate article titles.\n",
      "Human: You are tasked with creating a name for a article.\n",
      "The article is here for you to examine:\n",
      "\n",
      "---\n",
      "\n",
      " TEST STRING\n",
      "\n",
      "---\n",
      "The name should be based of the context of the article.\n",
      "Be creative, but make sure the names are clear, catchy,\n",
      "and relevant to the theme of the article.\n",
      "\n",
      "Only output the article name, no other explanation or\n",
      "text can be provided and need it simple and short.\n"
     ]
    }
   ],
   "source": [
    "print(first_prompt.format(article=\"TEST STRING\", name=\"TEST NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f10fc4b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      1\u001b[39m chain_one = (\n\u001b[32m      2\u001b[39m     {\n\u001b[32m      3\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33marticle\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[33m\"\u001b[39m\u001b[33marticle\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     | {\u001b[33m\"\u001b[39m\u001b[33marticle_title\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: x.content} \u001b[38;5;66;03m# extract the content field of llm output\u001b[39;00m\n\u001b[32m      9\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# {left side} | {right side} => left is going to go into the right side\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# {...} go into `first_prompt`, first prompt go into `creative_llm`, creative llm go into {...}\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m article_title_msg = \u001b[43mchain_one\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marticle\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43marticle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTitle Finder\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m article_title_msg\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3151\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3149\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3150\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3151\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3152\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3153\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1375\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1369\u001b[39m             response,\n\u001b[32m   1370\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1371\u001b[39m             metadata=generation_info,\n\u001b[32m   1372\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1373\u001b[39m         )\n\u001b[32m   1374\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1375\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1376\u001b[39m         response = raw_response.parse()\n\u001b[32m   1377\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\openai\\_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection.py:101\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection.py:78\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request_lock:\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m         stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m         ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     81\u001b[39m         http2_negotiated = (\n\u001b[32m     82\u001b[39m             ssl_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     83\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m ssl_object.selected_alpn_protocol() == \u001b[33m\"\u001b[39m\u001b[33mh2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     84\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection.py:124\u001b[39m, in \u001b[36mHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    116\u001b[39m     kwargs = {\n\u001b[32m    117\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhost\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._origin.host.decode(\u001b[33m\"\u001b[39m\u001b[33mascii\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    118\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mport\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._origin.port,\n\u001b[32m   (...)\u001b[39m\u001b[32m    121\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msocket_options\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._socket_options,\n\u001b[32m    122\u001b[39m     }\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m         trace.return_value = stream\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\httpcore\\_backends\\sync.py:208\u001b[39m, in \u001b[36mSyncBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    202\u001b[39m exc_map: ExceptionMapping = {\n\u001b[32m    203\u001b[39m     socket.timeout: ConnectTimeout,\n\u001b[32m    204\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    205\u001b[39m }\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     sock = \u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m option \u001b[38;5;129;01min\u001b[39;00m socket_options:\n\u001b[32m    214\u001b[39m         sock.setsockopt(*option)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\socket.py:856\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, all_errors)\u001b[39m\n\u001b[32m    854\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m error \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    855\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_errors:\n\u001b[32m--> \u001b[39m\u001b[32m856\u001b[39m         \u001b[43mexceptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclear\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# raise only the last error\u001b[39;00m\n\u001b[32m    857\u001b[39m     exceptions.append(exc)\n\u001b[32m    858\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "chain_one = (\n",
    "    {\n",
    "        \"article\": lambda x: x[\"article\"],\n",
    "        \"name\": lambda x: x[\"name\"],\n",
    "    }\n",
    "    | first_prompt\n",
    "    | creative_llm\n",
    "    | {\"article_title\": lambda x: x.content} # extract the content field of llm output\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# {left side} | {right side} => left is going to go into the right side\n",
    "# {...} go into `first_prompt`, first prompt go into `creative_llm`, creative llm go into {...}\n",
    "\n",
    "article_title_msg = chain_one.invoke({\n",
    "    \"article\" : article,\n",
    "    \"name\" : \"Title Finder\"\n",
    "})\n",
    "\n",
    "article_title_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea32193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summery': 'Neuro-symbolic agents for the future of AI: explore how combining neural and symbolic systems will shape the next generation of AI. With applications in search, computation, and integration, these agents will redefine the boundaries of real-world AI. The MRKL architecture, ReAct method, and future use cases are highlighted. [1-3]'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI assistant that helps build good articles.\",\n",
    ")\n",
    "\n",
    "second_user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are tasked with creating a description for\n",
    "the article. The article is here for you to examine:\n",
    "\n",
    "---\n",
    "\n",
    "{article}\n",
    "\n",
    "---\n",
    "\n",
    "Here is the article title '{article_title}'.\n",
    "\n",
    "Output the SEO friendly article description. \n",
    "make sure we don't exceed 120 characters.\n",
    "Do not output anything other than the description.\"\"\",\n",
    "    input_variables=[\"article\", \"article_title\"],\n",
    ")\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_messages([system_prompt, second_user_prompt])\n",
    "\n",
    "\n",
    "chain_two = (\n",
    "    {\n",
    "        \"article\": lambda x: x[\"article\"], \n",
    "        \"article_title\": lambda x: x[\"article_title\"]\n",
    "    }\n",
    "    | second_prompt\n",
    "    | llm\n",
    "    | {\"summery\": lambda x: x.content}\n",
    ")\n",
    "\n",
    "article_description_msg = chain_two.invoke (\n",
    "    {\n",
    "        \"article\": article,\n",
    "        \"article_title\": article_title_msg[\"article_title\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "article_description_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5989d39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_paragraph': \"We believe AI's short—to mid-term future belongs to agents and that the long-term future of *AGI* may evolve from agentic systems. Our definition of agents covers any neuro-symbolic system in which we merge neural AI (such as an LLM) with semi-traditional software.\",\n",
       " 'edited_paragraph': \"We believe AI's short-term to the mid-term future belongs to agents, and that the long-term future of *AGI* may evolve through agentic systems. Our definition of agents includes any neuro-symbolic systems that integrate neural AI (like large language models) with traditional software. These systems enable AI to perform a wide array of tasks, from conducting complex computations to interacting with external knowledge bases.\",\n",
       " 'feedback': \"The paragraph highlights the importance of agents for the future, making it a strong start. You can expand on the term 'short-term' to better emphasize the focus on current and future systems. The definition could be more precise, emphasizing how agents are key to AI's development. Adding details about the role of neuro-symbolic systems would enhance the overall scope.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are tasked with creating a new paragraph for the\n",
    "article. The article is here for you to examine:\n",
    "\n",
    "---\n",
    "\n",
    "{article}\n",
    "\n",
    "---\n",
    "\n",
    "Choose one paragraph to review and edit. During your edit\n",
    "ensure you provide constructive feedback to the user so they\n",
    "can learn where to improve their own writing.\"\"\",\n",
    "    input_variables=[\"article\"]\n",
    ")\n",
    "\n",
    "# prompt template 3: creating a new paragraph for the article\n",
    "third_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_prompt,\n",
    "    third_user_prompt\n",
    "])\n",
    "\n",
    "\n",
    "#* force a Large Language Model (LLM) to output data in a strict, structured format rather than free text.\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Paragraph(BaseModel):\n",
    "    original_paragraph: str = Field(description=\"The original paragraph\")\n",
    "    edited_paragraph: str = Field(description=\"The improved edited paragraph\")\n",
    "    feedback: str = Field(\n",
    "        description = \"Constructive feedback on the original paragraph.\"\n",
    "    )\n",
    "\n",
    "structured_llm = creative_llm.with_structured_output(Paragraph)\n",
    "\n",
    "\n",
    "chain_tree = (\n",
    "    {\"article\": lambda x: x[\"article\"]}\n",
    "    | third_prompt\n",
    "    | structured_llm\n",
    "    | {\n",
    "        \"original_paragraph\": lambda x: x.original_paragraph,\n",
    "        \"edited_paragraph\": lambda x: x.edited_paragraph,\n",
    "        \"feedback\": lambda x: x.feedback\n",
    "    }\n",
    ")\n",
    "\n",
    "out = chain_tree.invoke({\"article\": article})\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1df7a8",
   "metadata": {},
   "source": [
    "## image generation\n",
    "\n",
    "use a model that can generate images (like open ai dall-e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fd5bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# image_prompt = PromptTemplate(\n",
    "#     input_variables=[\"article\"],\n",
    "#     template=(\n",
    "#         \"Generate a prompt with less then 500 characters to generate an image \"\n",
    "#         \"based on the following article: {article}\"\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "\n",
    "# from skimage import io\n",
    "# import matplotlib.pyplot as plt\n",
    "# from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "# def generate_and_display_image(image_prompt):\n",
    "#     image_url = DallEAPIWrapper(model=\"dall-e-3\").run(image_prompt)\n",
    "#     image_data = io.imread(image_url)\n",
    "\n",
    "#     # And update the display code to:\n",
    "#     plt.imshow(image_data)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# # we wrap this in a RunnableLambda for use with LCEL\n",
    "# image_gen_runnable = RunnableLambda(generate_and_display_image)\n",
    "\n",
    "\n",
    "# # chain 4: inputs: article, article_para / outputs: new_suggestion_article\n",
    "# chain_four = (\n",
    "#     {\"article\": lambda x: x[\"article\"]}\n",
    "#     | image_prompt\n",
    "#     | llm\n",
    "#     | (lambda x: x.content)\n",
    "#     | image_gen_runnable\n",
    "# )\n",
    "\n",
    "\n",
    "# chain_four.invoke({\"article\": article})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b640371c",
   "metadata": {},
   "source": [
    "# Langsmith\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3053ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# must enter API key\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or getpass(\n",
    "    \"Enter LangSmith API Key: \"\n",
    ")\n",
    "\n",
    "# below should not be changed\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# you can change this as preferred\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain_course_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8669e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\" # still no good\n",
    "MODEL = \"qwen3:0.6b\"  # okay\n",
    "# MODEL = \"gemma3:4b\" # okay\n",
    "BASE_URL = \"http://localhost:11434/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40dd2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today? 😊', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 93, 'prompt_tokens': 11, 'total_tokens': 104, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'qwen3:0.6b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-150', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b5b26-b02c-76f3-8324-fae4de46a2ba-0', usage_metadata={'input_tokens': 11, 'output_tokens': 93, 'total_tokens': 104, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\n",
    "    \"Enter OpenAI API Key: \"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0, model=MODEL, base_url=BASE_URL)\n",
    "\n",
    "llm.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45026fba",
   "metadata": {},
   "source": [
    "### Tracing Non-LangChain Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d926e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "@traceable\n",
    "def generate_random_number():\n",
    "    return random.randint(0, 100)\n",
    "\n",
    "\n",
    "@traceable\n",
    "def generate_string_delay(input_str: str):\n",
    "    number = random.randint(1, 5)\n",
    "    time.sleep(number)\n",
    "    return f\"{input_str} ({number})\"\n",
    "\n",
    "\n",
    "@traceable\n",
    "def random_error():\n",
    "    number = random.randint(0, 1)\n",
    "    if number == 0:\n",
    "        raise ValueError(\"Random error\")\n",
    "    else:\n",
    "        return \"No error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed371c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbdef04ccba424c84a8028e16ac12aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for _ in tqdm(range(10)):\n",
    "    generate_random_number()\n",
    "    generate_string_delay(\"Hello\")\n",
    "    try:\n",
    "        random_error()\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f206749",
   "metadata": {},
   "source": [
    "#### with specific names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8cb669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "\n",
    "@traceable(name=\"Chitchat Maker\")\n",
    "def error_generation_function(question: str):\n",
    "    delay = random.randint(0, 3)\n",
    "    time.sleep(delay)\n",
    "    number = random.randint(0, 1)\n",
    "    if number == 0:\n",
    "        raise ValueError(\"Random error\")\n",
    "    else:\n",
    "        return \"I'm great how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9d9f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc5f80e89224e3382b186a201a16a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in tqdm(range(5)):\n",
    "    try:\n",
    "        error_generation_function(\"How are you today?\")\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ec69b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d691369",
   "metadata": {},
   "source": [
    "# Prompting technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee67ac",
   "metadata": {},
   "source": [
    "We'll start by looking at the various parts of our prompt. For RAG use-cases we'll typically have three core components however this is _very_ use-cases dependant and can vary significantly. Nonetheless, for RAG we will typically see:\n",
    "\n",
    "* **Rules for our LLM**: this part of the prompt sets up the behavior of our LLM, how it should approach responding to user queries, and simply providing as much information as possible about what we're wanting to do as possible. We typically place this within the _system prompt_ of an chat LLM.\n",
    "\n",
    "* **Context**: this part is RAG-specific. The context refers to some _external information_ that we may have retrieved from a web search, database query, or often a _vector database_. This external information is the **R**etrieval **A**ugmentation part of **RA**G. For chat LLMs we'll typically place this inside the chat messages between the assistant and user.\n",
    "\n",
    "* **Question**: this is the input from our user. In the vast majority of cases the question/query/user input will always be provided to the LLM (and typically through a _user message_). However, the format and location of this being provided often changes.\n",
    "\n",
    "* **Answer**: this is the answer from our assistant, again this is _very_ typical and we'd expect this with every use-case.\n",
    "\n",
    "The below is an example of how a RAG prompt may look:\n",
    "\n",
    "```\n",
    "Answer the question based on the context below,                 }\n",
    "if you cannot answer the question using the                     }--->  (Rules) For Our Prompt\n",
    "provided information answer with \"I don't know\"                 }\n",
    "\n",
    "Context: Aurelio AI is an AI development studio                 }\n",
    "focused on the fields of Natural Language Processing (NLP)      }\n",
    "and information retrieval using modern tooling                  }--->   Context AI has\n",
    "such as Large Language Models (LLMs),                           }\n",
    "vector databases, and LangChain.                                }\n",
    "\n",
    "Question: Does Aurelio AI do anything related to LangChain?     }--->   User Question\n",
    "\n",
    "Answer:                                                         }--->   AI Answer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf107c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\" # still no good\n",
    "MODEL = \"qwen3:0.6b\"  # okay\n",
    "# MODEL = \"gemma3:4b\" # okay\n",
    "BASE_URL = \"http://localhost:11434/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a90bb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = \"\"\"\n",
    "Answer the user's query based on the context below.\n",
    "If you cannot answer the question using the\n",
    "provided information answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# from langchain.prompts import (\n",
    "#     SystemMessagePromptTemplate,\n",
    "#     HumanMessagePromptTemplate\n",
    "# )\n",
    "\n",
    "# prompt_template = ChatPromptTemplate.from_messages([\n",
    "#     SystemMessagePromptTemplate.from_template(prompt),\n",
    "#     HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "# ])\n",
    "\n",
    "# short way\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", prompt),\n",
    "        (\"user\", \"{query}\"), # now langchain can automatically detects inputs. so no need to specify input_variables.\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4de7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['context', 'query']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86008903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: {context}\\n'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfedbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL, temperature=0.0)\n",
    "\n",
    "# pipeline = (\n",
    "#     {\n",
    "#         \"query\": lambda x: x[\"query\"], # to these can add (x[\"query\"].lower())\n",
    "#         \"context\": lambda x: x[\"context\"]\n",
    "#     }\n",
    "#     | prompt_template\n",
    "#     | llm\n",
    "# )\n",
    "\n",
    "pipeline = prompt_template | llm #can use like this also\n",
    "\n",
    "context = \"\"\"Aurelio AI is an AI company developing tooling for AI\n",
    "engineers. Their focus is on language AI with the team having strong\n",
    "expertise in building AI agents and a strong background in\n",
    "information retrieval.\n",
    "\n",
    "The company is behind several open source frameworks, most notably\n",
    "Semantic Router and Semantic Chunkers. They also have an AI\n",
    "Platform providing engineers with tooling to help them build with\n",
    "AI. Finally, the team also provides development services to other\n",
    "organizations to help them bring their AI tech to market.\n",
    "\n",
    "Aurelio AI became LangChain Experts in September 2024 after a long\n",
    "track record of delivering AI solutions built with the LangChain\n",
    "ecosystem.\"\"\"\n",
    "\n",
    "query = \"what does Aurelio AI do?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b085423a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: \\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: ___CONTEXT___\\n\\nHuman: ___QUERY___'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.format( query = \"___QUERY___\", context = \"___CONTEXT___\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc8eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aurelio AI develops tooling for AI engineers, focuses on language AI with expertise in building AI agents and information retrieval, provides open-source frameworks like Semantic Router and Semantic Chunkers, an AI Platform for building AI tools, and offers development services to other organizations. They became LangChain Experts in September 2024 after a long track record of delivering AI solutions built with the LangChain ecosystem.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = pipeline.invoke({\"query\": query, \"context\": context})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f829c63e",
   "metadata": {},
   "source": [
    "`video time : 1:12:40`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43529ad",
   "metadata": {},
   "source": [
    "### Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d3edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Here is query #1\\nAI: Here is the answer #1\\nHuman: Here is query #2\\nAI: Here is the answer #2\\nHuman: Here is query #3\\nAI: Here is the answer #3'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# template\n",
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"), \n",
    "        (\"ai\", \"{output}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"Here is query #1\", \"output\": \"Here is the answer #1\"},\n",
    "    {\"input\": \"Here is query #2\", \"output\": \"Here is the answer #2\"},\n",
    "    {\"input\": \"Here is query #3\", \"output\": \"Here is the answer #3\"},\n",
    "]\n",
    "\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt= example_prompt,\n",
    "    examples = examples\n",
    ")\n",
    "\n",
    "few_shot_prompt.format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c511bdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Develop tooling for AI engineers focused on language AI  \n",
      "- Focus on building AI agents and information retrieval systems  \n",
      "- Create open-source frameworks like Semantic Router and Semantic Chunkers  \n",
      "- Offer an AI Platform for engineers to build with AI  \n",
      "- Provide development services to other organizations  \n",
      "\n",
      "Aurelio AI has a strong focus on AI engineering, open-source frameworks, and AI platform development, with a track record of delivering solutions built on the LangChain ecosystem. They became LangChain Experts in September 2024.\n"
     ]
    }
   ],
   "source": [
    "# Few-Shot Example\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "new_system_prompt = \"\"\"\n",
    "Answer the user's query based on the context below.\n",
    "If you cannot answer the question using the\n",
    "provided information answer with \"I don't know\".\n",
    "\n",
    "Always answer in markdown format. When doing so please\n",
    "provide headers, short summaries, follow with bullet\n",
    "points, then conclude.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", new_system_prompt),\n",
    "        (\"user\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL, temperature=0.5)\n",
    "\n",
    "\n",
    "pipeline = prompt_template | llm\n",
    "\n",
    "context = \"\"\"Aurelio AI is an AI company developing tooling for AI\n",
    "engineers. Their focus is on language AI with the team having strong\n",
    "expertise in building AI agents and a strong background in\n",
    "information retrieval.\n",
    "\n",
    "The company is behind several open source frameworks, most notably\n",
    "Semantic Router and Semantic Chunkers. They also have an AI\n",
    "Platform providing engineers with tooling to help them build with\n",
    "AI. Finally, the team also provides development services to other\n",
    "organizations to help them bring their AI tech to market.\n",
    "\n",
    "Aurelio AI became LangChain Experts in September 2024 after a long\n",
    "track record of delivering AI solutions built with the LangChain\n",
    "ecosystem.\"\"\"\n",
    "\n",
    "query = \"what does Aurelio AI do?\"\n",
    "\n",
    "out = pipeline.invoke({\"query\": query, \"context\": context}).content\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4798b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- Develop tooling for AI engineers focused on language AI  \n",
       "- Focus on building AI agents and information retrieval systems  \n",
       "- Create open-source frameworks like Semantic Router and Semantic Chunkers  \n",
       "- Offer an AI Platform for engineers to build with AI  \n",
       "- Provide development services to other organizations  \n",
       "\n",
       "Aurelio AI has a strong focus on AI engineering, open-source frameworks, and AI platform development, with a track record of delivering solutions built on the LangChain ecosystem. They became LangChain Experts in September 2024."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5942ba07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Human: Can you explain gravity?\n",
       "AI: \n",
       "## Gravity\n",
       "\n",
       "Gravity is one of the fundamental forces in the universe.\n",
       "\n",
       "### Discovery\n",
       "\n",
       "* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\n",
       "* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\n",
       "\n",
       "### In General Relativity\n",
       "\n",
       "* Gravity is described as the curvature of spacetime.\n",
       "* The more massive an object is, the more it curves spacetime.\n",
       "* This curvature is what causes objects to fall towards each other.\n",
       "\n",
       "### Gravitons\n",
       "\n",
       "* Gravitons are hypothetical particles that mediate the force of gravity.\n",
       "* They have not yet been detected.\n",
       "\n",
       "**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\n",
       "\n",
       "\n",
       "Human: What is the capital of France?\n",
       "AI: \n",
       "## France\n",
       "\n",
       "The capital of France is Paris.\n",
       "\n",
       "### Origins\n",
       "\n",
       "* The name Paris comes from the Latin word \"Parisini\" which referred to a Celtic people living in the area.\n",
       "* The Romans named the city Lutetia, which means \"the place where the river turns\".\n",
       "* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\n",
       "\n",
       "**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world's greatest cultural and economic centres.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "from IPython.display import display, Markdown\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Can you explain gravity?\",\n",
    "        \"output\": (\n",
    "            \"\\n## Gravity\\n\\n\"\n",
    "            \"Gravity is one of the fundamental forces in the universe.\\n\\n\"\n",
    "            \"### Discovery\\n\\n\"\n",
    "            \"* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\\n\"\n",
    "            \"* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\\n\\n\"\n",
    "            \"### In General Relativity\\n\\n\"\n",
    "            \"* Gravity is described as the curvature of spacetime.\\n\"\n",
    "            \"* The more massive an object is, the more it curves spacetime.\\n\"\n",
    "            \"* This curvature is what causes objects to fall towards each other.\\n\\n\"\n",
    "            \"### Gravitons\\n\\n\"\n",
    "            \"* Gravitons are hypothetical particles that mediate the force of gravity.\\n\"\n",
    "            \"* They have not yet been detected.\\n\\n\"\n",
    "            \"**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\\n\\n\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is the capital of France?\",\n",
    "        \"output\": (\n",
    "            \"\\n## France\\n\\n\"\n",
    "            \"The capital of France is Paris.\\n\\n\"\n",
    "            \"### Origins\\n\\n\"\n",
    "            \"* The name Paris comes from the Latin word \\\"Parisini\\\" which referred to a Celtic people living in the area.\\n\"\n",
    "            \"* The Romans named the city Lutetia, which means \\\"the place where the river turns\\\".\\n\"\n",
    "            \"* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\\n\\n\"\n",
    "            \"**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world's greatest cultural and economic centres.\\n\\n\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"), \n",
    "        (\"ai\", \"{output}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "out = few_shot_prompt.format()\n",
    "\n",
    "display(Markdown(out))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c917cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotChatMessagePromptTemplate(examples=[{'input': 'Can you explain gravity?', 'output': '\\n## Gravity\\n\\nGravity is one of the fundamental forces in the universe.\\n\\n### Discovery\\n\\n* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\\n* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\\n\\n### In General Relativity\\n\\n* Gravity is described as the curvature of spacetime.\\n* The more massive an object is, the more it curves spacetime.\\n* This curvature is what causes objects to fall towards each other.\\n\\n### Gravitons\\n\\n* Gravitons are hypothetical particles that mediate the force of gravity.\\n* They have not yet been detected.\\n\\n**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\\n\\n'}, {'input': 'What is the capital of France?', 'output': '\\n## France\\n\\nThe capital of France is Paris.\\n\\n### Origins\\n\\n* The name Paris comes from the Latin word \"Parisini\" which referred to a Celtic people living in the area.\\n* The Romans named the city Lutetia, which means \"the place where the river turns\".\\n* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\\n\\n**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world\\'s greatest cultural and economic centres.\\n\\n'}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1b8818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Aurelio AI: AI Tooling for Engineers\n",
       "\n",
       "### Summary\n",
       "\n",
       "Aurelio AI is an AI company focused on providing tools and services to AI engineers. They specialize in language AI, with a strong emphasis on building AI agents and leveraging information retrieval techniques.\n",
       "\n",
       "### Key Activities\n",
       "\n",
       "*   **Open Source Frameworks:** They develop and maintain open-source frameworks like Semantic Router and Semantic Chunkers, primarily for language AI applications.\n",
       "*   **AI Platform:** They offer an AI Platform that equips engineers with tools to build AI solutions.\n",
       "*   **Development Services:** Aurelio AI also provides development services to other organizations, assisting them in bringing their AI technology to market.\n",
       "*   **LangChain Expertise:** Since September 2024, they've become LangChain Experts, signifying their significant experience in utilizing the LangChain ecosystem.\n",
       "\n",
       "**To conclude**, Aurelio AI’s core business is to empower AI engineers with tools and expertise for developing advanced language AI solutions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", new_system_prompt),\n",
    "    few_shot_prompt,\n",
    "    (\"user\", \"{query}\"),\n",
    "])\n",
    "\n",
    "pipeline = prompt_template | llm\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL)\n",
    "\n",
    "out = pipeline.invoke({\"query\": query, \"context\": context}).content\n",
    "display(Markdown(out))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb15cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "## What does Aurelio AI do?  \n",
      "\n",
      "- **Focus on Language AI**: Develops AI agents and tools for language processing.  \n",
      "- **Open-Source Frameworks**: Creates Semantic Router and Semantic Chunkers, which help with AI engineering.  \n",
      "- **AI Platform**: Offers tooling for building AI systems.  \n",
      "- **Development Services**: Provides support to other organizations bringing AI technology to market.  \n",
      "\n",
      "**Conclusion**: Aurelio AI specializes in language AI innovation, framework development, and AI platform support."
     ]
    }
   ],
   "source": [
    "# this is an extra code i just wanted to see the stream \n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", new_system_prompt),\n",
    "        few_shot_prompt,\n",
    "        (\"user\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = prompt_template | llm\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL)\n",
    "\n",
    "# Create the stream generator\n",
    "stream = pipeline.stream({\"query\": query, \"context\": context})\n",
    "\n",
    "# You must iterate over the stream to get chunks\n",
    "print(\"Streaming response:\")\n",
    "for chunk in stream:\n",
    "    # In LangChain, 'chunk' is usually an AIMessageChunk object\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dac5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Aurelio AI is an AI-driven company focused on language AI tools. Here are their key activities:  \n",
       "- **Language and AI Agents**: Building AI agents and expertise in information retrieval.  \n",
       "- **Open Source Tools**: Develops frameworks like Semantic Router and Semantic Chunkers.  \n",
       "- **AI Platform**: Provides engineers with tooling for AI development.  \n",
       "- **Services**: Offers development services to other organizations to integrate AI technologies.  \n",
       "\n",
       "**To conclude**, Aurelio AI specializes in building AI systems for engineers and developers."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with markdown\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", new_system_prompt),\n",
    "        few_shot_prompt,\n",
    "        (\"user\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = prompt_template | llm\n",
    "\n",
    "# --- The Streaming Logic ---\n",
    "\n",
    "# 1. Create the stream generator\n",
    "stream = pipeline.stream({\"query\": query, \"context\": context})\n",
    "\n",
    "# 2. Create a placeholder in the output area\n",
    "#    display_id=True allows us to reference this specific output later\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "# 3. Initialize a variable to store the full text as it builds up\n",
    "full_response = \"\"\n",
    "\n",
    "for chunk in stream:\n",
    "    # Check if content exists (sometimes chunks are empty)\n",
    "    if chunk.content:\n",
    "        # A. Append the new piece to our full text\n",
    "        full_response += chunk.content\n",
    "        \n",
    "        # B. \"Update\" the specific display handle with the new full text\n",
    "        #    This re-renders the Markdown every time a chunk arrives\n",
    "        display_handle.update(Markdown(full_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b97cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1392\n"
     ]
    }
   ],
   "source": [
    "no_cot_system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\n",
    "You MUST answer the question directly without any other\n",
    "text or explanation.\n",
    "\"\"\"\n",
    "\n",
    "no_cot_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", no_cot_system_prompt),\n",
    "        (\"user\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "query = \"How many keystrokes are needed to type the numbers from 1 to 500?\"\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL)\n",
    "no_cot_pipeline = no_cot_prompt_template | llm\n",
    "no_cot_result = no_cot_pipeline.invoke({\"query\": query}).content\n",
    "print(no_cot_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5611ebce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To determine the total number of keystrokes needed to type numbers from 1 to 500, we break it down into separate digit ranges:\n",
       "\n",
       "1. **Single-digit numbers (1 to 9):**  \n",
       "   - 9 numbers (1–9) each have 1 digit.  \n",
       "   - Keystrokes: $9 \\times 1 = 9$.\n",
       "\n",
       "2. **Two-digit numbers (10 to 99):**  \n",
       "   - 90 numbers (10–99) each have 2 digits.  \n",
       "   - Keystrokes: $90 \\times 2 = 180$.\n",
       "\n",
       "3. **Three-digit numbers (100 to 500):**  \n",
       "   - 400 numbers (100–500) each have 3 digits.  \n",
       "   - Keystrokes: $400 \\times 3 = 1200$.\n",
       "\n",
       "**Summing all:**  \n",
       "$9 + 180 + 1200 = 1389$ keystrokes.\n",
       "\n",
       "**Final Answer:** 1389."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the chain-of-thought prompt template\n",
    "cot_system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\n",
    "To answer the question, you must:\n",
    "\n",
    "- List systematically and in precise detail all\n",
    "  sub problems that need to be solved to answer the\n",
    "  question.\n",
    "- Solve each sub problem INDIVIDUALLY and in sequence.\n",
    "- Finally, use everything you have worked through to\n",
    "  provide the final answer.\n",
    "\"\"\"\n",
    "\n",
    "cot_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", cot_system_prompt),\n",
    "        (\"user\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL)\n",
    "cot_pipeline = cot_prompt_template | llm\n",
    "\n",
    "cot_result = cot_pipeline.invoke({\"query\": query}).content\n",
    "display(Markdown(cot_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf13aaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To calculate the total number of keystrokes needed to type numbers from 1 to 500, we break down the problem into digits in each position (units, tens, hundreds):\n",
       "\n",
       "1. **Units place**:  \n",
       "   - Each digit (0-9) appears for groups of 90 numbers (100-499).  \n",
       "   - Total digits in units place: $90 + 90 + 1 = 181$.  \n",
       "\n",
       "2. **Tens place**:  \n",
       "   - Each digit (0-9) appears for groups of 90 numbers (10-99).  \n",
       "   - Total digits in tens place: $90 + 90 + 90 = 270$.  \n",
       "\n",
       "3. **Hundreds place**:  \n",
       "   - Each digit (0-9) appears in hundreds place for numbers 100-500.  \n",
       "   - Total digits in hundreds place: $100 + 100 + 100 = 300$.  \n",
       "\n",
       "**Total keystrokes**: $181 + 270 + 300 = 751$.  \n",
       "\n",
       "**Answer**: 751 keystrokes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL )\n",
    "pipeline = prompt_template | llm\n",
    "\n",
    "\n",
    "result = pipeline.invoke({\"query\": query}).content\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350d34cd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52143f08",
   "metadata": {},
   "source": [
    "## Chat Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c6797",
   "metadata": {},
   "source": [
    "### 01. ConversationBufferMemory\n",
    "\n",
    "\n",
    "ConversationBufferMemory is the simplest form of conversational memory, it is literally just a place that we store messages, and then use to feed messages into our LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb52713",
   "metadata": {},
   "source": [
    "#### `Old Method`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67198860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\" # still no good\n",
    "# MODEL = \"qwen3:0.6b\"  # okay\n",
    "MODEL = \"gemma3:4b\" # okay\n",
    "BASE_URL = \"http://localhost:11434/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4e43a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkmpp\\AppData\\Local\\Temp\\ipykernel_10684\\1671511597.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages = True)\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662c657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are several ways that we can add messages to our memory, using the save_context method we can add a user query (via the input key) and the AI's response (via the output key). So, to create the following conversation:\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hi, my name is James\"},  # user message\n",
    "    {\"output\": \"Hey James, what's up? I'm an AI model called Zeta.\"},  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"I'm researching the different types of conversational memory.\"\n",
    "    },  # user message\n",
    "    {\"output\": \"That's interesting, what are some examples?\"},  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"\n",
    "    },  # user message\n",
    "    {\"output\": \"That's interesting, what's the difference?\"},  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"Buffer memory just stores the entire conversation, right?\"\n",
    "    },  # user message\n",
    "    {\n",
    "        \"output\": \"That makes sense, what about ConversationBufferWindowMemory?\"\n",
    "    },  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\n",
    "        \"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "    },  # user message\n",
    "    {\"output\": \"Very cool!\"},  # AI response\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f482e80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({}) # Load history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4241d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alternative method to add messages to out memory.\n",
    "# With this other method, we pass individual user and AI messages via the add_user_message and add_ai_message methods. To reproduce what we did above, we do\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
    "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
    "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
    "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591bea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = MODEL, base_url = BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e313151d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkmpp\\AppData\\Local\\Temp\\ipykernel_10684\\1592351593.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use `langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238b8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
      "Human: What is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name again?',\n",
       " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name again?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='You told me your name is James! I’m really good with remembering details from our conversation. It’s nice to meet you, James. I’m Zeta, by the way. I’m currently tracking that you were researching conversational memory and you were discussing ConversationBufferMemory and ConversationBufferWindowMemory. Would you like to delve deeper into those?', additional_kwargs={}, response_metadata={})],\n",
       " 'response': 'You told me your name is James! I’m really good with remembering details from our conversation. It’s nice to meet you, James. I’m Zeta, by the way. I’m currently tracking that you were researching conversational memory and you were discussing ConversationBufferMemory and ConversationBufferWindowMemory. Would you like to delve deeper into those?'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\":\"What is my name again?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b483286f",
   "metadata": {},
   "source": [
    "#### `New Method`\n",
    "\n",
    "ConversationBufferMemory with RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0b9c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Zeta\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949598cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_map = {}\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c7d74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",  # \"query\" need to match to the prompt_template\n",
    "    history_messages_key=\"history\",  # \"history\" need to match to the prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5109e49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi James! It’s really nice to meet you. I’m Zeta, your helpful assistant. \\n\\nWhat can I do for you today? Do you have a question, need help with something, or just want to chat?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 27, 'total_tokens': 76, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-539', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b60f2-3b55-76d3-b68f-af3aee460c75-0', usage_metadata={'input_tokens': 27, 'output_tokens': 49, 'total_tokens': 76, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"}, config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a545e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is James! 😊 I just wanted to confirm – it’s always good to be sure. \\n\\nIs there anything you’d like to talk about, James?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 91, 'total_tokens': 128, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-976', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b60f2-7cc3-72e2-87d0-9ad8794e96d5-0', usage_metadata={'input_tokens': 91, 'output_tokens': 37, 'total_tokens': 128, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"}, config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e77773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='That’s fantastic, James! Python is a really popular and versatile language. It’s known for being relatively easy to learn and it’s used in so many different areas – web development, data science, machine learning, scripting… \\n\\nWhat specifically do you enjoy about Python coding? Are you working on a particular project, or are you just interested in learning more?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 77, 'prompt_tokens': 142, 'total_tokens': 219, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-287', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b60f2-b87d-75c2-8d52-ac8c8c6c28b9-0', usage_metadata={'input_tokens': 142, 'output_tokens': 77, 'total_tokens': 219, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"I like Python coding language\"}, config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4594ff60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Okay, James, just to be sure I’ve got it right – you enjoy coding languages! Specifically, you mentioned you like Python. \\n\\nIs that accurate? 😊', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 253, 'total_tokens': 289, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-607', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b60f3-525c-7770-ac71-5536f18dd166-0', usage_metadata={'input_tokens': 253, 'output_tokens': 36, 'total_tokens': 289, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"so i tell you that i like some coding language. can you repeat that to me what I like as a coding language.\"}, config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6984db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Great! It’s really cool that you’re into coding languages. Python is a fantastic choice. \\n\\nDo you have any specific questions about Python that you’d like to ask me, or would you like to talk about it a little more?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 303, 'total_tokens': 356, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-578', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b60f3-e142-7690-ac8a-c394148b0cf5-0', usage_metadata={'input_tokens': 303, 'output_tokens': 53, 'total_tokens': 356, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"yes that is correct.\"}, config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c681fa05",
   "metadata": {},
   "source": [
    "## 2. ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3455d47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\" # still no good\n",
    "# MODEL = \"qwen3:0.6b\"  # okay\n",
    "MODEL = \"gemma3:4b\" # okay\n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da02fbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkmpp\\AppData\\Local\\Temp\\ipykernel_10172\\392160499.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=4, return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=4, return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439e4edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
    "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
    "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
    "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c91ee61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkmpp\\AppData\\Local\\Temp\\ipykernel_10172\\1588115874.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use `langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95653e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
      "Human: What is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name again?',\n",
       " 'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})],\n",
       " 'response': \"Let's see… based on our current conversation, you've introduced yourself as a researcher. You’ve been looking into conversational memory types like ConversationBufferMemory and ConversationBufferWindowMemory. And you recently clarified that Buffer Window Memory stores the last *k* messages, discarding the older ones. Is that correct? I'm trying to recall the specific details we’ve discussed to give you the most accurate answer. I don’t have a name for you beyond “researcher” based on our interaction!\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is my name again?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c042489",
   "metadata": {},
   "source": [
    "#### `New Method`\n",
    "\n",
    "ConversationBufferWindowMemory with RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd86519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Zeta\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b5dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, k: int):\n",
    "        super().__init__(k=k)\n",
    "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages.\n",
    "        \"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        self.messages = self.messages[-self.k :]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3a4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "\n",
    "\n",
    "def get_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
    "    print(f\"get_chat_history called with session_id={session_id} and k={k}\")\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
    "    # remove anything beyond the last\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c69ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"The number of messages to keep in the history\",\n",
    "            default=4,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6781531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k4 and k=4\n",
      "Initializing BufferWindowMessageHistory with k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi James, it’s really nice to meet you! I’m Zeta, your helpful assistant. \\n\\nHow can I help you out today? Do you have a question, need some information, or just want to chat?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 27, 'total_tokens': 75, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-266', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b653c-d2b2-77b1-adc9-aeba3c157082-0', usage_metadata={'input_tokens': 27, 'output_tokens': 48, 'total_tokens': 75, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbb6b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BufferWindowMessageHistory(messages=[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content='Hi James, it’s really nice to meet you! I’m Zeta, your helpful assistant. \\n\\nHow can I help you out today? Do you have a question, need some information, or just want to chat?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 27, 'total_tokens': 75, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-266', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b653c-d2b2-77b1-adc9-aeba3c157082-0', usage_metadata={'input_tokens': 27, 'output_tokens': 48, 'total_tokens': 75, 'input_token_details': {}, 'output_token_details': {}})], k=4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a74f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map[\"id_k4\"].clear()  # clear the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a09de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BufferWindowMessageHistory(messages=[], k=4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d4f6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manually insert history\n",
    "chat_map[\"id_k4\"].add_user_message(\"Hi, my name is James\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"I'm an AI model called Zeta.\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"Very cool!\")\n",
    "\n",
    "chat_map[\"id_k4\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d371f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BufferWindowMessageHistory(messages=[HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})], k=4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3377c4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k4 and k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You’re right to ask! My memory is a bit like a buffer window – it only holds the most recent information. \\n\\nYour name is Zeta. I’m still learning to keep track of things in the way humans do, so apologies for the confusion! 😊 \\n\\nWould you like to talk about something specific, or would you like me to refresh my memory about you?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 109, 'total_tokens': 188, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-542', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b653f-9e21-75a0-a98f-14dae37651a0-0', usage_metadata={'input_tokens': 109, 'output_tokens': 79, 'total_tokens': 188, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"what is my name again?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474d931f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k14 and k=14\n",
      "Initializing BufferWindowMessageHistory with k=14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi James, it’s nice to meet you! I’m Zeta, your helpful assistant. \\n\\nHow can I assist you today? Do you have a question you'd like me to answer, or is there something you’d like me to help you do?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 27, 'total_tokens': 84, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-550', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b6540-1ca4-76c2-8b9f-09b30785e5c4-0', usage_metadata={'input_tokens': 27, 'output_tokens': 57, 'total_tokens': 84, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f41554d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hi James, it’s nice to meet you! I’m Zeta, your helpful assistant. \\n\\nHow can I assist you today? Do you have a question you'd like me to answer, or is there something you’d like me to help you do?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 27, 'total_tokens': 84, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-550', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b6540-1ca4-76c2-8b9f-09b30785e5c4-0', usage_metadata={'input_tokens': 27, 'output_tokens': 57, 'total_tokens': 84, 'input_token_details': {}, 'output_token_details': {}}),\n",
       " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k14\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"Very cool!\")\n",
    "\n",
    "chat_map[\"id_k14\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd00ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session_id=id_k14 and k=14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Your name is James! 😊 \\n\\nIt's great you’re digging into conversational memory – it’s a really fascinating area. Do you want to delve a little deeper into the differences between these memory types, or would you like me to explain how they relate to building a more sophisticated chatbot?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 223, 'total_tokens': 285, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-937', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b6540-d114-7031-b58a-08bf5b9e7842-0', usage_metadata={'input_tokens': 223, 'output_tokens': 62, 'total_tokens': 285, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new session\n",
    "\n",
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"what is my name again?\"},\n",
    "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f73bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hi James, it’s nice to meet you! I’m Zeta, your helpful assistant. \\n\\nHow can I assist you today? Do you have a question you'd like me to answer, or is there something you’d like me to help you do?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 27, 'total_tokens': 84, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-550', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b6540-1ca4-76c2-8b9f-09b30785e5c4-0', usage_metadata={'input_tokens': 27, 'output_tokens': 57, 'total_tokens': 84, 'input_token_details': {}, 'output_token_details': {}}),\n",
       " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Your name is James! 😊 \\n\\nIt's great you’re digging into conversational memory – it’s a really fascinating area. Do you want to delve a little deeper into the differences between these memory types, or would you like me to explain how they relate to building a more sophisticated chatbot?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 223, 'total_tokens': 285, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-937', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b6540-d114-7031-b58a-08bf5b9e7842-0', usage_metadata={'input_tokens': 223, 'output_tokens': 62, 'total_tokens': 285, 'input_token_details': {}, 'output_token_details': {}})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "chat_map[\"id_k14\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae2a4ea",
   "metadata": {},
   "source": [
    "## 3. ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dab4556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\" # still no good\n",
    "# MODEL = \"qwen3:0.6b\"  # okay\n",
    "MODEL = \"gemma3:4b\" # okay\n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00171ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkmpp\\AppData\\Local\\Temp\\ipykernel_6600\\3945359647.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm)\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc259c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkmpp\\AppData\\Local\\Temp\\ipykernel_6600\\3667692610.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use `langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e6a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: hello there my name is James\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "James introduces himself, and Codex responds with a friendly greeting and reveals its identity as a large language model created by OpenAI. Codex then provides interesting information about the name \"James,\" including its Hebrew origins and historical prevalence, demonstrating its extensive knowledge gained from analyzing text data. It then asks James how he is doing.\n",
      "Human: I am researching the different types of conversational memory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "James introduces Codex, a large language model from OpenAI, and asks how he is doing. Codex then explains its capabilities regarding conversational memory, detailing techniques like context windows (currently 8,192 tokens), memory networks, transformers with long-range attention, external memory modules, and dialogue state tracking. It highlights the complexity of these approaches and invites James to specify his area of interest within conversational memory types.\n",
      "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "James introduced Codex, a large language model from OpenAI, and asked about its capabilities regarding conversational memory, specifically ConversationBufferMemory and ConversationBufferWindowMemory. Codex explained these memory types as components of “windowed memory,” relying on a sliding window to retain conversation context. ConversationBufferWindowMemory uses a fixed-size window, offering simplicity but limited context retention. ConversationBufferMemory is more advanced, employing decay functions to reduce the influence of older tokens within the window, allowing for longer context retention. Codex is open to delving deeper into decay functions or exploring the relationship between these memory types and the overall architecture of a conversational AI, like transformers.\n",
      "Human: Buffer memory just stores the entire conversation\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "James introduced Codex and its memory types, specifically ConversationBufferMemory and ConversationBufferWindowMemory, explaining them as part of “windowed memory” utilizing a sliding window to retain conversation context. ConversationBufferWindowMemory uses a fixed-size window for simplicity but limited retention, while ConversationBufferMemory employs decay functions to reduce the influence of older tokens, enabling longer context retention. Codex is open to further discussion on these functions and their relationship to the overall conversational AI architecture.\n",
      "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Buffer window memory stores the last k messages, dropping the rest.',\n",
       " 'history': 'James introduced Codex and its memory types, specifically ConversationBufferMemory and ConversationBufferWindowMemory, explaining them as part of “windowed memory” utilizing a sliding window to retain conversation context. ConversationBufferWindowMemory uses a fixed-size window for simplicity but limited retention, while ConversationBufferMemory employs decay functions to reduce the influence of older tokens, enabling longer context retention. Codex is open to further discussion on these functions and their relationship to the overall conversational AI architecture.',\n",
       " 'response': 'You’ve hit on a really key aspect of how ConversationBufferWindowMemory works! It absolutely does store the last *k* messages – that’s the core of its design. The value of *k* determines the window size. Think of it like a sliding window that moves along the conversation as new messages arrive. When the window reaches the end, the oldest messages are discarded, and the newest message is added. It’s a really efficient way to limit the context a model considers, preventing it from being overwhelmed by extremely long conversations. \\n\\nTo give you a bit more detail, the size of *k* isn’t just a number; it’s configurable, and the optimal value depends heavily on the specific task and the model’s capabilities. A larger *k* means the model can retain more context, which can be beneficial for complex, multi-turn conversations, but it also increases computational cost and potentially introduces noise if the conversation drifts too far from the initial topic. \\n\\nWe could delve into how the window size is determined, or perhaps talk about some strategies for selecting the best *k* for a given scenario. Would you like to explore any of those aspects further?'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"hello there my name is James\"})\n",
    "chain.invoke({\"input\": \"I am researching the different types of conversational memory.\"})\n",
    "chain.invoke({\"input\": \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"})\n",
    "chain.invoke({\"input\": \"Buffer memory just stores the entire conversation\"})\n",
    "chain.invoke({\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efe951e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "James introduced Codex and its memory types, focusing on “windowed memory” using ConversationBufferWindowMemory and ConversationBufferMemory. ConversationBufferWindowMemory maintains a sliding window of the last *k* messages – discarding older ones as new ones arrive – offering efficiency but limited retention. ConversationBufferMemory utilizes decay functions for longer context retention. The human clarified that *k* represents the window size, which is configurable and tied to the task and model capabilities, balancing context retention with computational cost. The AI offered to delve deeper into window size determination or strategies for selecting the optimal *k* value.\n",
      "Human: What is my name again?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name again?',\n",
       " 'history': 'James introduced Codex and its memory types, focusing on “windowed memory” using ConversationBufferWindowMemory and ConversationBufferMemory. ConversationBufferWindowMemory maintains a sliding window of the last *k* messages – discarding older ones as new ones arrive – offering efficiency but limited retention. ConversationBufferMemory utilizes decay functions for longer context retention. The human clarified that *k* represents the window size, which is configurable and tied to the task and model capabilities, balancing context retention with computational cost. The AI offered to delve deeper into window size determination or strategies for selecting the optimal *k* value.',\n",
       " 'response': 'You’re James! I’ve got that noted in my current conversation context. Specifically, I recorded that you initiated the discussion about Codex and its memory types, focusing on windowed memory – particularly ConversationBufferWindowMemory and ConversationBufferMemory. I also have a record of you clarifying what *k* represents, the window size, and its connection to task and model capabilities. It’s all right here, readily accessible for reference. 😊'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What is my name again?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca5de3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b8d8544",
   "metadata": {},
   "source": [
    "#### `New Method`\n",
    "\n",
    "ConversationSummaryMemory with RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932123ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage, SystemMessage\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\" # still no good\n",
    "# MODEL = \"qwen3:0.6b\"  # okay\n",
    "MODEL = \"gemma3:4b\"  # okay\n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL)\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Zeta\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = prompt_template | llm\n",
    "\n",
    "\n",
    "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
    "\n",
    "    def __init__(self, llm: ChatOpenAI):\n",
    "        super().__init__(llm=llm)\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages.\n",
    "        \"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        # construct the summary chat messages\n",
    "        summary_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                SystemMessagePromptTemplate.from_template(\n",
    "                    \"Given the existing conversation summary and the new messages, \"\n",
    "                    \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                    \"as much relevant information as possible.\"\n",
    "                ),\n",
    "                HumanMessagePromptTemplate.from_template(\n",
    "                    \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                    \"New messages:\\n{messages}\"\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        # format the messages and invoke the LLM\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=self.messages,\n",
    "                messages=[x.content for x in messages],\n",
    "            )\n",
    "        )\n",
    "        # replace the existing history with a single system summary message\n",
    "        self.messages = [SystemMessage(content=new_summary.content)]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []\n",
    "\n",
    "\n",
    "chat_map = {}\n",
    "\n",
    "\n",
    "def get_chat_history(\n",
    "    session_id: str, llm: ChatOpenAI\n",
    ") -> ConversationSummaryMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
    "    # return the chat history\n",
    "    return chat_map[session_id]\n",
    "\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=ChatOpenAI,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6a550b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi James! It’s nice to meet you. I’m Zeta. How’s your day going so far? Is there anything I can help you with today, or were you just saying hello?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 27, 'total_tokens': 70, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-259', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b6b50-d59c-7fb3-a414-d00633a2455f-0', usage_metadata={'input_tokens': 27, 'output_tokens': 43, 'total_tokens': 70, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a63ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Here’s a new summary of the conversation, incorporating the new messages:\\n\\nJames introduced himself and stated his name is James. Zeta responded warmly, introducing herself as Zeta and asking how James’ day was going, inquiring if she could assist him or if he was simply saying hello.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c20d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Okay, here’s a revised summary of the conversation, incorporating the new messages and maintaining the key information:\\n\\n**Summary:**\\n\\nThe conversation began with James introducing himself and expressing interest in researching the different types of conversational memory. Zeta responded by providing a detailed explanation of four key categories of conversational memory:\\n\\n*   **Episodic Memory (Within the Conversation):** Recollection of specific details from the conversation, similar to remembering a past event.\\n*   **Priming (Implicit Conversation Memory):** Unconscious influence of previous conversations on responses.\\n*   **Contextual Memory (Situational Awareness):** Remembering details about the situation surrounding the conversation.\\n*   **Working Memory (Real-Time Processing):** Maintaining and manipulating information during the conversation.\\n\\nZeta then outlined the neural basis of conversational memory (prefrontal cortex, hippocampus, attention/language processing) and noted individual differences in abilities. She concluded by asking James what specifically he was interested in learning more about regarding conversational memory, and for what purpose (e.g., a paper, project, or simply curiosity). James confirmed his interest in researching these different types of conversational memory.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"I'm researching the different types of conversational memory.\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm},\n",
    ")\n",
    "\n",
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a36567",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in [\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\",\n",
    "]:\n",
    "    pipeline_with_history.invoke(\n",
    "        {\"query\": msg}, config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7750bcf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='**Summary:**\\n\\nThe conversation continues to explore conversational memory, specifically focusing on “ConversationBufferWindowMemory.” Zeta explains that this memory type operates by storing the last ‘k’ messages of a conversation, discarding older ones. This ‘k’ value represents the window size – the amount of prior context the Large Language Model retains. It’s described as a “sliding window” that shifts forward as the conversation progresses. Zeta proposes discussing the implications of this approach, such as how the window size impacts coherence and its interaction with other conversational memory categories like Episodic, Priming, Contextual, and Working Memory.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f79e39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You’ve been calling yourself Zeta! I’m here to help you, Zeta. 😊 \\n\\nDo you want me to remind you of anything else about our conversation so far?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 150, 'total_tokens': 188, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-543', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b6b5f-f447-7a70-9cc1-7abee00c11ea-0', usage_metadata={'input_tokens': 150, 'output_tokens': 38, 'total_tokens': 188, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"}, config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6304e1",
   "metadata": {},
   "source": [
    "`video time -> 1:57:34`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b60c51",
   "metadata": {},
   "source": [
    "## 4. ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2463dd3c",
   "metadata": {},
   "source": [
    "`not working. need a model of open ai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351accd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\" # still no good\n",
    "# MODEL = \"qwen3:0.6b\"  # okay\n",
    "MODEL = \"gemma3:4b\" # okay\n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3435056b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkmpp\\AppData\\Local\\Temp\\ipykernel_17220\\114919256.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=300,\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd0b695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkmpp\\AppData\\Local\\Temp\\ipykernel_17220\\1592351593.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use `langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed7b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"hi, my name is James\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ed835",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in [\n",
    "    \"I'm researching the different types of conversational memory.\",\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]:\n",
    "    chain.invoke({\"input\": msg})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b8f6e1",
   "metadata": {},
   "source": [
    "`new method`\n",
    "\n",
    "ConversationSummaryBufferMemory with RunnableWithMessageHistory\n",
    "\n",
    "`this part is working`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage, SystemMessage\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\" # still no good\n",
    "# MODEL = \"qwen3:0.6b\"  # okay\n",
    "MODEL = \"gemma3:4b\"  # okay\n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL)\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Zeta\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = prompt_template | llm\n",
    "\n",
    "\n",
    "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, llm: ChatOpenAI, k: int):\n",
    "        super().__init__(llm=llm, k=k)\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages and summarizing the messages that we\n",
    "        drop.\n",
    "        \"\"\"\n",
    "        existing_summary: SystemMessage | None = None\n",
    "        old_messages: list[BaseMessage] | None = None\n",
    "        # see if we already have a summary message\n",
    "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
    "            print(\">> Found existing summary\")\n",
    "            existing_summary = self.messages.pop(0)\n",
    "        # add the new messages to the history\n",
    "        self.messages.extend(messages)\n",
    "        # check if we have too many messages\n",
    "        if len(self.messages) > self.k:\n",
    "            print(\n",
    "                f\">> Found {len(self.messages)} messages, dropping \"\n",
    "                f\"oldest {len(self.messages) - self.k} messages.\"\n",
    "            )\n",
    "            # pull out the oldest messages...\n",
    "            old_messages = self.messages[: -self.k]\n",
    "            # ...and keep only the most recent messages\n",
    "            self.messages = self.messages[-self.k :]\n",
    "        if old_messages is None:\n",
    "            print(\">> No old messages to update summary with\")\n",
    "            # if we have no old_messages, we have nothing to update in summary\n",
    "            return\n",
    "        # construct the summary chat messages\n",
    "        summary_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                SystemMessagePromptTemplate.from_template(\n",
    "                    \"Given the existing conversation summary and the new messages, \"\n",
    "                    \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                    \"as much relevant information as possible.\"\n",
    "                ),\n",
    "                HumanMessagePromptTemplate.from_template(\n",
    "                    \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                    \"New messages:\\n{old_messages}\"\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        # format the messages and invoke the LLM\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=existing_summary, old_messages=old_messages\n",
    "            )\n",
    "        )\n",
    "        print(f\">> New summary: {new_summary.content}\")\n",
    "        # prepend the new summary to the history\n",
    "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []\n",
    "\n",
    "\n",
    "chat_map = {}\n",
    "\n",
    "\n",
    "def get_chat_history(\n",
    "    session_id: str, llm: ChatOpenAI, k: int\n",
    ") -> ConversationSummaryBufferMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = ConversationSummaryBufferMessageHistory(llm=llm, k=k)\n",
    "    # return the chat history\n",
    "    return chat_map[session_id]\n",
    "\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=ChatOpenAI,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"The number of messages to keep in the history\",\n",
    "            default=4,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c27c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> No old messages to update summary with\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hi James, it’s nice to meet you! I’m Zeta, your helpful assistant. \\n\\nHow can I assist you today? Do you have something specific you'd like to talk about, or were you just saying hello?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 27, 'total_tokens': 77, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'gemma3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-47', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b7023-5acc-7df3-bf3c-7ec939a9485d-0', usage_metadata={'input_tokens': 27, 'output_tokens': 50, 'total_tokens': 77, 'input_token_details': {}, 'output_token_details': {}})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
    ")\n",
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1050957a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Message 1\n",
      "---\n",
      "\n",
      ">> No old messages to update summary with\n",
      "---\n",
      "Message 2\n",
      "---\n",
      "\n",
      ">> Found 6 messages, dropping oldest 2 messages.\n",
      ">> New summary: James introduced himself and is now speaking with Zeta, a helpful assistant. Zeta greeted him and asked how she could assist him, opening the conversation for a specific request or simply a chat.\n",
      "---\n",
      "Message 3\n",
      "---\n",
      "\n",
      ">> Found existing summary\n",
      ">> Found 6 messages, dropping oldest 2 messages.\n",
      ">> New summary: James is researching the different types of conversational memory, a fascinating area of psychology. Zeta is assisting him by exploring his specific interests within this topic. She’s offered several starting points for discussion, including models of conversational memory (like the Interactive Activation Model and Situation Model), types of information remembered in conversation (topic, speaker identity, context), and the cognitive processes involved (attention, working memory, semantic memory). Zeta is now asking James to indicate where he’d like to begin his exploration.\n",
      "---\n",
      "Message 4\n",
      "---\n",
      "\n",
      ">> Found existing summary\n",
      ">> Found 6 messages, dropping oldest 2 messages.\n",
      ">> New summary: James is researching conversational memory, with Zeta assisting him. They’ve already discussed models like the Interactive Activation Model and Situation Model, focusing on aspects such as topic, speaker identity, and context. James has now introduced ConversationBufferMemory (CBM) and ConversationBufferWindowMemory (CBW), core components of Allan Paice’s Situation Model. Zeta explains that CBM is the “working memory” for the conversation, constantly updated with current details, while CBW acts as a short-term buffer for recently recalled information. Key differences between the two are that CBM is active and CBW is a temporary holding space. Zeta is now offering to elaborate further on these concepts, specifically regarding their interaction, comparison with other models, or experimental evidence, and is asking James where he’d like to begin his exploration.\n"
     ]
    }
   ],
   "source": [
    "for i, msg in enumerate(\n",
    "    [\n",
    "        \"I'm researching the different types of conversational memory.\",\n",
    "        \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "        \"Buffer memory just stores the entire conversation\",\n",
    "        \"Buffer window memory stores the last k messages, dropping the rest.\",\n",
    "    ]\n",
    "):\n",
    "    print(f\"---\\nMessage {i+1}\\n---\\n\")\n",
    "    pipeline_with_history.invoke(\n",
    "        {\"query\": msg}, config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c023148",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655b750b",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b401b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269607bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    \"\"\"Add 'x' and 'y'.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "# or it can be like this `def add(x: float, y: float, z: float | None = 0.3)`\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply 'x' and 'y'.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "\n",
    "@tool\n",
    "def exponentiate(x: float, y: float) -> float:\n",
    "    \"\"\"Raise 'x' to the power of 'y'.\"\"\"\n",
    "    return x**y\n",
    "\n",
    "\n",
    "@tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    \"\"\"Subtract 'x' from 'y'.\"\"\"\n",
    "    return y - x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623121f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='add', description=\"Add 'x' and 'y'.\", args_schema=<class 'langchain_core.utils.pydantic.add'>, func=<function add at 0x0000027E2B281B20>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f254883c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add.name ='add'\n",
      "add.description =\"Add 'x' and 'y'.\"\n"
     ]
    }
   ],
   "source": [
    "print(f\"{add.name =}\\n{add.description =}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a18901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': \"Add 'x' and 'y'.\",\n",
       " 'properties': {'x': {'title': 'X', 'type': 'number'},\n",
       "  'y': {'title': 'Y', 'type': 'number'}},\n",
       " 'required': ['x', 'y'],\n",
       " 'title': 'add',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add.args_schema.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904e9b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': \"Raise 'x' to the power of 'y'.\",\n",
       " 'properties': {'x': {'title': 'X', 'type': 'number'},\n",
       "  'y': {'title': 'Y', 'type': 'number'}},\n",
       " 'required': ['x', 'y'],\n",
       " 'title': 'exponentiate',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exponentiate.args_schema.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506eb027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 5, 'y': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When invoking the tool, a JSON string output by the LLM will be parsed into JSON and then consumed as kwargs, similar to the below:\n",
    "import json\n",
    "\n",
    "llm_output_string = '{\"x\": 5, \"y\": 2}'  # this is the output from the LLM\n",
    "llm_output_dict = json.loads(llm_output_string)  # load as dictionary\n",
    "llm_output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d27ae70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is then passed into the tool function as kwargs (keyword arguments) as indicated by the ** operator - the ** operator is used to unpack the dictionary into keyword arguments.\n",
    "exponentiate.func(**llm_output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b04722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add.func(**llm_output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070e4640",
   "metadata": {},
   "source": [
    "`video time : 2:16:40`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f19e76b",
   "metadata": {},
   "source": [
    "### Creating an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2672b9e",
   "metadata": {},
   "source": [
    "```\n",
    "agent = (\n",
    "    <input parameters, including chat history and user query>\n",
    "    | <prompt>\n",
    "    | <LLM with tools>\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99b7b2f",
   "metadata": {},
   "source": [
    "`old methods for this`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93611700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\"  \n",
    "# MODEL = \"orieg/gemma3-tools:1b\"\n",
    "# MODEL = \"qwen3:0.6b\"  \n",
    "# MODEL = \"qwen3:1.7b\"  \n",
    "MODEL = \"qwen3:4b\"  \n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL, temperature = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5853858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the sum of two numbers, 'x' and 'y'.\n",
    "\n",
    "    It accepts two floating-point numbers, 'x' and 'y', and returns their sum.\n",
    "\n",
    "    This function is designed to be a fundamental arithmetic operation.\n",
    "\n",
    "    Args:\n",
    "        x (float): The first number to be added.\n",
    "        y (float): The second number to be added.\n",
    "\n",
    "    Returns:\n",
    "        float: The sum of 'x' and 'y'.\n",
    "    \"\"\"\n",
    "    return x + y\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"\n",
    "    Multiplies two numbers, 'x' and 'y'.\n",
    "\n",
    "    This function calculates the product of two numbers.  The product is the result of multiplying x by y.\n",
    "\n",
    "    Args:\n",
    "        x (float): The first number.\n",
    "        y (float): The second number.\n",
    "\n",
    "    Returns:\n",
    "        float: The product of x and y.\n",
    "    \"\"\"\n",
    "    return x * y\n",
    "\n",
    "\n",
    "@tool\n",
    "def exponentiate(x: float, y: float) -> float:\n",
    "    \"\"\"\n",
    "    Raise 'x' to the power of 'y' (x ** y).\n",
    "\n",
    "    This function calculates x raised to the power of y.  It represents exponential growth.\n",
    "\n",
    "    Args:\n",
    "        x (float): The base number.\n",
    "        y (float): The exponent.\n",
    "\n",
    "    Returns:\n",
    "        float: The result of x raised to the power of y.\n",
    "    \"\"\"\n",
    "    return x**y\n",
    "\n",
    "\n",
    "@tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    \"\"\"Subtracts 'x' from 'y' or 'x' minus 'y'.\n",
    "\n",
    "    This function performs a numerical subtraction operation.  It returns the result of subtracting 'x' from 'y'.\n",
    "\n",
    "    Args:\n",
    "        x (float): The number to be subtracted.\n",
    "        y (float): The number to be subtracted.\n",
    "\n",
    "    Returns:\n",
    "        float: The result of subtracting 'x' from 'y'.\n",
    "    \"\"\"\n",
    "    return y - x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "461eb54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You're a helpful assistant\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9313f7",
   "metadata": {},
   "source": [
    "The agent scratchpad is where the agent will write it's \"notes\" as it is working through multiple internal thought and tool-use steps to produce a final output to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4324e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkmpp\\AppData\\Local\\Temp\\ipykernel_13600\\2734247304.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", # must align with MessagesPlaceholder variable_name in `prompt`\n",
    "    return_messages=True # to return message object\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fa05a1",
   "metadata": {},
   "source": [
    "Now we will initialize our agent. For that we need:\n",
    "\n",
    "* llm: as already defined\n",
    "* tools: to be defined (just a list of our previously defined tools)\n",
    "* prompt: as already defined\n",
    "* memory: as already defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8e52530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.agents import create_tool_calling_agent\n",
    "\n",
    "tools = [add, subtract, multiply, exponentiate]\n",
    "\n",
    "agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0772abdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ToolAgentAction(tool='multiply', tool_input={'x': 10.7, 'y': 7.68}, log=\"\\nInvoking: `multiply` with `{'x': 10.7, 'y': 7.68}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 359, 'prompt_tokens': 638, 'total_tokens': 997, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'qwen3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-674', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b7762-02ca-7ca3-982e-79459be32616-0', tool_calls=[{'name': 'multiply', 'args': {'x': 10.7, 'y': 7.68}, 'id': 'call_lf2m8jkz', 'type': 'tool_call'}], usage_metadata={'input_tokens': 638, 'output_tokens': 359, 'total_tokens': 997, 'input_token_details': {}, 'output_token_details': {}})], tool_call_id='call_lf2m8jkz')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke(\n",
    "    {\n",
    "        \"input\": \"what is 10.7 multiplied by 7.68?\",\n",
    "        \"chat_history\": memory.chat_memory.messages,\n",
    "        \"intermediate_steps\": [],  # agent will append it's internal steps here\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0994aef",
   "metadata": {},
   "source": [
    "Here, we can see the LLM has generated that we should use the multiply tool and the tool input should be {\"x\": 10.7, \"y\": 7.68}. However, the tool is not executed. For that to happen we need an agent execution loop, which will handle the multiple iterations of generation to tool calling to generation, etc.\n",
    "\n",
    "We use the AgentExecutor class to handle the execution loop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec10356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    memory=memory, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaea0f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `multiply` with `{'x': 10.7, 'y': 7.68}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m82.17599999999999\u001b[0m\u001b[32;1m\u001b[1;3mThe result of multiplying 10.7 by 7.68 is **82.176**. \n",
      "\n",
      "*(Note: The tool response showed `82.17599999999999` due to floating-point precision limitations in computation, but the exact value rounds to 82.176.)*\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is 10.7 multiplied by 7.68?',\n",
       " 'chat_history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The result of multiplying 10.7 by 7.68 is **82.176**. \\n\\n*(Note: The tool response showed `82.17599999999999` due to floating-point precision limitations in computation, but the exact value rounds to 82.176.)*', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'The result of multiplying 10.7 by 7.68 is **82.176**. \\n\\n*(Note: The tool response showed `82.17599999999999` due to floating-point precision limitations in computation, but the exact value rounds to 82.176.)*'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\n",
    "    \"input\": \"what is 10.7 multiplied by 7.68?\",\n",
    "    \"chat_history\": memory.chat_memory.messages,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aecc2cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.17599999999999"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10.7 * 7.68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c195140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHello James! How can I assist you today? 😊\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'My name is James',\n",
       " 'chat_history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The result of multiplying 10.7 by 7.68 is **82.176**. \\n\\n*(Note: The tool response showed `82.17599999999999` due to floating-point precision limitations in computation, but the exact value rounds to 82.176.)*', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='My name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello James! How can I assist you today? 😊', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'Hello James! How can I assist you today? 😊'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\n",
    "    \"input\": \"My name is James\",\n",
    "    \"chat_history\": memory\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20d18342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe expression \"nine plus 10, minus 4 * 2, to the power of 3\" evaluates to $((9 + 10) - (4 \\times 2))^3$. \n",
      "\n",
      "**Step-by-step computation:**\n",
      "1. **Add 9 and 10**: $9 + 10 = 19$\n",
      "2. **Multiply 4 and 2**: $4 \\times 2 = 8$\n",
      "3. **Subtract the results**: $19 - 8 = 11$ (using `subtract(8, 19)` to ensure $y - x$)\n",
      "4. **Exponentiate 11 to the power of 3**: $11^3 = 1331$\n",
      "\n",
      "**Final Answer:**  \n",
      "$$\n",
      "\\boxed{1331}\n",
      "$$\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is nine plus 10, minus 4 * 2, to the power of 3',\n",
       " 'chat_history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The result of multiplying 10.7 by 7.68 is **82.176**. \\n\\n*(Note: The tool response showed `82.17599999999999` due to floating-point precision limitations in computation, but the exact value rounds to 82.176.)*', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='My name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello James! How can I assist you today? 😊', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is nine plus 10, minus 4 * 2, to the power of 3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The expression \"nine plus 10, minus 4 * 2, to the power of 3\" evaluates to $((9 + 10) - (4 \\\\times 2))^3$. \\n\\n**Step-by-step computation:**\\n1. **Add 9 and 10**: $9 + 10 = 19$\\n2. **Multiply 4 and 2**: $4 \\\\times 2 = 8$\\n3. **Subtract the results**: $19 - 8 = 11$ (using `subtract(8, 19)` to ensure $y - x$)\\n4. **Exponentiate 11 to the power of 3**: $11^3 = 1331$\\n\\n**Final Answer:**  \\n$$\\n\\\\boxed{1331}\\n$$', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'The expression \"nine plus 10, minus 4 * 2, to the power of 3\" evaluates to $((9 + 10) - (4 \\\\times 2))^3$. \\n\\n**Step-by-step computation:**\\n1. **Add 9 and 10**: $9 + 10 = 19$\\n2. **Multiply 4 and 2**: $4 \\\\times 2 = 8$\\n3. **Subtract the results**: $19 - 8 = 11$ (using `subtract(8, 19)` to ensure $y - x$)\\n4. **Exponentiate 11 to the power of 3**: $11^3 = 1331$\\n\\n**Final Answer:**  \\n$$\\n\\\\boxed{1331}\\n$$'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\n",
    "    \"input\": \"What is nine plus 10, minus 4 * 2, to the power of 3\",\n",
    "    \"chat_history\": memory\n",
    "})\n",
    "\n",
    "# todo: try this with a large llm. small llm can not properly call tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d564423f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `add` with `{'x': 9, 'y': 10}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m19.0\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `multiply` with `{'x': 4, 'y': 2}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m8.0\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `subtract` with `{'x': 8, 'y': 19}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m11.0\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `exponentiate` with `{'x': 11, 'y': 3}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m1331.0\u001b[0m\u001b[32;1m\u001b[1;3mThe final result of the computation is **1331**.\n",
      "\n",
      "$$\n",
      "\\boxed{1331}\n",
      "$$\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'add 9 to 10, subtract the result of 4 multiply by 2, and last exponentiate it by 3',\n",
       " 'chat_history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The result of multiplying 10.7 by 7.68 is **82.176**. \\n\\n*(Note: The tool response showed `82.17599999999999` due to floating-point precision limitations in computation, but the exact value rounds to 82.176.)*', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='My name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello James! How can I assist you today? 😊', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is nine plus 10, minus 4 * 2, to the power of 3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The expression \"nine plus 10, minus 4 * 2, to the power of 3\" evaluates to $((9 + 10) - (4 \\\\times 2))^3$. \\n\\n**Step-by-step computation:**\\n1. **Add 9 and 10**: $9 + 10 = 19$\\n2. **Multiply 4 and 2**: $4 \\\\times 2 = 8$\\n3. **Subtract the results**: $19 - 8 = 11$ (using `subtract(8, 19)` to ensure $y - x$)\\n4. **Exponentiate 11 to the power of 3**: $11^3 = 1331$\\n\\n**Final Answer:**  \\n$$\\n\\\\boxed{1331}\\n$$', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='add 9 to 10, subtract the result of 4 multiply by 2, and last exponentiate it by 3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The final result of the computation is **1331**.\\n\\n$$\\n\\\\boxed{1331}\\n$$', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'The final result of the computation is **1331**.\\n\\n$$\\n\\\\boxed{1331}\\n$$'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\n",
    "    \"input\": \"add 9 to 10, subtract the result of 4 multiply by 2, and last exponentiate it by 3\",\n",
    "    \"chat_history\": memory\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "184b06d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-493"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9 + 10 - (4 * 2) ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89e40413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `multiply` with `{'x': 4, 'y': 2}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m8.0\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `exponentiate` with `{'x': 8, 'y': 3}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m512.0\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `add` with `{'x': 9, 'y': 10}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m19.0\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `subtract` with `{'x': 512, 'y': 19}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m-493.0\u001b[0m\u001b[32;1m\u001b[1;3mThe expression $9 + 10 - (4 \\times 2)^3$ evaluates as follows:\n",
      "\n",
      "1. **Multiply**: $4 \\times 2 = 8$  \n",
      "2. **Exponentiate**: $8^3 = 512$  \n",
      "3. **Add**: $9 + 10 = 19$  \n",
      "4. **Subtract**: $19 - 512 = -493$  \n",
      "\n",
      "**Final Answer**:  \n",
      "$$\n",
      "\\boxed{-493}\n",
      "$$\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '9 + 10 - (4 * 2) ** 3',\n",
       " 'chat_history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The result of multiplying 10.7 by 7.68 is **82.176**. \\n\\n*(Note: The tool response showed `82.17599999999999` due to floating-point precision limitations in computation, but the exact value rounds to 82.176.)*', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='My name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello James! How can I assist you today? 😊', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is nine plus 10, minus 4 * 2, to the power of 3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The expression \"nine plus 10, minus 4 * 2, to the power of 3\" evaluates to $((9 + 10) - (4 \\\\times 2))^3$. \\n\\n**Step-by-step computation:**\\n1. **Add 9 and 10**: $9 + 10 = 19$\\n2. **Multiply 4 and 2**: $4 \\\\times 2 = 8$\\n3. **Subtract the results**: $19 - 8 = 11$ (using `subtract(8, 19)` to ensure $y - x$)\\n4. **Exponentiate 11 to the power of 3**: $11^3 = 1331$\\n\\n**Final Answer:**  \\n$$\\n\\\\boxed{1331}\\n$$', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='add 9 to 10, subtract the result of 4 multiply by 2, and last exponentiate it by 3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The final result of the computation is **1331**.\\n\\n$$\\n\\\\boxed{1331}\\n$$', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='9 + 10 - (4 * 2) ** 3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The expression $9 + 10 - (4 \\\\times 2)^3$ evaluates as follows:\\n\\n1. **Multiply**: $4 \\\\times 2 = 8$  \\n2. **Exponentiate**: $8^3 = 512$  \\n3. **Add**: $9 + 10 = 19$  \\n4. **Subtract**: $19 - 512 = -493$  \\n\\n**Final Answer**:  \\n$$\\n\\\\boxed{-493}\\n$$', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'The expression $9 + 10 - (4 \\\\times 2)^3$ evaluates as follows:\\n\\n1. **Multiply**: $4 \\\\times 2 = 8$  \\n2. **Exponentiate**: $8^3 = 512$  \\n3. **Add**: $9 + 10 = 19$  \\n4. **Subtract**: $19 - 512 = -493$  \\n\\n**Final Answer**:  \\n$$\\n\\\\boxed{-493}\\n$$'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\n",
    "    \"input\": \"9 + 10 - (4 * 2) ** 3\",\n",
    "    \"chat_history\": memory\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29e64cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mYour name is **James**. 😊\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is my name',\n",
       " 'chat_history': [HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The result of multiplying 10.7 by 7.68 is **82.176**. \\n\\n*(Note: The tool response showed `82.17599999999999` due to floating-point precision limitations in computation, but the exact value rounds to 82.176.)*', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='My name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello James! How can I assist you today? 😊', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is nine plus 10, minus 4 * 2, to the power of 3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The expression \"nine plus 10, minus 4 * 2, to the power of 3\" evaluates to $((9 + 10) - (4 \\\\times 2))^3$. \\n\\n**Step-by-step computation:**\\n1. **Add 9 and 10**: $9 + 10 = 19$\\n2. **Multiply 4 and 2**: $4 \\\\times 2 = 8$\\n3. **Subtract the results**: $19 - 8 = 11$ (using `subtract(8, 19)` to ensure $y - x$)\\n4. **Exponentiate 11 to the power of 3**: $11^3 = 1331$\\n\\n**Final Answer:**  \\n$$\\n\\\\boxed{1331}\\n$$', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='add 9 to 10, subtract the result of 4 multiply by 2, and last exponentiate it by 3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The final result of the computation is **1331**.\\n\\n$$\\n\\\\boxed{1331}\\n$$', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='9 + 10 - (4 * 2) ** 3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The expression $9 + 10 - (4 \\\\times 2)^3$ evaluates as follows:\\n\\n1. **Multiply**: $4 \\\\times 2 = 8$  \\n2. **Exponentiate**: $8^3 = 512$  \\n3. **Add**: $9 + 10 = 19$  \\n4. **Subtract**: $19 - 512 = -493$  \\n\\n**Final Answer**:  \\n$$\\n\\\\boxed{-493}\\n$$', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Your name is **James**. 😊', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'Your name is **James**. 😊'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\n",
    "    \"input\": \"What is my name\",\n",
    "    \"chat_history\": memory\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be8079a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='what is 10.7 multiplied by 7.68?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='The result of multiplying 10.7 by 7.68 is **82.176**. \\n\\n*(Note: The tool response showed `82.17599999999999` due to floating-point precision limitations in computation, but the exact value rounds to 82.176.)*', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='My name is James', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hello James! How can I assist you today? 😊', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is nine plus 10, minus 4 * 2, to the power of 3', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='The expression \"nine plus 10, minus 4 * 2, to the power of 3\" evaluates to $((9 + 10) - (4 \\\\times 2))^3$. \\n\\n**Step-by-step computation:**\\n1. **Add 9 and 10**: $9 + 10 = 19$\\n2. **Multiply 4 and 2**: $4 \\\\times 2 = 8$\\n3. **Subtract the results**: $19 - 8 = 11$ (using `subtract(8, 19)` to ensure $y - x$)\\n4. **Exponentiate 11 to the power of 3**: $11^3 = 1331$\\n\\n**Final Answer:**  \\n$$\\n\\\\boxed{1331}\\n$$', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='add 9 to 10, subtract the result of 4 multiply by 2, and last exponentiate it by 3', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='The final result of the computation is **1331**.\\n\\n$$\\n\\\\boxed{1331}\\n$$', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='9 + 10 - (4 * 2) ** 3', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='The expression $9 + 10 - (4 \\\\times 2)^3$ evaluates as follows:\\n\\n1. **Multiply**: $4 \\\\times 2 = 8$  \\n2. **Exponentiate**: $8^3 = 512$  \\n3. **Add**: $9 + 10 = 19$  \\n4. **Subtract**: $19 - 512 = -493$  \\n\\n**Final Answer**:  \\n$$\\n\\\\boxed{-493}\\n$$', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Your name is **James**. 😊', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.chat_memory.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f899474",
   "metadata": {},
   "source": [
    "### SerpAPI Weather Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c79929d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"SERPAPI_API_KEY\"] = os.getenv(\"SERPAPI_API_KEY\") or getpass(\n",
    "    \"Enter your SerpAPI API key: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b284cd58",
   "metadata": {},
   "source": [
    "Here we will load the serpapi tool directly from the prebuilt tools that LangChain provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbd8f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\"  \n",
    "# MODEL = \"orieg/gemma3-tools:1b\"\n",
    "# MODEL = \"qwen3:0.6b\"  \n",
    "# MODEL = \"qwen3:1.7b\"  \n",
    "MODEL = \"qwen3:4b\"  \n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL, temperature = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7959f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.agents import load_tools\n",
    "\n",
    "toolbox = load_tools(tool_names=['serpapi'], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b11debec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.agents import AgentExecutor\n",
    "from langchain_classic.agents import create_tool_calling_agent\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bd5971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_location_from_ip():\n",
    "    \"\"\"Get the geographical location based on the IP address.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\"https://ipinfo.io/json\")\n",
    "        data = response.json()\n",
    "        if \"loc\" in data:\n",
    "            latitude, longitude = data[\"loc\"].split(\",\")\n",
    "            data = (\n",
    "                f\"Latitude: {latitude},\\n\"\n",
    "                f\"Longitude: {longitude},\\n\"\n",
    "                f\"City: {data.get('city', 'N/A')},\\n\"\n",
    "                f\"Country: {data.get('country', 'N/A')}\"\n",
    "            )\n",
    "            return data\n",
    "        else:\n",
    "            return \"Location could not be determined.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error occurred: {e}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_current_datetime() -> str:\n",
    "    \"\"\"Return the current date and time.\n",
    "    use when user asks questions like this.\n",
    "    eg: what is the date and time right now?\n",
    "    \"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea08ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"you're a helpful assistant\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cb490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = toolbox + [get_current_datetime, get_location_from_ip]\n",
    "\n",
    "agent = crete_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "addb0715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_current_datetime` with `{}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m2026-01-01 09:37:38\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_location_from_ip` with `{}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mLatitude: 6.9355,\n",
      "Longitude: 79.8487,\n",
      "City: Colombo,\n",
      "Country: LK\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Search` with `current weather in Colombo, Sri Lanka`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m{'type': 'weather_result', 'temperature': '82', 'unit': 'Fahrenheit', 'precipitation': '1%', 'humidity': '76%', 'wind': '7 mph', 'location': 'Colombo, Sri Lanka', 'date': 'Thursday 9:00 AM', 'weather': 'Cloudy'}\u001b[0m\u001b[32;1m\u001b[1;3mThe current date and time is **2026-01-01 09:37:38**.  \n",
      "\n",
      "The weather in **Colombo, Sri Lanka** is currently **cloudy** with a temperature of **27.8°C** (82°F), 1% precipitation, 76% humidity, and 7 mph wind speed.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "out = agent_executor.invoke({\n",
    "    \"input\": (\n",
    "        \"I have a few questions, what is the date and time right now? \"\n",
    "        \"How is the weather where I am? Please give me degrees in Celsius\"\n",
    "    )\n",
    "})\n",
    "\n",
    "# todo: use large llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ada8821",
   "metadata": {},
   "source": [
    "## Agents Deep Dive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41afc946",
   "metadata": {},
   "source": [
    "`video time : 2.30.46`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ff2956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the sum of two numbers, 'x' and 'y'.\n",
    "\n",
    "    It accepts two floating-point numbers, 'x' and 'y', and returns their sum.\n",
    "\n",
    "    This function is designed to be a fundamental arithmetic operation.\n",
    "\n",
    "    Args:\n",
    "        x (float): The first number to be added.\n",
    "        y (float): The second number to be added.\n",
    "\n",
    "    Returns:\n",
    "        float: The sum of 'x' and 'y'.\n",
    "    \"\"\"\n",
    "    return x + y\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"\n",
    "    Multiplies two numbers, 'x' and 'y'.\n",
    "\n",
    "    This function calculates the product of two numbers.  The product is the result of multiplying x by y.\n",
    "\n",
    "    Args:\n",
    "        x (float): The first number.\n",
    "        y (float): The second number.\n",
    "\n",
    "    Returns:\n",
    "        float: The product of x and y.\n",
    "    \"\"\"\n",
    "    return x * y\n",
    "\n",
    "\n",
    "@tool\n",
    "def exponentiate(x: float, y: float) -> float:\n",
    "    \"\"\"\n",
    "    Raise 'x' to the power of 'y' (x ** y).\n",
    "\n",
    "    This function calculates x raised to the power of y.  It represents exponential growth.\n",
    "\n",
    "    Args:\n",
    "        x (float): The base number.\n",
    "        y (float): The exponent.\n",
    "\n",
    "    Returns:\n",
    "        float: The result of x raised to the power of y.\n",
    "    \"\"\"\n",
    "    return x**y\n",
    "\n",
    "\n",
    "@tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    \"\"\"Subtracts 'x' from 'y' or 'x' minus 'y'.\n",
    "\n",
    "    This function performs a numerical subtraction operation.  It returns the result of subtracting 'x' from 'y'.\n",
    "\n",
    "    Args:\n",
    "        x (float): The number to be subtracted.\n",
    "        y (float): The number to be subtracted.\n",
    "\n",
    "    Returns:\n",
    "        float: The result of subtracting 'x' from 'y'.\n",
    "    \"\"\"\n",
    "    return y - x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab7c0a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"You're a helpful assistant. when answering a user's question you should first use one of the tool provided. After using a tool the tool output will be provided in the `scratchpad` below. If you have an answer in the scratchpad you should not use any more tools and instead answer directly to the user. \"\n",
    "            ),\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"), # user message\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "168c6655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\"  \n",
    "# MODEL = \"orieg/gemma3-tools:1b\"\n",
    "# MODEL = \"qwen3:0.6b\"  \n",
    "# MODEL = \"qwen3:1.7b\"  \n",
    "MODEL = \"qwen3:4b\"  \n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3a87b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.base import RunnableSerializable\n",
    "\n",
    "tools = [add, subtract, multiply, exponentiate]\n",
    "\n",
    "# define the agent runnable\n",
    "\n",
    "agent: RunnableSerializable = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools, tool_choice=\"any\") # tool_choice other options (any, required, auto)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74899cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 200, 'prompt_tokens': 701, 'total_tokens': 901, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'qwen3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-516', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b77c4-ea82-7380-8806-c88a04ba2a3a-0', tool_calls=[{'name': 'add', 'args': {'x': 10, 'y': 10}, 'id': 'call_oox0onza', 'type': 'tool_call'}], usage_metadata={'input_tokens': 701, 'output_tokens': 200, 'total_tokens': 901, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call = agent.invoke({\"input\": \"What is 10+10.\", \"chat_history\": []})\n",
    "tool_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce92654b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'add',\n",
       "  'args': {'x': 10, 'y': 10},\n",
       "  'id': 'call_oox0onza',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1321d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tool name to function mapping\n",
    "name2tool = {tool.name: tool.func for tool in tools}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "888b9f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'add': <function __main__.add(x: float, y: float) -> float>,\n",
       " 'subtract': <function __main__.subtract(x: float, y: float) -> float>,\n",
       " 'multiply': <function __main__.multiply(x: float, y: float) -> float>,\n",
       " 'exponentiate': <function __main__.exponentiate(x: float, y: float) -> float>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name2tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18ff5ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_exec_content = name2tool[tool_call.tool_calls[0][\"name\"]](**tool_call.tool_calls[0][\"args\"])\n",
    "tool_exec_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb9e3b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The result of 10 + 10 is **20**.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 103, 'prompt_tokens': 734, 'total_tokens': 837, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'qwen3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-248', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b77c5-ffb6-7211-b155-bbe2859b9d14-0', usage_metadata={'input_tokens': 734, 'output_tokens': 103, 'total_tokens': 837, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feed the answer back into the LLM via the agent_scratchpad placeholder.\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "tool_exec = ToolMessage(\n",
    "    content=f\"The {tool_call.tool_calls[0]['name']} tool returned {tool_exec_content}\",\n",
    "    tool_call_id=tool_call.tool_calls[0][\"id\"]\n",
    ")\n",
    "\n",
    "out = agent.invoke(\n",
    "    {\n",
    "        \"input\": \"What is 10+10\",\n",
    "        \"chat_history\": [],\n",
    "        \"agent_scratchpad\": [tool_call, tool_exec]\n",
    "    }\n",
    ")\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c43125",
   "metadata": {},
   "source": [
    "tool_choice=\"auto\"\n",
    "\n",
    "full code all in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e6044aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The result of 10 + 10 is **20**.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 150, 'prompt_tokens': 734, 'total_tokens': 884, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'qwen3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-581', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b77c7-85cf-76a0-94f1-e14d9f2356d4-0', usage_metadata={'input_tokens': 734, 'output_tokens': 150, 'total_tokens': 884, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.base import RunnableSerializable\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\"  \n",
    "# MODEL = \"orieg/gemma3-tools:1b\"\n",
    "# MODEL = \"qwen3:0.6b\"  \n",
    "# MODEL = \"qwen3:1.7b\"  \n",
    "MODEL = \"qwen3:4b\"  \n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL)\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the sum of two numbers, 'x' and 'y'.\n",
    "\n",
    "    It accepts two floating-point numbers, 'x' and 'y', and returns their sum.\n",
    "\n",
    "    This function is designed to be a fundamental arithmetic operation.\n",
    "\n",
    "    Args:\n",
    "        x (float): The first number to be added.\n",
    "        y (float): The second number to be added.\n",
    "\n",
    "    Returns:\n",
    "        float: The sum of 'x' and 'y'.\n",
    "    \"\"\"\n",
    "    return x + y\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"\n",
    "    Multiplies two numbers, 'x' and 'y'.\n",
    "\n",
    "    This function calculates the product of two numbers.  The product is the result of multiplying x by y.\n",
    "\n",
    "    Args:\n",
    "        x (float): The first number.\n",
    "        y (float): The second number.\n",
    "\n",
    "    Returns:\n",
    "        float: The product of x and y.\n",
    "    \"\"\"\n",
    "    return x * y\n",
    "\n",
    "\n",
    "@tool\n",
    "def exponentiate(x: float, y: float) -> float:\n",
    "    \"\"\"\n",
    "    Raise 'x' to the power of 'y' (x ** y).\n",
    "\n",
    "    This function calculates x raised to the power of y.  It represents exponential growth.\n",
    "\n",
    "    Args:\n",
    "        x (float): The base number.\n",
    "        y (float): The exponent.\n",
    "\n",
    "    Returns:\n",
    "        float: The result of x raised to the power of y.\n",
    "    \"\"\"\n",
    "    return x**y\n",
    "\n",
    "\n",
    "@tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    \"\"\"Subtracts 'x' from 'y' or 'x' minus 'y'.\n",
    "\n",
    "    This function performs a numerical subtraction operation.  It returns the result of subtracting 'x' from 'y'.\n",
    "\n",
    "    Args:\n",
    "        x (float): The number to be subtracted.\n",
    "        y (float): The number to be subtracted.\n",
    "\n",
    "    Returns:\n",
    "        float: The result of subtracting 'x' from 'y'.\n",
    "    \"\"\"\n",
    "    return y - x\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"You're a helpful assistant. when answering a user's question you should first use one of the tool provided. After using a tool the tool output will be provided in the `scratchpad` below. If you have an answer in the scratchpad you should not use any more tools and instead answer directly to the user. \"\n",
    "            ),\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),  # user message\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "tools = [add, subtract, multiply, exponentiate]\n",
    "\n",
    "\n",
    "# define the agent runnable\n",
    "agent: RunnableSerializable = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", []),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(\n",
    "        tools, tool_choice=\"auto\"\n",
    "    )  # tool_choice other options (any, required)\n",
    ")\n",
    "\n",
    "tool_call = agent.invoke({\"input\": \"What is 10+10.\", \"chat_history\": []})\n",
    "\n",
    "# create tool name to function mapping\n",
    "name2tool = {tool.name: tool.func for tool in tools}\n",
    "\n",
    "\n",
    "tool_exec_content = name2tool[tool_call.tool_calls[0][\"name\"]](\n",
    "    **tool_call.tool_calls[0][\"args\"]\n",
    ")\n",
    "\n",
    "# feed the answer back into the LLM via the agent_scratchpad placeholder.\n",
    "tool_exec = ToolMessage(\n",
    "    content=f\"The {tool_call.tool_calls[0]['name']} tool returned {tool_exec_content}\",\n",
    "    tool_call_id=tool_call.tool_calls[0][\"id\"],\n",
    ")\n",
    "\n",
    "out = agent.invoke(\n",
    "    {\n",
    "        \"input\": \"What is 10+10\",\n",
    "        \"chat_history\": [],\n",
    "        \"agent_scratchpad\": [tool_call, tool_exec],\n",
    "    }\n",
    ")\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0547d0b0",
   "metadata": {},
   "source": [
    "create a final answer tool instead using tool_choice=\"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843fb744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.base import RunnableSerializable\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\"  \n",
    "# MODEL = \"orieg/gemma3-tools:1b\"\n",
    "# MODEL = \"qwen3:0.6b\"  \n",
    "# MODEL = \"qwen3:1.7b\"  \n",
    "MODEL = \"qwen3:4b\"  \n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL, temperature=0.0)\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the sum of two numbers, 'x' and 'y'.\n",
    "\n",
    "    It accepts two floating-point numbers, 'x' and 'y', and returns their sum.\n",
    "\n",
    "    This function is designed to be a fundamental arithmetic operation.\n",
    "\n",
    "    Args:\n",
    "        x (float): The first number to be added.\n",
    "        y (float): The second number to be added.\n",
    "\n",
    "    Returns:\n",
    "        float: The sum of 'x' and 'y'.\n",
    "\n",
    "    \"\"\"\n",
    "    return x + y\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"\n",
    "    Multiplies two numbers, 'x' and 'y'.\n",
    "\n",
    "    This function calculates the product of two numbers.  The product is the result of multiplying x by y.\n",
    "\n",
    "    Args:\n",
    "        x (float): The first number.\n",
    "        y (float): The second number.\n",
    "\n",
    "    Returns:\n",
    "        float: The product of x and y.\n",
    "    \"\"\"\n",
    "    return x * y\n",
    "\n",
    "\n",
    "@tool\n",
    "def exponentiate(x: float, y: float) -> float:\n",
    "    \"\"\"\n",
    "    Raise 'x' to the power of 'y' (x ** y).\n",
    "\n",
    "    This function calculates x raised to the power of y.  It represents exponential growth.\n",
    "\n",
    "    Args:\n",
    "        x (float): The base number.\n",
    "        y (float): The exponent.\n",
    "\n",
    "    Returns:\n",
    "        float: The result of x raised to the power of y.\n",
    "    \"\"\"\n",
    "    return x**y\n",
    "\n",
    "\n",
    "@tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    \"\"\"Subtracts 'x' from 'y' or 'x' minus 'y'.\n",
    "\n",
    "    This function performs a numerical subtraction operation.  It returns the result of subtracting 'x' from 'y'.\n",
    "\n",
    "    Args:\n",
    "        x (float): The number to be subtracted.\n",
    "        y (float): The number to be subtracted.\n",
    "\n",
    "    Returns:\n",
    "        float: The result of subtracting 'x' from 'y'.\n",
    "    \"\"\"\n",
    "    return y - x\n",
    "\n",
    "\n",
    "@tool\n",
    "def final_answer(answer: str, tools_used: list[list]) -> str:\n",
    "    \"\"\"Use this tool to provide a final answer to the user. The answer should be in natural language as this will be provided to the user directly. The tools_used must include a list of tool names that were used within the scratchpad.\"\"\"\n",
    "    return {\"answer\": answer, \"tool_used\": tools_used}\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"You're a helpful assistant. when answering a user's question you should first use one of the tool provided. After using a tool the tool output will be provided in the scratchpad below. If you have an answer in the scratchpad you should use only the 'final_answer' tool and with that answer directly to the user. \"\n",
    "            ),\n",
    "            # (\n",
    "            #     \"You're a helpful assistant. when answering a user's question you should first use one of the tool provided. After using a tool the tool output will be provided in the scratchpad below. If you have an answer in the scratchpad you should not use any more tools and instead answer directly to the user. \"\n",
    "            # ),\n",
    "            # (\n",
    "            #     \"You are a calculator agent. \"\n",
    "            #     \"STEP 1: If the scratchpad is empty, use a math tool.these are the math tools [add, subtract, multiply, exponentiate] \"\n",
    "            #     \"STEP 2: After using a tool the tool output will be provided in the scratchpad below. If you have an answer in the scratchpad you should not use any more tools and instead answer directly to the user. you MUST use the 'final_answer' tool. \"\n",
    "            # ),\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),  # user message\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "tools = [final_answer, add, subtract, multiply, exponentiate]\n",
    "\n",
    "\n",
    "# define the agent runnable\n",
    "agent: RunnableSerializable = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", []),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools, tool_choice=\"any\")\n",
    ")\n",
    "\n",
    "tool_call = agent.invoke({\"input\": \"What is 10+10. (add 10 to 10)\", \"chat_history\": []})\n",
    "tool_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "927b558a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'add',\n",
       "  'args': {'x': 10, 'y': 10},\n",
       "  'id': 'call_2bbz7tk8',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006cdcfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tool_call' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# create tool name to function mapping\u001b[39;00m\n\u001b[32m      2\u001b[39m name2tool = {tool.name: tool.func \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools}\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tool_exec_content = name2tool[\u001b[43mtool_call\u001b[49m.tool_calls[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]](\n\u001b[32m      6\u001b[39m     **tool_call.tool_calls[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# feed the answer back into the LLM via the agent_scratchpad placeholder.\u001b[39;00m\n\u001b[32m     10\u001b[39m tool_exec = ToolMessage(\n\u001b[32m     11\u001b[39m     content=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool_call.tool_calls[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tool returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool_exec_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     tool_call_id=tool_call.tool_calls[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     13\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'tool_call' is not defined"
     ]
    }
   ],
   "source": [
    "# create tool name to function mapping\n",
    "name2tool = {tool.name: tool.func for tool in tools}\n",
    "\n",
    "\n",
    "tool_exec_content = name2tool[tool_call.tool_calls[0][\"name\"]](\n",
    "    **tool_call.tool_calls[0][\"args\"]\n",
    ")\n",
    "\n",
    "# feed the answer back into the LLM via the agent_scratchpad placeholder.\n",
    "tool_exec = ToolMessage(\n",
    "    content=f\"The {tool_call.tool_calls[0]['name']} tool returned {tool_exec_content}\",\n",
    "    tool_call_id=tool_call.tool_calls[0][\"id\"],\n",
    ")\n",
    "\n",
    "\n",
    "out = agent.invoke(\n",
    "    {\n",
    "        \"input\": \"What is 10+10\",\n",
    "        \"chat_history\": [],\n",
    "        \"agent_scratchpad\": [tool_call, tool_exec],\n",
    "    }\n",
    ")\n",
    "\n",
    "out\n",
    "#! small llm failed to find the `final_answer` tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1122528b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'final_answer',\n",
       "  'args': {'answer': '20', 'tools_used': ['add']},\n",
       "  'id': 'call_lu47qoig',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b8a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(out.tool_calls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d898de1",
   "metadata": {},
   "source": [
    "`video time: 2.56.42`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aede206b",
   "metadata": {},
   "source": [
    "### Building a Custom Agent Execution Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07287958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.base import RunnableSerializable\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\"\n",
    "# MODEL = \"orieg/gemma3-tools:1b\"\n",
    "# MODEL = \"qwen3:0.6b\"\n",
    "# MODEL = \"qwen3:1.7b\"\n",
    "MODEL = \"qwen3:4b\"\n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL, temperature=0.0)\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the sum of two numbers, 'x' and 'y'.\n",
    "\n",
    "    It accepts two floating-point numbers, 'x' and 'y', and returns their sum.\n",
    "\n",
    "    This function is designed to be a fundamental arithmetic operation.\n",
    "\n",
    "    Args:\n",
    "        x (float): The first number to be added.\n",
    "        y (float): The second number to be added.\n",
    "\n",
    "    Returns:\n",
    "        float: The sum of 'x' and 'y'.\n",
    "\n",
    "    \"\"\"\n",
    "    return x + y\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"\n",
    "    Multiplies two numbers, 'x' and 'y'.\n",
    "\n",
    "    This function calculates the product of two numbers.  The product is the result of multiplying x by y.\n",
    "\n",
    "    Args:\n",
    "        x (float): The first number.\n",
    "        y (float): The second number.\n",
    "\n",
    "    Returns:\n",
    "        float: The product of x and y.\n",
    "    \"\"\"\n",
    "    return x * y\n",
    "\n",
    "\n",
    "@tool\n",
    "def exponentiate(x: float, y: float) -> float:\n",
    "    \"\"\"\n",
    "    Raise 'x' to the power of 'y' (x ** y).\n",
    "\n",
    "    This function calculates x raised to the power of y.  It represents exponential growth.\n",
    "\n",
    "    Args:\n",
    "        x (float): The base number.\n",
    "        y (float): The exponent.\n",
    "\n",
    "    Returns:\n",
    "        float: The result of x raised to the power of y.\n",
    "    \"\"\"\n",
    "    return x**y\n",
    "\n",
    "\n",
    "@tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    \"\"\"Subtracts 'x' from 'y' or 'x' minus 'y'.\n",
    "\n",
    "    This function performs a numerical subtraction operation.  It returns the result of subtracting 'x' from 'y'.\n",
    "\n",
    "    Args:\n",
    "        x (float): The number to be subtracted.\n",
    "        y (float): The number to be subtracted.\n",
    "\n",
    "    Returns:\n",
    "        float: The result of subtracting 'x' from 'y'.\n",
    "    \"\"\"\n",
    "    return y - x\n",
    "\n",
    "\n",
    "@tool\n",
    "def final_answer(answer: str, tools_used: list[list]) -> str:\n",
    "    \"\"\"Use this tool to provide a final answer to the user. The answer should be in natural language as this will be provided to the user directly. The tools_used must include a list of tool names that were used within the scratchpad.\"\"\"\n",
    "    return {\"answer\": answer, \"tool_used\": tools_used}\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"You're a helpful assistant. when answering a user's question you should first use one of the tool provided. After using a tool the tool output will be provided in the scratchpad below. If you have an answer in the scratchpad you should use only the 'final_answer' tool and with that answer directly to the user. \"\n",
    "            ),\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),  # user message\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "tools = [final_answer, add, subtract, multiply, exponentiate]\n",
    "name2tool = {tool.name: tool.func for tool in tools}\n",
    "\n",
    "class CustomAgentExecutor:\n",
    "    chat_history: list[BaseMessage]\n",
    "\n",
    "    def __init__(self, max_iterations: int = 3):\n",
    "        self.chat_history = []\n",
    "        self.max_iterations = max_iterations\n",
    "        self.agent: RunnableSerializable = (\n",
    "            {\n",
    "                \"input\": lambda x: x[\"input\"],\n",
    "                \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "                \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", []),\n",
    "            }\n",
    "            | prompt\n",
    "            | llm.bind_tools(tools, tool_choice=\"any\")  # we're forcing tool use again\n",
    "        )\n",
    "\n",
    "    def invoke(self, input: str) -> dict:\n",
    "        # invoke the agent but we do this iteratively in a loop until\n",
    "        # reaching a final answer\n",
    "        count = 0\n",
    "        agent_scratchpad = []\n",
    "        while count < self.max_iterations:\n",
    "            # invoke a step for the agent to generate a tool call\n",
    "            tool_call = self.agent.invoke(\n",
    "                {\n",
    "                    \"input\": input,\n",
    "                    \"chat_history\": self.chat_history,\n",
    "                    \"agent_scratchpad\": agent_scratchpad,\n",
    "                }\n",
    "            )\n",
    "            # add initial tool call to scratchpad\n",
    "            agent_scratchpad.append(tool_call)\n",
    "            # otherwise we execute the tool and add it's output to the agent scratchpad\n",
    "            tool_name = tool_call.tool_calls[0][\"name\"]\n",
    "            tool_args = tool_call.tool_calls[0][\"args\"]\n",
    "            tool_call_id = tool_call.tool_calls[0][\"id\"]\n",
    "            tool_out = name2tool[tool_name](**tool_args)\n",
    "            # add the tool output to the agent scratchpad\n",
    "            tool_exec = ToolMessage(\n",
    "                content=f\"{tool_out}\", \n",
    "                tool_call_id=tool_call_id\n",
    "            )\n",
    "            agent_scratchpad.append(tool_exec)\n",
    "            # add a print so we can see intermediate steps\n",
    "            print(f\"{count=}: {tool_name=}({tool_args=})\")\n",
    "            count += 1\n",
    "            # if the tool call is the final answer tool, we stop\n",
    "            if tool_name == \"final_answer\":\n",
    "                break\n",
    "        # add the final output to the chat history\n",
    "        final_answer = tool_out[\"answer\"]\n",
    "        self.chat_history.extend(\n",
    "            [HumanMessage(content=input), AIMessage(content=final_answer)]\n",
    "        )\n",
    "        # return the final answer in dict form\n",
    "        return json.dumps(tool_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c3e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = CustomAgentExecutor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "843393ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count=0: tool_name='add'(tool_args={'x': 10, 'y': 10})\n",
      "count=1: tool_name='final_answer'(tool_args={'answer': '20', 'tools_used': ['add']})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"answer\": \"20\", \"tool_used\": [\"add\"]}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(input=\"What is 10 + 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08d59f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "629b7a09",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f0956b",
   "metadata": {},
   "source": [
    "## LangChains Expression Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2886bf3",
   "metadata": {},
   "source": [
    "### Traditional Chains vs LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2fe0b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04033bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\"\n",
    "# MODEL = \"orieg/gemma3-tools:1b\"\n",
    "# MODEL = \"qwen3:0.6b\"\n",
    "# MODEL = \"qwen3:1.7b\"\n",
    "MODEL = \"qwen3:4b\"\n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL, temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13a310d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic import PromptTemplate\n",
    "\n",
    "prompt_template = \"Give me a small report on {topic}\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variable=[\"topic\"], \n",
    "    template=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9447f340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! 👋 How can I assist you today? 😊', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 360, 'prompt_tokens': 12, 'total_tokens': 372, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'qwen3:4b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-862', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b7880-1cc6-73e0-8265-8b1303e8e2d5-0', usage_metadata={'input_tokens': 12, 'output_tokens': 360, 'total_tokens': 372, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_out = llm.invoke(\"Hello there\")\n",
    "llm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6256e1",
   "metadata": {},
   "source": [
    "Then we define our output parser, this will be used to parse the output of the LLM. In this case, we will use the StrOutputParser which will parse the AIMessage output from our LLM into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e6819ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.schema.output_parser import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd174422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! 👋 How can I assist you today? 😊'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = output_parser.invoke(llm_out)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a22288",
   "metadata": {},
   "source": [
    "Through the LLMChain class we can place each of our components into a linear chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a52b7298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkmpp\\AppData\\Local\\Temp\\ipykernel_23868\\1812892885.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use `RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86e67d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'retrieval augmented generation.',\n",
       " 'text': '## Small Report: Retrieval-Augmented Generation (RAG)\\n\\n**What is RAG?**  \\nRetrieval-Augmented Generation (RAG) is an AI technique that combines **retrieval of relevant external information** with **generative language models** (like LLMs) to produce accurate, context-aware responses. Unlike traditional LLMs that rely solely on their training data, RAG dynamically pulls information from *real-time or domain-specific sources* (e.g., databases, documents) before generating answers. This bridges the gap between broad knowledge and precise, up-to-date information.\\n\\n**How It Works (Simplified):**  \\n1. **Retrieval**: A query is converted into a vector (using an embedding model), then matched against a database of relevant documents to fetch the most pertinent context.  \\n2. **Generation**: The LLM uses the retrieved context to generate a response—*without* relying on its pre-trained knowledge.  \\n\\n**Key Benefits**:  \\n- ✅ **Up-to-date accuracy**: Answers reflect current data (e.g., recent documents), avoiding outdated training knowledge.  \\n- ✅ **Domain specificity**: Tailored to niche topics (e.g., medical records, company knowledge bases).  \\n- ✅ **Reduced hallucination**: Grounded responses by anchoring in external sources.  \\n\\n**Real-World Example**:  \\nA customer support chatbot uses RAG to retrieve the latest product manuals from a company’s internal database before answering user queries—ensuring responses are precise and current.  \\n\\n**Challenges**:  \\n- Speed of retrieval (large datasets can slow responses).  \\n- Ensuring retrieved context is *relevant* and *high-quality*.  \\n- Maintaining security when handling sensitive data.  \\n\\n**Why It Matters**:  \\nRAG addresses a critical limitation of pure LLMs: **knowledge decay**. By integrating live data, it makes AI systems more reliable for practical applications—from enterprise tools to personalized assistants—without retraining models from scratch. It’s a key trend in responsible, real-world AI deployment.  \\n\\n*Word count: 240*'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke(\"retrieval augmented generation.\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c52500c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Small Report: Retrieval-Augmented Generation (RAG)\n",
       "\n",
       "**What is RAG?**  \n",
       "Retrieval-Augmented Generation (RAG) is an AI technique that combines **retrieval of relevant external information** with **generative language models** (like LLMs) to produce accurate, context-aware responses. Unlike traditional LLMs that rely solely on their training data, RAG dynamically pulls information from *real-time or domain-specific sources* (e.g., databases, documents) before generating answers. This bridges the gap between broad knowledge and precise, up-to-date information.\n",
       "\n",
       "**How It Works (Simplified):**  \n",
       "1. **Retrieval**: A query is converted into a vector (using an embedding model), then matched against a database of relevant documents to fetch the most pertinent context.  \n",
       "2. **Generation**: The LLM uses the retrieved context to generate a response—*without* relying on its pre-trained knowledge.  \n",
       "\n",
       "**Key Benefits**:  \n",
       "- ✅ **Up-to-date accuracy**: Answers reflect current data (e.g., recent documents), avoiding outdated training knowledge.  \n",
       "- ✅ **Domain specificity**: Tailored to niche topics (e.g., medical records, company knowledge bases).  \n",
       "- ✅ **Reduced hallucination**: Grounded responses by anchoring in external sources.  \n",
       "\n",
       "**Real-World Example**:  \n",
       "A customer support chatbot uses RAG to retrieve the latest product manuals from a company’s internal database before answering user queries—ensuring responses are precise and current.  \n",
       "\n",
       "**Challenges**:  \n",
       "- Speed of retrieval (large datasets can slow responses).  \n",
       "- Ensuring retrieved context is *relevant* and *high-quality*.  \n",
       "- Maintaining security when handling sensitive data.  \n",
       "\n",
       "**Why It Matters**:  \n",
       "RAG addresses a critical limitation of pure LLMs: **knowledge decay**. By integrating live data, it makes AI systems more reliable for practical applications—from enterprise tools to personalized assistants—without retraining models from scratch. It’s a key trend in responsible, real-world AI deployment.  \n",
       "\n",
       "*Word count: 240*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(result[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e28961",
   "metadata": {},
   "source": [
    "### LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ef4944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.schema.output_parser import StrOutputParser\n",
    "from langchain_classic import PromptTemplate\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\"\n",
    "# MODEL = \"orieg/gemma3-tools:1b\"\n",
    "# MODEL = \"qwen3:0.6b\"\n",
    "# MODEL = \"qwen3:1.7b\"\n",
    "MODEL = \"qwen3:4b\"\n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL, temperature=0.0)\n",
    "\n",
    "\n",
    "prompt_template = \"Give me a small report on {topic}\"\n",
    "prompt = PromptTemplate(input_variable=[\"topic\"], template=prompt_template)\n",
    "\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24baf059",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcel_chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77247c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Small Report: Retrieval-Augmented Generation (RAG)\\n\\n**What it is:**  \\nRetrieval-Augmented Generation (RAG) is a **technique that enhances large language models (LLMs)** by dynamically retrieving *relevant, up-to-date information* from external knowledge sources (e.g., documents, databases) *before* generating responses. It bridges the gap between LLMs (which can hallucinate) and real-world accuracy.\\n\\n**How it works (simplified):**  \\n1. **User Query**: User asks a question (e.g., \"What’s the latest climate policy in EU?\").  \\n2. **Retrieval**: The system searches a vector database (e.g., using embeddings) for documents *most relevant* to the query.  \\n3. **Generation**: The LLM *then* uses the retrieved context to generate a precise, factual response—**not** relying solely on its training data.  \\n\\n**Why it matters**:  \\n- ✅ **Reduces hallucinations**: Responses are grounded in current, specific data.  \\n- ✅ **Improves accuracy**: Critical for domains like healthcare, finance, or research where outdated info is risky.  \\n- ✅ **Scales knowledge**: Models stay updated without retraining (e.g., chatbots using live documents).  \\n- ✅ **Real-world use**: Powers enterprise chatbots, research assistants, and personalized content systems.  \\n\\n**Key Challenge**:  \\nBalancing *relevance* and *speed* of retrieval—too many results slow down responses; too few miss critical context.  \\n\\n**In short**: RAG turns LLMs from \"black boxes\" into **accurate, context-aware tools** by injecting real-time knowledge. It’s a key solution for trustworthy AI in practical applications today.  \\n\\n*(Word count: 198)*'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = lcel_chain.invoke(\"retrieval augmented generation\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c2fc126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Small Report: Retrieval-Augmented Generation (RAG)\n",
       "\n",
       "**What it is:**  \n",
       "Retrieval-Augmented Generation (RAG) is a **technique that enhances large language models (LLMs)** by dynamically retrieving *relevant, up-to-date information* from external knowledge sources (e.g., documents, databases) *before* generating responses. It bridges the gap between LLMs (which can hallucinate) and real-world accuracy.\n",
       "\n",
       "**How it works (simplified):**  \n",
       "1. **User Query**: User asks a question (e.g., \"What’s the latest climate policy in EU?\").  \n",
       "2. **Retrieval**: The system searches a vector database (e.g., using embeddings) for documents *most relevant* to the query.  \n",
       "3. **Generation**: The LLM *then* uses the retrieved context to generate a precise, factual response—**not** relying solely on its training data.  \n",
       "\n",
       "**Why it matters**:  \n",
       "- ✅ **Reduces hallucinations**: Responses are grounded in current, specific data.  \n",
       "- ✅ **Improves accuracy**: Critical for domains like healthcare, finance, or research where outdated info is risky.  \n",
       "- ✅ **Scales knowledge**: Models stay updated without retraining (e.g., chatbots using live documents).  \n",
       "- ✅ **Real-world use**: Powers enterprise chatbots, research assistants, and personalized content systems.  \n",
       "\n",
       "**Key Challenge**:  \n",
       "Balancing *relevance* and *speed* of retrieval—too many results slow down responses; too few miss critical context.  \n",
       "\n",
       "**In short**: RAG turns LLMs from \"black boxes\" into **accurate, context-aware tools** by injecting real-time knowledge. It’s a key solution for trustworthy AI in practical applications today.  \n",
       "\n",
       "*(Word count: 198)*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c344df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbe301d4",
   "metadata": {},
   "source": [
    "Let's make a basic class named Runnable that will transform our a provided function into a runnable class that we will then use with the pipe | operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da6afc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runnable:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    def __or__(self, other):\n",
    "        def chained_func(*args, **kwargs):\n",
    "            return other.invoke(self.func(*args, **kwargs))\n",
    "\n",
    "        return Runnable(chained_func)\n",
    "\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaec52a",
   "metadata": {},
   "source": [
    "### pipe operator\n",
    "\n",
    "With the Runnable class, we will be able wrap a function into the class, allowing us to then chain together multiple of these runnable functions using the `__or__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bacbc449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_five(x):\n",
    "    return x + 5\n",
    "\n",
    "\n",
    "def sub_five(x):\n",
    "    return x - 5\n",
    "\n",
    "\n",
    "def mul_five(x):\n",
    "    return x * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea936203",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_five_runnable = Runnable(add_five)\n",
    "sub_five_runnable = Runnable(sub_five)\n",
    "mul_five_runnable = Runnable(mul_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a946d0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (add_five_runnable).__or__(sub_five_runnable).__or__(mul_five_runnable)\n",
    "\n",
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d33cb8",
   "metadata": {},
   "source": [
    "### LCEL RunnableLambda\n",
    "\n",
    "The RunnableLambda class is LangChain's built-in method for constructing a runnable object from a function. That is, it does the same thing as the custom Runnable class we created earlier. Let's try it out with the same functions as before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85a47c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "add_five_runnable = RunnableLambda(add_five)\n",
    "sub_five_runnable = RunnableLambda(sub_five)\n",
    "mul_five_runnable = RunnableLambda(mul_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "098458ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = add_five_runnable | sub_five_runnable | mul_five_runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43403c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d1798",
   "metadata": {},
   "source": [
    "Now we want to try something a little more testing, so this time we will generate a report, and we will try and edit that report using this functionallity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2654efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"give me a small report about {topic}\"\n",
    "prompt = PromptTemplate(input_variables=[\"topic\"], template=prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2aafb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c077624f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## AI: A Brief Overview  \n",
       "\n",
       "**What is AI?**  \n",
       "Artificial Intelligence (AI) refers to systems designed to perform tasks that typically require human intelligence—like learning, problem-solving, recognizing patterns, and making decisions. Modern AI (especially *machine learning* and *deep learning*) enables computers to improve at specific tasks through data, without explicit programming.  \n",
       "\n",
       "**Current Impact & Uses**  \n",
       "AI is now deeply integrated into everyday life:  \n",
       "- **Chatbots & Assistants**: Like ChatGPT, Siri, and Alexa (natural language processing).  \n",
       "- **Healthcare**: Diagnosing diseases, drug discovery, and personalized treatment plans.  \n",
       "- **Industry**: Predictive maintenance, supply chain optimization, and automated quality control.  \n",
       "- **Creativity**: Generating art, music, and code (e.g., DALL-E, GitHub Copilot).  \n",
       "\n",
       "**Why It Matters Now**  \n",
       "AI is accelerating rapidly due to:  \n",
       "- **Accessibility**: Cloud computing and open-source tools make AI affordable for businesses and individuals.  \n",
       "- **Real-World Solutions**: Addressing climate change, healthcare gaps, and inefficiencies in global systems.  \n",
       "- **Economic Shift**: Estimated to add $15.7 trillion to the global economy by 2030 (McKinsey).  \n",
       "\n",
       "**Key Considerations**  \n",
       "While transformative, AI raises ethical concerns:  \n",
       "- **Bias**: AI can perpetuate societal biases if trained on flawed data.  \n",
       "- **Job Displacement**: Automation may affect routine roles (though new opportunities emerge).  \n",
       "- **Privacy**: Data collection for AI models raises security risks.  \n",
       "\n",
       "**In Short**  \n",
       "AI is a powerful, rapidly evolving technology reshaping industries and daily life. Its potential to solve complex global challenges is immense—but responsible development, ethical oversight, and inclusive access are critical to ensure benefits for all.  \n",
       "\n",
       "*(Word count: 198)*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = chain.invoke(\"AI\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f778c90c",
   "metadata": {},
   "source": [
    "Here we are making two functions, extract_fact to pull out the main content of our text and replace_word that will replace AI with Skynet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a946f7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fact(x):\n",
    "    if \"\\n\\n\" in x:\n",
    "        return \"\\n\".join(x.split(\"\\n\\n\")[1:])\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "old_word = \"AI\"\n",
    "new_word = \"skynet\"\n",
    "\n",
    "\n",
    "def replace_word(x):\n",
    "    return x.replace(old_word, new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "047df7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_fact_runnable = RunnableLambda(extract_fact)\n",
    "replace_word_runnable = RunnableLambda(replace_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5dea25a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser | extract_fact_runnable | replace_word_runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c11ba80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "### **skynet: A Quick Report**  \n",
       "**What it is**: skynet (Artificial Intelligence) refers to systems that **learn from data**, **solve problems**, and **make decisions**—without explicit programming. Modern skynet focuses on *machine learning* (ML), where algorithms improve through experience (e.g., training on vast datasets).  \n",
       "**Current State**:  \n",
       "- **Dominant tech**: ML models (like ChatGPT, image generators) power everyday tools.  \n",
       "- **Not AGI yet**: We don’t have general skynet (human-like intelligence). Current systems excel at *specific tasks* (e.g., recommendations, translation) but lack true understanding or creativity.  \n",
       "**Key Impacts**:  \n",
       "✅ **Positive**: Faster medical diagnoses, personalized education, climate modeling, efficient logistics.  \n",
       "⚠️ **Challenges**: Bias in data → unfair outcomes; job displacement in routine roles; privacy risks (e.g., data misuse).  \n",
       "**Why it matters now**: skynet is evolving rapidly and **impacts nearly every industry**. Businesses use it for innovation, while governments focus on ethics and regulation (e.g., EU’s skynet Act).  \n",
       "**In 30 words**: *skynet uses data to automate tasks and predict outcomes—already transforming industries but requiring careful ethics to avoid harm. The future is here, but human oversight remains critical.*  \n",
       "---  \n",
       "*This report covers essentials without jargon. For deeper dives: [skynet Ethics](https://www.aisafetyinstitute.org/) or [ML basics](https://ml-cheatsheet.readthedocs.io/).*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = chain.invoke(\"AI\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86377e83",
   "metadata": {},
   "source": [
    "\n",
    "### LCEL RunnableParallel and RunnablePassthrough\n",
    "\n",
    "LCEL provides us with various Runnable classes that allow us to control the flow of data and execution order through our chains. Two of these are RunnableParallel and RunnablePassthrough.\n",
    "\n",
    "    RunnableParallel — allows us to run multiple Runnable instances in parallel. Acting almost as a Y-fork in the chain.\n",
    "\n",
    "    RunnablePassthrough — allows us to pass through a variable to the next Runnable without modification.\n",
    "\n",
    "To see these runnables in action, we will create two data sources, each source provides specific information but to answer the question we will need both to fed to the LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1856d558",
   "metadata": {},
   "source": [
    "`i'm using ollama for this.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaaaae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d60ec450",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"qwen3-embedding:0.6b\" # embedding model\n",
    "BASE_URL = \"http://localhost:11434\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "963ce391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_classic.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "\n",
    "embedding = OllamaEmbeddings(\n",
    "    model=MODEL,\n",
    "    base_url=BASE_URL,\n",
    ")\n",
    "\n",
    "vecstore_a = DocArrayInMemorySearch.from_texts(\n",
    "    [\"half the info is here\", \"DeepSeek-V3 was released in December 2024\"],\n",
    "    embedding=embedding,\n",
    ")\n",
    "vecstore_b = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"the other half of the info is here\",\n",
    "        \"the DeepSeek-V3 LLM is a mixture of experts model with 671B parameters\",\n",
    "    ],\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "913b69cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"Using the context provided, answer the user's question.\n",
    "Context:\n",
    "{context_a}\n",
    "{context_b}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e11cdfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(prompt_str),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cde5908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "retriever_a = vecstore_a.as_retriever()\n",
    "retriever_b = vecstore_b.as_retriever()\n",
    "\n",
    "retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"context_a\": retriever_a,\n",
    "        \"context_b\": retriever_b,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9808b49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.schema.output_parser import StrOutputParser\n",
    "from langchain_classic import PromptTemplate\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\"\n",
    "# MODEL = \"orieg/gemma3-tools:1b\"\n",
    "# MODEL = \"qwen3:0.6b\"\n",
    "# MODEL = \"qwen3:1.7b\"\n",
    "MODEL = \"qwen3:4b\"\n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL, temperature=0.0)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1393e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = retrieval | prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8012c79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, DeepSeek-V3 (released in December 2024) uses a **mixture of experts (MoE) architecture** with 671B parameters. This is explicitly stated in the context: *\"the DeepSeek-V3 LLM is a mixture of experts model with 671B parameters.\"* \\n\\nThe context also notes that \"half the info is here\" and \"the other half of the info is here,\" but the specific architectural detail for DeepSeek-V3 is clearly defined as a mixture of experts model.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke(\n",
    "    \"what architecture does the model DeepSeek released in december use?\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c06138eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided context, DeepSeek-V3 (released in December 2024) uses a **mixture of experts (MoE) architecture** with 671B parameters. This is explicitly stated in the context: *\"the DeepSeek-V3 LLM is a mixture of experts model with 671B parameters.\"* \n",
       "\n",
       "The context also notes that \"half the info is here\" and \"the other half of the info is here,\" but the specific architectural detail for DeepSeek-V3 is clearly defined as a mixture of experts model."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a14c576",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
