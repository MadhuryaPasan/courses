{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50646e10",
   "metadata": {},
   "source": [
    "langchain-core: The base abstractions (like runnable, messages, and standard interfaces).\n",
    "\n",
    "langchain: The chain, agent, and retrieval logic.\n",
    "\n",
    "langchain-community: The massive collection of specific tools, document loaders, vector stores, and chat models provided by the community and third parties (e.g., Wikipedia tools, FAISS vector store, specific PDF loaders)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca165fe",
   "metadata": {},
   "source": [
    "We will be taking an article draft and using LangChain to generate various useful items around this article. We'll be creating:\n",
    "\n",
    "01. An article title\n",
    "02. An article description\n",
    "03. Editor advice where we will insert an additional paragraph in the article\n",
    "04. A thumbnail / hero image for our article.\n",
    "\n",
    "Here we input our article to start with. Currently this is using an article from the Aurelio AI learning page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1c6f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "import textwrap\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\" # still no good\n",
    "MODEL = \"qwen3:0.6b\" # okay\n",
    "# MODEL = \"gemma3:4b\" # okay\n",
    "BASE_URL = \"http://localhost:11434/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af265f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL, temperature=0.0)\n",
    "\n",
    "creative_llm = ChatOpenAI(model=MODEL, base_url=BASE_URL, temperature=0.9)\n",
    "\n",
    "# response = model.invoke(\"Why do parrots talk?\")\n",
    "# response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "352a8450",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"\n",
    "\\\n",
    "We believe AI's shortâ€”to mid-term future belongs to agents and that the long-term future of *AGI* may evolve from agentic systems. Our definition of agents covers any neuro-symbolic system in which we merge neural AI (such as an LLM) with semi-traditional software.\n",
    "\n",
    "With agents, we allow LLMs to integrate with code â€” allowing AI to search the web, perform math, and essentially integrate into anything we can build with code. It should be clear the scope of use cases is phenomenal where AI can integrate with the broader world of software.\n",
    "\n",
    "In this introduction to AI agents, we will cover the essential concepts that make them what they are and why that will make them the core of real-world AI in the years to come.\n",
    "\n",
    "---\n",
    "\n",
    "## Neuro-Symbolic Systems\n",
    "\n",
    "Neuro-symbolic systems consist of both neural and symbolic computation, where:\n",
    "\n",
    "- Neural refers to LLMs, embedding models, or other neural network-based models.\n",
    "- Symbolic refers to logic containing symbolic logic, such as code.\n",
    "\n",
    "Both neural and symbolic AI originate from the early philosophical approaches to AI: connectionism (now neural) and symbolism. Symbolic AI is the more traditional AI. Diehard symbolists believed they could achieve true AGI via written rules, ontologies, and other logical functions.\n",
    "\n",
    "The other camp were the connectionists. Connectionism emerged in 1943 with a theoretical neural circuit but truly kicked off with Rosenblatt's perceptron paper in 1958 [1][2]. Both of these approaches to AI are fascinating but deserve more time than we can give them here, so we will leave further exploration of these concepts for a future chapter.\n",
    "\n",
    "Most important to us is understanding where symbolic logic outperforms neural-based compute and vice-versa.\n",
    "\n",
    "| Neural | Symbolic |\n",
    "| --- | --- |\n",
    "| Flexible, learned logic that can cover a huge range of potential scenarios. | Mostly hand-written rules which can be very granular and fine-tuned but hard to scale. |\n",
    "| Hard to interpret why a neural system does what it does. Very difficult or even impossible to predict behavior. | Rules are written and can be understood. When unsure why a particular ouput was produced we can look at the rules / logic to understand. |\n",
    "| Requires huge amount of data and compute to train state-of-the-art neural models, making it hard to add new abilities or update with new information. | Code is relatively cheap to write, it can be updated with new features easily, and latest information can often be added often instantaneously. |\n",
    "| When trained on broad datasets can often lack performance when exposed to unique scenarios that are not well represented in the training data. | Easily customized to unique scenarios. |\n",
    "| Struggles with complex computations such as mathematical operations. | Perform complex computations very quickly and accurately. |\n",
    "\n",
    "Pure neural architectures struggle with many seemingly simple tasks. For example, an LLM *cannot* provide an accurate answer if we ask it for today's date.\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is commonly used to provide LLMs with up-to-date knowledge on a particular subject or access to proprietary knowledge.\n",
    "\n",
    "### Giving LLMs Superpowers\n",
    "\n",
    "By 2020, it was becoming clear that neural AI systems could not perform tasks symbolic systems typically excelled in, such as arithmetic, accessing structured DB data, or making API calls. These tasks require discrete input parameters that allow us to process them reliably according to strict written logic.\n",
    "\n",
    "In 2022, researchers at AI21 developed Jurassic-X, an LLM-based \"neuro-symbolic architecture.\" Neuro-symbolic refers to merging the \"neural computation\" of large language models (LLMs) with more traditional (i.e. symbolic) computation of code.\n",
    "\n",
    "Jurassic-X used the Modular Reasoning, Knowledge, and Language (MRKL) system [3]. The researchers developed MRKL to solve the limitations of LLMs, namely:\n",
    "\n",
    "- Lack of up-to-date knowledge, whether that is the latest in AI or something as simple as today's date.\n",
    "- Lack of proprietary knowledge, such as internal company docs or your calendar bookings.\n",
    "- Lack of reasoning, i.e. the inability to perform operations that traditional software is good at, like running complex mathematical operations.\n",
    "- Lack of ability to generalize. Back in 2022, most LLMs had to be fine-tuned to perform well in a specific domain. This problem is still present today but far less prominent as the SotA models generalize much better and, in the case of MRKL, are able to use tools relatively well (although we could certainly take the MRKL solution to improve tool use performance even today).\n",
    "\n",
    "MRKL represents one of the earliest forms of what we would now call an agent; it is an LLM (neural computation) paired with executable code (symbolic computation).\n",
    "\n",
    "## ReAct and Tools\n",
    "\n",
    "There is a misconception in the broader industry that an AI agent is an LLM contained within some looping logic that can generate inputs for and execute code functions. This definition of agents originates from the huge popularity of the ReAct agent framework and the adoption of a similar structure with function/tool calling by LLM providers such as OpenAI, Anthropic, and Ollama.\n",
    "\n",
    "![ReAct agent flow with the Reasoning-Action loop [4]. When the action chosen specifies to use a normal tool, the tool is used and the observation returned for another iteration through the Reasoning-Action loop. To return a final answer to the user the LLM must choose action \"answer\" and provide the natural language response, finishing the loop.](/images/posts/ai-agents/ai-agents-00.png)\n",
    "\n",
    "<small>ReAct agent flow with the Reasoning-Action loop [4]. When the action chosen specifies to use a normal tool, the tool is used and the observation returned for another iteration through the Reasoning-Action loop. To return a final answer to the user the LLM must choose action \"answer\" and provide the natural language response, finishing the loop.</small>\n",
    "\n",
    "Our \"neuro-symbolic\" definition is much broader but certainly does include ReAct agents and LLMs paired with tools. This agent type is the most common for now, so it's worth understanding the basic concept behind it.\n",
    "\n",
    "The **Re**ason **Act**ion (ReAct) method encourages LLMs to generate iterative *reasoning* and *action* steps. During *reasoning,* the LLM describes what steps are to be taken to answer the user's query. Then, the LLM generates an *action,* which we parse into an input to some executable code, which we typically describe as a tool/function call.\n",
    "\n",
    "![ReAct method. Each iteration includes a Reasoning step followed by an Action (tool call) step. The Observation is the output from the previous tool call. During the final iteration the agent calls the answer tool, meaning we generate the final answer for the user.](/images/posts/ai-agents/ai-agents-01.png)\n",
    "\n",
    "<small>ReAct method. Each iteration includes a Reasoning step followed by an Action (tool call) step. The Observation is the output from the previous tool call. During the final iteration the agent calls the answer tool, meaning we generate the final answer for the user.</small>\n",
    "\n",
    "Following the reason and action steps, our action tool call returns an observation. The logic returns the observation to the LLM, which is then used to generate subsequent reasoning and action steps.\n",
    "\n",
    "The ReAct loop continues until the LLM has enough information to answer the original input. Once the LLM reaches this state, it calls a special *answer* action with the generated answer for the user.\n",
    "\n",
    "## Not only LLMs and Tool Calls\n",
    "\n",
    "LLMs paired with tool calling are powerful but far from the only approach to building agents. Using the definition of neuro-symbolic, we cover architectures such as:\n",
    "\n",
    "- Multi-agent workflows that involve multiple LLM-tool (or other agent structure) combinations.\n",
    "- More deterministic workflows where we may have set neural model-tool paths that may fork or merge as the use case requires.\n",
    "- Embedding models that can detect user intents and decide tool-use or LLM selection-based selection in vector space.\n",
    "\n",
    "These are just a few high-level examples of alternative agent structures. Far from being designed for niche use cases, we find these alternative options to frequently perform better than the more common ReAct or Tool agents. We will cover all of these examples and more in future chapters.\n",
    "\n",
    "---\n",
    "\n",
    "Agents are fundamental to the future of AI, but that doesn't mean we should expect that future to come from agents in their most popular form today. ReAct and Tool agents are great and handle many simple use cases well, but the scope of agents is much broader, and we believe thinking beyond ReAct and Tools is key to building future AI.\n",
    "\n",
    "---\n",
    "\n",
    "You can sign up for the [Aurelio AI newsletter](https://b0fcw9ec53w.typeform.com/to/w2BDHVK7) to stay updated on future releases in our comprehensive course on agents.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "[1] The curious case of Connectionism (2019) [https://www.degruyter.com/document/doi/10.1515/opphil-2019-0018/html](https://www.degruyter.com/document/doi/10.1515/opphil-2019-0018/html)\n",
    "\n",
    "[2] F. Rosenblatt, [The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf) (1958), Psychological Review\n",
    "\n",
    "[3] E. Karpas et al. [MRKL Systems: A Modular, Neuro-Symbolic Architecture That Combines Large Language Models, External Knowledge Sources and Discrete Reasoning](https://arxiv.org/abs/2205.00445) (2022), AI21 Labs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9284bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI assistant called {name} that helps generate article titles.\",\n",
    "    input_variable=[\"name\"],\n",
    ")\n",
    "\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are tasked with creating a name for a article.\n",
    "The article is here for you to examine:\n",
    "\n",
    "---\n",
    "\n",
    " {article}\n",
    " \n",
    "---\n",
    "The name should be based of the context of the article.\n",
    "Be creative, but make sure the names are clear, catchy,\n",
    "and relevant to the theme of the article.\n",
    "\n",
    "Only output the article name, no other explanation or\n",
    "text can be provided and need it simple and short.\"\"\",\n",
    "    input_variables=[\"article\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3676e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_prompt.format(article=\"TEST STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd42664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview of the user prompt output. Later for the variable `article` we can give a real value.\n",
    "# print(user_prompt.format(article=\"TEST STRING\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1cc339da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ce70286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: You are an AI assistant called TEST NAME that helps generate article titles.\\nHuman: You are tasked with creating a name for a article.\\nThe article is here for you to examine:\\n\\n---\\n\\n TEST STRING\\n\\n---\\nThe name should be based of the context of the article.\\nBe creative, but make sure the names are clear, catchy,\\nand relevant to the theme of the article.\\n\\nOnly output the article name, no other explanation or\\ntext can be provided and need it simple and short.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format `first_prompt` also\n",
    "\n",
    "first_prompt.format(article=\"TEST STRING\", name=\"TEST NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3194c727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an AI assistant called TEST NAME that helps generate article titles.\n",
      "Human: You are tasked with creating a name for a article.\n",
      "The article is here for you to examine:\n",
      "\n",
      "---\n",
      "\n",
      " TEST STRING\n",
      "\n",
      "---\n",
      "The name should be based of the context of the article.\n",
      "Be creative, but make sure the names are clear, catchy,\n",
      "and relevant to the theme of the article.\n",
      "\n",
      "Only output the article name, no other explanation or\n",
      "text can be provided and need it simple and short.\n"
     ]
    }
   ],
   "source": [
    "print(first_prompt.format(article=\"TEST STRING\", name=\"TEST NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f10fc4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article_title': 'Neuro-Symbolic Agents: Bridging the Gap in AI Innovation.'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_one = (\n",
    "    {\n",
    "        \"article\": lambda x: x[\"article\"],\n",
    "        \"name\": lambda x: x[\"name\"],\n",
    "    }\n",
    "    | first_prompt\n",
    "    | creative_llm\n",
    "    | {\"article_title\": lambda x: x.content} # extract the content field of llm output\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# {left side} | {right side} => left is going to go into the right side\n",
    "# {...} go into `first_prompt`, first prompt go into `creative_llm`, creative llm go into {...}\n",
    "\n",
    "article_title_msg = chain_one.invoke({\n",
    "    \"article\" : article,\n",
    "    \"name\" : \"Title Finder\"\n",
    "})\n",
    "\n",
    "article_title_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ea32193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summery': 'Neuro-symbolic agents for the future of AI: explore how combining neural and symbolic systems will shape the next generation of AI. With applications in search, computation, and integration, these agents will redefine the boundaries of real-world AI. The MRKL architecture, ReAct method, and future use cases are highlighted. [1-3]'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI assistant that helps build good articles.\",\n",
    ")\n",
    "\n",
    "second_user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are tasked with creating a description for\n",
    "the article. The article is here for you to examine:\n",
    "\n",
    "---\n",
    "\n",
    "{article}\n",
    "\n",
    "---\n",
    "\n",
    "Here is the article title '{article_title}'.\n",
    "\n",
    "Output the SEO friendly article description. \n",
    "make sure we don't exceed 120 characters.\n",
    "Do not output anything other than the description.\"\"\",\n",
    "    input_variables=[\"article\", \"article_title\"],\n",
    ")\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_messages([system_prompt, second_user_prompt])\n",
    "\n",
    "\n",
    "chain_two = (\n",
    "    {\n",
    "        \"article\": lambda x: x[\"article\"], \n",
    "        \"article_title\": lambda x: x[\"article_title\"]\n",
    "    }\n",
    "    | second_prompt\n",
    "    | llm\n",
    "    | {\"summery\": lambda x: x.content}\n",
    ")\n",
    "\n",
    "article_description_msg = chain_two.invoke (\n",
    "    {\n",
    "        \"article\": article,\n",
    "        \"article_title\": article_title_msg[\"article_title\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "article_description_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5989d39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_paragraph': \"We believe AI's shortâ€”to mid-term future belongs to agents and that the long-term future of *AGI* may evolve from agentic systems. Our definition of agents covers any neuro-symbolic system in which we merge neural AI (such as an LLM) with semi-traditional software.\",\n",
       " 'edited_paragraph': \"We believe AI's short-term to the mid-term future belongs to agents, and that the long-term future of *AGI* may evolve through agentic systems. Our definition of agents includes any neuro-symbolic systems that integrate neural AI (like large language models) with traditional software. These systems enable AI to perform a wide array of tasks, from conducting complex computations to interacting with external knowledge bases.\",\n",
       " 'feedback': \"The paragraph highlights the importance of agents for the future, making it a strong start. You can expand on the term 'short-term' to better emphasize the focus on current and future systems. The definition could be more precise, emphasizing how agents are key to AI's development. Adding details about the role of neuro-symbolic systems would enhance the overall scope.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are tasked with creating a new paragraph for the\n",
    "article. The article is here for you to examine:\n",
    "\n",
    "---\n",
    "\n",
    "{article}\n",
    "\n",
    "---\n",
    "\n",
    "Choose one paragraph to review and edit. During your edit\n",
    "ensure you provide constructive feedback to the user so they\n",
    "can learn where to improve their own writing.\"\"\",\n",
    "    input_variables=[\"article\"]\n",
    ")\n",
    "\n",
    "# prompt template 3: creating a new paragraph for the article\n",
    "third_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_prompt,\n",
    "    third_user_prompt\n",
    "])\n",
    "\n",
    "\n",
    "#* force a Large Language Model (LLM) to output data in a strict, structured format rather than free text.\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Paragraph(BaseModel):\n",
    "    original_paragraph: str = Field(description=\"The original paragraph\")\n",
    "    edited_paragraph: str = Field(description=\"The improved edited paragraph\")\n",
    "    feedback: str = Field(\n",
    "        description = \"Constructive feedback on the original paragraph.\"\n",
    "    )\n",
    "\n",
    "structured_llm = creative_llm.with_structured_output(Paragraph)\n",
    "\n",
    "\n",
    "chain_tree = (\n",
    "    {\"article\": lambda x: x[\"article\"]}\n",
    "    | third_prompt\n",
    "    | structured_llm\n",
    "    | {\n",
    "        \"original_paragraph\": lambda x: x.original_paragraph,\n",
    "        \"edited_paragraph\": lambda x: x.edited_paragraph,\n",
    "        \"feedback\": lambda x: x.feedback\n",
    "    }\n",
    ")\n",
    "\n",
    "out = chain_tree.invoke({\"article\": article})\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1df7a8",
   "metadata": {},
   "source": [
    "## image generation\n",
    "\n",
    "use a model that can generate images (like open ai dall-e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58fd5bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# image_prompt = PromptTemplate(\n",
    "#     input_variables=[\"article\"],\n",
    "#     template=(\n",
    "#         \"Generate a prompt with less then 500 characters to generate an image \"\n",
    "#         \"based on the following article: {article}\"\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "\n",
    "# from skimage import io\n",
    "# import matplotlib.pyplot as plt\n",
    "# from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "# def generate_and_display_image(image_prompt):\n",
    "#     image_url = DallEAPIWrapper(model=\"dall-e-3\").run(image_prompt)\n",
    "#     image_data = io.imread(image_url)\n",
    "\n",
    "#     # And update the display code to:\n",
    "#     plt.imshow(image_data)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# # we wrap this in a RunnableLambda for use with LCEL\n",
    "# image_gen_runnable = RunnableLambda(generate_and_display_image)\n",
    "\n",
    "\n",
    "# # chain 4: inputs: article, article_para / outputs: new_suggestion_article\n",
    "# chain_four = (\n",
    "#     {\"article\": lambda x: x[\"article\"]}\n",
    "#     | image_prompt\n",
    "#     | llm\n",
    "#     | (lambda x: x.content)\n",
    "#     | image_gen_runnable\n",
    "# )\n",
    "\n",
    "\n",
    "# chain_four.invoke({\"article\": article})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b640371c",
   "metadata": {},
   "source": [
    "# Langsmith\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3053ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# must enter API key\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or getpass(\n",
    "    \"Enter LangSmith API Key: \"\n",
    ")\n",
    "\n",
    "# below should not be changed\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# you can change this as preferred\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain_course_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8669e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\" # still no good\n",
    "MODEL = \"qwen3:0.6b\"  # okay\n",
    "# MODEL = \"gemma3:4b\" # okay\n",
    "BASE_URL = \"http://localhost:11434/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b40dd2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today? ðŸ˜Š', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 93, 'prompt_tokens': 11, 'total_tokens': 104, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'qwen3:0.6b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-150', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b5b26-b02c-76f3-8324-fae4de46a2ba-0', usage_metadata={'input_tokens': 11, 'output_tokens': 93, 'total_tokens': 104, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\n",
    "    \"Enter OpenAI API Key: \"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0, model=MODEL, base_url=BASE_URL)\n",
    "\n",
    "llm.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45026fba",
   "metadata": {},
   "source": [
    "### Tracing Non-LangChain Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d926e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "@traceable\n",
    "def generate_random_number():\n",
    "    return random.randint(0, 100)\n",
    "\n",
    "\n",
    "@traceable\n",
    "def generate_string_delay(input_str: str):\n",
    "    number = random.randint(1, 5)\n",
    "    time.sleep(number)\n",
    "    return f\"{input_str} ({number})\"\n",
    "\n",
    "\n",
    "@traceable\n",
    "def random_error():\n",
    "    number = random.randint(0, 1)\n",
    "    if number == 0:\n",
    "        raise ValueError(\"Random error\")\n",
    "    else:\n",
    "        return \"No error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed371c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbdef04ccba424c84a8028e16ac12aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for _ in tqdm(range(10)):\n",
    "    generate_random_number()\n",
    "    generate_string_delay(\"Hello\")\n",
    "    try:\n",
    "        random_error()\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f206749",
   "metadata": {},
   "source": [
    "#### with specific names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db8cb669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "\n",
    "@traceable(name=\"Chitchat Maker\")\n",
    "def error_generation_function(question: str):\n",
    "    delay = random.randint(0, 3)\n",
    "    time.sleep(delay)\n",
    "    number = random.randint(0, 1)\n",
    "    if number == 0:\n",
    "        raise ValueError(\"Random error\")\n",
    "    else:\n",
    "        return \"I'm great how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b9d9f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc5f80e89224e3382b186a201a16a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in tqdm(range(5)):\n",
    "    try:\n",
    "        error_generation_function(\"How are you today?\")\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ec69b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d691369",
   "metadata": {},
   "source": [
    "# Prompting technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee67ac",
   "metadata": {},
   "source": [
    "We'll start by looking at the various parts of our prompt. For RAG use-cases we'll typically have three core components however this is _very_ use-cases dependant and can vary significantly. Nonetheless, for RAG we will typically see:\n",
    "\n",
    "* **Rules for our LLM**: this part of the prompt sets up the behavior of our LLM, how it should approach responding to user queries, and simply providing as much information as possible about what we're wanting to do as possible. We typically place this within the _system prompt_ of an chat LLM.\n",
    "\n",
    "* **Context**: this part is RAG-specific. The context refers to some _external information_ that we may have retrieved from a web search, database query, or often a _vector database_. This external information is the **R**etrieval **A**ugmentation part of **RA**G. For chat LLMs we'll typically place this inside the chat messages between the assistant and user.\n",
    "\n",
    "* **Question**: this is the input from our user. In the vast majority of cases the question/query/user input will always be provided to the LLM (and typically through a _user message_). However, the format and location of this being provided often changes.\n",
    "\n",
    "* **Answer**: this is the answer from our assistant, again this is _very_ typical and we'd expect this with every use-case.\n",
    "\n",
    "The below is an example of how a RAG prompt may look:\n",
    "\n",
    "```\n",
    "Answer the question based on the context below,                 }\n",
    "if you cannot answer the question using the                     }--->  (Rules) For Our Prompt\n",
    "provided information answer with \"I don't know\"                 }\n",
    "\n",
    "Context: Aurelio AI is an AI development studio                 }\n",
    "focused on the fields of Natural Language Processing (NLP)      }\n",
    "and information retrieval using modern tooling                  }--->   Context AI has\n",
    "such as Large Language Models (LLMs),                           }\n",
    "vector databases, and LangChain.                                }\n",
    "\n",
    "Question: Does Aurelio AI do anything related to LangChain?     }--->   User Question\n",
    "\n",
    "Answer:                                                         }--->   AI Answer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bdf107c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "# MODEL = \"functiongemma:270m\" # still no good\n",
    "MODEL = \"qwen3:0.6b\"  # okay\n",
    "# MODEL = \"gemma3:4b\" # okay\n",
    "BASE_URL = \"http://localhost:11434/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a90bb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = \"\"\"\n",
    "Answer the user's query based on the context below.\n",
    "If you cannot answer the question using the\n",
    "provided information answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# from langchain.prompts import (\n",
    "#     SystemMessagePromptTemplate,\n",
    "#     HumanMessagePromptTemplate\n",
    "# )\n",
    "\n",
    "# prompt_template = ChatPromptTemplate.from_messages([\n",
    "#     SystemMessagePromptTemplate.from_template(prompt),\n",
    "#     HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "# ])\n",
    "\n",
    "# short way\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", prompt),\n",
    "        (\"user\", \"{query}\"), # now langchain can automatically detects inputs. so no need to specify input_variables.\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e4de7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['context', 'query']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86008903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: {context}\\n'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfedbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=MODEL, base_url=BASE_URL, temperature=0.0)\n",
    "\n",
    "# pipeline = (\n",
    "#     {\n",
    "#         \"query\": lambda x: x[\"query\"], # to these can add (x[\"query\"].lower())\n",
    "#         \"context\": lambda x: x[\"context\"]\n",
    "#     }\n",
    "#     | prompt_template\n",
    "#     | llm\n",
    "# )\n",
    "\n",
    "pipeline = prompt_template | llm #can use like this also\n",
    "\n",
    "context = \"\"\"Aurelio AI is an AI company developing tooling for AI\n",
    "engineers. Their focus is on language AI with the team having strong\n",
    "expertise in building AI agents and a strong background in\n",
    "information retrieval.\n",
    "\n",
    "The company is behind several open source frameworks, most notably\n",
    "Semantic Router and Semantic Chunkers. They also have an AI\n",
    "Platform providing engineers with tooling to help them build with\n",
    "AI. Finally, the team also provides development services to other\n",
    "organizations to help them bring their AI tech to market.\n",
    "\n",
    "Aurelio AI became LangChain Experts in September 2024 after a long\n",
    "track record of delivering AI solutions built with the LangChain\n",
    "ecosystem.\"\"\"\n",
    "\n",
    "query = \"what does Aurelio AI do?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b085423a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: \\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: ___CONTEXT___\\n\\nHuman: ___QUERY___'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.format( query = \"___QUERY___\", context = \"___CONTEXT___\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc8eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aurelio AI develops tooling for AI engineers, focusing on language AI with expertise in building AI agents and information retrieval. They offer open-source frameworks like Semantic Router and Semantic Chunkers, an AI Platform for engineers to build AI tools, and development services to other organizations. Additionally, they became LangChain Experts in September 2024 after a long track record of delivering AI solutions built with the LangChain ecosystem.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = pipeline.invoke({\"query\": query, \"context\": context})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f829c63e",
   "metadata": {},
   "source": [
    "`video time : 1:12:40`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d846d7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
