{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c0c410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install -qU langchain-community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff0dbf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"D:\\Library\\Sliit\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf\"\n",
    "loader = PyPDFLoader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09b87999",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35003af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 0, 'page_label': 'C1'}, page_content=''),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 1, 'page_label': 'C2'}, page_content=''),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 2, 'page_label': 'i'}, page_content='Data Smart\\nJohn W. Foreman\\nUsing Data Science to \\nTransform Information \\ninto Insight'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 3, 'page_label': 'ii'}, page_content='Data Smart: Using Data Science to Transform Information into Insight\\nPublished by\\nJohn Wiley & Sons, Inc.\\n10475 Crosspoint Boulevard\\nIndianapolis, IN 46256\\nwww . wi l ey . com\\nCopyright © 2014 by John Wiley & Sons, Inc., Indianapolis, Indiana\\nPublished simultaneously in Canada\\nISBN: 978-1-118-66146-8\\nISBN: 978-1-118-66148-2 (ebk)\\nISBN: 978-1-118-83986-7 (ebk)\\nManufactured in the United States of America\\n10 9 8 7 6 5 4 3 2 1\\nNo part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or \\nby any means, electronic, mechanical, photocopying, recording, scanning or otherwise, except as permit-\\nted under Sections 107 or 108 of the 1976 United States Copyright Act, without either the prior written \\npermission of the Publisher, or authorization through payment of the appropriate per-copy fee to the \\nCopyright Clearance Center, 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax (978) 646-\\n8600. Requests to the Publisher for permission should be addressed to the Permissions Department, John \\nWiley & Sons, Inc., 111 River Street, Hoboken, NJ  07030, (201) 748-6011, fax (201) 748-6008, or online \\nat \\nhttp://www.wiley.com/go/permissions.\\nLimit of Liability/Disclaimer of Warranty:  The publisher and the author make no representations or war-\\nranties with respect to the accuracy or completeness of the contents of this work and speciﬁ  cally disclaim all \\nwarranties, including without limitation warranties of ﬁ  tness for a particular purpose. No warranty may be \\ncreated or extended by sales or promotional materials. The advice and strategies contained herein may not \\nbe suitable for every situation. This work is sold with the understanding that the publisher is not engaged in \\nrendering legal, accounting, or other professional services. If professional assistance is required, the services \\nof a competent professional person should be sought. Neither the publisher nor the author shall be liable for \\ndamages arising herefrom. The fact that an organization or Web site is referred to in this work as a citation \\nand/or a potential source of further information does not mean that the author or the publisher endorses \\nthe information the organization or website may provide or recommendations it may make. Further, readers \\nshould be aware that Internet websites listed in this work may have changed or disappeared between when \\nthis work was written and when it is read.\\nFor general information on our other products and services please contact our Customer Care \\nDepartment within the United States at (877) 762-2974, outside the United States at (317) 572-3993 or fax \\n(317) 572-4002.\\nWiley publishes in a variety of print and electronic formats and by print-on-demand. Some material \\nincluded with standard print versions of this book may not be included in e-books or in print-on-demand. \\nIf this book refers to media such as a CD or DVD that is not included in the version you purchased, you \\nmay download this material at \\nhttp://booksupport.wiley.com. For more information about Wiley \\nproducts, visit www.wiley.com.\\nLibrary of Congress Control Number:  2013946768\\nTrademarks: Wiley and the Wiley logo are trademarks or registered trademarks of John Wiley & Sons, Inc. \\nand/or its affi  liates, in the United States and other countries, and may not be used without written permission. \\nAll other trademarks are the property of their respective owners. John Wiley & Sons, Inc. is not associated \\nwith any product or vendor mentioned in this book.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 4, 'page_label': 'iii'}, page_content='To my wife, Lydia. What you do each day is impossibly rad. If it weren’t for you, \\nI’d have lost my hair (and my mind) eons ago.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 5, 'page_label': 'iv'}, page_content='Executive Editor\\nCarol Long\\nSenior Project Editor\\nKevin Kent\\nTechnical Editors\\nGreg Jennings \\nEvan Miller \\nProduction Editor\\nChristine Mugnolo\\nCopy Editor\\nKezia Endsley \\nEditorial Manager\\nMary Beth Wakeﬁ eld \\nFreelancer Editorial Manager\\nRosemarie Graham\\nAssociate Director of Marketing\\nDavid Mayhew\\nMarketing Manager\\nAshley Zurcher\\nBusiness Manager\\nAmy Knies\\nVice President and Executive Group \\nPublisher\\nRichard Swadley\\nAssociate Publisher\\nJim Minatel\\nProject Coordinator, Cover\\nKatie Crocker\\nProofreader\\nNancy Carrasco\\nIndexer\\nJohnna van Hoose Dinse\\nCover Image\\nCourtesy of John W. Foreman\\nCover Designer\\nRyan Sneed\\nCredits'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 6, 'page_label': 'v'}, page_content='John W. Foreman  is the Chief Data Scientist for MailChimp.com. He’s also a \\nrecovering management consultant who’s done a lot of analytics work for large \\nbusinesses (Coca-Cola, Royal Caribbean, Intercontinental Hotels) and the gov-\\nernment (DoD, IRS, DHS, FBI). John can often be found speaking about the trials \\nand travails of implementing analytic solutions in business—check \\nJohn-Foreman\\n.com to see if he’s headed to your town.\\nWhen he’s not playing with data, John spends his time hiking, watching copious \\namounts of television, eating all sorts of terrible food, and raising three smelly boys.\\nAbout the Author\\nGreg Jennings is a data scientist, software engineer, and co-founder of ApexVis. After \\ncompleting a master’s degree in materials science from the University of Virginia, he \\nbegan his career with the Analytics group of Booz Allen Hamilton, where he grew \\na team providing predictive analytics and data visualization solutions for planning \\nand scheduling problems.\\nAfter leaving Booz Allen Hamilton, Greg cofounded his ﬁ  rst startup, Decision \\nForge, where he served as CTO and helped develop a web-based data mining plat-\\nform for a government client. He also worked with a major media organization to \\ndevelop an educational product that assists teachers in accessing targeted content for \\ntheir students, and with a McLean-based startup to help develop audience modeling \\napplications to optimize web advertising campaigns.\\nAfter leaving Decision Forge, he cofounded his current business ApexVis, focused \\non helping enterprises get maximum value from their data through custom data \\nvisualization and analytical software solutions. He lives in Alexandria, Virginia, \\nwith his wife and two daughters.\\nEvan Miller  received his bachelor’s degree in physics from Williams College in \\n2006 and is currently a PhD student in economics at the University of Chicago. \\nHis research interests include speciﬁ cation testing and computational methods in \\neconometrics. Evan is also the author of Wizard, a popular Mac program for per-\\nforming statistical analysis, and blogs about statistics problems and experiment \\ndesign at \\nhttp://www.evanmiller.org.\\nAbout the Technical Editors'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 7, 'page_label': 'vi'}, page_content='T\\nhis book started after an improbable number of folks checked out my analytics \\nblog, Analytics Made Skeezy. So I’d like to thank those readers as well as my \\ndata science Twitter pals who’ve been so supportive. And thanks to Aarron Walter, \\nChris Mills, and Jon Duckett for passing the idea for this book on to Wiley based \\non my blog’s silly premise.\\nI’d also like to thank the crew at MailChimp for making this happen. Without \\nthe supportive and adventurous culture fostered at MailChimp, I’d not have felt \\nconﬁ dent enough to do something so stupid as to write a technical book while \\nworking a job and raising three boys. Speciﬁ  cally, I couldn’t have done it without \\nthe daily assistance of Neil Bainton and Michelle Riggin-Ransom. Also, I’m indebted \\nto Ron Lewis, Josh Rosenbaum, and Jason Travis for their work on the cover and \\nmarketing video for the book.\\nThanks to Carol Long at Wiley for taking a chance on me and to all the editors \\nfor their expertise and hard work. Big thanks to Greg Jennings for working all the \\nspreadsheets!\\nMany thanks to my parents for reading my sci-ﬁ  novel and not telling me to quit \\nwriting.\\nAcknowledgments'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 8, 'page_label': 'vii'}, page_content='Contents\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii\\n 1 Everything You Ever Needed to Know about Spreadsheets but Were \\nToo Afraid to Ask  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1\\nSome Sample Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .2\\nMoving Quickly with the Control Button . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .2\\nCopying Formulas and Data Quickly . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4\\nFormatting Cells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .5\\nPaste Special Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7\\nInserting Charts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8\\nLocating the Find and Replace Menus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . .9\\nFormulas for Locating and Pulling Values  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .10\\nUsing VLOOKUP to Merge Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\nFiltering and Sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\nUsing PivotTables  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\nUsing Array Formulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\nSolving Stuff with Solver  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\nOpenSolver: I Wish We Didn’t Need This, but We Do . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\nWrapping Up  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .27\\n 2 Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  . . . . . . . . 29\\nGirls Dance with Girls, Boys Scratch Their Elbows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\nGetting Real: K-Means Clustering Subscribers in E-mail Marketing . . . . . . . . . . . . . . . . . . . . . . .35\\nJoey Bag O’ Donuts Wholesale Wine Emporium  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .36\\nThe Initial Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .36\\nDetermining What to Measure  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .38\\nStart with Four Clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\nEuclidean Distance: Measuring Distances as the Crow Flies . . . . . . . . . . . . . . . . . . . . . . . . . 41\\nDistances and Cluster Assignments for Everybody! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\nSolving for the Cluster Centers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\nMaking Sense of the Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 9, 'page_label': 'viii'}, page_content='Contents viii\\nGetting the Top Deals by Cluster . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\\nThe Silhouette: A Good Way to Let Different K Values \\nDuke It Out . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .53\\nHow about Five Clusters?  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\nSolving for Five Clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\nGetting the Top Deals for All Five Clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\\nComputing the Silhouette for 5-Means Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\\nK-Medians Clustering and Asymmetric Distance Measurements . . . . . . . . . . . . . . . . . . . . . . . . 66\\nUsing K-Medians Clustering  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\\nGetting a More Appropriate Distance Metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .67\\nPutting It All in Excel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\\nThe Top Deals for the 5-Medians Clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\\nWrapping Up  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .75\\n 3 Naïve Bayes and the Incredible Lightness of Being an Idiot . . . . . . . . . . . . . . . . . . . . 77\\nWhen You Name a Product Mandrill, You’re Going to Get Some Signal and \\nSome Noise  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .77\\nThe World’s Fastest Intro to Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .79\\nTotaling Conditional Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\\nJoint Probability, the Chain Rule, and Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\\nWhat Happens in a Dependent Situation? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\\nBayes Rule  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\\nUsing Bayes Rule to Create an AI Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .83\\nHigh-Level Class Probabilities Are Often Assumed to Be Equal . . . . . . . . . . . . . . . . . . . . . 84\\nA Couple More Odds and Ends . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .85\\nLet’s Get This Excel Party Started . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .87\\nRemoving Extraneous Punctuation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .87\\nSplitting on Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . 88\\nCounting Tokens and Calculating Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\\nAnd We Have a Model! Let’s Use It. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\\nWrapping Up  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\\n 4 Optimization Modeling: Because That “Fresh Squeezed” Orange Juice \\nAin’t Gonna Blend Itself . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .10 1\\nWhy Should Data Scientists Know Optimization?  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .102'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 10, 'page_label': 'ix'}, page_content='Contents ix\\nStarting with a Simple Trade-Off . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .103\\nRepresenting the Problem as a Polytope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .103\\nSolving by Sliding the Level Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .105\\nThe Simplex Method: Rooting around the Corners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\\nWorking in Excel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\\nThere’s a Monster at the End of This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\\nFresh from the Grove to Your Glass...with a Pit Stop Through a Blending Model . . . . . . . . . 118\\nYou Use a Blending Model  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\\nLet’s Start with Some Specs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\\nComing Back to Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\\nPutting the Data into Excel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\\nSetting Up the Problem in Solver . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1 2 4\\nLowering Your Standards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .126\\nDead Squirrel Removal: The Minimax Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\\nIf-Then and the “Big M” Constraint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1 3 3\\nMultiplying Variables: Cranking Up the Volume to 11  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\\nModeling Risk  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .144\\nNormally Distributed Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\\nWrapping Up  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .154\\n 5 Cluster Analysis Part II: Network Graphs and Community Detection . . . . . . . . . . .155\\nWhat Is a Network Graph? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .156\\nVisualizing a Simple Graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\\nBrief Introduction to Gephi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\\nGephi Installation and File Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\\nLaying Out the Graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .162\\nNode Degree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .165\\nPretty Printing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .166\\nTouching the Graph Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .168\\nBuilding a Graph from the Wholesale Wine Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .170\\nCreating a Cosine Similarity Matrix  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .172\\nProducing an r-Neighborhood Graph  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\\nHow Much Is an Edge Worth? Points and Penalties in Graph Modularity . . . . . . . . . . . . . . . . 178'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 11, 'page_label': 'x'}, page_content='Contents x\\nWhat’s a Point and What’s a Penalty? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .179\\nSetting Up the Score Sheet  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183\\nLet’s Get Clustering! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .185\\nSplit Number 1  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .185\\nSplit 2: Electric Boogaloo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190\\nAnd…Split 3: Split with a Vengeance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .192\\nEncoding and Analyzing the Communities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193\\nThere and Back Again: A Gephi Tale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .197\\nWrapping Up  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202\\n 6 The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression . . . . . . . . . . . . 205\\nWait, What? You’re Pregnant? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\\nDon’t Kid Yourself . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206\\nPredicting Pregnant Customers at RetailMart Using Linear Regression . . . . . . . . . . . . . . . . . 207\\nThe Feature Set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\\nAssembling the Training Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209\\nCreating Dummy Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .210\\nLet’s Bake Our Own Linear Regression  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213\\nLinear Regression Statistics: R-Squared, F Tests, t Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . .221\\nMaking Predictions on Some New Data and Measuring Performance . . . . . . . . . . . . . . 230\\nPredicting Pregnant Customers at RetailMart Using Logistic Regression . . . . . . . . . . . . . . . . 239\\nFirst You Need a Link Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240\\nHooking Up the Logistic Function and Reoptimizing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\\nBaking an Actual Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 4\\nModel Selection—Comparing the Performance of the Linear \\nand Logistic Regressions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .245\\nFor More Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248\\nWrapping Up  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249\\n 7 Ensemble Models: A Whole Lot of Bad Pizza . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .251\\nUsing the Data from Chapter 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .252\\nBagging: Randomize, Train, Repeat  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254\\nDecision Stump Is an Unsexy Term for a Stupid Predictor  . . . . . . . . . . . . . . . . . . . . . . . . 254\\nDoesn’t Seem So Stupid to Me! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .255\\nYou Need More Power! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .257'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 12, 'page_label': 'xi'}, page_content='Contents xi\\nLet’s Train It . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258\\nEvaluating the Bagged Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267\\nBoosting: If You Get It Wrong, Just Boost and \\nTry Again . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272\\nTraining the Model—Every Feature Gets a Shot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272\\nEvaluating the Boosted Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280\\nWrapping Up  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283\\n 8 Forecasting: Breathe Easy; You Can’t Win . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285\\nThe Sword Trade Is Hopping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286\\nGetting Acquainted with Time Series Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286\\nStarting Slow with Simple Exponential Smoothing  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288\\nSetting Up the Simple Exponential Smoothing Forecast . . . . . . . . . . . . . . . . . . . . . . . . . . 290\\nYou Might Have a Trend . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296\\n Holt’s Trend-Corrected Exponential Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299\\nSetting Up Holt’s Trend-Corrected Smoothing in a Spreadsheet . . . . . . . . . . . . . . . . . . 300\\nSo Are You Done? Looking at Autocorrelations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306\\nMultiplicative Holt-Winters Exponential Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313\\nSetting the Initial Values for Level, Trend, and Seasonality . . . . . . . . . . . . . . . . . . . . . . . . . 315\\nGetting Rolling on the Forecast. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319\\nAnd...Optimize! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .324\\nPlease Tell Me We’re Done Now!!! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .326\\nPutting a Prediction Interval around the Forecast . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .327\\nCreating a Fan Chart for Effect  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331\\nWrapping Up  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .333\\n 9 Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re \\nUnimportant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\\nOutliers Are (Bad?) People, Too . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .335\\nThe Fascinating Case of Hadlum v. Hadlum  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .336\\nTukey Fences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .337\\nApplying Tukey Fences in a Spreadsheet  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .338\\nThe Limitations of This Simple Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340\\nTerrible at Nothing, Bad at Everything . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341\\nPreparing Data for Graphing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .342'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 13, 'page_label': 'xii'}, page_content='Contents xii\\nCreating a Graph  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .345\\nGetting the k Nearest Neighbors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347\\nGraph Outlier Detection Method 1: Just Use the Indegree . . . . . . . . . . . . . . . . . . . . . . . . 348\\nGraph Outlier Detection Method 2: Getting Nuanced with k-Distance . . . . . . . . . . . . . 351\\nGraph Outlier Detection Method 3: Local Outlier Factors Are Where It’s At . . . . . . . .353\\nWrapping Up  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .358\\n 10 Moving from Spreadsheets into R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .361\\nGetting Up and Running with R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . 362\\nSome Simple Hand-Jamming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .363\\nReading Data into R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370\\nDoing Some Actual Data Science . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .372\\nSpherical K-Means on Wine Data in Just a Few Lines  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .372\\nBuilding AI Models on the Pregnancy Data  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .378\\nForecasting in R  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385\\nLooking at Outlier Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389\\nWrapping Up  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394\\n  Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395\\nWhere Am I? What Just Happened? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . .395\\nBefore You Go-Go . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .395\\nGet to Know the Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396\\nWe Need More Translators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .397\\nBeware the Three-Headed Geek-Monster: Tools, Performance, and \\nMathematical Perfection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .397\\nYou Are Not the Most Important Function of Your Organization . . . . . . . . . . . . . . . . . 400\\nGet Creative and Keep in Touch! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400\\n  Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . 401'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 14, 'page_label': 'xiii'}, page_content='What Am I Doing Here?\\nYou’ve probably heard the term data science ﬂ oating around recently in the media, in \\nbusiness books and journals, and at conferences. Data science can call presidential races, \\nreveal more about your buying habits than you’d dare tell your mother, and predict just \\nhow many years those chili cheese burritos have been shaving off  your life. \\nData scientists, the elite practitioners of this art, were even labeled “sexy” in a recent \\nHarvard Business Review article, although there’s apparently such a shortage that it’s kind \\nof like calling a unicorn sexy. There’s just no way to verify the claim, but if you could see \\nme as I type this book with my neck beard and the tired eyes of a parent of three boys, \\nyou’d know that sexy is a bit of an overstatement.\\nI digress. The point is that there’s a buzz about data science these days, and that buzz \\nis creating pressure on a lot of businesses. If you’re not doing data science, you’re gonna \\nlose out to the competition. Someone’s going to come along with some new product called \\nthe “BlahBlahBlahBigDataGraphThing” and destroy your business. \\nTake a deep breath. \\nThe truth is most people are going about data science all wrong. They’re starting with \\nbuying the tools and hiring the consultants. They’re spending all their money before they \\neven know what they want, because a purchase order seems to pass for actual progress \\nin many companies these days.\\nBy reading this book, you’re gonna have a leg up on those jokers, because you’re going \\nto learn exactly what these techniques in data science are and how they’re used. When it \\ncomes time to do the planning, and the hiring, and the buying, you’ll already know how \\nto identify the data science opportunities within your own organization.\\nThe purpose of this book is to introduce you to the practice of data science in a com-\\nfortable and conversational way. When you’re done, I hope that much of that data science \\nanxiety you’re feeling is replaced with excitement and with ideas about how you can use \\ndata to take your business to the next level.\\nIntroduction'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 15, 'page_label': 'xiv'}, page_content='xiv Introduction\\nA Workable Deﬁ nition of Data Science\\nTo an extent, data science is synonymous with or related to terms like business analytics, \\noperations research, business intelligence, competitive intelligence, data analysis and modeling, \\nand knowledge extraction (also called knowledge discovery in databases or KDD). It’s just a \\nnew spin on something that people have been doing for a long time.\\nThere’s been a shift in technology since the heyday of those other terms. Advancements \\nin hardware and software have made it easy and inexpensive to collect, store, and analyze \\nlarge amounts of data whether that be sales and marketing data, HTTP requests from \\nyour website, customer support data, and so on. Small businesses and nonproﬁ  ts can \\nnow engage in the kind of analytics that were previously the purview of large enterprises.\\nOf course, while data science is used as a catch-all buzzword for analytics today, data \\nscience is most often associated with data mining techniques such as artiﬁ cial intelligence, \\nclustering, and outlier detection. Thanks to the cheap technology-enabled proliferation \\nof transactional business data, these computational techniques have gained a foothold in \\nbusiness in recent years where previously they were too cumbersome to use in produc-\\ntion settings.\\nIn this book, I’m going to take a broad view of data science. Here’s the deﬁ nition I’ll \\nwork from: \\nData science is the transformation of data using mathematics and statistics into valuable \\ninsights, decisions, and products. \\nThis is a business-centric deﬁ nition. It’s about a usable and valuable end product derived \\nfrom data. Why? Because I’m not in this for research purposes or because I think data \\nhas aesthetic merit. I do data science to help my organization function better and create \\nvalue; if you’re reading this, I suspect you’re after something similar. \\nWith that deﬁ nition in mind, this book will cover mainstay analytics techniques such \\nas optimization, forecasting, and simulation, as well as more “hot” topics such as artiﬁ cial \\nintelligence, network graphs, clustering, and outlier detection.\\nSome of these techniques are as old as World War II. Others were introduced in the \\nlast 5 years. And you’ll see that age has no bearing on diffi  culty or usefulness. All these \\ntechniques—whether or not they’re currently the rage—are equally useful in the right \\nbusiness context. \\nAnd that’s why you need to understand how they work, how to choose the right tech-\\nnique for the right problem, and how to prototype with them. There are a lot of folks out'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 16, 'page_label': 'xv'}, page_content='xvIntroduction\\nthere who understand one or two of these techniques, but the rest aren’t on their radar. If \\nall I had in my toolbox was a hammer, I’d probably try to solve every problem by smack-\\ning it real hard. Not unlike my two-year-old.\\nBetter to have a few other tools at your disposal.\\nBut Wait, What about Big Data?\\nYou’ve heard the term big data even more than data science most likely. Is this a book on \\nbig data?\\nThat depends on how you deﬁ ne big data. If you deﬁ ne big data as computing simple \\nsummary statistics on unstructured garbage stored in massive, horizontally scalable, \\nNoSQL databases, then no, this is not a book on big data.\\nIf you deﬁ ne big data as turning transactional business data into decisions and insight \\nusing cutting-edge analytics (regardless of where that data is stored), then yes, this is a \\nbook about big data. \\nThis is not a book that will be covering database technologies, like MongoDB and HBase. \\nThis is not a book that will be covering data science coding packages like Mahout, NumPy, \\nvarious R libraries, and so on. There are other books out there for that stuff .\\nBut that’s a good thing. This book ignores the tools, the storage, and the code. Instead, \\nit focuses as much as possible on the techniques. There are many folks out there who \\nthink that data storage and retrieval, with a little bit of cleanup and aggregation mixed \\nin, constitutes all there is to know about big data. \\nThey’re wrong. This book will take you beyond the spiel you’ve been hearing from the \\nbig data software sales reps and bloggers to show you what’s really possible with your data. \\nAnd the cool thing is that for many of these techniques, your dataset can be any size, small \\nor large. You don’t have to have a petabyte of data and the expenses that come along with \\nit in order to predict the interests of your customer base. If you have a massive dataset, \\nthat’s great, but there are some businesses that don’t have it, need it, and will likely never \\ngenerate it. Like my local butcher. But that doesn’t mean his e-mail marketing couldn’t \\nbeneﬁ t from a little bacon versus sausage cluster detection.\\nIf data science books were workouts, this book would be all calisthenics—no machine \\nweights, no ergs. Once you understand how to implement the techniques with even the \\nmost barebones of tools, you’ll ﬁ nd yourself free to implement them in a variety of tech-\\nnologies, prototype with them with ease, buy the correct data science products from \\nconsultants, delegate the correct approach to your developers, and so on.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 17, 'page_label': 'xvi'}, page_content='xvi Introduction\\nWho Am I?\\nLet me pause a moment to tell you my story. It’ll go a long way to explaining why I teach \\ndata science the way I do. Many moons ago, I was a management consultant. I worked \\non analytics problems for organizations such as the FBI, DoD, the Coca-Cola Company, \\nIntercontinental Hotels Group, and Royal Caribbean International. And through all these \\nexperiences I walked away having learned one thing—more people than just the scientists \\nneed to understand data science. \\nI worked with managers who bought simulations when they needed an optimization \\nmodel. I worked with analysts who only understood Gantt charts, so everything needed \\nto be solved with Gantt charts. As a consultant, it wasn’t hard to win over a customer \\nwith any old white paper and a slick PowerPoint deck, because they couldn’t tell AI from \\nBI or BI from BS.\\nThe point of this book is to broaden the audience of who understands and can imple-\\nment data science techniques. I’m not trying to turn you into a data scientist against your \\nwill. I just want you to be able to integrate data science as best as you can into the role \\nyou’re already good at.\\nAnd that brings me to who you are.\\nWho Are You?\\nNo, I haven’t been using data science to spy on you. I have no idea who you are, but thanks \\nfor shelling out some money for this book. Or supporting your local library. You can do \\nthat, too. \\nHere are some archetypes (or personas for you marketing folks) I had in mind when \\nwriting this book. Maybe you are:\\n• The vice president of marketing who wants to use her transactional business data \\nmore strategically to price products and segment customers. But she doesn’t under-\\nstand the approaches her software developers and overpriced consultants are rec-\\nommending she try.\\n• The demand forecasting analyst who knows his organization’s historical purchase \\ndata holds more insight about his customers than just the next quarter’s projections. \\nBut he doesn’t know how to extract that insight.\\n• The CEO of an online retail start-up who wants to predict when a customer is likely \\nto be interested in buying an item based on their past purchases.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 18, 'page_label': 'xvii'}, page_content='xviiIntroduction\\n• The business intelligence analyst who sees money going down the tubes from the \\ninfrastructure and supply chain costs her organization is accruing, but doesn’t know \\nhow to systematically make cost-saving decisions.\\n• The online marketer who wants to do more with his company’s free text customer \\ninteractions taking place in e-mail, Facebook, and Twitter, but right now they’re \\njust being read and saved.\\nI have in mind that you are a reader who would beneﬁ  t directly from knowing more \\nabout data science but hasn’t found a way to get a foothold into all the techniques. The \\npurpose of this book is to strip away all the distractions around data science (the code, \\nthe tools, and the hype) and teach the techniques using practical use cases that someone \\nwith a semester of linear algebra or calculus in college can understand. Assuming you \\ndidn’t fail that semester. If you did, just read slower and use Wikipedia liberally.\\nNo Regrets. Spreadsheets Forever\\nThis is not a book about coding. In fact, I’m giving you my “no code” guarantee (until \\nChapter 10 at least). Why?\\nBecause I don’t want to spend a hundred pages at the beginning of this book messing \\nwith Git, setting environment variables, and doing the dance of Emacs versus Vi. \\nIf you run Windows and Microsoft Offi  ce almost exclusively. If you work for the govern-\\nment, and they don’t let you download and install random open source stuff  on your box. \\nEven if MATLAB or your TI-83 scared the hell out of you in college, you need not be afraid. \\nDo you need to know how to write code to put most of these techniques in automated, \\nproduction settings? Absolutely! Or at least someone you work with needs to be able to \\nhandle code and storage technologies.\\nDo you need to know how to write code in order to understand, distinguish between, \\nand prototype with these techniques? Absolutely not!\\nThis is why I go over every technique in spreadsheet software. \\nNow, this is all a bit of a lie. The ﬁ nal chapter in this book is actually on moving to the \\ndata science-focused programming language, R. It’s for those of you that want to use this \\nbook as a jumping-off  point to deeper things.\\nBut Spreadsheets Are So Démodé!\\nSpreadsheets are not the sexiest tools around. In fact, they’re the Wilford-Brimley-selling-\\nColonial-Penn of the analytics tool world. Completely unsexy. Sorry, Wilford.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 19, 'page_label': 'xviii'}, page_content='xviii Introduction\\nBut that’s the point. Spreadsheets stay out of the way. They allow you to see the data \\nand to touch (or at least click on) the data. There’s a freedom there. In order to learn these \\ntechniques, you need something vanilla, something everyone understands, but nonethe-\\nless, something that will let you move fast and light as you learn. That’s a spreadsheet. \\nSay it with me: “I am a human. I have dignity. I should not have to write a map-reduce \\njob in order to learn data science.”\\nAnd spreadsheets are great for prototyping! You’re not running a production AI model \\nfor your online retail business out of Excel, but that doesn’t mean you can’t look at purchase \\ndata, experiment with features that predict product interest, and prototype a targeting \\nmodel. In fact, it’s the perfect place to do just that.\\nUse Excel or LibreOfﬁ ce\\nAll the examples you’re going to work through will be visualized in the book in Excel. \\nOn the book’s website (www.wiley.com/go/datasmart) are posted companion spread-\\nsheets for each chapter so that you can follow along. If you’re really adventurous, you can \\nclear out all but the starting data in the spreadsheet and replicate all the work yourself.\\nThis book is compatible with Excel versions 2007, 2010, 2011 for Mac, and 2013. Chapter \\n1 will discuss the version diff erences most in depth.\\nMost of you have access to Excel, and you probably already use it for reporting or \\nrecordkeeping at work. But if for some reason you don’t have a copy of Excel, you can \\neither buy it or go for LibreOffi  ce (\\nwww.libreoffice.org) instead. \\nLibreOffi  ce is open source, free, and has nearly all of the same functionality as Excel. \\nI think its native solver is actual preferable to Excel’s. So if you want to go that route for \\nthis book, feel free.\\nWHAT ABOUT GOOGLE DRIVE?\\nNow, some of you might be wondering whether you can use Google Drive. It’s an appeal-\\ning option since Google Drive is in the cloud and can run on your mobile devices as \\nwell as your beige box. But it just won’t work.\\nGoogle Drive is great for simple spreadsheets, but for where you’re going, Google \\njust can’t hang. Adding rows and columns in Drive is a constant annoyance, the imple-\\nmentation of Solver is dreadful, and the charts don’t even have trendlines. I wish it were \\notherwise.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 20, 'page_label': 'xix'}, page_content='xixIntroduction\\nConventions\\nTo help you get the most from the text and keep track of what’s happening, I’ve used a \\nnumber of conventions throughout the book.\\nWARNING\\nWarnings hold important, not-to-be-forgotten information that is directly relevant to \\nthe surrounding text.\\nNOTE\\nNotes cover tips, hints, tricks, or asides to the current discussion.\\nFrequently in this text I’ll reference little snippets of Excel code like this:\\n=CONCATENATE(“THIS IS A FORMULA”, “ IN EXCEL!”)\\nWe highlight new terms and important words when we introduce them. We show ﬁ  le \\nnames, URLs, and formulas within the text like so:\\nhttp://www .john-foreman.com.\\nLet’s Get Going\\nIn the ﬁ rst chapter, I’m going to ﬁ ll in a few holes in your Excel knowledge. After that, \\nyou’ll move right into use cases. By the end of this book, you’ll not only know about but \\nactually have experience implementing from scratch the following techniques:\\n• Optimization using linear and integer programming\\n• Working with time series data, detecting trends and seasonal patterns, and forecast-\\ning with exponential smoothing\\nSIDEBARS\\nSidebars, like the one you just read about Google Drive, touch upon some side issue \\nrelated to the text in detail.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 21, 'page_label': 'xx'}, page_content='xx Introduction\\n• Using Monte Carlo simulation in optimization and forecasting scenarios to quantify \\nand address risk\\n• Artiﬁ cial intelligence using the general linear model, logistic link functions, ensem-\\nble methods, and naïve Bayes\\n• Measuring distances between customers using cosine similarity, creating kNN \\ngraphs, calculating modularity, and clustering customers\\n• Detecting outliers in a single dimension with Tukey fences or in multiple dimen-\\nsions with local outlier factors\\n• Using R packages to “stand on the shoulders” of other analysts in conducting these \\ntasks\\nIf any of that sounds exciting, read on! If any of that sounds scary, I promise to keep \\nthings as clear and enjoyable as possible.\\nIn fact, I prefer clarity well above mathematical correctness, so if you’re an academician \\nreading this, there may be times where you should close your eyes and think of England. \\nWithout further ado, then, let’s get number-crunching.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 22, 'page_label': '1'}, page_content='1\\nT\\nhis book relies on you having a working knowledge of spreadsheets, and I’m going to \\nassume that you already understand the basics. If you’ve never used a formula before \\nin your life, then you’ve got a slight uphill battle here. I’d recommend going through a For \\nDummies book or some other intro-level tutorial for Excel before diving into this.\\nThat said, even if you’re a seasoned Excel veteran, there’s some functionality that’ll keep \\ncropping up in this text that you may not have had to use before. It’s not diffi  cult stuff ; \\njust things I’ve noticed not everyone has used in Excel. You’ll be covering a wide variety \\nof little features in this chapter, and the example at this stage might feel a bit disjointed. \\nBut you can learn what you can here, and then, when you encounter it organically later \\nin the book, you can slip back to this chapter as a reference.\\nAs Samuel L. Jackson says in Jurassic Park, “Hold on to your butts!”\\nEXCEL VERSION DIFFERENCES\\nAs mentioned in the book’s introduction, these chapters work with Excel 2007, 2010, \\n2013, 2011 for Mac, and LibreOffi  ce. Sadly, in each version of Excel, Microsoft has \\nmoved stuff  around for the heck of it.\\nFor example, things on the Layout tab on 2011 are on the View tab in the other ver-\\nsions. Solver is the same in 2010 and 2013, but the performance is actually better in \\n2007 and 2011 even though 2007’s Solver interface is grotesque.\\nThe screen captures in this text will be from Excel 2011. If you have an older or newer \\nversion, sometimes your interactions will look a little diff erent—mostly when it comes \\nto where things are on the menu bar. I will do my best to call out these diff erences. If \\nyou can’t ﬁ nd something, Excel’s help feature and Google are your friends.\\nThe good news is that whenever we’re in the “spreadsheet part of the spreadsheet,” \\neverything works exactly the same.\\nAs for LibreOffi  ce, if you’ve chosen to use open source software for this book, then \\nI’m assuming you’re a do-it-yourself kind of person, and I won’t be referencing the \\nLibreOffi  ce interface directly. Never you mind, though. It’s a dead ringer for Excel.\\nEverything You Ever \\nNeeded to Know about \\nSpreadsheets but Were \\nToo Afraid to Ask'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 23, 'page_label': '2'}, page_content='2 Data Smart\\nSome Sample Data\\nNOTE\\nThe Excel workbook used in this chapter, “Concessions.xlsx,” is available for download \\nat the book’s website at www.wiley.com/go/datasmart.\\nImagine you’ve been terribly unsuccessful in life, and now you’re an adult, still living \\nat home, running the concession stand during the basketball games played at your old \\nhigh school. (I swear this is only semi-autobiographical.) \\nYou have a spreadsheet full of last night’s sales, and it looks like Figure 1-1.\\nFigure 1-1: Concession stand sales\\nFigure 1-1 shows each sale, what the item was, what type of food or drink it was, the \\nprice, and the percentage of the sale going toward proﬁ t.\\nMoving Quickly with the Control Button\\nIf you want to peruse the records, you can scroll down the sheet with your scroll wheel, \\ntrack pad, or down arrow. As you scroll, it’s helpful to keep the header row locked at \\nthe top of the sheet, so you can remember what each column means. To do that, choose'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 24, 'page_label': '3'}, page_content='3Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \\nFreeze Panes or Freeze Top Row from the “View” tab on Windows (“Layout” tab on Mac \\n2011 as shown in Figure 1-2).\\nFigure 1-2: Freezing the top row\\nTo move quickly to the bottom of the sheet to look at how many transactions you have, \\nyou can select a value in one of the populated columns and press Ctrl+ ↓ (Command+↓ \\non a Mac). You’ll zip right to the last populated cell in that column. In this sheet, the ﬁ nal \\nrow is 200. Also, note that using Ctrl/Command to jump around the sheet from left to \\nright works much the same.\\nIf you want to take an average of the sales prices for the night, below the price column, \\ncolumn C, you can jot the following formula:\\n=AVERAGE(C2:C200)\\nThe average is $2.83, so you won’t be retiring wealthy anytime soon. Alternatively, you \\ncan select the last cell in the column, C200, hold Shift+Ctrl+↑ to highlight the whole col-\\numn, and then select the Average calculation from the status bar in the bottom right of the \\nspreadsheet to see the simple summary statistic (see Figure 1-3). On Windows, you’ll need \\nto right-click the status bar to select the average if it’s not there. On Mac, if your status bar \\nis turned off , click the View menu and select “Status Bar” to turn it on.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 25, 'page_label': '4'}, page_content='4 Data Smart\\nFigure 1-3: Average of the price column in the status bar\\nCopying Formulas and Data Quickly\\nPerhaps you’d like to view your proﬁ ts in actual dollars rather than as percentages. You \\ncan add a header to column E called “Actual Proﬁ t.” In E2, you need only to multiply the \\nprice and proﬁ t columns together to obtain this:\\n=C2*D2 \\nFor beer, it’s $2. You don’t have to rewrite this formula in every cell in the column. \\nInstead, Excel lets you grab the right-bottom corner of the cell and drag the formula \\nwhere you like. The referenced cells in columns C and D will update relative to where you \\ncopy the formula. If, as in the case of the concession data, the column to the left is fully \\npopulated, you can double-click the bottom-right corner of the formula to have Excel ﬁ ll \\nthe whole column (see Figure 1-4). Try this double-click action for yourself, because I’ll \\nbe using it all over the place in this book, and if you get the hang of it now, you’ll save \\nyourself a whole lot of heartache. \\nNow, what if you don’t want the cells in the formula to change relative to the target when \\nthey’re dragged or copied? Whatever you don’t want changed, just add a \\n$ in front of it.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 26, 'page_label': '5'}, page_content='5Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \\nFor example, if you changed the formula in E2 to: \\n=C$2*D$2\\nFigure 1-4:  Filling in a formula by dragging the corner\\nThen when you copy the formula down, nothing changes. The formula continues to \\nreference row 2. \\nIf you copy the formula to the right, however, C would become D, D would become E, \\nand so on. If you don’t want that behavior, you need to put a $ in front of the column refer-\\nences as well. This is called an absolute reference as opposed to a relative reference.\\nFormatting Cells\\nExcel off ers static and dynamic options for formatting values. Take a look at column E, the \\nActual Proﬁ t column you just created. Select column E by clicking on the gray E column \\nlabel. Then right-click the selection and choose Format Cells.\\nFrom within the Format Cells menu, you can tell Excel the type of number to be found \\nin column E. In this case you want it to be Currency. And you can set the number of \\ndecimal places. Leave it at two decimals, as shown in Figure 1-5. Also available in Format \\nCells are options for changing font colors, text alignment, ﬁ ll colors, borders, and so on.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 27, 'page_label': '6'}, page_content='6 Data Smart\\nFigure 1-5: The Format Cells menu\\nBut here’s a conundrum. What if you want to format only the cells that have a certain \\nvalue or range of values in them? And what if you want that formatting to change with \\nthe values?\\nThat’s called conditional formatting, and this book makes liberal use of it. \\nCancel out of the Format Cells menu and navigate to the Home tab. In the Styles \\nsection (Mac calls it Format), you’ll find the Conditional Formatting button (see \\nFigure 1-6). Click the button to drop down a menu of options. The conditional formatting \\nmost used in this text is Color Scales. Pick a scale for column E and note how each cell \\nin the column is colored based on its high or low value.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 28, 'page_label': '7'}, page_content='7Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \\nFigure 1-6:  Applying conditional formatting to the proﬁ  t\\nTo remove conditional formatting, use the Clear Rules options under the Conditional \\nFormatting menu.\\nPaste Special Values\\nIt’s often in your best interest not to have a formula lying around like you see in Column E \\nin Figure 1-4. If you were using the \\nRAND() formula to generate a random value, for example, \\nit changes each time the spreadsheet auto-recalculates, which while awesome, can also be \\nextremely annoying. The solution is to copy and paste these cells back to the sheet as ﬂ at \\nvalues. \\nTo convert formulas to values only, simply copy a column ﬁ  lled with formulas (grab \\ncolumn E) and paste it back using the Paste Special option (found on the Home tab under \\nthe Paste option on Windows and under the Edit menu on Mac). In the Paste Special win-\\ndow, choose to paste as values (see Figure 1-7). Note also that Paste Special allows you to \\ntranspose the data from vertical to horizontal and vice versa when pasting. You’ll be using \\nthat a fair bit in the chapters to come.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 29, 'page_label': '8'}, page_content='8 Data Smart\\nFigure 1-7: The Paste Special window in Excel 2011\\nInserting Charts\\nIn the concession stand sales workbook, there’s also a tab called Calories with a tiny table \\nthat shows the calorie count of each item the concession stand sells. You can chart data \\nlike this in Excel easily. On the Insert tab (Charts on a Mac), there is a charts section that \\nprovides diff erent visualization options such as bar charts, line graphs, and pie charts. \\nNOTE\\nIn this book, we’re going to use mostly column charts, line graphs, and scatter plots. \\nNever be caught using a pie chart. And especially never use the 3D pie charts Excel \\noff ers, or my ghost will personally haunt you when I die. They’re ugly, they don’t com-\\nmunicate data well, and the 3D eff ect has less aesthetic value than the seashell paintings \\nhanging on the wall of my dentist’s offi  ce.\\nHighlighting columns A:B on the Calories workbook, you can select a Clustered Column \\nchart to visualize the data. Play around with the graph. Sections can be right-clicked to \\nbring up formatting menus. For example, right-clicking the bars, you can select “Format'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 30, 'page_label': '9'}, page_content='9Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \\nData Series…” under which you can change the ﬁ  ll color on the bars from the default \\nExcel blue to any number of pleasing shades—black, for instance. \\nThere’s no reason for the default legend, so you should select it and press delete to \\nremove it. You might also want to select various text sections on the graph and increase \\nthe size of their font (font size is under the Home tab in Excel). This gives the graph \\nshown in Figure 1-8.\\nFigure 1-8:  Inserting a calories column chart\\nLocating the Find and Replace Menus\\nYou’re going to use ﬁ nd and replace a fair bit in this book. On Windows you can either \\npress Ctrl+F to open up the Find window (Ctrl+H for replace) or navigate to the Home \\ntab and use the Find button in the Editing section. On Mac, there’s a search ﬁ eld on the \\ntop right of the sheet (press the down arrow for the Replace menu), or you can just press \\nCmd+F to bring up the Find and Replace menu.\\nJust to test it out, open up the replace menu on the Calories sheet. You can replace every \\ninstance of the word “Calories” with the word “Energy” (see Figure 1-9) by popping the \\nwords in the Find and Replace window and pressing Replace All.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 31, 'page_label': '10'}, page_content='10 Data Smart\\nFigure 1-9: Running a Find and Replace\\nFormulas for Locating and Pulling Values\\nIf I didn’t assume you at least knew some formulas in Excel (SUM, MAX, MIN, PERCENTILE, and \\nso on), we’d be here all day. And I want to get started. But there are some formulas used a \\nlot in this book that you’ve probably not used unless you’ve dug deep into the wonderful \\nworld of spreadsheets. These formulas deal with ﬁ nding a value in a range and returning its \\nlocation or on the ﬂ ip side ﬁ nding a location in a range and returning its value. \\nI want to cover a few of those on the Calories tab.\\nSometimes you want to know the place in line of some element in a column or row. Is it \\nﬁ rst, second, third? The \\nMATCH formula handles that quite nicely. Below your calorie data, \\nlabel A18 as Match. You can implement the formula one cell over in B18 to ﬁ nd where in \\nthe item list above the word “Hamburger” appears. To use the formula, you supply it a \\nvalue to look for, a range to search in, and a 0 to force it to give you back the position of \\nthe keyword itself:\\n=MATCH(\"Hamburger\",A2:A15,0)\\nThis yields a 6, because “Hamburger” is the sixth item in the list (see Figure 1-10).\\nNext up is the \\nINDEX formula. Label A19 as Index. \\nThis formula takes in a range of values and a row and column number and returns \\nthe value in the range at that location. For example, you can feed the INDEX formula our \\ncalorie table A1:B15, and to pull back the calorie count for bottled water, feed in 3 rows \\ndown and 2 columns over:\\n=INDEX(A1:B15,3,2)'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 32, 'page_label': '11'}, page_content='11Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \\nThis yields a calorie count of 0 as expected (see Figure 1-10).\\nAnother formula you’ll see a lot in this text is OFFSET. Go ahead and label A20 as Off set, \\nand you can play with the formula in B20. \\nWith this formula, you provide a range that acts like a cursor which is moved around \\nwith row and column off sets (similar to INDEX for the single valued case except it’s 0-based). \\nFor example, you can provide OFFSET with a reference to the top left of the sheet, A1, and \\nthen pull back the value 3 cells below by providing a row off set of 3 and a column off set \\nof 0:\\n=OFFSET(A1,3,0)\\nThis returns the name of the third item on the list, “Chocolate Bar.” See Figure 1-10.\\nThe last formula I want to look at in this section is \\nSMALL (it has a counterpart called \\nLARGE that works the same way). If you have a list of values and you want to return, say, \\nthe third smallest, SMALL does that for you. To see this, label A21 as Small and in B21 feed \\nin the list of calorie counts and an index of 3:\\n=SMALL(B2:B15,3)\\nThis hands back a value of 150 which is the third smallest after 0 (bottled water) and \\n120 (soda). See Figure 1-10. \\nNow, there’s one more formula used for looking up values that’s kind of like MATCH on \\nsteroids and that’s VLOOKUP (and its horizontal counterpart HLOOKUP). That’s got its own \\nsection next because it’s a beast.\\nFigure 1-10:  Formulas you should learn'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 33, 'page_label': '12'}, page_content='12 Data Smart\\nUsing VLOOKUP to Merge Data\\nGo ahead and ﬂ ip back to the Basketball Game Sales tab. You can still reference a cell \\nhere from the previous tab, Calories, by simply placing the tab name and “!” in front of a \\nreferenced cell. For example, \\nCalories!B2 is a reference to the calories in beer regardless \\nof what sheet you’re working in.\\nNow, what if you wanted to toss the calorie data into a column back on the sales sheet \\nso that next to each item sold the appropriate calorie count was listed? You’d somehow \\nhave to look up the calorie count of each item sold and place it into a column next to the \\ntransaction. Well, it turns out there’s a formula for that called \\nVLOOKUP.\\nGo ahead and label Column F in the spreadsheet Calories for this purpose. Cell F2 \\nwill include the calorie count for the ﬁ rst beer transaction from the Calories table. Using \\nthe VLOOKUP  formula, you supply the item name from cell A2, a reference to the table \\nCalories!$A$1:$B$15 , and the relative column off set you want your return value to be \\nread out of, which is to say the second column:\\n=VLOOKUP(A2,Calories!$A$1:$B$15,2,FALSE)\\nThe FALSE at the end of the VLOOKUP formula means that you will not accept approximate \\nmatches for “Beer.” If the formula can’t ﬁ nd “Beer” on the calories table, it returns an error.  \\nWhen you enter the formula, you can see that 200 calories is read in from the table \\non the Calories tab. Since you’ve put the $ in front of the table references in the formula, \\nyou can copy this formula down the column by double-clicking the bottom-right corner \\nof the cell. Voila! As shown in Figure 1-11, you have calorie counts for every transaction.\\nFigure 1-11: Using VLOOKUP to grab calorie counts'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 34, 'page_label': '13'}, page_content='13Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \\nFiltering and Sorting\\nNow that you have calories in there, say you now want to view only those transactions \\nfrom the Frozen Treats category. What you want to do then is ﬁ lter the sheet. To do so, ﬁ rst \\nyou select the data in range A1:F200. You can put the cursor in A1 and press Shift+Ctrl+↓ \\nthen →. An even easier method is to click the top of column A and hold the click as you \\nmouse over to column F to highlight all six columns. \\nThen to place auto-ﬁ ltering on these six columns, you press the Filter button in the \\nData section of the ribbon. It looks like a gray funnel as shown in Figure 1-12.\\nFigure 1-12: Place auto-ﬁ lter on a selected range\\nOnce auto-ﬁ lter is activated, you can click the drop-down menu that appears in cell B1 \\nand choose to show only certain categories (in this case, only the Frozen Treats transac-\\ntions will be displayed). See Figure 1-13. \\nOnce you’ve ﬁ ltered, highlighting columns of data allows the summary bar in Excel to \\ngive you rolled-up information just on the cells that remain. For example, having ﬁ ltered \\njust the Frozen Treats, we can highlight the values in column E and use the summary bar \\nto get a quick total of proﬁ t just from that category. See Figure 1-14.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 35, 'page_label': '14'}, page_content='14 Data Smart\\nFigure 1-13: Filtering on category\\nFigure 1-14: Summarizing a ﬁ  ltered column\\nAuto-ﬁ lter allows you to sort as well. For example, if you want to sort by proﬁ  t, just \\nclick the auto-ﬁ lter menu on the Proﬁ t cell (D1) and select Sort Ascending (or “Smallest \\nto Largest” in some versions). See Figure 1-15.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 36, 'page_label': '15'}, page_content='15Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \\nFigure 1-15: Sorting in ascending order by proﬁ  t\\nTo remove all the ﬁ ltering you’ve applied, either you can go back into the Category ﬁ lter \\nmenu and check the other boxes, or you can un-toggle the ﬁ lter button on the ribbon that \\nyou pressed in the ﬁ rst place. You’ll see that although you have all of your data back, the \\nFrozen Treats are still in the order you sorted them in.\\nExcel also off ers the Sort interface for doing more complex sorts than might be possible \\nwith auto-ﬁ lter. To use the feature, you highlight the data to be sorted (grab A:F again) \\nand select Sort from the Sort & Filter section of the Data tab in Excel. This will bring up \\nthe sort menu. On Mac, to get this window, you must press the down arrow in the sort \\nbutton and select Custom Sort….\\nIn the sort menu, shown in Figure 1-16, you can note whether your data has column \\nheaders or not, and if it does have headers like this example does, then you can select, by \\nname, the columns to be sorted.\\nNow, the most awesome part of this sorting interface is that under the “Options…” \\nbutton, you can select to sort left to right instead of column data. That’s something you \\ncannot do with auto-ﬁ lter. In top to bottom of this book you’ll need to randomly sort data \\nby both columns and rows in two quick steps, and this interface is going to be your friend. \\nFor now, just cancel out of it as the data is already ordered the way you want it.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 37, 'page_label': '16'}, page_content='16 Data Smart\\nFigure 1-16: Using the Sort menu\\nUsing PivotTables\\nWhat if you wanted to know the total counts of each item type you sold? Or you wanted \\nto know revenue totals by item?\\nThese questions are akin to “aggregate” or “group by” queries that you’d run in a tra-\\nditional SQL database. But this data isn’t in a database. It’s in a spreadsheet. That’s where \\nPivotTables come to the rescue.\\nJust as when you ﬁ ltered your data, you start by selecting the data you want to manipu-\\nlate—in this case, the purchase data in the range A1:F:200. From the Insert tab (Data tab \\non Mac), you can press the PivotTable button and select for Excel to create a new sheet \\nwith a PivotTable. While some versions of Excel allow you to insert a PivotTable into an \\nexisting sheet, it’s standard practice to select the new sheet option unless you have a really \\ngood reason not to. \\nIn this new sheet, the PivotTable Builder will be aligned to the right of the table (it ﬂ oats \\non a Mac). The builder allows you to take the columns from the original selected data and \\nuse them as report ﬁ lters, column and row labels for grouping, or values. A report ﬁ  lter \\nis similar in function to a ﬁ lter from the previous section—it allows you to select only a \\nsubset of the data, such as Frozen Treats. The Column Labels and Row Labels ﬁ  ll in the \\nmeat of the PivotTable report with distinct values from the selected columns.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 38, 'page_label': '17'}, page_content='17Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \\nOn Windows, the initial PivotTable built will be completely empty, while on Mac it is \\noften prepopulated with distinct values from the ﬁ rst selected column down the rows of \\nthe table and distinct values from the second column across the columns. If you’re on a \\nMac, go ahead and uncheck all the boxes in the builder, so that you can work along from \\nan empty table.\\nNow, say you wanted to know total revenue by item. To get at that, you’d drag the Item \\ntile in the PivotTable Builder into the Rows section and the Price tile into the Values sec-\\ntion. This means that you’ll be operating on revenue grouped by item name.\\nInitially, however, the PivotTable is set up to merely count the number of price records \\nthat are within a group. For example, there are 20 Beer rows. See Figure 1-17. \\nFigure 1-17: The PivotTable builder and a count of sales by item\\nYou need to change the count to a sum in order to examine revenue. To do so, on \\nWindows, drop the menu down on the Price tile in the Values section of the builder and \\nselect “Value Field Settings….” On Mac, press the little “i” button. From there, “sum” can \\nbe selected from the various summary options.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 39, 'page_label': '18'}, page_content='18 Data Smart\\nWhat if you wanted to break out these sums by category? To do so, you drag the Category \\ntile into the Columns section of the builder. This gives the table shown in Figure 1-18. \\nNote that the PivotTable in the ﬁ gure automatically totals up rows and columns for you.\\nFigure 1-18: Revenue by item and category\\nAnd if you want to ever get rid of something from the table, just uncheck it or grab the \\ntile from the section it’s in and drag it out of the sheet as if you were tossing it away. Go \\nahead and drop the Category tile.\\nOnce you get a report you want in a PivotTable, you can always select the values and \\npaste them to another sheet to work on further. In this example, you can copy the table \\n(A5:B18 on Mac) and Paste Special its values into a new tab called Revenue By Item (see \\nFigure 1-19).\\nFeel free to swap in various row and column labels until you get the hang of what’s \\ngoing on. For instance, try to get a total calorie count sold by category using a PivotTable.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 40, 'page_label': '19'}, page_content=\"19Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \\nFigure 1-19: Revenue by Item tab created by pasting values from a PivotTable\\nUsing Array Formulas\\nIn the concession transaction workbook, there is a tab called Fee Schedule. As it turns out, \\nCoach O’Shaughnessy would let you run the snack stand only if you kicked some of the \\nproﬁ t back to him (perhaps to subsidize his tube sock-buying habit). The Fee Schedule \\ntab shows the percent cut he takes on each item sold. \\nSo how much money do you owe him for last night’s game? To answer that question, \\nyou need to multiply the total revenue of each item from the PivotTable by the cut for the \\ncoach and sum them all up.\\nThere’s a great formula for this operation that will do all the multiplication and sum-\\nmation in a single step. Rather creatively named, it’s called \\nSUMPRODUCT . In cell E1 on \\nthe Revenue By Item sheet, add a label called Total Cut for Coach. In C2, determine the \\nSUMPRODUCT of the revenue and the fees by adding this formula:\\n=SUMPRODUCT(B2:B15,'Fee Schedule'!B2:O2)\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 41, 'page_label': '20'}, page_content=\"20 Data Smart\\nUh oh. There’s an error; the cell just reads #Value. What’s going wrong?\\nEven though you’ve selected two ranges of equal size and put them in SUMPRODUCT , \\nthe formula can’t see that the ranges are equal because one range is vertical and one’s \\nhorizontal.\\nFortunately, Excel has a function for ﬂ ipping arrays in the right direction. It’s called \\nTRANSPOSE. You need to write the formula like this:\\n=SUMPRODUCT(B2:B15,TRANSPOSE('Fee Schedule'!B2:O2))\\nNope! Still getting an error.\\nThe reason you’re still getting an error is that every formula in Excel, by default, returns \\na single value. Even TRANSPOSE returns the ﬁ rst value in the transposed array. If you want \\nthe whole array returned, you have to turn TRANSPOSE into an “array formula,” which means \\nexactly what you might think. Array formulas hand you back arrays, not single values.\\nYou don’t have to change the way you type your SUMPRODUCT to make this happen. All \\nyou need to do is when you’re done typing the formula, instead of pressing Enter, press \\nCtrl+Shift+Enter. On the Mac, you use Command+Return.\\nVictory! As shown in Figure 1-20, the calculation now reads $57.60. But I suggest round-\\ning that down to $50, because how many socks does Coach really need?\\nFigure 1-20: Taking a SUMPRODUCT with an array formula \\nSolving Stuff with Solver\\nMany of the techniques you’ll study in this book can be boiled down to optimization mod-\\nels. An optimization problem is one where you have to make the best decision (choose \\nthe best investments, minimize your company’s costs, ﬁ  nd the class schedule with the\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 42, 'page_label': '21'}, page_content='21Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \\nfewest morning classes, or so on). In optimization models then, the words “minimize” \\nand “maximize” come up a lot when articulating an objective.\\nIn data science, many of the practices, whether that’s artiﬁ cial intelligence, data mining, \\nor forecasting, are actually just some data prep plus a model-ﬁ tting step that’s actually an \\noptimization model. So it’d make sense to teach optimization ﬁ rst. But learning all there \\nis to know about optimization is tough to do straight off  the bat. So you’ll do an in-depth \\noptimization study in Chapter 4 after you do some more fun machine learning problems \\nin Chapters 2 and 3. To ﬁ ll in the gaps though, it’s best if you get a little practice with \\noptimization now. Just a taste.\\nIn Excel, optimization problems are solved using an Add-In that ships with Excel \\ncalled Solver. \\n• On Windows, Solver may be added in by going to File (in Excel 2007 it’s the top \\nleft Windows button) ➪ Options ➪ Add-ins, and under the Manage drop-down \\nchoosing Excel Add-ins and pressing the Go button. Check the Solver Add-In box \\nand press OK. \\n• On Mac, Solver is added by going to Tools then Add-ins and selecting Solver.xlam \\nfrom the menu.\\nA Solver button will appear in the Analysis section of the Data tab in every version.\\nAll right! Now that Solver is installed, here’s an optimization problem: You are told you \\nneed 2,400 calories a day. What’s the fewest number of items you can buy from the snack \\nstand to achieve that? Obviously, you could buy 10 ice cream sandwiches at 240 calories \\na piece, but is there a way to do it for fewer items than that?\\nSolver can tell you!\\nTo start, make a copy of the Calories sheet, name the sheet Calories-Solver, and clear \\nout everything but the calories table on the copy. If you don’t know how to make a copy \\nof a sheet in Excel, you simply right-click the tab you’d like to copy and select the Move \\nor Copy menu. This gives you the new sheet shown in Figure 1-21.\\nTo get Solver to work, you need to provide it with a range of cells it can set with deci-\\nsions. In this case, Solver needs to decide how many of each item to buy. So in Column C \\nnext to the calorie counts, label the column How many? (or whatever you feel like), and \\nyou can allow Solver to store its decisions in this column.\\nExcel considers blank cells to be 0s so you needn’t ﬁ ll in these cells with anything to \\nstart. Solver will do that for you.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 43, 'page_label': '22'}, page_content='22 Data Smart\\nFigure 1-21: The copied Calories-Solver sheet\\nIn cell C16, sum up the number of items to be bought above as:\\n=SUM(C2:C15)\\nAnd below that you can sum up the total calorie count of these items (which you’ll \\nwant eventually to equal 2,400) using the SUMPRODUCT formula:\\n=SUMPRODUCT(B2:B15,C2:C15)\\nThis gives the initial sheet shown in Figure 1-22.\\nNow you’re ready to build the model, so bring up the Solver window by pressing the \\nSolver button on the Data tab.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 44, 'page_label': '23'}, page_content='23Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \\nFigure 1-22: Getting calorie and item counts set up\\nNOTE\\nThe Solver window, shown in Figure 1-23 in Excel 2011, looks pretty similar in Excel \\n2010, 2011, and 2013. In Excel 2007, the layout is slightly diff  erent, but the only \\nsubstantive diff erence is that there is no algorithm selection box. Rather, there’s an \\n“Assume Linear Model” checkbox under the Options menu. We’ll learn all about these \\nelements later.\\nThe main elements you plug into Solver to solve a problem, as shown in Figure 1-23, \\nare an objective cell, an optimization direction (minimization or maximization), some \\ndecision variables that can be changed by Solver, and some constraints.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 45, 'page_label': '24'}, page_content='24 Data Smart\\nFigure 1-23: The uninitialized Solver window\\nIn your case, the objective is to minimize the total items in cell C16. The cells that can \\nbe altered are the item selections in C2:C15. And the constraints are that C17, the total \\ncalories, needs to be equal to 2,400. Also, we’ll need to add a constraint that our decisions \\nbe counting numbers, so we’ll need to check the non-negative box (under the options menu \\nin Excel 2007) and add an integer constraint to the decisions. After all, you can’t buy 1.7 \\nsodas. These integer constraints will be covered in depth in Chapter 4.\\nTo add in the total calorie constraint, press the Add button and set C17 equal to 2,400 \\nas shown in Figure 1-24. \\nFigure 1-24: Adding the calorie constraint\\nSimilarly, add a constraint setting C2:C15 to be integers as shown in Figure 1-25.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 46, 'page_label': '25'}, page_content='25Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \\nFigure 1-25: Adding an integer constraint\\nPress OK. \\nIn Excel 2010, 2011, and 2013, make sure the solving method is set to Simplex LP. \\nSimplex LP is appropriate for this problem, because this problem is linear (the “L ” in LP \\nstands for linear as you’ll see in Chapter 4). By linear, I mean that the problem involves \\nnothing but linear combinations of the decisions in C2 through C15 (sums, products with \\nconstants such as calorie counts, etc.). \\nIf we had non-linear calculations in the model (perhaps a square root of a decision, a \\nlogarithm, or an exponential function), then we could use one of the other algorithms \\nExcel provides in Solver. Chapter 4 covers this in great detail. \\nIn Excel 2007, you would denote the problem as linear by clicking the Assume Linear \\nModel under the Options screen. Your ﬁ nal setup should appear as in Figure 1-26. \\nFigure 1-26: Final Solver setup for minimizing items needed for 2,400 calories'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 47, 'page_label': '26'}, page_content='26 Data Smart\\nAll right! Go ahead and press the Solve button. Excel should ﬁ  nd a solution almost \\nimmediately. And that solution, as shown in Figure 1-27, is 5. Now, your Excel might \\npick a diff erent 5 items than mine in the screenshot, but the minimum is 5 nonetheless.\\nFigure 1-27: The optimized item selection\\nOpenSolver: I Wish We Didn’t Need This, but We Do\\nThis book was originally designed to work completely with Excel’s built-in Solver. However, \\nas it turns out, functionality was removed from Solver in later versions for mysterious and \\nunadvertised reasons.\\nWhat that means is that while this whole book works using vanilla Solver in Excel 2007 \\nand Excel 2011 for Mac, in Excel 2010 and Excel 2013, the built-in Solver will occasion-\\nally complain that a linear optimization model is too large (I’ll give you a heads-up in this \\nbook whenever a model gets that complex).\\nLuckily, there’s an excellent free tool called OpenSolver that’s available for the Windows \\nversions of Excel that addresses this deﬁ ciency. With OpenSolver, you can still build your \\nmodel in the regular Solver interface, but OpenSolver provides a button that you press to \\nuse its Simplex LP algorithm implementation, which is blazingly fast.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 48, 'page_label': '27'}, page_content='27Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \\nTo set up OpenSolver, navigate to http://OpenSolver.org and download the zip ﬁ le. \\nUncompress the ﬁ le into a folder, and whenever you want to solve a beefy model, just set \\nit up in a spreadsheet like normal and double-click the OpenSolver.xlam ﬁ le, which will \\ngive you an OpenSolver section on the Data tab in Excel. Press the Solve button to solve \\nan existing model. As shown in Figure 1-28, I’ve applied OpenSolver in Excel 2013 to the \\nmodel from the previous section, and it buys ﬁ ve slices of pizza.\\nFigure 1-28: OpenSolver buys pizza like a madman\\nWrapping Up\\nAll right, you’ve learned how to navigate and select ranges quickly, how to leverage absolute \\nreferences, how to paste special values, how to use VLOOKUP and other matching formulas, \\nhow to sort and ﬁ lter data, how to create PivotTables and charts, how to execute array \\nformulas, and how and when to bust out Solver.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 49, 'page_label': '28'}, page_content='28 Data Smart\\nHere’s either a depressing or fun fact depending on your perspective. I’ve known man-\\nagement consultants at prominent ﬁ rms who earn excellent salaries by doing what I call \\nthe “consulting two-step”:\\n 1. Talk about nonsense with clients (sports, vacation, barbeque ... not that there’s \\nanything nonsensical about smoked meats). \\n 2. Summarize data in Excel.\\nYou may not know all there is to know about college football (I certainly don’t), but if \\nyou internalize this chapter, you’ll have point number two knocked out.\\nBut you’re not here to become a management consultant. You’re here to drive deep into \\ndata science, and that starts in the next chapter where we’ll get started with a little bit of \\nunsupervised machine lear ning.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 50, 'page_label': '29'}, page_content='2\\nI \\nwork in the e-mail marketing industry for a website called MailChimp.com. We help \\ncustomers send e-mail newsletters to their audience, and every time someone uses the \\nterm “e-mail blast,” a little part of me dies. \\nWhy? Because e-mail addresses are no longer black boxes that you lob “blasts” at like \\nﬂ ash grenades. No, in e-mail marketing (as with many other forms of online engagement, \\nincluding tweets, Facebook posts, and Pinterest campaigns), a business receives feedback \\non how their audience is engaging at the individual level through click tracking, online \\npurchases, social sharing, and so on. This data is not noise. It characterizes your audience. \\nBut to the uninitiated, it might as well be Greek. Or Esperanto.\\nHow do you take a bunch of transactional data from your customers (or audience, users, \\nsubscribers, citizens, and so on) and use it to understand them? When you’re dealing with \\nlots of people, it’s hard to understand each customer personally, especially if they all have \\ntheir own diff erent ways in which they’ve engaged with you. Even if you could understand \\neveryone at a personal level, that can be tough to act on.\\nYou need to take this customer base and ﬁ  nd a happy medium between “blasting” \\neveryone as if they were the same faceless entity and understanding everything about \\neveryone to create personalized marketing for each individual recipient. One way to strike \\nthis balance is to use clustering to create a market segmentation of your customers so that \\nyou can market to segments of your base with targeted content, deals, etc.\\nCluster analysis is the practice of gathering up a bunch of objects and separating them \\ninto groups of similar objects. By exploring these diff  erent groups—determining how \\nthey’re similar and how they’re diff erent—you can learn a lot about the previously amor-\\nphous pile of data you had. And that insight can help you make better decisions at a level \\nthat’s more detailed than before.\\nIn this way, clustering is called exploratory data mining, because these clustering tech-\\nniques help tease out relationships in large datasets that are too hard to identify with an \\neyeball. And revealing relationships in your population is useful across industries whether \\nit’s for recommending ﬁ lms based on the habits of folks in a taste cluster, identifying crime \\nCluster Analysis \\nPart I: Using K-Means \\nto Segment Your \\nCustomer Base'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 51, 'page_label': '30'}, page_content='30 Data Smart\\nhot spots within urban areas, or grouping return-related ﬁ nancial investments to ensure \\na diversiﬁ ed portfolio spans clusters.\\nOne of my favorite uses for clustering is image clustering—lumping together image \\nﬁ les that “look the same” to the computer. For example, in photo sharing services like \\nFlickr, a user will generate a lot of content, and there may end up being too many photos \\nto navigate simply. But using clustering techniques, you can cluster similar images together \\nand allow users to navigate between these clusters before drilling down.\\nThis chapter looks at the most common type of clustering, called k-means clustering, \\nwhich originated in the 1950s and has since become a go-to clustering technique for \\nknowledge discovery in databases (KDD) across industries and the government. \\nK-means isn’t the most mathematically rigorous of techniques. It’s born of the kind \\nof practicality and common sense you might see in soul food. Soul food doesn’t have the \\nsnooty pedigree of French cuisine, but it hits the spot sometimes. Cluster analysis with \\nk-means, as you’ll soon see, is part math, part story-telling. But its intuitive simplicity is \\npart of the attraction. \\nTo see how it works, you’ll start with a simple example.\\nGirls Dance with Girls, Boys Scratch Their Elbows\\nThe goal in k-means clustering is to take some points in space and put them into k groups \\n(where k is any number you want to pick). Those k groups are each deﬁ ned by a point in \\nthe center, kind of like a ﬂ ag stuck in the moon that says, “Hey, this is the center of my \\nSUPERVISED VERSUS UNSUPERVISED MACHINE LEARNING\\nBy deﬁ nition, in exploratory data mining, you don’t know ahead of time what you’re \\nlooking for. You’re an explorer. Like Dora. You may be able to articulate when two \\ncustomers look the same and when they look diff erent, but you don’t know the best \\nway to segment your customer base. So when you ask a computer to segment your \\ncustomers for you, that’s called unsupervised machine learning, because you’re not \\n“supervising”—telling the computer how to do its job.\\nThis is in contrast to supervised machine learning, which usually crops up when artiﬁ -\\ncial intelligence makes the front page of the paper. If I know I want to divide customers \\ninto two groups—say “likely to purchase” and “not likely to purchase”—and I provide \\nthe computer with historical examples of such customers and tell it to assign all new \\nleads to one of these two groups, that’s supervised.\\nIf instead I say, “here’s what I know about my customers and here’s how to measure \\nwhether they’re diff erent or similar. Tell me what’s interesting,” that’s unsupervised.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 52, 'page_label': '31'}, page_content='31Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\ngroup. Join me if you’re closer to this ﬂ ag than any others.” This group center (formally \\ncalled the cluster centroid) is the mean from which k-means gets its name.\\nTake as an example a middle school dance. If you’ve blocked the horror of middle school \\ndances from your mind, I apologize for resurfacing such painful memories.\\nThose in attendance at the McAcne Middle School dance, romantically called the “Under \\nthe Sea Gala,” are scattered about the ﬂ oor as shown in Figure 2-1. I’ve even Photoshopped \\nsome parquet ﬂ oor into the ﬁ gure to help with the illusion.\\nAnd here’s a sampling of the songs these young leaders of the free world will be dancing \\nawkwardly to if you’d like to listen along in Spotify:\\n• Styx: Come Sail Away\\n• Everything But the Girl: Missing\\n• Ace of Bass: All that She Wants\\n• Soft Cell: Tainted Love\\n• Montell Jordan: This is How We Do It\\n• Eiff el 65: Blue\\nFigure 2-1: McAcne Middle School students tearing up the dance ﬂ  oor\\nNow, k-means clustering demands that you specify how many clusters you want to put \\nthe attendees in. Let’s pick three clusters to start (later in this chapter we’ll look at how'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 53, 'page_label': '32'}, page_content='32 Data Smart\\nto choose k). The algorithm is going to plant three ﬂ ags on the dance ﬂ oor, starting with \\nsome initial feasible solution, such as that pictured in Figure 2-2, where you have three \\ninitial means spread on the ﬂ oor, denoted by black circles. \\nFigure 2-2: Initial cluster centers placed\\nIn k-means clustering, dancers are assigned to the cluster that’s nearest them, so \\nbetween any two cluster centers on the ﬂ oor, you can draw a line of demarcation, whereby \\nif a dancer is on one side of the line they’re in one group, but if they’re on the other side, \\ntheir group changes (see Figure 2-3).\\nUsing these lines of demarcation, you can assign dancers to their groups and shade \\nthem appropriately, as in Figure 2-4. This diagram, one that divides the space into poly-\\ntopes based on which regions are assigned to which cluster centers by distance, is called \\na Voronoi diagram.\\nNow, this initial assignment doesn’t feel right, does it? You’ve sliced the space up in a \\nrather odd way, leaving the bottom-left group empty and a lot of folks on the border of \\nthe top-right group.\\nThe k-means clustering algorithm slides these three cluster centers around the dance \\nﬂ oor until it gets the best ﬁ t. \\nHow is “best ﬁ t” measured? Well, each attendee is some distance away from their clus-\\nter center. Whichever arrangement of cluster centers minimizes the average distance of \\nattendees from their center is best.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 54, 'page_label': '33'}, page_content='33Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\nFigure 2-3: Lines denote the borders of the clusters.\\nFigure 2-4:  Cluster assignments given by shaded regions in the Voronoi diagram'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 55, 'page_label': '34'}, page_content='34 Data Smart\\nNow, as I mentioned in Chapter 1, the word “minimize” is a tip-off   that you’ll need \\noptimization modeling to best place the cluster centers. So in this chapter, you’ll be busting \\nout Solver to move the cluster centers around. The way Solver is going to get the centers \\nplaced just right is by intelligently and iteratively moving them around, keeping track of \\nmany of the good placements it has found and combining them (literally mating them \\nlike race horses) to get the best placement.\\nSo while the diagram in Figure 2-4 looks pretty bad, Solver might eventually bump the \\ncenters to something like Figure 2-5. This gets the average distance between each dancer \\nand their center down a bit.\\nFigure 2-5: Moving the centers just a tad\\nEventually though, Solver would ﬁ  gure out that the centers should be placed in the \\nmiddle of our three groups of dancers as shown in Figure 2-6.\\nNice! This is what an ideal clustering looks like. The cluster centroids are at the centers \\nof each group of dancers, minimizing the average distance between dancer and nearest \\ncenter. And now that you have a clustering, you can move on to the fun part: trying to \\nunderstand what the clusters mean.\\nIf you investigated the dancers’ hair colors, political persuasions, or mile run speeds, \\nthe clusters may not make much sense. But the moment you were to evaluate the genders \\nand ages of the attendees in each cluster, you’d start to see some common themes. The \\nsmall group at the bottom is all old people—they must be the dance chaperones. The left \\ngroup is all young males, and the right group is all young females. Everyone is too afraid \\nto dance with each other.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 56, 'page_label': '35'}, page_content='35Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\nFigure 2-6: Optimal 3-means clustering of the McAcne dance\\nAll right! So k-means has allowed you to segment this dance attendee population and \\ncorrelate attendee descriptors with cluster membership to understand the why behind \\nthe assignments.\\nNow, you’re probably saying to yourself, “Yeah, but that’s stupid. I already knew the \\nanswer to start.” You’re right. In this example, you did. The reason this is a toy problem, is \\nthat you can already solve it by just looking at the points. Everything is in two-dimensional \\nspace, which is super easy for your eyeballs to cluster. \\nBut what if you ran a store that sold thousands of products? Some customers have bought \\none or two in the past year. Other customers have bought tens. And the items purchased \\nvary from customer to customer.\\nHow do you cluster them on their “dance ﬂ oor?” Well, your dance ﬂ oor isn’t in a two-\\ndimensional space or three-dimensional space. It’s in a thousand-dimensional product \\npurchase space in which a customer has either purchased or not purchased the product in \\neach single dimension. Very quickly, you see, a clustering problem can exceed the limits \\nof the “Mark I Eyeball,” as my military friends like to say.\\nGetting Real: K-Means Clustering Subscribers in \\nE-mail Marketing\\nLet’s move on to a more substantive use case. I’m an e-mail-marketing guy, so I’m going \\nto use an example from MailChimp.com where I work. But this same example would work \\non retail purchase data, ad conversion data, social media data, and so on. It works with'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 57, 'page_label': '36'}, page_content='36 Data Smart\\nbasically any type of data where you’re reaching out to customers with marketing mate-\\nrial, and they’re choosing to engage with you.\\nJoey Bag O’ Donuts Wholesale Wine Emporium\\nLet’s imagine that you live in New Jersey where you run Joey Bag O’ Donuts Wholesale \\nWine Emporium. It’s an import-export business focused on bringing bulk wine to the \\nstates and selling it to select wine and liquor stores across the country. The way the busi-\\nness works is that Joey Bags travels the globe ﬁ nding incredible deals on large quantities of \\nwine. Joey ships it back to Jersey, and it’s your job to sell this stuff  on to stores at a proﬁ t.\\nYou reach out to customers in a number of ways—a Facebook page, Twitter, even the \\noccasional direct mailing—but the e-mail newsletter drums up the most business. For the \\npast year, you’ve sent one newsletter per month. Usually there are two or three wine deals \\nin each e-mail, perhaps one would be on Champagne, another on Malbec. Some deals are \\namazing, 80 percent or more off  of retail. In total, you’ve off ered 32 deals this year, all of \\nwhich have gone quite well. \\nBut just because things are going well, doesn’t mean you can’t do better. It’d be nice \\nif you could understand the customers a little more. Sure, you can look at a particular \\npurchase—like how some person with the last name Adams bought some Espumante \\nin July at a 50 percent discount—but you can’t tell whether that’s because he liked that \\nthe minimum purchase requirement was one six-bottle box or the price or that it hadn’t \\npassed its peak yet. \\nIt would be nice if you could segment the list into groups based on interest. Then, you \\ncould customize the newsletter to each segment and maybe drum up some more busi-\\nness. Whichever deal you thought matched up better with the segment could go in the \\nsubject line and would come ﬁ rst in the newsletter. That type of targeting can result in a \\nbump in sales. \\nBut how do you segment the list? Where do you start? \\nThis is an opportunity to let the computer segment the list for you. Using k-means \\nclustering, you can ﬁ nd the best segments and then try to understand why they’re the \\nbest segments.\\nThe Initial Dataset\\nNOTE\\nThe Excel workbook used in this chapter, “WineKMC.xlsx,” is available for down-\\nload at the book’s website at www.wiley.com/go/datasmart. This workbook includes \\nall the initial data if you want to work from it. Or you can just read along using the \\nsheets I’ve put together in the workbook.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 58, 'page_label': '37'}, page_content='37Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\nStarting out, you have two interesting sources of data:\\n• The metadata on each off er is saved in a spreadsheet, including varietal, minimum \\nbottle quantity for purchase, discount off  retail, whether the wine is past its peak, \\nand country or state of origin. This data is housed in a tab called Off erInformation, \\nas shown in Figure 2-7\\n• You also know which customers bought which off ers, so you can dump that infor-\\nmation out of MailChimp and into the spreadsheet with the off er metadata in a tab \\ncalled Transactions. This transactional data, as shown in Figure 2-8, is simply rep-\\nresented as the customer who made the purchase and which off er they purchased.\\nFigure 2-7: The details of the last 32 offers'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 59, 'page_label': '38'}, page_content='38 Data Smart\\nFigure 2-8: A list of offers taken by customer \\nDetermining What to Measure\\nSo here’s a conundrum. In the middle school dance problem, measuring distances between \\ndancers and cluster centers was easy, right? Just break out the measuring tape!\\nBut what do you do here?\\nYou know there were 32 deals offered in the last year, and you have a list in the \\nTransactions tab of the 324 purchases, broken out by customer. But in order to measure \\nthe distance between each customer and a cluster center, you need to position them in \\nthis 32-deal space. In other words, you need to understand the deals they did not take, \\nand create a matrix of deals-by-customers, where each customer gets their own 32-deal \\ncolumn full of 1s for the deals they took and 0s for the ones they didn’t. \\nIn other words, you need to take this row-oriented Transactions tab and turn it into a \\nmatrix with customers in columns and off ers in rows. And the best way to create such a \\nmatrix is to use a PivotTable.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 60, 'page_label': '39'}, page_content='39Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\nNOTE\\nFor a primer on PivotTables, see Chapter 1.\\nSo here’s what you’re going to do. In the Transactions tab, highlight columns A and \\nB and then insert a PivotTable. Using the PivotTable Builder, simply select deals as row \\nlabels, customers as column labels, and take a count of deals for the values. This count \\nwill be 1 if a customer/deal pair was present in the original data and 0 otherwise (0 ends \\nup as a blank cell in this case). The resulting PivotTable is pictured in Figure 2-9.  \\nFigure 2-9: PivotTable of deals versus customers \\nNow that you have your purchases in matrix form, copy the Off erInformation tab and \\nname it Matrix. In this new sheet, paste the values from the PivotTable (you don’t need \\nto copy and paste the deal number, because it’s already in the off er information) into the \\nnew tab starting at column H. You end up with a ﬂ  eshed out version of the matrix that \\nhas consolidated the deal descriptions with the purchase data, as pictured in Figure 2-10.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 61, 'page_label': '40'}, page_content='40 Data Smart\\nFigure 2-10: Deal description and purchase data merged into a single matrix \\nSTANDARDIZING YOUR DATA\\nIn this chapter, each dimension of your data is the same type of binary purchase \\ndata. But in many clustering problems, this is not the case. Envision a scenario \\nwhere people are clustered based on height, weight, and salary. These three types of \\ndata are all on diff erent scales. Height may range from 60 inches to 80 inches while \\nweight may range from 100 to 300 pounds. \\nIn this context, measuring the distance between customers (like dancers on the dance \\nﬂ oor) gets tricky. So it’s common to standardize each column of data by subtracting out \\nthe average and dividing through by a measure of spread we’ll encounter in Chapter \\n4 called the standard deviation. This puts each column on the same scale, centered \\naround 0.\\nWhile our data in Chapter 2 does not require standardization, you can see it in action \\nin the outlier detection chapter, Chapt er 9.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 62, 'page_label': '41'}, page_content='41Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\nStart with Four Clusters\\nAll right, so now you have all of your data consolidated into a single, useable format. \\nIn order to begin clustering, you need to pick k, which is the number of clusters in the \\nk-means clustering algorithm. Often the approach in k-means is to try a bunch of diff er-\\nent values for k (I’ll get to how to choose between them later), but for the sake of starting, \\nyou need to choose just one. \\nYou’ll want to choose a number of clusters to start with that’s in the ball park of what \\nyou’re willing to act on. You’re not going to create 50 clusters and send 50 targeted ad \\ncampaigns to a couple of folks in each group. That defeats the purpose of the exercise in \\nthe ﬁ rst place. You want something small in this case. For this example, then, start with \\nfour—in an ideal world, maybe you’d get your list divided into four perfectly understand-\\nable groups of 25 customers each (this isn’t likely).\\nAll right then, if you were to split the customers into four groups, what are the best \\nfour groups for that?\\nRather than dirty up the pretty Matrix tab, copy the data into a new tab and call it 4MC. \\nYou can then insert four columns after Past Peak in columns H through K that will be the \\ncluster centers. (To insert a column, right-click Column H and select Insert. A column \\nwill be added to the left.) Label these clusters Cluster 1 through Cluster 4. You can also \\nplace some conditional formatting on them so that whenever each cluster center is set \\nyou can see how they diff er.\\nThe 4MC tab will appear as shown in Figure 2-11. \\nThese cluster centers are all 0s at this point. But technically, they can be anything \\nyou want, and what you’d like to see is that they, like in the middle school dance case, \\ndistribute themselves to minimize the distances between each customer and their closest \\ncluster center.\\nObviously then, these centers will have values between 0 and 1 for each deal since all \\nthe customer vectors are binary.\\nBut what does it mean to measure the distance between a cluster center and a customer?\\nEuclidean Distance: Measuring Distances as the Crow Flies\\nYou now have a single column per customer, so how do you measure the dance-ﬂ  oor \\ndistance between them? Well, the offi  cial term for that is “as-the-crow-ﬂ ies,” measuring \\ntape distance is the Euclidean distance.\\nLet’s return to the dance ﬂ oor problem to understand how to compute it.\\nI’m going to lay down a horizontal and a vertical axis on the dance ﬂ oor, and in Figure \\n2-12, you can see that you have a dancer at (8, 2) and a cluster center at (4, 4). To compute \\nthe Euclidean distance between them, you have to remember the Pythagorean theorem \\nyou learned back in middle school.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 63, 'page_label': '42'}, page_content='42 Data Smart\\nFigure 2-11: Blank cluster centers placed on the 4MC tab\\n(8,2)\\n(4,4)\\n1\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n23456 789 1 0\\nFigure 2-12: A dancer at (8,2) and a cluster center at (4,4)'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 64, 'page_label': '43'}, page_content='43Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\nThese two points are 8 – 4 = 4 feet apart in the vertical direction. They’re 4 – 2 = 2 feet \\napart in the horizontal direction. By the Pythagorean theorem then, the squared distance \\nbetween these two points is 4^2 + 2^2 = 16 + 4 = 20 feet. So the distance between them is \\nthe square root of 20, which is approximately 4.47 feet (see Figure 2-13). \\n(8,2)\\n(4,4)\\n2\\n4\\n1\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n234567891 0\\n42 +  22 =  4.47\\nFigure 2-13: Euclidean distance is the square root of the sum of squared distances in each \\nsingle direction\\nIn the context of the newsletter subscribers, you have more than two dimensions, but \\nthe same concept applies. Distance between a customer and a cluster center is calculated \\nby taking the diff erence between the two points for each deal, squaring them, summing \\nthem up, and taking the square root.\\nSo for instance, let’s say in the 4MC tab, you wanted to take the Euclidean distance \\nbetween the Cluster 1 center in column H and the purchases of customer Adams in col-\\numn L.\\nIn cell L34, below Adams’ purchases, you can take the diff erence of Adams’ vector and \\nthe cluster center, square it, sum it, and square root the sum, using the following array \\nformula (note the absolute references that allow you to drag this formula to the right or \\ndown without the cluster center reference changing):\\n{=SQRT(SUM((L$2:L$33-$H$2:$H$33)^2))}'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 65, 'page_label': '44'}, page_content='44 Data Smart\\nYou have to use an array formula (enter the formula and press Ctrl + Shift + Enter or \\nCmd + Return on Mac as covered in Chapter 1) because the (L2:L33 – H2:H33)^2 por-\\ntion of the formula needs to know to go item by item taking diff  erences and squaring \\nthem. The end result, however, is a single number: 1.732 in this case (see Figure 2-14). \\nThis makes sense because Adams took three deals, but the initial cluster center is all 0s, \\nand the square root of 3 is 1.732. \\nFigure 2-14: The distance between Adams and Cluster 1 \\nIn the spreadsheet shown in Figure 2-14, I’ve frozen panes (see Chapter 1) between \\ncolumns G and H and labeled row 34 in G34 as Distance to Cluster 1 just to keep track \\nof things when you scroll to the right.\\nDistances and Cluster Assignments for Everybody!\\nSo now you know how to calculate the distance between a purchase vector and a cluster \\ncenter.\\nIt’s time to add the distance calculations for Adams to the other centers by dragging \\ncell L34 down through L37 and then changing the cluster center reference manually from \\ncolumn H to I, J, and K in the descending cells. You end up with the following 4 formulas \\nin L34:L37:\\n{=SQRT(SUM((L$2:L$33-$H$2:$H$33)^2))}\\n{=SQRT(SUM((L$2:L$33-$I$2:$I$33)^2))}'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 66, 'page_label': '45'}, page_content='45Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\n{=SQRT(SUM((L$2:L$33-$J$2:$J$33)^2))}\\n{=SQRT(SUM((L$2:L$33-$K$2:$K$33)^2))}\\nSince you’ve used absolute references (the $ sign in the formulas; see Chapter 1 for \\nmore details) for the cluster centers, you can drag L34:L37 over through DG34:DG37 to \\ncalculate distances between each customer and all four cluster centers. Also, in column \\nG, label rows 35 through 37 Distance to Cluster 2, and so on. These new distances are \\npictured in Figure 2-15.\\nFigure 2-15: Distance calculations from each customer to each cluster \\nFor each customer then, you know their distance to all four cluster centers. Their cluster \\nassignment is to the nearest one, which you can calculate in two steps.\\nFirst, going back to customer Adams in column L, let’s calculate the minimum distance \\nto a cluster center in cell L38. That’s just:\\n=MIN(L34:L37) \\nAnd then to determine which cluster center matches that minimum distance, you can \\nuse the MATCH formula (see Chapter 1 for more details). Placing the following MATCH formula \\nin L39, you can determine which cell index in the range L34 to L37 counting up from 1 \\nmatches the minimum distance:\\n=MATCH(L38,L34:L37,0)\\nIn this case the minimum distance is a tie between all four clusters, so MATCH picks the \\nﬁ rst (L34) by returning index 1 (see Figure 2-16).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 67, 'page_label': '46'}, page_content='46 Data Smart\\nYou can drag these two formulas across the sheet through DG38:DG39 as well. Add \\nMinimum Cluster Distance and Assigned Cluster in Column G as labels for rows 38 and \\n39 just to keep things organized. \\nFigure 2-16: Cluster matches added into the sheet \\nSolving for the Cluster Centers\\nYou now have distance calculations and cluster assignments in the spreadsheet. To set the \\ncluster centers to their best locations, you need to ﬁ nd the values in columns H through \\nK that minimize the total distance between the customers and their assigned clusters \\ndenoted on row 39 beneath each customer. \\nAnd if you read Chapter 1, you know exactly what to think when you hear the word \\nminimize: This is an optimization step, and an optimization step means using Solver.\\nIn order to use Solver, you need an objective cell, so in cell A36, let’s sum up all the \\ndistances between customers and their cluster assignments:\\n=SUM(L38:DG38)\\nThis sum of customers’ distances from their closest cluster center is exactly the objec-\\ntive function encountered earlier when clustering on the McAcne Middle School dance'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 68, 'page_label': '47'}, page_content='47Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\nﬂ oor. But Euclidean distance with its squares and square roots is crazy non-linear (read \\n“wicked non-linear” if you live in Massachusetts), so you need to use the evolutionary \\nsolving method instead of the simplex method to set the cluster centers. \\nIn Chapter 1, you used the simplex algorithm. Simplex is faster than other methods \\nwhen it’s allowable, but it’s not possible when you’re squaring, square rooting, or other-\\nwise, taking non-linear functions of your decisions. Likewise, OpenSolver (introduced \\nin Chapter 1), which uses an implementation of simplex on steroids is of no use here.\\nIn this case, the evolutionary algorithm built into Solver uses a combination of random \\nsearch and good solution “breeding” to ﬁ nd good solutions similarly to how evolution \\nworks in biological contexts.\\nNOTE\\nFor a full treatment of optimization, see Chapter 4.\\nNotice that you have everything you need to set up a problem in Solver:\\n• Objective: Minimize the total distances of customers from their cluster cen-\\nters (A36).\\n• Decision variables : The deal values of each row within the cluster center \\n(H2:K33).\\n• Constraints: Cluster centers should have values somewhere between 0 and 1.\\nOpen Solver and hammer in the requirements. You’ll set Solver to minimize A36 by \\nchanging H2:K33 with the constraint that H2:K33 be <= 1 just like all the deal vectors. \\nMake sure that the variables are checked as non-negative and that the evolutionary solver \\nis chosen. See Figure 2-17.\\nAlso, setting these clusters isn’t a cakewalk for Solver, so you should beef up some of \\nthe evolutionary solver’s options by pressing the options button within the Solver win-\\ndow and toggling over to the evolutionary tab. It’s useful to bump up the Maximum Time \\nWithout Improvement parameter somewhere north of 30 seconds, depending on how long \\nyou want to wait for the Solver to ﬁ nish. In Figure 2-18, I’ve set mine to 600 seconds (10 \\nminutes). That way, I can set the Solver to run and go to dinner. And if you ever want to \\nkill Solver early, just press Escape and then exit with the best solution it’s found so far.\\nIf you’re curious, the inner workings of the evolutionary solver are covered in greater \\ndetail in Chapter 4 and at \\nhttp://www.solver.com.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 69, 'page_label': '48'}, page_content='48 Data Smart\\nFigure 2-17: The Solver setup for 4-means clustering \\nFigure 2-18: The evolutionary solver options tab\\nPress Solve and watch Excel do its thing until the evolutionary algorithm converges.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 70, 'page_label': '49'}, page_content='49Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\nMaking Sense of the Results\\nOnce Solver gives you the optimal cluster centers, the fun starts. You get to mine the \\ngroups for insight! So in Figure 2-19, you can see that Solver calculated an optimal total \\ndistance of 140.7, and the four cluster centers, thanks to the conditional formatting, all \\nlook very diff erent.\\nNote that your cluster centers may look diff erent from the spreadsheet provided with \\nthe book, because the evolutionary algorithm employs random numbers and does not \\ngive the same answer each time. The clusters may be fundamentally diff  erent or, more \\nlikely, they may be in a diff erent order (for example, my Cluster 1 is very close to your \\nCluster 4, and so on).\\nBecause you pasted the deal descriptions in columns B through G when you set up the \\ntab, you can read off  the details of the deals in Figure 2-19 that seem important to the \\ncluster centers.\\nFigure 2-19: The four optimal cluster centers \\nFor Cluster 1 in column H, the conditional formatting calls out deals 24, 26, 17, and to \\na lesser degree, 2. Reading through the details of those deals, the main thing they have \\nin common: They’re all Pinot Noir. \\nIf you look at column I, the green cells all have a low minimum quantity in common. \\nThese are the buyers who don’t want to have to buy in bulk to get a deal.\\nBut I’ll be honest; the last two cluster centers are kind of hard to interpret. Well, how \\nabout instead of interpreting the cluster center, you investigate the members of the cluster \\nand determine which deals they like? That might be more elucidating.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 71, 'page_label': '50'}, page_content=\"50 Data Smart\\nGetting the Top Deals by Cluster\\nSo instead of looking at which dimensions are closer to 1 for a cluster center, let’s check \\nwho is assigned to each cluster and which deals they prefer.\\nTo do this, let’s start by making a copy of the Off erInformation tab and calling it 4MC – \\nTopDealsByCluster. On this new tab, label columns H through K as 1, 2, 3, and 4 (see \\nFigure 2-20).\\nFigure 2-20: Setting up a tab to count popular deals by cluster\\nBack on tab 4MC, you have cluster assignments listed (1-4) on row 39. All you need to \\ndo to get deal counts by cluster is check the column title on tab 4MC – TopDealsByCluster \\nin columns H through K, see who on 4MC was assigned to that cluster using row 39, and \\nthen sum up their values for each deal row. That’ll give you the total customers from a \\ngiven cluster that took a deal.\\nStart with cell H2, that is, the count of customers in Cluster 1 who took off  er #1, the \\nJanuary Malbec off er. You want to sum across L2:DG2 on the 4MC tab but only for those \\ncustomers who are in Cluster 1, and that is a classic use case for the \\nSUMIF formula. The \\nformula looks like this:\\n=SUMIF('4MC'!$L$39:$DG$39,'4MC - TopDealsByCluster'!H$1,'4MC'!$L2:$DG2)\\nThe way the SUMIF statement works is that you provide it with some values to check \\nin the ﬁ rst section '4MC'!$L$39:$DG$39, which are checked against the 1 in the column \\nheader ('4MC - TopDealsByCluster'!H$1), and then for any match, you sum up row 2 by \\nspecifying '4MC'!$L2:$DG2 in the third section of the formula.\\nNote that you’ve used absolute references (the $ in the formula) in front of everything \\nin the cluster assignment row, in front of the row number for our column headers, and \\nin front of the column letter for our deals taken. By making these references absolute, \\nyou can then drag this formula through range H2:K33 to get deal counts for every cluster \\ncenter and deal combination, as pictured in Figure 2-21. You can place some conditional \\nformatting on these columns to make them more readable.\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 72, 'page_label': '51'}, page_content='51Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\nBy selecting columns A through K and auto-ﬁ  ltering (see Chapter 1), you can make \\nthis data sortable. Sorting from high to low on column H, you can then see which deals \\nare most popular within Cluster 1 (see Figure 2-22).\\nJust as noted earlier, the four top deals for this cluster are all Pinot. These folks have \\nwatched Sideways one too many times. When you sort on Cluster 2, it becomes abundantly \\nclear that these are the low volume buyers (see Figure 2-23).\\nBut when you sort on Cluster 3, things aren’t quite as clear. There are more than a hand-\\nful of top deals; the drop-off  between in deals and out deals is not as stark. But the most \\npopular ones seem to have a few things in common—the discounts are quite good, ﬁ  ve \\nout of the top six deals are bubbly in nature, and France is in three of the top four deals. \\nBut nothing is conclusive (see Figure 2-24).\\nAs for Cluster 4, these folks really loved the August Champaign deal for whatever rea-\\nson. Also, ﬁ ve out of the top six deals are from France, and nine of the top 10 deals are \\nhigh volume (see Figure 2-25). Perhaps this is the French-leaning high volume Cluster? \\nThe overlap between clusters 3 and 4 is somewhat troubling.\\nThis leads to a question: Is 4 the right number for k in k-means clustering? Perhaps \\nnot. But how do you tell?\\nFigure 2-21: Totals of each deal taken broken out by cluster\\nFigure 2-22: Sorting on Cluster 1—Pinot, Pinot, Pinot!'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 73, 'page_label': '52'}, page_content='52 Data Smart\\nFigure 2-23: Sorting on Cluster 2—small-timers\\nFigure 2-24: Sorting on Cluster 3 is a bit of a mess'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 74, 'page_label': '53'}, page_content='53Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\nFigure 2-25: Sorting on Cluster 4—these folks just like Champagne in August? \\nThe Silhouette: A Good Way to Let Different K Values \\nDuke It Out\\nThere’s nothing wrong with just doing k-means clustering for a few values of k until you \\nﬁ nd something that makes intuitive sense to you. Of course, maybe the reason that a \\ngiven k doesn’t “read well” is not because k is wrong but because the off er information is \\nleaving something out that would help describe the clusters better.\\nSo is there another way (other than just eyeballing the clusters) to give a thumbs-up \\nor -down to a particular value of k? \\nThere is—by computing a score for your clusters called the silhouette. The cool thing \\nabout the silhouette is that it’s relatively agnostic to the value of k, so you can compare \\ndiff erent values of k using this single score.\\nThe Silhouette at a High Level: How Far Are Your Neighbors from You?\\nYou can compare the average distance between each customer and their friends in the \\ncluster they’ve been assigned to with the average distance to the customers in the cluster \\nwith the next nearest center.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 75, 'page_label': '54'}, page_content='54 Data Smart\\nIf I’m a lot closer to the people in my cluster than to the people in the neighboring \\ncluster, these folks are a good group for me, right? But what if the folks from the next \\nnearest cluster are nearly as close to me as my own clustered brethren? Well, then my \\ncluster assignment is a bit shaky, isn’t it? \\nA formal way to write this value is:\\n(Average distance to those in the nearest neighboring cluster – Average distance to those \\nin my cluster)/The maximum of those two averages\\nThe denominator in the calculation keeps the value between -1 and 1. \\nThink about that formula. As the residents of the next closet cluster get farther and \\nfarther away (more ill-suited to me), the value approaches 1. And if the two average dis-\\ntances are nearly the same? Then the value approaches 0.\\nTaking the average of this calculation for each customer gives you the silhouette. If the \\nsilhouette is 1, it’s perfect. If it’s 0, the clusters are rather ill suited. If it’s less than 0, lots \\nof customers are better off  hanging out in another cluster, which is the pits. \\nAnd for diff erent values of k, you can compare silhouettes to see if you’re improving.\\nTo see this concept more clearly, go back to the middle school dance example. Figure 2-26 \\nshows an illustration of the distance calculations used in forming the silhouette. Note \\nthat one of the chaperone’s distance from the other two chaperones is being compared \\nto the distances from the next nearest cluster, which is the ﬂ ock of middle school boys. \\nNow, the other two chaperones are by far closer than the herd of awkward teenagers, \\nso that would make the distance ratio calculation far greater than 0 for this chaperone.\\nFigure 2-26: The distances considered for a chaperone’s contribution to the silhouette calculation'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 76, 'page_label': '55'}, page_content='55Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\nCreating a Distance Matrix\\nIn order to implement the silhouette, there’s one major piece of data you need: the distance \\nbetween customers. And while cluster centers may move around, the distance between \\ntwo customers never changes. So you can just create a single Distances tab and use it \\nin all of your silhouette calculations no matter what value of k you use or where those \\ncenters end up.\\nLet’s start by creating a blank sheet called Distances and pasting in customers across \\nthe top and down the rows. A cell in the matrix will hold the distance between the cus-\\ntomer on the row and the customer on the column. To paste customers down the rows, \\ncopy H1:DC1 from the Matrix tab and use Paste Special to paste the values, making sure \\nto choose the Transpose option in the Paste Special window.  \\nYou need to keep track of where customers are on the Matrix tab, so number the cus-\\ntomers from 0 to 99 in both directions. Let’s put these numbers in column A and row 1, \\nso insert blank rows and columns to the left and above the names you’ve already pasted \\nby right-clicking column A and row 1 and inserting a new row 1 and a new column A. \\nNOTE\\nFYI, there are a lot of ways to put those 0–99 counts into Excel. For instance, you \\ncan type the ﬁ rst few in, 0, 1, 2, 3, and then highlight them and drag the bottom \\ncorner of the selection through the rest of the customers. Excel will understand and \\nextend the count. The resulting empty matrix is pictured in Figure 2-27.\\nConsider cell C3, which is the distance between Adams and Adams, in other words \\nbetween Adams and himself. This should be 0, right? You can’t get any closer to you \\nthan you! \\nSo how do you calculate that? Well, column H on the Matrix tab shows Adams’ deal \\nvector. To calculate the Euclidean distance between Adams and himself, it’s just column \\nH minus column H, square the diff erences, sum them up, and take the square root.\\nBut how do you drag that calculation around to every cell in the matrix? I’d hate to \\ntype them in manually. That’d take forever. What you need to use is the \\nOFFSET formula \\nin cell C3 (see Chapter 1 for an explanation of OFFSET).\\nThe OFFSET formula takes in an anchoring range of cells; in this case make it Adams’ \\ndeal vector Matrix!$H$2:$H$33, and moves the entire range a given number of rows and \\ncolumns in the direction you specify.\\nSo for instance, OFFSET(Matrix!$H$2:$H$33,0,0) is just Adams’ deal vector because \\nyou’re moving the original range 0 rows down and 0 columns to the right.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 77, 'page_label': '56'}, page_content='56 Data Smart\\nFigure 2-27: The bare bones Distances tab \\nBut OFFSET(Matrix!$H$2:$H$33,0,1) is Allen’s deal column. \\nOFFSET(Matrix!$H$2:$H$33,0,2) is Anderson, and so on.\\nAnd this is where those indices 0 – 99 in row 1 and column A are going to come in \\nhandy. For example:\\n{=SQRT(SUM((OFFSET(Matrix!$H$2:$H$33,0,Distances!C$1)-OFFSET(Matrix!$H$2:$\\nH$33,0,Distances!$A3))^2))}\\nThat’s the distance between Adams and himself. Note that you’re pulling Distances!C$1 \\nfor the column off set in the ﬁ rst deal vector and Distances!$A3 for the column off set in \\nthe second deal vector. \\nThat way, when you drag this calculation across and down in the sheet, everything \\nis anchored to Adams’ deal vector, but the OFFSET  formula shifts the vectors over the \\nappropriate amount using the indices in column A and row 1. This way, it will grab the \\nappropriate two deal vectors for the customers you care about. Figure 2-28 shows the \\nﬁ lled out distance matrix.\\nAlso, keep in mind that just like on tab 4MC, these distances are array formulas.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 78, 'page_label': '57'}, page_content=\"57Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\nFigure 2-28: The completed distance matrix \\nImplementing the Silhouette in Excel\\nAll right, now that you have a Distances tab, you can create another tab called 4MC \\nSilhouette for the ﬁ nal silhouette calculation.\\nTo start, let’s copy the customers and their community assignments from the 4MC tab \\nand Paste Special the customer names down column A and the assignments down B (don’t \\nforget to check that Transpose box in the Paste Special window).\\nNext, you can use the Distances tab to calculate the average distance between each \\ncustomer and those in a particular cluster. So label columns C through F Distance from \\nPeople in 1 through Distance from People in 4.\\nIn my workbook, Adams has been assigned to Cluster 2, so calculate in cell C2 the \\ndistance between him and all the customers in Cluster 1. You need to look up customers \\nand see which ones are in Cluster 1 and then average their distances from Adams on row \\n3 of the Distances tab.\\nSounds like a case for the \\nAVERAGEIF formula:\\n=AVERAGEIF('4MC'!$L$39:$DG$39,1,Distances!$C3:$CX3)\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 79, 'page_label': '58'}, page_content='58 Data Smart\\nAVERAGEIF checks the cluster assignments and matches them to Cluster 1 before aver-\\naging the appropriate distances from C3:CX3.\\nFor columns D through F, the formulas are the same except Cluster 1 is replaced with \\n2, 3, and 4 in the formula. You can then double-click these formulas to copy them to all \\ncustomers, yielding the table shown in Figure 2-29. \\nFigure 2-29: Average distance between each customer and the customers in every cluster \\nIn column G, you can calculate the closest group of customers using the MIN formula. \\nFor instance, for Adams, it’s simply:\\n=MIN(C2:F2) \\nAnd in column H, you can calculate the second closest group of customers using the \\nSMALL formula (the 2 in the formula is for second place):\\n=SMALL(C2:F2,2) \\nLikewise, you can calculate the distance to your own community members (which is \\nprobably the same as column G but not always) in column I as:\\n=INDEX(C2:F2,B2) \\nThe INDEX  formula is used to count over to the appropriate distance column in C \\nthrough F using the assignment value in B as an index.\\nAnd for the silhouette calculation, you also need the distance to the closest group of \\ncustomers who are not in your cluster, which is most likely column H but not always. To \\nget this in column J, you check your own cluster distance in I against the closest cluster \\nin G, and if they match, the value is H. Otherwise, it’s G.\\n=IF(I2=G2,H2,G2) \\nCopying all these values down, you’ll get the spreadsheet shown in Figure 2-30.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 80, 'page_label': '59'}, page_content='59Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\nFigure 2-30: Average distances to the folks in my own cluster and to the closest group whose cluster \\nI’m not in \\nOnce you’ve placed those values together, adding the silhouette values for a particular \\ncustomer in column K is simple:\\n=(J2-I2)/MAX(J2,I2) \\nYou can just copy that formula down the sheet to get these ratios for each customer. \\nYou’ll notice that for some customers, these values are closer to 1. For example, the \\nsilhouette value for Anderson in my clustering solution is 0.544 (see Figure 2-31). Not \\nbad! But for other customers, such as Collins, the value is actually less than 0, implying \\nthat all things being equal Collins would be better off  in his neighboring cluster than in \\nhis current one. Poor guy.\\nNow, you can average these values to get the ﬁ nal silhouette ﬁ gure. In my case, as shown \\nin Figure 2-31, it’s 0.1492, which seems a lot closer to 0 than 1. That’s disheartening, but \\nnot entirely surprising. After all, two out of four of the clusters were very shaky when you \\ntried to interpret them with the deal descriptions.\\nFigure 2-31: The ﬁ nal silhouette for 4-means clustering'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 81, 'page_label': '60'}, page_content='60 Data Smart\\nOkay. Now what?\\nSure, the silhouette is 0.1492. But what does that mean? How do you use it? You try \\nother values of k! Then you can use the silhouette to see if you’re doing better.\\nHow about Five Clusters?\\nTry bumping k up to 5 and see what happens.\\nHere’s the good news: Because you’ve already done four clusters, you don’t have to start \\nthe spreadsheets from scratch. You don’t have to do anything with the Distances sheet at \\nall. That one’s good to go.\\nYou start by creating a copy of the 4MC tab and calling it 5MC. All you need to do is \\nadd a ﬁ fth cluster to the sheet and work it into your calculations.\\nFirst, let’s right-click column L and insert a new column called Cluster 5. You also need \\nto insert a Distance to Cluster 5 row by right-clicking row 38 and selecting Insert. You can \\ncopy down the Distance to Cluster 4 row into row 38 and change column K to L, to create \\nthe Distance to Cluster 5 row. As for the Minimum Cluster Distance and Assigned Cluster \\nrows, references to row 37 should be revised to 38 to include the new cluster distance.\\nYou’ll end up with the sheet pictured in Figure 2-32. \\nFigure 2-32: The 5-means clustering tab\\nSolving for Five Clusters\\nOpening up Solver, you need only change $H$2:$K$33 to $H$2:$L$33 in both the decision vari-\\nables and constraints sections to include the new ﬁ fth cluster. Everything else stays the same.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 82, 'page_label': '61'}, page_content='61Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\nPress Solve and let this new problem run. \\nIn my run, the Solver terminated with a total distance of 135.1, as shown in Figure 2-33.\\nFigure 2-33: The optimal 5-means clusters\\nGetting the Top Deals for All Five Clusters\\nAll right. Let’s see how you did. \\nYou can create a copy of the 4MC – TopDealsByCluster tab and rename it 5MC – \\nTopDealsByCluster, but you’ll need to revise a few of the formulas to get it to work.\\nFirst of all, you need to make sure that this worksheet is ordered by Off er # in column \\nA. Then label column L with a 5 and drag the formulas from K over to L. You should also \\nhighlight columns A through L and reapply the auto-ﬁ  ltering to make Cluster 5’s deal \\npurchases sortable.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 83, 'page_label': '62'}, page_content=\"62 Data Smart\\nEverything on this sheet is currently pointing to tab 4MC, so it’s time to break out the \\nol’ Find and Replace. The cluster assignments on tab 5MC are shifted one row down and \\none column to the right, so the reference to \\n'4MC'!$L$39:$DG$39 in the SUMIF formulas \\nshould become '5MC'!$M$40:$DH$40 . As shown in Figure 2-34, you can use Find and \\nReplace to change this.\\nFigure 2-34  Replacing 4-means cluster assignments with 5-means cluster assignments \\nNOTE\\nKeep in mind that your results will diff er from mine due to the evolutionary solver. \\nSorting on Cluster 1, you clearly have your Pinot Noir cluster again (see Figure 2-35). \\nFigure 2-35: Sorting on Cluster 1—Pinot Noir out the ears\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 84, 'page_label': '63'}, page_content='63Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\nCluster 2 is the low-volume buyer cluster (see Figure 2-36).\\nFigure 2-36: Sorting on Cluster 2—small quantities only, please \\nAs for Cluster 3, this one hurts my head. It seems only to be a South African Espumante \\nthat’s important for some reason (Figure 2-37).\\nFigure 2-37: Sorting on Cluster 3—is Espumante that important? \\nThe Cluster 4 customers are interested in high volume, primarily French deals with \\ngood discounts. There may even be a propensity toward sparkling wines. This cluster is \\ntough to read; there’s a lot going on (see Figure 2-38).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 85, 'page_label': '64'}, page_content='64 Data Smart\\nFigure 2-38: Sorting on Cluster 4—all sorts of interests  \\nSorting on Cluster 5 gives you results similar to Cluster 4, although high volume and \\nhigh discounts seem to be the primary drivers (see Figure 2-39).\\nComputing the Silhouette for 5-Means Clustering\\nYou may be wondering whether ﬁ ve clusters did any better than four. From an eyeball \\nperspective, there doesn’t seem to be a whole lot of diff erence. Let’s compute the silhouette \\nfor ﬁ ve clusters and see what the computer thinks.\\nStart by making a copy of 4MC Silhouette and renaming it 5MC Silhouette. Next, right-\\nclick column G, insert a new column, and name it Distance From People in 5. Drag the \\nformula from F2 over into G2, change the cluster check from 4 to 5, and then double-click \\nthe cell to shoot it down the sheet.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 86, 'page_label': '65'}, page_content=\"65Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\nFigure 2-39: Sorting on Cluster 5—high volume \\nIdentical to the previous section, you’ll need to Find and Replace '4MC'!$L$39:$DG$39 \\nwith '5MC'!$M$40:$DH$40. \\nIn cells H2, I2, and J2, you should include distances to folks in Cluster 5 in your cal-\\nculations, so any ranges that stop at F2 should be expanded to include G2. You can then \\nhighlight H2:J2 and double-click the bottom right to send these updated calculations \\ndown the sheet.\\nLastly, you need to copy and Paste Special values from the cluster assignments on row \\n40 of the 5MC tab into column B on the 5MC Silhouette tab. This means you have to check \\nthe Transpose button when using Paste Special.\\nOnce you’ve revised the sheet, you should get something like what’s pictured in \\nFigure 2-40.\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 87, 'page_label': '66'}, page_content='66 Data Smart\\nFigure 2-40:  The silhouette for 5-means clustering \\nWell, this is depressing, isn’t it? The silhouette isn’t all that diff  erent. At 0.134, it’s \\nactually a little worse! But that’s not much of a surprise after mining the clusters. In both \\ncases, you had three clusters that really made sense. The others were noisy. Maybe you \\nshould go the other direction and try k=3? If you want to give this a shot, I leave it as an \\nexercise for you to try on your own.\\nInstead, let’s give a little thought to what may be going wrong here to cause these noisy, \\nperplexing clusters. \\nK-Medians Clustering and Asymmetric Distance \\nMeasurements\\nUsually doing vanilla k-means clustering with Euclidean distance is just ﬁ  ne, but you’ve \\nrun into some problems here that many who do clustering on sparse data (whether that’s \\nin retail or text classiﬁ cation or bioinformatics) often encounter.\\nUsing K-Medians Clustering\\nThe ﬁ rst obvious problem is that your cluster centers are decimals even though each \\ncustomer’s deal vector is made of solid 0s and 1s. What does 0.113 of a deal really mean? \\nI want cluster centers that commit to a deal or don’t!\\nIf you modify the clustering algorithm to use only values present in the customers’ deal \\nvectors, this is called K-medians clustering, rather than K-means clustering.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 88, 'page_label': '67'}, page_content='67Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\nAnd if you wanted to stick with Euclidean distance, all you’d need to do is add a binary \\nconstraint, (bin) in Solver to all of your cluster centers. \\nBut if you make your cluster centers binary, is Euclidean distance what you want?\\nGetting a More Appropriate Distance Metric\\nWhen folks switch from k-means to k-medians, they typically stop using Euclidean dis-\\ntance and start using something called Manhattan distance. \\nAlthough a crow can ﬂ y from point A to B in a straight line, a cab in Manhattan has to \\nstay on the grid of straight streets; it can only go north, south, east, and west. So while in \\nFigure 2-13, you saw that the distance between a middle school dancer and their cluster \\ncenter was approximately 4.47, their Manhattan distance was 6 feet (that’s 4 feet down \\nplus 2 feet across).\\nIn terms of binary data, like the purchase data, the Manhattan distance between a \\ncluster center and a customer’s purchase vector is just the count of the mismatches. If the \\ncluster center has a 0 and I have a 0, in that direction there’s a distance of 0, and if you \\nhave mismatched 0 and 1, you have a distance of 1 in that direction. Summing them up, \\nyou get the total distance, which is just the number of mismatches. When working with \\nbinary data like this, Manhattan distance is also commonly called Hamming distance.\\nDoes Manhattan Distance Solve the Issues?\\nBefore you dive headﬁ rst into doing k-medians clustering using Manhattan distance, stop \\nand think about the purchase data. \\nWhat does it mean when customers take a deal? It means they really wanted that \\nproduct!\\nWhat does it mean when customers don’t take a deal? Does it mean that they didn’t \\nwant the product as much as they did want the one they bought? Is a negative signal as \\nstrong as a positive one? Perhaps they like Champagne but already have a lot in stock. \\nMaybe they just didn’t see your e-mail newsletter that month. There are a lot of reasons \\nwhy someone doesn’t take an action, but there are few reasons why someone does.\\nIn other words, you should care about purchases, not non-purchases.\\nThe fancy way to say this is that there’s an “asymmetry” in the data. The 1s are worth \\nmore than the 0s. If a customer matches another customer on three 1s, that’s more impor-\\ntant than matching some other customer on three 0s. What stinks though is that while \\nthe 1s are so important, there are very few of them in the data—hence, the term “sparse.”\\nBut think about what it means for a customer to be close to a cluster center from a \\nEuclidean perspective. If I have a customer with a 1 for one deal and a 0 for another, both \\nof those are just as important in calculating whether a customer is near a cluster center.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 89, 'page_label': '68'}, page_content='68 Data Smart\\nWhat you need is an asymmetric distance calculation. And for binary encoded transac-\\ntional data, like these wine purchases, there are a bunch of good ones.\\nPerhaps the most widely used asymmetric distance calculation for 0-1 data is something \\ncalled cosine distance.\\nCosine Distance Isn’t Scary Despite the Trigonometry\\nThe easiest way to explain cosine distance is to explain its opposite: cosine similarity.\\nSay you had a couple of two-dimensional binary purchase vectors (1,1) and (1,0). In the \\nﬁ rst vector, both products were purchased, whereas in the second, only the ﬁ rst product \\nwas purchased. You can visualize these two purchase vectors in space and see that they \\nhave a 45-degree angle between them (see Figure 2-41). Go on, break out the protractor \\nand check it.\\nYou can say that they have a cosine similarity then of cos(45 degrees) = 0.707. But why?\\nIt turns out the cosine of an angle between two binary purchase vectors is equal to: \\nThe count of matched purchases in the two vectors divided by the product of the square \\nroot of the number of purchases in the first vector times the square root of the number of \\npurchases in the second vector.\\nIn the case of the two vectors (1,1) and (1,0), they have one matched purchase, so the \\ncalculation is 1 divided by the square root of 2 (two deals taken), times the square root \\nof one deal taken. And that’s 0.707 (see Figure 2-41).\\nWhy is this calculation so cool?\\nThree reasons:\\n• The numerator in the calculation counts numbers of matched purchases only, so \\nthis is an asymmetric measure, which is what you’re looking for.\\n• By dividing through by the square root of the number of purchases in each vec-\\ntor, you’re accounting for the fact that a vector where everything is purchased, call \\nit a promiscuous purchase vector, is farther away from another vector than one \\nwho matches on the same deals and has not taken as many other deals. You want \\nto match up vectors whose taste matches, not where one vector encompasses the \\ntaste of another.\\n• For binary data, this similarity value ranges between 0 and 1, where two vectors \\ndon’t get a 1 unless their purchases are identical. This means that 1 – cosine similarity'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 90, 'page_label': '69'}, page_content='69Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\ncan be used as a distance metric called cosine distance, which also ranges between \\n0 and 1.\\n(1,1)\\n(1,0)45 degree angle\\ncos(45°) ==  .7071 matched purchase\\n2 purchases 1 purchase\\nFigure 2-41:  An illustration of cosine similarity on two binary purchase vectors \\nPutting It All in Excel\\nIt’s time to give k-medians clustering with cosine distance in Excel a shot. \\nNOTE\\nClustering with cosine distance is also sometimes called spherical k-means. In Chapter 10, \\nyou’ll look at spherical k-means in R.\\nFor consistency’s sake, continue using k = 5.\\nStart by making a copy of the 5MC tab and naming it 5MedC. Since the cluster centers \\nneed to be binary, you might as well delete what Solver left in there.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 91, 'page_label': '70'}, page_content='70 Data Smart\\nThe only items you need to change (other than adding the binary constraint in Solver \\nfor k-medians) are the distance calculations on rows 34 through 38. Start in cell M34, \\nwhich is the distance between Adams and the center of Cluster 1.\\nTo count the deal matches between Adams and Cluster 1, you need to take a \\nSUMPRODUCT \\nof the two columns. If either or both have 0s, they get nothing for that row, but if both \\nhave a 1, that match will get totaled by the \\nSUMPRODUCT (since 1 times 1 is 1 after all).\\nAs for taking the square root of the number of deals taken in a vector, that’s just a SQRT \\nlaid on a SUM of the vector. Thus, the overall distance equation can be written as:\\n=1-SUMPRODUCT(M$2:M$33,$H$2:$H$33)/\\n    (SQRT(SUM(M$2:M$33))*SQRT(SUM($H$2:$H$33)))\\nNote the 1– at the beginning of the formula, which changes from cosine similarity to \\ndistance. Also, unlike with Euclidean distance, the cosine distance calculation does not \\nrequire the use of array formulas.\\nHowever, when you stick this into cell M34, you should add an error check in case the \\ncluster center is all 0s:\\n=IFERROR(1-SUMPRODUCT(M$2:M$33,$H$2:$H$33)/\\n    (SQRT(SUM(M$2:M$33))*SQRT(SUM($H$2:$H$33))),1)\\nAdding the IFERROR formula prevents you from having a division by 0 situation. If for \\nsome reason Solver picks an all-0s cluster center, then you can consider that center to \\nhave a distance of 1 from everything instead (1 being the largest possible distance in this \\nbinary setup).\\nYou can then copy M34 down through M38 and change the references from column \\nH to I, J, K, and L respectively. Just like in the Euclidean distance case, you use absolute \\nreferences ($) in the formula so that you can drag it across without the cluster center \\ncolumns changing.\\nThis gives you a 5MedC sheet (see Figure 2-42) that’s remarkably similar to the earlier \\n5MC tab.\\nNow, to ﬁ nd the clusters, you need to open Solver and change the \\n<= 1 constraint for \\nH2:L33 to instead read as a binary or bin constraint.\\nPress Solve. You can take a load off  for a half hour while the computer ﬁ nds the optimal \\nclusters. Now, you’ll notice visually that the cluster centers are all binary, so likewise the \\nconditional formatting goes to two shades, which is much more stark.\\nThe Top Deals for the 5-Medians Clusters\\nWhen Solver completes, you end up with ﬁ ve cluster centers, each which have a smattering \\nof 1s, indicating which deals are preferred by that cluster. In my Solver run, I ended up with \\nan optimal objective value of 42.8, although yours may certainly vary (see Figure 2-43).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 92, 'page_label': '71'}, page_content='71Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\nFigure 2-42:  The 5MedC tab not yet optimized \\nFigure 2-43:  The ﬁ ve-cluster medians'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 93, 'page_label': '72'}, page_content='72 Data Smart\\nLet’s make sense of these clusters using the same deal counting techniques you’ve \\nused in k-means. To do so, the ﬁ  rst thing you need to do is make a copy of the 5MC – \\nTopDealsByCluster tab and rename it 5MedC – TopDealsByCluster.\\nOn this tab, all you need to do to make it work is to ﬁ nd and replace 5MC with 5MedC. \\nBecause the layout of rows and columns between these two sheets is identical, all the \\ncalculations carry over once the sheet reference is changed.\\nNow, your clusters may be slightly diff erent than mine in both order and composition \\ndue to the evolutionary algorithm, but hopefully not substantively so. Let’s walk through \\nmy clusters one at a time to see how the algorithm has partitioned the customers.\\nSorting on Cluster 1, it’s apparent that this is the low-volume cluster (see Figure 2-44). \\nFigure 2-44:  Sorting on Cluster 1—low-volume customers \\nCluster 2 has carved out customers who only buy sparkling wine. Champagne, Prosecco, \\nand Espumante dominate the top 11 spots in the cluster (see Figure 2-45). It’s interesting \\nto note that the k-means approach did not so clearly demonstrate the bubbly cluster with \\nk equal to 4 or 5. \\nCluster 3 is our Francophile cluster. The top ﬁ ve deals are all French (see Figure 2-46). \\nDon’t they know California wines are better?'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 94, 'page_label': '73'}, page_content='73Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\nFigure 2-45:  Sorting on Cluster 2—not all who sparkle are vampires \\nFigure 2-46:  Sorting on cluster—Francophiles'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 95, 'page_label': '74'}, page_content='74 Data Smart\\nAs for Cluster 4, all the deals are high volume. And the top rated deals are all well \\ndiscounted and not past their peak (Figure 2-47).\\nFigure 2-47: Sorting on Cluster 4—high volume for 19 deals in a row\\nCluster 5 is the Pinot Noir cluster once again (see Figure 2-48).\\nThat feels a lot cleaner doesn’t it? That’s because in the k-medians case, using an asym-\\nmetric distance measure like cosine distance, you can cluster customers based on their \\ninterests more than their disinterests. And that’s really what you care about.\\nWhat a diff erence a distance measure makes!\\nSo now you can take these ﬁ ve cluster assignments, import them back into MailChimp\\n.com as a merge ﬁ eld on the list of e-mails, and use the values to customize your e-mail \\nmarketing per cluster. This should help you better target customers and drive sales.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 96, 'page_label': '75'}, page_content='75Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \\nFigure 2-48:  Sorting on cluster 5—mainlining Pinot Noir \\nWrapping Up\\nThis chapter covered all sorts of good stuff . To summarize, you looked at:\\n• Euclidean distance\\n• k-means clustering using Solver to optimize the centers\\n• How to understand the clusters once you have them\\n• How to calculate the silhouette of a given k-means run\\n• K-medians clustering\\n• Manhattan/Hamming distance\\n• Cosine similarity and distance\\nIf you made it through the chapter, you should feel conﬁ  dent not only about how to \\ncluster data, but also which questions can be answered in business through clustering, \\nand how to prepare your data to make it ready to cluster.\\nK-means clustering has been around for decades and is deﬁ nitely the place to start for \\nanyone looking to segment and pull insights from their customer data. But it’s not the \\nmost “current” clustering technique. In Chapter 5, you’ll explore using network graphs'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 97, 'page_label': '76'}, page_content='76 Data Smart\\nto ﬁ nd communities of customers within this same dataset. You’ll even take a ﬁ  eld trip \\noutside of Excel, very brieﬂ y, to visualize the data.\\nIf you want to go further with k-means clustering, keep in mind that vanilla Excel \\ntops out at 200 decision variables in Solver, so you need to upgrade to a better non-linear \\nSolver (for example Premium Solver available at Solver.com or just migrate over to using \\nthe non-linear Solver in LibreOffi  ce) to cluster on data with many deal dimensions and \\na high value of k.\\nMost statistical software off ers clustering capabilities. For example, R comes with the \\nkmeans() function; however, the capabilities of the fastcluster package, which includes \\nk-medians and a variety of distance functions, is preferable. In Chapter 10, you’ll look at \\nthe \\nskmeans package for performing sphe rical k-means.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 98, 'page_label': '77'}, page_content='3\\nI\\nn the previous chapter, you hit the ground running with a bit of unsupervised learning. \\nYou looked at k-means clustering, which is like the chicken nugget of the data mining \\nworld: simple, intuitive, and useful. Delicious too. \\nIn this chapter you’re going to move from unsupervised into supervised artiﬁ cial intel-\\nligence models by training up a naïve Bayes model, which is, for lack of a better metaphor, \\nalso a chicken nugget, albeit a supervised one.\\nAs mentioned in Chapter 2, in supervised artiﬁ  cial intelligence, you “train” a model \\nto make predictions using data that’s already been classiﬁ  ed. The most common use of \\nnaïve Bayes is for document classiﬁ cation. Is this e-mail spam or ham? Is this tweet happy \\nor angry? Should this intercepted satellite phone call be classiﬁ ed for further investigation \\nby the spooks? You provide “training data,” i.e. classiﬁ ed examples, of these documents \\nto the training algorithm, and then going forward, the model can classify new documents \\ninto these categories using its knowledge.\\nThe example you’ll work through in this chapter is one that’s close to my own heart. \\nLet me explain. \\nWhen You Name a Product Mandrill, You’re Going to \\nGet Some Signal and Some Noise\\nRecently the company I work for, MailChimp, started a new product called Mandrill.com. \\nIt has the most frightening logo I’ve seen in a while (see Figure 3-1).\\nMandrill is a transactional e-mail product for software developers who want their apps \\nto send one-off  e-mails, receipts, password resets, and anything else that’s one-to-one. \\nBecause it allows you to track opens and clicks of individual transactional e-mails, you \\ncan even wire it into your personal e-mail account and track whether your relatives are \\nactually viewing those pictures of your cat you keep sending them. (Take it from a data \\nscientist—they’re not.) \\nNaïve Bayes and the \\nIncredible Lightness of \\nBeing an Idiot'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 99, 'page_label': '78'}, page_content='Data Smart78\\nFigure 3-1: The trance-inducing Mandrill logo\\nBut ever since Mandrill was released, one thing has perpetually annoyed me. Whereas \\na “MailChimp” is a something we invented, a mandrill, also a primate, has been kicking \\nit here on earth for a while. And they’re quite popular. Heck, Darwin called the mandrill’s \\ncolorful butt “extraordinary.” \\nThat means that if you go onto Twitter and want to look at any tweets mentioning the prod-\\nuct Mandrill, you get something like what you see in Figure 3-2. The bottom tweet is about \\na new module hooking up the Perl programming language to Mandrill. That one is relevant. \\nBut the two above it are about Spark Mandrill from the Super Nintendo game Megaman X \\nand a band called Mandrill.\\nFigure 3-2: Three tweets, only one of which matters'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 100, 'page_label': '79'}, page_content='79Naïve Bayes and the Incredible Lightness of Being an Idiot\\nYuck. \\nEven if you enjoyed Megaman X when you were a teen, many of these tweets aren’t \\nrelevant to your search. Indeed, there are more tweets about the band plus the game plus \\nthe animal plus other Twitter users with “mandrill” in their handle combined than there \\nare about Mandrill.com. That’s a lot of noise.\\n So is it possible to create a model that can distinguish the signal from the noise? Can \\nan AI model alert you only to the tweets about the e-mail product Mandrill?\\nThis then is a classic document classiﬁ cation problem. If a document, such as a Mandrill \\ntweet, can belong to multiple classes (about Mandrill.com, about other things), which \\nclass should it go in?\\nAnd the most typical way of attacking this problem is using a bag of words model in \\ncombination with a naïve Bayes classiﬁ  er. A bag of words model treats documents as a \\ncollection of unordered words. “John ate Little Debbie” is the same as “Debbie ate Little \\nJohn”; they both are treated as a collection of words {“ate,” “Debbie,” “John,” “Little”}.\\nA naïve Bayes classiﬁ er takes in a training set of these bags of words that are already \\nclassiﬁ ed. For instance, you might feed it some bags of about-Mandrill-the-app words and \\nsome bags of about-other-mandrills words and train it to distinguish between the two. \\nThen in the future, you can feed it an unknown bag of words, and it’ll classify it for you.\\nSo that’s what you’re going to build in this chapter—a naïve Bayes document classiﬁ er \\nthat treats the Mandrill tweets as bags of words and gives you back a classiﬁ  cation. And \\nit’s going to be really fun. Why?\\nBecause naïve Bayes is often called “idiot’s Bayes.” As you’ll see, you get to make lots of \\nsloppy, idiotic assumptions about your data, and it still works! It’s like the splatter-paint \\nof AI models, and because it’s so simple and easy to implement (it can be done in 50 lines \\nof code), companies use it all the time for simple classiﬁ cation jobs. You can use it to clas-\\nsify company e-mails, customer support transcripts, AP wire articles, the police blotter, \\nmedical documents, movie reviews, whatever!\\nNow, before you get started implementing this thing in Excel (which is really quite \\neasy), you’re going to have to learn some probability theory. My apologies. If you get lost \\nin the math, press on to the implementation, and you’ll see how simply it all shakes out.\\nThe World’s Fastest Intro to Probability Theory\\nIn the next couple sections I’m going to use the notation p() to talk about probability. \\nFor instance: \\n  p (Michael Bay’s next ﬁ lm will be terrible) = 1\\n  p (John Foreman will ever go vegan) = 0.0000001'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 101, 'page_label': '80'}, page_content='Data Smart80\\nSorry, it’s extremely unlikely that I’ll ever give up Conecuh smoked sausage—the one \\nthing I like that comes out of Alabama.\\nTotaling Conditional Probabilities\\nNow, the previous two examples are simple probabilities, but what you’re going to be work-\\ning with a lot in this chapter are conditional probabilities. Here’s a conditional probability:\\n  p (John Foreman will go vegan | you pay him $1B) = 1\\nAlthough the odds of me ever going vegan are extremely low, the probability of me \\ngoing vegan given you pay me a billion dollars is 100 percent. That vertical bar | in the \\nstatement is used to separate the event from what it’s being conditioned on.\\nHow do you reconcile the 0.0000001 overall vegan probability with the virtually assured \\nconditional probability? Well, you can use the law of total probability. The way it works is \\nthe probability of my going vegan equals the sum of the probabilities of my going vegan \\nconditioned on all possible cases times their probability of happening:\\n  p (vegan) = p($1B) * p(vegan | $1B) + p(not $1B)* p(vegan | not $1B) = .0000001\\nThe overall probability is the weighted sum of all conditional probabilities multiplied \\nby the probability of that condition. And the probability of the condition that you will \\npay me one billion dollars is 0 (pretty sure that’s a safe assumption). Which means that \\np(not $1B) is 1, so you get:\\n  p (vegan) = 0*p(vegan | $1B) + 1* p(vegan | not \\n$1B) = .0000001 \\n  p (vegan) = 0*1 + 1*.0000001 = .0000001 \\nJoint Probability, the Chain Rule, and Independence\\nAnother concept in probability theory is that of the joint probability, which is just a fancy \\nway of saying “and.” Think back to your SAT days.\\nHere’s the probability that I’ll eat Taco Bell for lunch today:\\n  p (John eats Taco Bell) = .2\\nIt’s a once-a-week thing for me. And here’s the probability that I’ll listen to some cheesy \\nelectronic music today:\\n  p (John listens to cheese) = .8\\nIt’s highly likely.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 102, 'page_label': '81'}, page_content='81Naïve Bayes and the Incredible Lightness of Being an Idiot\\nSo what are the odds that I do both today? That’s called the joint probability, and it’s \\nwritten as follows:\\n  p (John eats Taco Bell, John listens to cheese)\\nYou just separate the two events with a comma. \\nNow, in this case these events are independent . That means that my listening doesn’t \\naff ect my eating and vice versa. Given this independence, you can then multiply these \\ntwo probabilities together to get their joint likelihood:\\n  p (John eats Taco Bell, John listens to cheese) = .2 * .8 = .16\\nThis is sometimes called the multiplication rule of probability. Note that the joint prob-\\nability is less than the probability of either occurring, which makes perfect sense. Winning \\nthe lottery on the day you get struck by lightning is far less likely to happen than either \\nevent alone. \\nOne way to see this is through the chain rule of probability, which goes like this:\\n  p (John eats Taco Bell, John listens to cheese) = p(John eats Taco Bell) * \\np(John listens \\nto cheese | John eats Taco Bell)\\nThe joint probability is the probability of one event happening times the probability of \\nthe other event happening given that the ﬁ rst event happens. But since these two events \\nare independent, the condition doesn’t matter. I’m going to listen to cheesy techno the \\nsame amount regardless of lunch, so:\\n  p (John listens to cheese | John eats Taco Bell) = p(John listens to cheese)\\nThat reduces the chain rule setup to simply:\\n  p (John eats Taco Bell, John listens to cheese) = p(John eats Taco Bell) * p(John listens \\nto cheese) = .16\\nWhat Happens in a Dependent Situation?\\nI’ll introduce another probability, the probability that I listen to Depe che Mode today:\\n  p (John listens to Depeche Mode) = .3\\nThere’s a 30 percent chance I’ll rock some DM today. Don’t judge. I now have two events \\nthat have dependencies on each other: listening to Depeche Mode and listening to cheesy \\nelectronic music. Why? Because Depeche Mode is cheesy techno. That means that:\\n  p (John listens to cheese | John listens to Depeche Mode) = 1'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 103, 'page_label': '82'}, page_content='Data Smart82\\nIf I listen to Depeche Mode today then there’s a 100 percent chance I’m listening to \\ncheesy techno. It’s a tautology. Since Depeche Mode is cheesy, the probably that I’m listen-\\ning to cheesy techno given that I’m listening to Depeche Mode must be 1.\\nAnd that means that when I want to calculate their joint probability, I’m not just going \\nto get the product of the two probabilities. Using the chain rule:\\n  p (John listens to cheese, John listens to DM) = p(John listens to Depeche Mode) * p(John \\nlistens to cheese | John listens to Depeche Mode) = .3 * 1 = .3\\nBayes Rule\\nSince I’ve deﬁ ned Depeche Mode as cheesy techno, the probability of my listening to cheesy \\ntechno given I listen to Depeche Model is 1. But what about the other way around? You \\ndon’t yet have a probability for this statement:\\n  p (John listens to Depeche Mode | John listens to cheese)\\nAfter all, there are other techno groups out there. Kraftwerk anyone? The new Daft \\nPunk album, maybe?\\nWell, a kindly gentleman named Bayes came up with this rule: \\n  p (cheese) * p(DM | cheese) = p(DM) * p(cheese | DM)\\nThis rule allows you to relate the probability of a conditional event to the probability \\nwhen the event and condition are swapped.\\nRearranging the terms then, we can isolate the probability we do not know (the prob-\\nability that I’m listening to Depeche Mode given that I’m listening to cheesy music):\\np(DM | cheese) = p(DM) * p(cheese | DM) / p(cheese)\\nThe preceding formula is the way you’ll encounter Bayes Rule most often. It’s merely a \\nway of ﬂ ipping around conditional probabilities. When you know a conditional probability \\ngoing only one way, yet you know the total probabilities of the event and the condition, \\nyou can ﬂ ip everything around.\\nPlugging in values, you’ll get:\\np(DM | cheese) = .3 * 1 / .8 = .375\\nI typically have a 30 percent chance of listening to Depeche Mode on any day. However, \\nif I know I’m going to listen to some kind of cheesy techno today, the odds of listening to \\nDepeche Mode jump up to 37.5 percent given that knowledge. Cool!'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 104, 'page_label': '83'}, page_content='83Naïve Bayes and the Incredible Lightness of Being an Idiot\\nUsing Bayes Rule to Create an AI Model\\nAll right, it’s time to leave my music taste behind and think on this Mandrill tweet prob-\\nlem. You’re going to treat each tweet as a bag of words, meaning you’ll break each tweet \\nup into words (often called tokens) at spaces and punctuation. There are two classes of \\ntweets—called app for the Mandrill.com tweets and other for everything else. \\nYou care about these two probabilities:\\n  p (app | word\\n1, word2, word3, …)\\n  p (other | word1, word2, word3, …)\\nThese are the probabilities of a tweet being either about the app or about something \\nelse given that we see the words “word1,” “word2,” “word3,” etc.\\nThe standard implementation of a naïve Bayes model classiﬁ es a new document based \\non which of these two classes is most likely given the words. In other words, if:\\n  p (app | word1, word2, word3, …) > p(other | word1, word2, word3, …)\\nthen you have a tweet about the Mandrill app. \\nThis decision rule—which picks the class that’s most likely given the words—is called \\nthe maximum a posteriori rule (MAP rule).\\nBut how do you calculate these two probabilities? The ﬁ rst step is to use the Bayes Rule \\non them. Using the Bayes Rule, you can rewrite the conditional app probability as follows:\\n  p (app | word1, word2, …) = p(app) p(word1, word2, …| app) / p(word1, word2, …)\\nSimilarly, you get:\\n  p (other | word1, word2, …) = p(other) p(word1, word2, …| other) / p(word1, word2, …)\\nBut note that both of these calculations have the same denominator:\\np(word1, word2, …)\\nThis is just the probability of getting these words in a document in general. Because this \\nquantity doesn’t change based on the class, you can drop it out of the MAP comparison, \\nmeaning you care only about which of these two values is larger:\\np(app) p(word\\n1, word2, …| app)\\np(other) p(word1, word2, …| other)'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 105, 'page_label': '84'}, page_content='Data Smart84\\nBut how do you calculate the probability of getting a bag of words given that it’s an app \\ntweet or an other tweet?\\nThis is where things get idiotic!\\nAssume that the probabilities of these words being in the document are independent \\nfrom one another. Then you get:\\n  p (app) p(word\\n1, word2, …| app) = p(app) p(word1| app) p(word2| app) p(word3| app)…\\n  p (other ) p(word1, word2, …| other ) = p(other ) p(word1| other ) p(word2| other ) \\np(word3| other)…\\nThe independence assumption allows you to break that joint conditional probability of \\nthe bag of words given the class into probabilities of single words given the class.\\nAnd why is this idiotic? Because words are not independent of one another in \\na document!\\nIf you were classifying spam e-mails and you had two words in the document,—\\n“erectile” and “dysfunction”—this would assume:\\n  p (erectile, dysfunction | spam) = p(erectile | spam) p(dysfunction | spam)\\nBut this is idiotic, isn’t it? It’s naïve, because if I told you that I got a spam e-mail \\nwith the word “dysfunction” in it and I asked you to guess what the previous word was, \\nyou’d almost certainly guess “erectile.” There’s a dependency there that’s being blatantly \\nignored.\\nThe funny thing is though that for many practical applications, somehow this idiocy \\ndoesn’t matter. That’s because the MAP rule doesn’t really care that you calculated your \\nclass probabilities correctly; it just cares about which incorrectly calculated probability \\nis larger. And by assuming independence of words, you’re injecting all sorts of error into \\nthat calculation, but at least this sloppiness is across the board. The comparisons used in \\nthe MAP rule tend to come out in the same direction they would have had you applied all \\nsorts of fancier linguistic understanding to the model.\\nHigh-Level Class Probabilities Are Often Assumed to Be Equal\\nSo then to recap, in the case of the Mandrill app, you want to classify tweets based on \\nwhich of these two values is higher:\\n  p (app) p(word\\n1| app) p(word2| app) p(word3| app)…\\n  p (other) p(word1| other) p(word2| other) p(word3| other)…\\nSo what are p(app) and p(other)? You can log on to Twitter and see that p(app) is really \\nabout 20 percent. Eighty percent of tweets using the word mandrill are about other stuff .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 106, 'page_label': '85'}, page_content='85Naïve Bayes and the Incredible Lightness of Being an Idiot\\nAlthough this is true now, it may shift over time, and I’d prefer to get too many tweets \\nclassiﬁ ed as app tweets (false positives) rather than ﬁ  lter some relevant ones out (false \\nnegatives), so I’m going to assume my odds are 50/50. You’ll see this assumption con-\\nstantly in naïve Bayes classiﬁ cation in the real world, especially in spam ﬁ  ltering where \\nthe percentage of e-mail that’s spam shifts over time and may be hard to measure globally.\\nBut if you assume both p(app) and p(other) are 50 percent, then when comparing the \\ntwo values using the MAP decision rule, you might as well just drop them out. Thus, you \\ncan classify a tweet as app-related if:\\n  p (word\\n1| app) p(word2| app) … >= p(word1| other) p(word2| other) …\\nBut how do you calculate the probability of a word given the class it’s in? For example, \\ncontemplate the following probability:\\n  p (“spark” | app)\\nTo ﬁ gure this out, you can pull a set of training tweets in for the app, tokenize them \\ninto words, count up the words, and ﬁ gure out what percentage of those words are “spark.” \\nIt’ll probably be 0 percent since most “spark” mandrill tweets are about video games.\\nPause a moment and contemplate this point. To build a naïve Bayes classiﬁ cation model, \\nyou need only track frequencies of historic app-related and non-app-related words. Well \\nthat’s not hard!\\nA Couple More Odds and Ends\\nNow, before you get started in Excel, you have to address two practical hurdles in imple-\\nmenting naïve Bayes in Excel or in any programming language: \\n• Rare words\\n• Floating-point underﬂ ow\\nDealing with Rare Words\\nThe ﬁ rst is the problem of rare words. What if you get a tweet that you’re supposed to \\nclassify, but there’s the word “Tubal-cain” in it? Based on past data in the training set, \\nperhaps one or both classes have never seen this word. A place where this happens a lot \\non Twitter is with shortened URLs, since each new tweet of a URL might have a diff erent, \\nnever-seen-before encoding. \\nYou can assume:\\n  p (“Tubal-cain” | app) = 0\\nBut then you’d get:\\n  p (“Tubal-cain” | app) p(word\\n2| other) p(word3| other)… = 0'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 107, 'page_label': '86'}, page_content='Data Smart86\\nTubal-cain eff ectively “zeros out” the entire probability calculation. \\nInstead, assume that you’ve seen “Tubal-cain” once before. You can do this for all rare \\nwords.\\nBut wait—that’s unfair to the words you actually  have seen once. Okay, so add 1 to \\nthem, too. \\nBut that’s unfair to the words you’ve actually seen twice. Okay, so add one to every count. \\nThis is called additive smoothing, and it’s often used to accommodate heretofore-unseen \\nwords in bag of words models.\\nDealing with Floating-Point Underﬂ ow\\nNow that you’ve addressed rare words, the second problem you have to face is called \\nﬂ oating-point underﬂ ow. \\nA lot of these words are rare, so you end up with very small probabilities. In this data, \\nmost of the word probabilities will be less than 0.001. And because of the independence \\nassumption, you’ll be multiplying these individual word probabilities together.\\nWhat if you have a 15-word tweet with probabilities all under 0.001? You’ll end up \\nwith a value in the MAP comparison that’s tiny, such as 1x10\\n-45. Now, in truth, Excel can \\nhandle a number as small as 1x10-45. It craps out somewhere in the hundreds of 0s after \\nthe decimal place. So for classifying tweets, you’d probably be all right. But for longer \\ndocuments (e.g. e-mails, news articles), tiny numbers can wreak havoc on calculations.\\nJust to be on the safe side, you need to ﬁ  nd a way to not make the MAP evaluation \\ndirectly:\\n  p (word\\n1| app) p(word2| app) … >= p(word1| other) p(word2| other) …\\nYou can solve this problem using the log function (natural log in Excel is available \\nthrough the LN formula). \\nHere’s a math fun fact for you. Say you have a product:\\n  . 2 * .8\\nIf you take the log of it, the following is true:\\n  ln (.2 * .8) = ln(.2) + ln(.8)\\nAnd when you take the natural log of any value between 0 and 1, instead of getting a \\ntiny decimal, you get a solid negative number. So you can take the natural log of each of \\nthe probabilities and sum them to conduct the maximum a posteriori comparison. This \\ngives a value that the computer won’t barf on.\\nIf you’re a bit confused, don’t worry. This will become very clear in Excel.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 108, 'page_label': '87'}, page_content='87Naïve Bayes and the Incredible Lightness of Being an Idiot\\nLet’s Get This Excel Party Started\\nNOTE\\nThe Excel workbook used in this chapter, “Mandrill.xlsx,” is available for download \\nat the book’s website at www.wiley.com/go/datasmart.This workbook includes all the \\ninitial data if you want to work from that. Or you can just read along using the sheets \\nI’ve already put together in the workbook.\\nIn this chapter’s workbook, called Mandrill.xlsx, you have two tabs of input data to start \\nwith. One tab, AboutMandrillApp, contains 150 tweets, one per row, pertaining to Mandrill.\\ncom. The other tab, AboutOther, contains 150 tweets about other mandrill-related things.\\nI just want to say before you get started—welcome to the world of natural language \\nprocessing (NLP). Natural language processing concerns itself with chewing on human-\\nwritten text and spitting out knowledge. And that almost always means prepping that \\nhuman-written content (like tweets) for computer consumption. It’s time to get prepping.\\nRemoving Extraneous Punctuation\\nThe primary step in creating a bag of words from a tweet is tokenizing the words wherever \\nthere’s a space between them. But before you divide the words wherever there’s whitespace, \\nyou must lowercase everything and replace most of the punctuation with spaces since \\npunctuation in tweets isn’t always meaningful. The reason why you lowercase everything \\nis because the words “e-mail” and “E-mail” aren’t meaningfully diff erent.\\nSo in cell B2 on the two tweet tabs, add this formula:\\n=LOWER(A2)\\nThis will lowercase the ﬁ rst tweet. In C2, strip out any periods. You don’t want to mangle \\nthe URLs, so strip out any periods with a space after them using the SUBSTITUTE command:\\n=SUBSTITUTE(B2,\". \",\" \")\\nThis formula substitutes the string \". \" for a single space \" \".\\nYou can also point cell D2 at cell C2 and replace any colons with a space after them \\nwith a single space:\\n=SUBSTITUTE(C2,\": \",\" \")\\nIn cells E2 through H2, you should make similar substitutions with the strings \"?\", \\n\"!\", \";\", and \",\":\\n=SUBSTITUTE(D2,\"?\",\" \")\\n=SUBSTITUTE(E2,\"!\",\" \")'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 109, 'page_label': '88'}, page_content='Data Smart88\\n=SUBSTITUTE(F2,\";\",\" \")\\n=SUBSTITUTE(G2,\",\",\" \")\\nYou don’t need to add a space after the punctuation in the previous four formulas \\nbecause they don’t appear in URLs (especially in shortened links) that often.\\nHighlight cells B2:H2 on both tabs and double-click the formulas to send them down \\nthrough row 151. This gives you two tabs like the ones shown in Figure 3-3.\\nFigure 3-3: Prepped tweet data\\nSplitting on Spaces\\nNext, create two new tabs and call them AppTokens and OtherTokens.\\nYou need to count how many times each word is used across all tweets in a category. \\nThat means you need all the tweets’ words in a single column. It’s safe to assume that \\neach tweet contains no more than 30 words (feel free to expand this to 40 or 50 if you \\nlike), so if you’re going to extract one token from a tweet per row, that means you need \\n150 x 30 = 4,500 rows.\\nTo start, in these two tabs label A1 as Tweet. \\nHighlight A2:A4501 and Paste Special the tweet values from column H of the initial two \\ntabs. This will give you a list of the processed tweets, as shown in Figure 3-4. Note that \\nbecause you’re pasting 150 tweets into 4,500 rows, Excel automatically repeats everything \\nfor you. Ginchy.\\nThat means that if you extract the ﬁ rst word from the ﬁ rst tweet on row 2, that same \\ntweet is repeated to extract the second word from it on row 152, then the third word on \\nrow 302, and so on.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 110, 'page_label': '89'}, page_content='89Naïve Bayes and the Incredible Lightness of Being an Idiot\\nFigure 3-4:  The initial AppTokens sheet\\nIn column B, you need to indicate the position of each successive space between words \\nin a tweet. You can label this column something like Space Position. Because there is no \\nspace at the beginning of each tweet, begin by placing a 0 in A2:A151 to indicate that \\nwords begin at the ﬁ rst character of each tweet. \\nBeginning at B152 when the tweets repeat for the ﬁ rst time, you can calculate the next \\nspace as follows:\\n=FIND(\" \",A152,B2+1)\\nThe FIND formula will search the tweet for the next empty space beginning with the \\ncharacter after the previous space referenced in cell B2, which is 150 cells above. See \\nFigure 3-5.\\nFigure 3-5:  The space position of the second word in the tweet on row 152'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 111, 'page_label': '90'}, page_content='Data Smart90\\nHowever, note that this formula will give an error once you run out of spaces if there \\nare fewer words than the 30 you’ve planned for, so to accommodate this, you need \\nto wrap the formula in an \\nIFERROR statement and just return one plus the tweet length to \\nindicate the position after the last word:\\n=IFERROR(FIND(\" \",A152,B2+1),LEN(A152)+1)\\nYou can then double-click this formula to send it down the sheet through A4501. This \\nwill produce the sheet shown in Figure 3-6.\\nFigure 3-6:  Positions of each space in the tweet\\nNext in column C, you can begin to extract single tokens from the tweets. Label column \\nC as Token, and beginning in cell C2, you can pull the appropriate word from the tweet \\nusing the MID function. MID takes in a string of text, a start position, and the number of \\ncharacters to yank. So in C2, your text is in A2, the starting position is one past the last \\nspace (B2 + 1), and the length is the diff erence between the subsequent space position in \\ncell B152 and the current space position in B2 minus 1 (keeping in mind that identical \\ntweets are off set by 150 rows). \\nThis yields the following formula:\\nMID(A2,B2+1,B152-B2-1) \\nNow, once again, you can get into some tight spots at the end of the string when you run \\nout of words. So, if there’s an error, turn the token into \".\" so it will be easy to ignore later:\\n=IFERROR(MID(A2,B2+1,B152-B2-1),\".\")\\nYou can then double-click this formula and send it down the sheet to tokenize every \\ntweet, as shown in Figure 3-7.\\nAdd a Length column to column D, and in cell D2 take the length of the token in C2 as:\\n=LEN(C2)\\nYou can double-click this to send it down the sheet. This value allows you to ﬁ nd and \\ndelete any token three characters or less, which tend overall to be meaningless.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 112, 'page_label': '91'}, page_content='91Naïve Bayes and the Incredible Lightness of Being an Idiot\\nFigure 3-7: Every tweet token\\nNOTE\\nTypically in these kind of natural language processing tasks, rather than drop all the \\nshort words, a list of stop words for the particular language (English in this case) would \\nbe removed. Stop words are words which have very little lexical content, which is like \\nnutritional content, for bag of words models. \\nFor instance, “because” or “instead” might be stop words, because they’re common \\nand they don’t really do much to distinguish one type of document from another. The \\nmost common stop words in English do happen to be short, such as “a,” “and,” “the,” \\netc., which is why in this chapter you’ll take the easier, yet more Draconian, route of \\nremoving short words from tweets only.\\nIf you follow these steps, you’ll have the AppTokens sheet shown in Figure 3-8 (the \\nOtherTokens sheet is identical except for the tweets pasted in column A).\\nFigure 3-8:  App tokens with their respective lengths'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 113, 'page_label': '92'}, page_content='Data Smart92\\nCounting Tokens and Calculating Probabilities\\nNow that you’ve tokenized your tweets, you’re ready to calculate the conditional prob-\\nability of a token, p(token | class).\\nTo do so, you need to determine how many times each token is used. Start with the \\nAppTokens tab by selecting the token and length range C1:D4501 and then inserting the \\ndata into a PivotTable. Rename the created pivot table tab AppTokensProbability.\\nIn the PivotTable Builder, ﬁ lter on token length, make the tokens the row labels, and in \\nthe values box set the value to be a count of each token. This gives you the Builder setup \\nshown in Figure 3-9.\\nIn the actual pivot, drop down the length ﬁ  lter and uncheck tokens of length 0, 1, 2, \\nor 3 from being used. (On Windows you have to instruct Excel to Select Multiple Items \\nin the drop-down.) This is also pictured in Figure 3-9.\\nFigure 3-9: PivotTable Builder setup for token counting\\nYou now have only the longer tokens from each tweet, all counted up.\\nYou can now tack on the probabilities to each token, but before you run the numbers, apply \\nthe additive smoothing concept discussed earlier in the chapter by adding one to each token.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 114, 'page_label': '93'}, page_content='93Naïve Bayes and the Incredible Lightness of Being an Idiot\\nLabel column C Add One To Everything, and set C5 = B5+1 (C4 = B4+1 on Windows, \\nwhere Excel builds pivot tables one row higher just to annoy this book). You can double-\\nclick the formula to send it down the page.\\nSince you’ve added one to everything, you’ll also need a new grand total token count. \\nSo at the bottom of the table (row 828 in the AppTokensProbability tab), set the cell to \\nsum the counts above it. Once again, note that if you’re on Windows everything is one \\nrow higher (C4:C826 for the summation range):\\n=SUM(C5:C827)\\nIn column D, you can calculate the probability of each token as its count in column C \\ndivided by the total token count. Label column D as P(Token|App). The probability of the \\nﬁ rst token in D5 (D4 on Windows) is calculated as:\\n=C5/C$828\\nNote the absolute reference to the token total count. This allows you to double-click \\nthe formula and send it down column D. Then in column E (call it LN(P)), you can take \\nthe natural log of the probability in D5 as follows:\\n=LN(D5)\\nSending this down the sheet, you now have the values you need for the MAP rule. See \\nFigure 3-10.\\nFigure 3-10:  The logged probabilities for the app tokens\\nAlso, create an identical tab using the non-app tokens called OtherTokensProbabilies.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 115, 'page_label': '94'}, page_content='Data Smart94\\nAnd We Have a Model! Let’s Use It\\nUnlike with a regression model (which you’ll encounter in Chapter 6), there’s no optimi-\\nzation step here. No Solver, no model ﬁ tting. A naïve Bayes model is nothing more than \\nthese two conditional probability tables.\\nThis is one of the reasons why programmers love this model. There’s no complicated \\nmodel-ﬁ tting step—they just chunk up some tokens and count them. And you can dump \\nthat dictionary of tokens out to disk for later use. It’s terribly easy.\\nOkay, so now that the naïve Bayes model is trained, you can use it. In the TestTweets \\ntab of the workbook, you’ll ﬁ nd 20 tweets, 10 about the app and 10 about other mandrills. \\nYou’re going to prep these tweets, tokenize them (you’ll do the tokenizing a bit diff  er-\\nently this time for kicks), calculate their logged token probabilities for both classes, and \\ndetermine which class is most likely.\\nTo begin then, copy cells B2:H21 from AboutMandrillApp and paste them into D2:J21 of \\nthe TestTweets tab in order to prep the tweets. This gives you the sheet shown in Figure 3-11.\\nFigure 3-11: Prepped test tweets\\nNext, create a tab called TestPredictions. In the tab, paste the Number and Class col-\\numns from TestTweets. Name column C Prediction, which you’ll ﬁ ll in with the predicted \\nclass values. Then label column D as Tokens, and in D2:D21, paste the values from column \\nJ on the TestTweets tab. This gives you the sheet shown in Figure 3-12.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 116, 'page_label': '95'}, page_content='95Naïve Bayes and the Incredible Lightness of Being an Idiot\\nFigure 3-12: The TestPredictions tab \\nUnlike when you built the probability tables, you don’t want to combine these tokens \\nacross tweets. You want to evaluate each tweet separately, and this makes tokenizing \\nrather simple.\\nTo start, highlight the tweets in D2:D21 and choose Text to Columns on the Data tab of \\nthe Excel ribbon. In the Convert Text to Columns wizard that pops up, select Delimited \\nand press Next.\\nOn the second screen of the wizard, specify Tab and Space as delimiters. You can also \\nchoose Treat Consecutive Delimiters As One and make sure that the Text Qualiﬁ er is set \\nto {none}. This gives the setup shown in Figure 3-13.\\nFigure 3-13: The Text to Columns Wizard setup'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 117, 'page_label': '96'}, page_content='Data Smart96\\nPress Finish. This chunks up the tweets into columns going all the way out to column \\nAI (see Figure 3-14).\\nFigure 3-14: The tokens from the test tweets \\nBelow the tokens starting in column D on row 25, you should look up the app prob-\\nabilities for each token. To do so, you can use the VLOOKUP function (see Chapter 1 for \\nmore on VLOOKUP), starting with cell D25:\\n=VLOOKUP(D2,AppTokensProbability!$A$5:$E$827,5,FALSE)\\nThe VLOOKUP function takes the corresponding token from D2 and tries to ﬁ  nd it in \\ncolumn A on the AppTokensProbability tab. When it ﬁ  nds the token, the lookup grabs \\nthe value from column E.\\nBut this isn’t suffi  cient, because you need to deal with the rare words not on the lookup \\ntable—these tokens will get an N/A value from the VLOOKUP  as it stands. As discussed \\nearlier, these rare words should get a probability of 1 divided by the total token count in \\ncell B828 on the AppTokensProbability tab.\\nTo handle these rare words, you just wrap the \\nVLOOKUP in an ISNA check and slide in \\nthe rare word logged probability if needed:\\nIF(ISNA(VLOOKUP(D2,AppTokensProbability!$A$5:$E$827,5,FALSE)),\\nLN(1/AppTokensProbability!$C$828),VLOOKUP(D2,AppTokensProbability!\\n$A$5:$E$827,5,FALSE))\\nThe one thing this solution hasn’t addressed yet are the small tokens you want to throw \\naway. Since you’re going to sum these logged probabilities, you can set any small token’s \\nlogged probability to zero (this is akin to setting the probability to 1 on both sides, that \\nis, throwing it away).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 118, 'page_label': '97'}, page_content='97Naïve Bayes and the Incredible Lightness of Being an Idiot\\nTo do this, you just wrap the whole formula in one more IF statement that checks length:\\n=IF(LEN(D2)<=3,0,IF(ISNA(VLOOKUP(D2,AppTokensProbability!\\n$A$5:$E$827,5,FALSE)),LN(1/AppTokensProbability!$C$828),\\nVLOOKUP(D2,AppTokensProbability!$A$5:$E$827,5,FALSE)))\\nNote that absolute references are used on the AppTokensProbability tab so that you \\ncan drag this formula around.\\nSince the tweet tokens reach all the way to column AI, you can drag this formula from \\nD25 through AI44 to score each token. This gives the worksheet shown in Figure 3-15.\\nFigure 3-15: App logged probabilities assigned to tokens\\nStarting at cell D48, you can use the same formula as in D25 except that it should ref-\\nerence the OtherTokensProbability tab, and the range on the probability tab changes to \\n$A$5:$E$810 in the \\nVLOOKUP with the total token count being on $C$811.\\nThis then yields the sheet shown in Figure 3-16.\\nFigure 3-16: Both sets of logged probabilities assigned to the test tweets'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 119, 'page_label': '98'}, page_content='Data Smart98\\nIn column C, you can sum each row of probabilities, yielding the sheet shown in Figure \\n3-17. For example, C25 is simply:\\n=SUM(D25:AI25)\\nFigure 3-17: Sums of logged conditional token probabilities\\nIn cell C2, you can classify this ﬁ rst tweet by simply comparing its scores below in cells \\nC25 and C48 using the following IF statement:\\n=IF(C25>C48,\"APP\",\"OTHER\")\\nCopying this formula down through C21, you get all of the classiﬁ  cations, as shown \\nin Figure 3-18.\\nIt gets 19 out of 20 correct! Not bad. If you look at the one tweet that was misclassiﬁ ed, \\nthe language is quite vague—the scores are close to tied. \\nAnd that’s it. Model built, predictions done.\\nWrapping Up\\nThis chapter is super short compared to others in this book. Why? Because naïve Bayes \\nis easy! And that’s why folks love it. Naïve Bayes appears to be working some kind of \\ncomplex magic when in reality it just relies on the computer to have a good memory of \\nhow often each token in the training data showed up in each class.\\nThere’s a proverb that goes, “Experience is the father of wisdom and memory the \\nmother.” Nowhere is this truer than with naïve Bayes. Its entire faux-wisdom stems from \\na combination of past data and storage with a little bit of mathematical duct tape.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 120, 'page_label': '99'}, page_content='99Naïve Bayes and the Incredible Lightness of Being an Idiot\\nFigure 3-18: Test tweets classiﬁ ed\\nNaïve Bayes lends itself particularly well to simple implementations in code. For exam-\\nple, here’s a C# implementation:\\nhttp://msdn.microsoft.com/en-us/magazine/jj891056.aspx\\nHere’s a tiny version someone posted online in Python:\\nhttp://www.mustapps.com/spamfilter.py\\nHere’s one in Ruby:\\nhttp://blog.saush.com/2009/02/11/naive-bayesian-classifiers-and-ruby/\\nOne of the great things about this type of model is that it works well even when there \\nare a boatload of features (AI model inputs) you’re predicting with (in the case of this data, \\neach word was a feature). But that said, keep in mind that a simple bag of words model \\ndoes have some drawbacks. Chieﬂ  y, the naïve bit of the model can cause problems. I’ll \\ngive you an example.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 121, 'page_label': '100'}, page_content='Data Smart100\\nSuppose I build a naïve Bayes classiﬁ er that tries to classify tweets about movies into \\n“thumbs up” and “thumbs down.” When someone says something like:\\n  Michael  Bay’s new movie is a steaming pile of misogynistic garbage, full of explosions \\nand poor acting, signifying nothing. And I, for one, loved the ride!\\nIs the model going to get that correct? You have a bunch of thumbs-down tokens fol-\\nlowed by a thumbs-up token at the end.\\nSince a bag of words model throws away the structure of the text and tokens are \\nassumed to be unordered, this could be a problem. Many naïve Bayes models actually \\ntake in phrases rather than individual words as tokens. That helps contextualize words \\na little bit (and makes the naïve assumption even more ludicrous...but who cares!). You \\nneed more training data to make that work because the space of possible n-word phrases \\nis larger than the space of possible words.\\nFor something like this movie review you might need a model that actually cares about \\nthe position of a word in the review. Which phrase “had the last word?” Incorporating \\nthat kind of information immediately does away with this simple bag of words concept.\\nBut, hey, this is nitpicking. Naïve Bayes is a straightforward and versatile AI tool. It’s \\neasy to prototype and test with. So you can try out a modeling idea with naïve Bayes, and \\nif it works well enough, you’re good. If it shows promise but is poor, you can move on to \\nsomething beeﬁ er, like an ensemble model (which is covered in Chapt er 7).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 122, 'page_label': '101'}, page_content='4\\nB\\nusiness Week recently published an article about how The Coca-Cola Company uses a \\nlarge analytics model to determine how to blend raw orange juices to create the perfect \\nnot-from-concentrate product.\\nI was discussing this article with some folks, and one of them blurted something like, \\n“But you could never do that with an artiﬁ cial intelligence model!”\\nThey were right. You can’t. Because Coca-Cola doesn’t use an artiﬁ cial intelligence model. \\nIt uses an optimization model. Huh? What’s the diff erence?\\nAn artiﬁ cial intelligence model predicts the result of a process by analyzing its inputs. \\nThat’s not what Coca-Cola is doing. Coca-Cola doesn’t need to predict the outcome when \\nthey combine juice A with juice B. It needs to decide which combination of juice A, B, C, \\nD, and so on to buy and blend together. Coca-Cola is taking some data and some business \\nrules (their inventory, their demand, their specs, and so on) and deciding how to blend a \\nproduct. These decisions enable Coca-Cola to blend juices with complementary strengths \\nand weaknesses (maybe one is too sweet and another not sweet enough) to get exactly the \\nright taste for the minimum cost and the maximum proﬁ t.\\nThere’s no one outcome that needs predicting. The model gets to change the future. \\nOptimization modeling is analytics’ Arminianism to AI’s Calvinism. Free will, baby! \\n(Sorry, that’s the last historical theological joke in this book.)\\nCompanies across industries use optimization models every day to answer questions \\nsuch as these:\\n• How do I schedule my call center employees to accommodate their vacation requests, \\nbalance overtime, and eliminate back-to-back graveyard shifts for any one employee? \\n• Which oil drilling opportunities do I explore to maximize return while keeping \\nrisk under control? \\n• When do I place new orders to China, and how do I get them shipped to minimize \\ncost and meet anticipated demand?\\nOptimization \\nModeling: Because \\nThat “Fresh Squeezed” \\nOrange Juice Ain’t \\nGonna Blend Itself'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 123, 'page_label': '102'}, page_content='Data Smart102\\nOptimization, you see, is the practice of mathematically formulating a business problem \\nand then solving that mathematical representation for the best solution. And as noted in \\nChapter 1, this objective is always a minimization or a maximization where the “best solution” \\ngets to mean whatever you like—lowest cost, highest proﬁ t, or least likely to land you in jail.\\nThe most widely used and understood form of mathematical optimization, called linear \\nprogramming, was developed in secret by the Soviet Union in the late 1930s and gained \\ntraction through its extensive use in World War II for transportation planning and resource \\nallocation to minimize cost and risk and maximize damage to the enemy.\\nIn this chapter, I’ll go into detail on the linear  part of linear programming. The \\nprogramming part is a holdover from wartime terminology and has nothing to do with \\ncomputer programming. Just ignore it. \\nThis chapter covers linear, integer, and a bit of non-linear optimization. It focuses on \\nhow to formulate business problems in a language in which the computer can solve them. \\nThe chapter also discusses at a high level how the industry-standard optimization meth-\\nods built into Excel’s Solver tool attack these problems and close in on the best solutions.\\nWhy Should Data Scientists Know Optimization?\\nIf you watch a bunch of James Bond or Mission Impossible movies, you’ll notice that they \\noften have a big action sequence before the opening credits. Nothing draws viewers in \\nlike an explosion. \\nThe previous chapters on data mining and artiﬁ  cial intelligence were just that—our \\nexplosions. But now, like in any good action movie, the plot must advance. In Chapter 2 \\nyou used a bit of optimization modeling in ﬁ nding the optimal placement of cluster cen-\\ntroids, but you had only been given enough optimization knowledge in Chapter 1 to make \\nthat happen. In this chapter, you’re going to dive deep into optimization and get lots of \\nexperience with how to formulate models that solve business problems.\\nArtiﬁ cial intelligence is making waves these days for its use at tech companies and \\nstart-ups. Optimization, on the other hand, seems to be more of a Fortune 500 business \\npractice. Reengineering your supply chain to reduce the fuel costs of your ﬂ eet is anything \\nbut sexy. But optimization, whether it’s trimming the fat or making the most of economies \\nof scale, is fundamental  to eff ectively running a business. \\nAnd when we talk data science, the truth is that optimization is fundamental there \\ntoo. As you’ll see in this book, not only is optimization a worthwhile analytic practice to \\nunderstand on its own, but any data science practitioner worth their salt is going to need \\nto use optimization on the way to implementing other data science techniques. In this \\nbook alone, optimization makes a cameo in four other chapters:\\n• Determining optimal cluster centers in k-means clustering as seen in Chapter 2\\n• Maximizing modularity for community detection (Chapter 5)'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 124, 'page_label': '103'}, page_content='103Optimization Modeling\\n• Training coeffi  cients for an AI model (ﬁ tting a regression in Chapter 6)\\n• Optimally setting smoothing parameters in a forecasting model (Chapter 8)\\nOptimization problems are embedded everywhere in data science, so you need to master \\nsolving them before you move on.\\nStarting with a Simple Trade-Off\\nThis section begins by discussing economists’ two favorite resources—guns and butter. \\nThe year is 1941, and you’ve been airdropped behind enemy lines where you’ve assumed \\nthe identity of one Jérémie (or Ameline) Galiendo, a French dairy farmer. \\nYour day job: milking cows and selling sweet, creamy butter to the local populace.\\nYour night job: building and selling machine guns to the French resistance.\\nYour job is complex and fraught with peril. You’ve been cut off  from HQ and are left \\non your own to run the farm while not getting caught by the Nazis. You only have so \\nmuch money in the budget to make ends meet while producing guns and butter; you must \\nstay solvent throughout the war. You cannot lose the farm and your cover along with it.\\nAfter sitting and thinking about your plight, you’ve found a way to characterize your \\nsituation in terms of three elements:\\n• The objective: You get $195 dollars (or, uh, francs, although honestly my Excel is \\nset to dollars, and I’m not going to change it for the ﬁ gures here) in revenue from \\nevery machine gun you sell to your contact, Pierre. You get $150 for every ton of \\nbutter you sell in the market. You need to bring in as much revenue as you can each \\nmonth to keep the farm going.\\n• The decisions: You need to ﬁ gure out what mix of guns and tons of butter to pro-\\nduce each month to maximize total proﬁ t. \\n• The constraints:  It costs $100 to produce a ton of butter and $150 to produce a \\nmachine gun. You have a budget of $1,800 a month to devote to producing new \\nproduct for sale. You also have to store this stuff  in your 21 cubic meter cellar. Guns \\ntake up 0.5 cubic meters once packaged, and a ton of butter takes up 1.5 cubic \\nmeters. You can’t store the butter elsewhere or it’ll spoil. You can’t store the guns \\nelsewhere or you’ll get caught by the Nazis.\\nRepresenting the Problem as a Polytope\\nThis problem as it’s been laid out is called a linear program. A linear program is char-\\nacterized as a set of decisions that need to be made to optimize an objective in light of \\nsome constraints, where both the constraints and the objective are linear. Linear in this \\ncase means that any equation in the problem can only add decisions, subtract decisions, \\nmultiply decisions by constants, or some combination of those things.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 125, 'page_label': '104'}, page_content='104 Data Smart\\nIn linear programming, you can’t shove your decisions through any non-linear func-\\ntions, which might include: \\n• Multiplying decisions together (guns times butter cannot be used anywhere)\\n• Sending a decision variable through a kind of logic check, such as an if statement \\n(“If you only store butter in the cellar, then you can give it a little squish and make \\nthe capacity 22 cubic meters.”)\\nAs you’ll see later in this chapter, restrictions breed creativity. \\nNow, back to the problem. Start by graphing the “feasible region” for this problem. The \\nfeasible region is the set of possible solutions. Can you produce no guns and no butter? \\nSure, that’s feasible. It won’t maximize revenue, but it’s feasible. Can you produce 100 guns \\nand 1,000 tons of butter? Nope, not in the budget, and not in the cellar. Not feasible. \\nOkay, so where do you start graphing? Well, you can’t produce negative quantities of guns or \\nbutter. This isn’t theoretical physics. So you’re dealing with the ﬁ rst quadrant of the x-y plane. \\nIn terms of the budget, at $150 a pop you can make 12 guns from the $1,800 budget. \\nAt $100 a ton, you can make 18 tons of butter.\\nSo if you graph the budget constraint as a line on the x-y plane, it’d pass right through \\n12 guns and 18 tons of butter. As shown in Figure 4-1, the feasible region is then a triangle \\nof positive values in which you can produce, at most, 12 guns and 18 tons of butter, or \\nsome middling linear combination of the two extremes.\\n30 40\\nBudget Constraint\\nButter\\nGuns\\n5010\\n10\\n20\\n30\\n40\\n50\\n20\\nFigure 4-1: The budget constraint makes the feasible region a triangle.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 126, 'page_label': '105'}, page_content='105Optimization Modeling\\nNow, this triangle is more generally called a polytope. A polytope is nothing more than \\na geometric shape with ﬂ at sides. You’ve probably heard the term polygon. Well, a polygon \\nis just a polytope in a two-dimensional space. If you’ve got a big fat rock of an engagement \\nring on your hand…Bam! The diamond is a polytope.\\nAll linear programs can have their feasible regions expressed as polytopes. Some algo-\\nrithms, as you’ll see momentarily, exploit this fact to arrive quickly at solutions to linear \\nprogramming problems.\\nConcerning the problem at hand, it’s time to consider the second constraint—the cellar. \\nIf you produced only guns, you’d be able to pack 42 of them in the cellar. On the other \\nhand, you could shove 14 tons of butter in the cellar, maximum. So adding this constraint \\nto the polytope, you shave off  part of the feasible region, as shown in Figure 4-2.\\n30 40\\nCellar Constraint\\nButter\\nGuns\\n5010\\n10\\n20\\n30\\n40\\n50\\n20\\nBudget Constraint\\nFigure 4-2: The cellar constraint cuts a chunk out of the feasible region.\\nSolving by Sliding the Level Set\\nNow that you’ve determined the feasible region, you can begin to ask the question, “Where \\nin that region is the best guns/butter mix?”\\nTo answer that question, begin by deﬁ ning something called the level set. A level set for your \\noptimization model is a region in the polytope where all the points give the same revenue.\\nBecause your revenue function is $150*Butter + $195*Guns, each level set can be deﬁ ned \\nby the line $150*Butter + $195*Guns = C, where C is a ﬁ xed amount of revenue.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 127, 'page_label': '106'}, page_content='106 Data Smart\\nConsider the case where C is $1950.  For the level set $150*Butter + $195*Guns = $1950, both \\nthe points (0,10) and (13,0) exist in the level set as does any combination of guns and butter \\nwhere $150*Butter + $195*Guns comes out to $1950. This level set is pictured in Figure 4-3.\\nUsing this idea of the level set, you could then think of solving the revenue maximization \\nproblem by sliding the level set in the direction of increasing revenue (this is perpendicular \\nto the level set itself) until the last possible moment before you left the feasible region.\\nIn Figure 4-3, a level set is pictured with a dashed line, while the arrow and dashed \\nline together represent your objective function. \\n30 40\\nButter\\nGuns\\n50\\nLevel Set\\n10\\n10\\n20\\n30\\n40\\n50\\n20\\nFigure 4-3: The level set and objective function for the revenue optimization\\nThe Simplex Method: Rooting around the Corners\\nTo reiterate, if you want to know which feasible points are optimal, you can just slide that \\nlevel set along the direction of increasing revenue. Right at the border before the level set \\nleaves the polytope, that’s where the best points would be. And here’s what’s cool about that: \\nOne of these optimal points at the border will always be a corner of the polytope. \\nGo ahead and conﬁ rm this in Figure 4-3. Lay a pencil on the level set and move it up \\nand right in the direction of increasing revenue. See how it leaves the polytope at a corner?'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 128, 'page_label': '107'}, page_content='107Optimization Modeling\\nWhy is that cool? Well, the polytope in Figure 4-3 has an inﬁ  nite number of feasible \\nsolutions. Searching the entire space would be hell. Even the edges have an inﬁ nite num-\\nber of points! But there are only four corners, and there’s an optimal solution in one of \\nthem. Much better odds.\\nIt turns out there’s an algorithm that’s been designed to check corners. And even in \\nproblems with hundreds of millions of decisions, it’s very eff ective. The algorithm is called \\nthe simplex method.\\nBasically, the simplex method starts at a corner of the polytope and slides along edges \\nof the polytope that beneﬁ t the objective. When it hits a corner whose departing edges all \\nare detrimental to the objective, well, then that corner is the best one.\\nIn the case of selling guns and butter, assume that you start out at point (0,0). It’s a \\ncorner, but it’s got $0 in revenue. Surely you can do better. \\nWell, as seen in Figure 4-3, the bottom edge of the polytope increases revenue as you \\nmove right. So sliding along the bottom edge of the polytope in this direction, you hit the \\ncorner (14,0)—14 tons of butter and no guns will produce $2,100 dollars (see Figure 4-4).\\n30 40\\nButter\\nGuns\\n5010\\n10\\n20\\n30\\n40\\n50\\n20\\nFigure 4-4:  Testing out the all-butter corner\\nFrom the all-butter corner, you can then slide along the cellar storage edge in the direc-\\ntion of increasing revenue. The next corner you hit is (12.9, 3.4), which gives you revenue \\njust shy of $2,600. All the edges departing the corner lead to worse nodes, so you’re done. \\nAs pictured in Figure 4-5, this is the optimum!'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 129, 'page_label': '108'}, page_content='108 Data Smart\\n30 40\\nButter\\nGuns\\n5010\\n10\\n20\\n30\\n40\\n50\\n20\\nFigure 4-5:  Located the optimal corner\\nWorking in Excel\\nBefore you leave this simple problem behind for something a little tougher, I want to build \\nand solve it in Excel. The ﬁ rst thing you’re going to do in a blank Excel workbook is create \\nspaces for the objective and decision variables, so you’ll label cell B2 as the spot where the \\ntotal revenue will go and cells B4:C4 as the range where the production decisions will go.\\nBelow the objective and decision sections, add the size and price information for guns \\nand butter, the limits on storage space and budget, and each item’s contribution to revenue. \\nThe barebones spreadsheet should look like Figure 4-6.\\nFigure 4-6:  Guns and butter data placed, lovingly, in Excel'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 130, 'page_label': '109'}, page_content='109Optimization Modeling\\nTo this data, you need to add several calculations, namely, the constraint calculations \\nand the revenue calculation. In Column E, next to the Limit cells, you can multiply the \\namounts of guns and butter produced times their respective sizes and prices, and sum \\nthem up in a Used column. For example, in E7 you can place how much space is used in \\nthe cellar using the formula:\\n=SUMPRODUCT(B4:C4,B7:C7)\\nNote that this formula is linear because only one range, B4:C4, is a decision range. The \\nother range just houses the storage coeffi  cients. You can do the same calculation to gather \\nthe total amount spent on guns and butter.\\nFor the objective function, you need only take a SUMPRODUCT  of the purchased \\nquantities on row 4 with their revenue on row 9. Placing a feasible solution, such as \\n1 gun, 1 ton of butter, into the decision cells now yields a sheet like that pictured \\nin Figure 4-7.\\nFigure 4-7: Revenue and constraint calculations within the guns and butter problem\\nAll right, so how do you now get Excel to set the decision variables to their optimal \\nvalues? To do this, you use Solver! Start by popping open an empty Solver window (pic-\\ntured in Figure 4-8). For more on adding Solver to Excel see Chapter 1.\\nJust as was mocked up earlier in the chapter, you need to provide Solver with an objec-\\ntive, decisions, and constraints. The objective is the revenue cell created in B1. Also, make \\nsure that you choose the Max radio button since you’re maximizing, not minimizing, \\nrevenue. If you were working a problem with cost or risk in the objective function, you \\nwould use the Min option instead.\\nThe decisions are in B4:C4. After you add them to the “By Changing Variable Cells” \\nsection, the Solver window will look like Figure 4-9.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 131, 'page_label': '110'}, page_content='110 Data Smart\\nFigure 4-8:  The Solver window\\nAs for the constraints, there are two you have to add. Start with the cellar storage \\nconstraint. Click on the Add button next to the constraints section. Filling out the small \\ndialog box, you need to indicate that cell E7 must be less than or equal to (≤) cell D7 (see \\nFigure 4-10). The amount of space you’re using must be less than the limit.\\nNOTE\\nNote that Solver will add absolute references ($) to everything in your formulation. It \\ndoesn’t matter that Solver does this. Honestly, I don’t know why it does because you \\ncan’t drag formulas in the context of a Solver model. See Chapter 1 for more on absolute \\nreferences.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 132, 'page_label': '111'}, page_content='111Optimization Modeling\\nFigure 4-9: Objective and decisions populated in Solver\\nNOTE\\nBefore pressing OK, look at the other constraint types Solver off ers you. Beyond ≤, ≥, \\nand =, there are some funky ones, namely int, bin, and dif. These odd constraints can \\nbe placed on cells to make them integers, binary (0 or 1), or “all diff  erent.” Keep the \\nint constraint in mind. You’re going to return to it in a second.\\nPress OK to add the constraint, and then add the budget constraint the same way (E8 ≤ \\nD8). Conﬁ rm also that the Make Unconstrained Variables Non-Negative box is checked to \\nmake sure the guns and butter production doesn’t become negative for some odd reason. \\n(Alternatively, you can just add a B4:C4 ≥ 0 constraint, but the check box makes it easy.) \\nNow, from Select a Solving Method, make sure the Simplex LP algorithm is selected. \\nYou’re ready to go (see Figure 4-11).\\nFigure 4-10:  The Add Constraint dialog box'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 133, 'page_label': '112'}, page_content='112 Data Smart\\nUSING EXCEL 2007\\nIn Excel 2007, there is no Make Unconstrained Variables Non-Negative checkbox. \\nInstead, go to the Options screen and check off  the Assume Non-negative box. Also, \\nthere’s no Solving Method selection. Instead, check the Assume Linear Model box in \\norder to activate the simplex algorithm.\\nWhen you press Solve, Excel quickly ﬁ nds the solution to the problem and pops up a \\nbox letting you know. You can either accept the solution found or restore the values in the \\ndecision cells (see Figure 4-12). If you press OK to accept the solution, you would see that \\nit’s 3.43 guns and 12.86 tons of butter just like you’d graphed (see Figure 4-13).\\nFigure 4-11: Completed Guns and Butter formulation in Solver\\nFigure 4-12: Solver lets you know when it’s solved the problem.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 134, 'page_label': '113'}, page_content='113Optimization Modeling\\nFigure 4-13: Optimized guns and butter workbook\\nBut You Can’t Make 3.43 Guns\\nNow, your French alter ego is most likely shouting, “Zut alors!” Why? Because you can’t \\nmake 43 percent of a gun. And I concede this point. \\nWhen working with linear programs, the fractional solutions can sometimes be an \\nannoyance. If you were producing guns and butter in the millions, the decimal could be \\nignored without too much danger of infeasibility or revenue changes. But for this problem, \\nthe numbers are small enough to where you really need Solver to make them integers.\\nSo, hopping back into the Solver window, add a constraint to force the decision cells \\nB4:C4 to be integers (see Figure 4-14). Click OK to return to the Solver Parameters window.\\nFigure 4-14: Making the guns and butter decisions integers\\nUnder the Options section next to Simplex LP, make sure that the Ignore Integer \\nConstraints box is not checked. Press OK.\\nPress Solve and a new solution pops up. At $2,580, you’ve only lost about $17. Not bad! \\nNote that by forcing the decisions to be integers, you can never do better, only worse, \\nbecause you’re tightening up the possible solutions.\\nGuns have moved up to an even 4 while butter has dropped to 12. And while the budget \\nis completely used up, note that you’ve got a spare 1 cubic meter of storage left in the cellar.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 135, 'page_label': '114'}, page_content='114 Data Smart\\nSo why not just make your decisions integers all the time? Well, sometimes you just \\ndon’t need them. For instance, if you’re blending liquids, fractions can be just ﬁ ne. \\nAlso, behind the scenes the algorithm Solver uses actually changes when integers are \\nintroduced, and performance degrades as a result. The algorithm Solver uses when it \\nencounters the integer or binary constraints is called “Branch and Bound,” and at a high \\nlevel, it has to run the simplex algorithm over and over again on pieces of your original \\nproblem, rooting around for integer-feasible solutions at each step.\\nLet’s Make the Problem Non-Linear for Kicks\\nEven though you’ve added an integer constraint to the decisions, the basic problem at \\nhand is still a linear one.\\nWhat if you got a $500 bonus from your contact Pierre if you were able to bring him 5 \\nor more guns each month? Well, you can place an \\nIF statement in the revenue function \\nthat checks gun production in cell B4:\\n=SUMPRODUCT(B9:C9,B4:C4) + IF(B4>=5,500,0)\\nOnce you tack on that IF statement, the objective function becomes non-linear. By \\ngraphing the IF statement in Figure 4-15, you can easily see the large non-linear discon-\\ntinuity at 5 guns.\\n$0\\n$500\\nGuns\\nBonus from Pierre\\n51 01 52 0\\nFigure 4-15: A graph of Pierre’s $500 bonus\\nIf you were to open Solver and use Simplex LP again to solve this problem, Excel would \\npolitely complain that “the linearity conditions required by this LP Solver are not satis-\\nﬁ ed” (see Figure 4-16).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 136, 'page_label': '115'}, page_content='115Optimization Modeling\\nFigure 4-16:  Excel won’t let you put the decision variables through an IF statement when using \\nSimplex LP.\\nLuckily, Solver provides two other algorithms for resolving this problem, called the \\n“Evolutionary” and “GRG Nonlinear” algorithms. You’ll give the evolutionary approach \\na shot here, with which you’re already familiar if you’ve worked through Chapter 2. (In \\nExcel 2007, since there is no algorithm selection box, leaving the Assume Linear Model \\nbox unchecked will activate a non-linear optimization algorithm.)\\nThe way an evolutionary algorithm works is loosely modeled on the way evolution \\nworks in biology:\\n• Generate a pool of initial solutions (kind of like a “gene pool”), some feasible and \\nsome infeasible.\\n• Each solution has some level of ﬁ tness for survival.\\n• Solutions breed through crossover, meaning components are selected and combined \\nfrom two or three existing solutions.\\n• Solutions mutate to create new solutions.\\n• Some amount of local search takes place, wherein new solutions are generated within \\nthe close vicinity of the current best solution in the population.\\n• Selection occurs when randomly selected poor performing candidate solutions are \\ndropped from the gene pool.\\nNote that this approach does not inherently require that the problem structure be \\nlinear, quadratic, or otherwise. To an extent, the problem can be treated like a black box.\\nWhat that means is that when modeling a linear program in Excel, you’re limited to \\nthings like the +/- signs, the SUM and AVERAGE  formulas, and the SUMPRODUCT  formula, \\nwhere only one range contains decisions. But with the evolutionary solver, your formula'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 137, 'page_label': '116'}, page_content='116 Data Smart\\nchoices expand to just about anything your little heart desires, including these useful \\nnon-linear functions:\\n• Logical checks: \\n• IF\\n• COUNTIF\\n• SUMIF\\n• Statistical functions: \\n• MIN\\n• MAX\\n• MEDIAN\\n• LARGE\\n• NORMDIST, BINOMDIST, and so on\\n• Lookup functions: \\n• VLOOKUP\\n• HLOOKUP\\n• OFFSET\\n• MATCH\\n• INDEX\\nNow, I know you’re getting pumped, so let me deﬂ ate the excitement just a little bit. \\nThere are a number of problems with the evolutionary solver:\\n• It gives no guarantees that it can ﬁ nd an optimal solution. All it does is keep track \\nof the best solution in a population until time runs out, until the population hasn’t \\nchanged enough in a while to merit continuing, or until you kill Solver with the \\nEsc key. You can modify these “stopping criteria” in the evolutionary algorithm \\noptions section of Excel Solver.\\n• The evolutionary solver can be quite slow. With complex feasible regions, it often \\nbarfs, unable to ﬁ nd even a good starting place.\\n• In order to get the evolutionary algorithm to work well in Excel, you should specify \\nhard bounds for each decision variable. If you have a decision that’s more or less \\nunbounded, you have to pick a really large number to bound it. \\nConcerning this last bullet point, for the guns and butter problem, you should add \\na constraint that both decisions must stay below 25, giving the new setup pictured in \\nFigure 4-17.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 138, 'page_label': '117'}, page_content='117Optimization Modeling\\nFigure 4-17: Formulation for the evolutionary solver\\nPress OK then Solve. The algorithm kicks off  and should eventually ﬁ nd a solution of \\n6 guns and 9 tons of butter. So the evolutionary algorithm decided to take Pierre up on his \\n$500 bonus. Nice! But notice that even on such a small problem, this took a while. About \\n30 seconds on my laptop. Think about what that might mean for a production model.\\nThere’s a Monster at the End of This Chapter\\nOkay, so that’s an imaginary problem. In the next section, I’m going to demonstrate the \\npowers of Solver on something a bit meatier. You’ll also spend time learning how to model \\nnon-linear functions (such as Pierre’s $500 gun bonus) in linear ways, so that you can still \\nuse the fast Simplex LP algorithm.\\nIf you’re chomping at the bit to move on to another topic, you now know most of what \\nyou need to know to succeed in the following chapters. Stick around at least through the \\nIf-Then and the “Big M” Constraint section of this chapter in order to learn what you \\nneed for Chapter 5 on clustering in graphs. Or, better yet, strap in and work through all \\nthe remaining problems here! But be warned, the last two business rules modeled in this \\nchapter are monsters.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 139, 'page_label': '118'}, page_content='118 Data Smart\\nOTHER TOOLS\\nHuge models don’t ﬁ t very well in Excel. The version of Solver that comes packaged \\nwith Excel allows only 100 – 200 decision variables and constraints, depending on \\nthe version you’re running. That’s going to limit the size of the problems you can \\nattack in this book. \\nIf you want to go larger in Excel, you can buy a bigger version of Solver from Frontline \\nSystems. Even better, if you’re on a Windows box, use OpenSolver just as you’ll do in \\nthe later sections of this chapter. OpenSolver, introduced in Chapter 1, calls an open \\nsource solver called COIN Branch and Cut (\\nhttp://www.coin-or.org/) that is excellent \\nfor midsized optimization problems. I’ve used OpenSolver on hundreds of thousands \\nof variables eff ectively.\\nOther beeﬁ er linear programming engines include Gurobi and CPLEX. I generally \\nrecommend that developers and other people who like their software “in the cloud” \\ncheck out Gurobi, whereas CPLEX, owned by IBM, is the go-to enterprise solution.\\nInterfacing with these industrial strength tools happens in all sorts of ways. For \\ninstance, CPLEX comes packaged with an environment called OPL where you can write \\nmodels in a specialized language that’s got excellent hooks into spreadsheets. There \\nare plenty of hooks into programming languages for embedding these algorithms and \\nmodels within production systems.\\nMy favorite tool for plugging into the heavy-duty solvers like CPLEX and Gurobi is \\ncalled AIMMS (\\nwww.AIMMS.com). The software lets you build out optimization models \\nand then slap a user interface on them without having to write code. Also, the software \\ncan talk to spreadsheets and databases. \\nFor the rest of this book, you’re going to stick with Excel and Solver, but just know \\nthat there are cutting-edge modeling environments out there for solving bigger problems, \\nshould your needs grow beyond what Excel can handle.\\nFresh from the Grove to Your Glass...with a Pit Stop \\nthrough a Blending Model\\nNOTE\\nThe Excel workbook used in this chapter, “OrangeJuiceBlending.xlsx,” is available \\nfor download at the book’s website at www.wiley.com/go/datasmart .This workbook \\nincludes all the initial data if you want to work from that. Or you can just read along \\nusing the sheets I’ve already put together in the workbook.”'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 140, 'page_label': '119'}, page_content='119Optimization Modeling\\nWhen you were a child, perhaps there came that day when someone explained to you that \\nSanta Claus didn’t exist, outside of men with bad rosacea dressed up at the mall.\\nWell, today I’m going to shatter another belief: your not-from-concentrate premium \\norange juice was not hand squeezed. In fact, the pulp in it is probably from diff erent oranges \\nthan the juice, and the juice has been pulled from diff  erent vats and blended according \\nto mathematical models to ensure that each carafe you drink tastes the same as the last.\\nConsistent taste in OJ year round isn’t something that just anyone can pull off . Oranges \\naren’t in season in Florida year round. And at diff erent times of the year, diff erent orange \\nvarietals are ripe. Pull fruit too early and it tastes “green.” Get fruit from another country \\nthat’s in season instead, and the juice might be another color. Or sweeter. Consumers \\ndemand consistency. That might be easy with Sunny D, but how do you get that out of a \\nbunch of vats of freshly squeezed, very chilled orange juice?\\nYou Use a Blending Model\\nOn the hit TV show Downton Abbey, the wealthy Lord Grantham invests all his family’s \\nmoney in a single railroad venture. It’s risky. And he loses big. Apparently in the early \\n1900s, diversiﬁ cation was not a popular concept. \\nBy averaging the risk and return of an investment portfolio across multiple investments, \\nthe odds of you striking it rich probably decrease, but so do the odds of your going broke. \\nThis same approach applies to orange juice production today.\\nJuice can be procured from all around the world, from diff  erent oranges in diff erent \\nseasons. Each product has diff  erent specs—some might be a bit more tart, some a bit \\nmore astringent, and others might be sickly sweet. By blending this “portfolio” of juices, \\na single consistent taste can be maintained.\\nThat’s the problem you’ll work through in this section. How do you build a blending \\nmodel that reduces cost while maintaining quality, and what type of wrenches might get \\nthrown into the works that would need to get mathematically formulated along the way?\\nLet’s Start with Some Specs\\nLet’s say you’re an analyst working at JuiceLand and your boss, Mr. Juice R. Landingsly \\nIII (your company is full of nepotism), has asked you to plan the procurement of juice \\nfrom your suppliers for January, February, and March of this coming year. Along with this \\nassignment, Mr. Landingsly hands you a sheet of specs from your suppliers containing \\nthe country of origin and varietal, the quantity available for purchase over the next three \\nmonths, and the price and shipping cost per 1,000 gallons.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 141, 'page_label': '120'}, page_content='120 Data Smart\\nThe specs sheet rates the color of the juice on a scale from one to ten and three ﬂ avor \\ncomponents:\\n• Brix/Acid ratio: Brix is a measure of sweetness in the juice, so Brix/Acid ratio is a \\nmeasure of sweetness to tartness, which in the end, is really what orange juice is \\nall about.\\n• Acid (%): Acid as a percentage of the juice is broken out individually, because at \\na certain point, it doesn’t really matter how sweet the juice is, it’s still too acidic.\\n• Astringency (1–10 scale):  A measure of the “green” quality of the juice. It’s that \\nbitter, unripe, planty ﬂ avor that can creep in. This scale is assessed by a panel of \\ntasters at each juicing facility on a scale of 1–10.\\nAll of these speciﬁ cations are represented in the speciﬁ  cations spreadsheet pictured \\nin Figure 4-18.\\nFigure 4-18: The specs sheet for raw orange juice procurement\\nWhatever juice you choose to buy will be shipped to your blending facility in large, \\naseptic chilled tanks, either by cargo ship or rail. That’s why there isn’t a shipping cost \\nfor the Florida Valencia oranges—the blending facility is located in your Florida grove \\n(where, back in the good old days, you grew all the oranges you needed).\\nLook over the specs pictured in Figure 4-18. What can you say about them? The juice \\nis coming from an international selection of varietals and localities.\\nSome juice, such as that from Mexico, is cheap but a bit off . In Mexico’s case, the astrin-\\ngency is very high. In other cases, such as the Sunstar oranges from Texas, the juice is \\nsweeter and less astringent, but the cost is higher.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 142, 'page_label': '121'}, page_content='121Optimization Modeling\\nWhich juice you buy for the next three months depends on some considerations:\\n• If you’re minimizing cost, can you buy whatever you want?\\n• How much juice do you need?\\n• What are the ﬂ avor and color bounds for each batch?\\nComing Back to Consistency\\nThrough taste tests and numerous customer interviews, JuiceLand has determined what \\ntheir orange juice should taste and look like. Any deviation outside the allowable range \\nof these specs and customers are more likely to label the juice as generic, cheap, or even \\nworse, from concentrate. Eek.\\nMr. Landingsly III lays out the requirements for you:\\n• He wants the lowest cost purchase plan for January, February, and March that \\nmeets a projected demand of 600,000 gallons of juice in January and February and \\n700,000 gallons in March.\\n• JuiceLand has entered an agreement with the state of Florida which provides the \\ncompany tax incentives so long as the company buys at least 40 percent of its juice \\neach month from Florida Valencia growers. Under no circumstances are you to \\nviolate this agreement.\\n• The Brix/Acid ratio (BAR) must stay between 11.5 and 12.5 in each month’s blend.\\n• The acid level must remain between 0.75 and 1 percent.\\n• The astringency level must stay at 4 or lower.\\n• Color must remain between 4.5 and 5.5. Not too watery, not too dark.\\nReal quickly shove those requirements into an outline of an LP formulation:\\n• Objective: Minimize procurement costs.\\n• Decisions: Amount of each juice to buy each month\\n• Constraints:\\n• Demand\\n• Supply\\n• Florida Valencia requirement\\n• Flavor\\n• Color\\nPutting the Data into Excel\\nTo model the problem in Excel, the ﬁ rst thing you need to do is create a new tab to house \\nthe formulation. Call it Optimization Model.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 143, 'page_label': '122'}, page_content='122 Data Smart\\nIn cell A2, under the label Total Cost, put a placeholder for the objective. \\nBelow that, in cell A5, paste everything from the Specs tab, but insert four columns \\nbetween the Region and Qty Available columns to make way for the decision variables as \\nwell as their totals by row. \\nThe ﬁ rst three columns will be labeled January, February, and March, while the fourth \\nwill be their sum, labeled Total Ordered. In the Total Ordered column, you need to sum \\nthe three cells to the left, so for example in the case of Brazilian Hamlin oranges, cell F6 \\ncontains:\\n=SUM(C6:E6)\\nYou can drag cell F6 down through F16. Placing some conditional formatting on the \\nrange C6:E16, the resulting spreadsheet looks like the one in Figure 4-19.\\nFigure 4-19: Setting up the blending spreadsheet\\nBelow the monthly purchase ﬁ  elds, add some ﬁ  elds for monthly procurement and \\nshipping costs. For January, place the monthly procurement cost in cell C17 as follows:\\n=SUMPRODUCT(C6:C16,$L6:$L16)\\nOnce again, since only the C column is a decision variable, this calculation is linear. \\nSimilarly, you need to add the following calculation to C18 to calculate shipping costs \\nfor the month:\\n=SUMPRODUCT(C6:C16,$M6:$M16)\\nDragging these formulas across columns D and E, you’ll have all of your procurement \\nand shipping costs calculated. You can then set the objective function in cell A2 as the \\nsum of C17:E18. The resulting spreadsheet is pictured in Figure 4-20.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 144, 'page_label': '123'}, page_content='123Optimization Modeling\\nFigure 4-20: Cost calculations added to the juice blending worksheet\\nNow add the calculations you need to satisfy the demand and Florida Valencia con-\\nstraints. On row 20, sum the total quantity of juice procured on that month, and on row 21, \\nplace the required levels of 600, 600, and 700, respectively into columns C through E.\\nAs for total Valencia ordered from Florida, map C8:E8 to cells C23:E23 and place the \\nrequired 40 percent of total demand (240, 240, 280) below the values.\\nThis yields the spreadsheet shown in Figure 4-21.\\nNow that you’ve covered the objective function, the decision variables, and the supply, \\ndemand, and Valencia calculations, all you have left are the taste and color calculations \\nbased on what you order.\\nLet’s tackle Brix/Acid ratio ﬁ rst. In cell B27, put the minimum BAR of the blend, which \\nis 11.5. Then in cell C27, you can use the \\nSUMPRODUCT of the January orders (column C) \\nwith their Brix/Acid specs in column H, divided by total demand, to get the average Brix/\\nAcid ratio. \\nWARNING\\nDo not divide through by total ordered, as that’s a function of your decision variables! \\nDecisions divided by decisions are highly non-linear.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 145, 'page_label': '124'}, page_content='124 Data Smart\\nFigure 4-21: Demand and Valencia calculations added\\nJust remember, you’ll be setting the total ordered amount equal to projected demand \\nas a constraint, so there’s no reason not to just divide through by demand when getting \\nthe average BAR of the blend. Thus, cell C27 looks as follows:\\n=SUMPRODUCT(C$6:C$16,$H$6:$H$16)/C$21\\nYou can drag that formula to the right through column E. In column F, you’ll ﬁ  nish \\noff  the row by typing in the maximum BAR of 12.5. You can then repeat these steps to \\nset up calculations for acid, astringency, and color in rows 28 through 30. The resulting \\nspreadsheet is pictured in Figure 4-22.\\nSetting Up the Problem in Solver\\nAll right, so you have all the data and calculations you need to set up the blending prob-\\nlem in Solver. The ﬁ rst thing you need to specify in Solver is the total cost function in A2 \\nthat you’re minimizing.\\nThe decision variables are the monthly purchase amounts of each varietal housed in \\nthe cell range C6:E16. Once again, these decisions can’t be negative, so make sure the'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 146, 'page_label': '125'}, page_content='125Optimization Modeling\\nMake Unconstrained Variables Non-Negative box is checked (Assume Linear Model is \\nchecked in Excel 2007).\\nFigure 4-22: Adding taste and color constraints to the worksheet\\nWhen it comes to adding constraints, this problem really deviates from the guns and \\nbutter example. There are a lot of them.\\nThe ﬁ rst constraint is that the orders on row 20 must equal demand on row 21 for each \\nmonth. Similarly, the Florida Valencia orders on row 23 should be greater than or equal \\nto the required amount on row 24. Also, the total quantity ordered from each geography, \\ncalculated in F6:F16, should be less than or equal to what’s available in G6:G16.\\nWith supply and demand constraints added, you need to add the taste and color \\nconstraints. \\nNow, Excel won’t let you put a constraint on two diff erently sized ranges, so if you enter \\nC27:E30 ≥ B27:B30, it’s not going to understand how to handle that. (I ﬁ nd this terribly \\nirritating.) Instead, you have to add constraints for columns C, D, and E individually. For \\nexample, for January orders you have C27:C30 ≥ B27:B30 and C27:C30 ≤ F27:F30. And \\nthe same goes for February and March.\\nAfter you add all those constraints, make sure that Simplex LP is the chosen solving \\nmethod. The ﬁ nal formulation should look like Figure 4-23.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 147, 'page_label': '126'}, page_content='126 Data Smart\\nFigure 4-23: The populated Solver dialog for the blending problem\\nSolving, you get an optimal cost of $1.23 million dollars in procurement costs (see \\nFigure 4-24). Note how Florida Valencia purchases hug their lower bound. Obviously, \\nthese oranges aren’t the best deal, but the model is being forced to make do for tax pur-\\nposes. The second most popular orange is the Verna out of Mexico, which is dirt cheap \\nbut otherwise pretty awful. The model balances this bitter, acidic juice with mixtures of \\nBelladonna, Biondo Commune, and Gardner, which are all milder, sweeter, and superior \\nin color. Pretty neat!\\nLowering Your Standards\\nExcited, you bring your optimal blend plan to your manager, Mr. Landingsly III. You \\nexplain how you arrived at your answer, and he eyes it with suspicion. Even though you \\nclaim it’s optimal, he wants you to shave an additional 5 percent off  the cost. He explains \\nhis seemingly nonsensical position using mostly sports analogies about “playing all four \\nquarters” and “giving 110 percent.”\\nThere’s no use arguing against sports analogies. If $1,170,000 is the sweet spot, then so \\nbe it. You explain that there’s no way to achieve that within the current quality bounds, \\nand he merely grunts and tells you to “bend reality a bit.”\\nHmmm…'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 148, 'page_label': '127'}, page_content='127Optimization Modeling\\nYou return to your spreadsheet ﬂ ustered.\\nHow do you get the best blend for a cost of $1,170,000?\\nAfter the heart to heart with Mr. Landingsly, cost is no longer an objective. It’s a con-\\nstraint! So what’s the objective? \\nFigure 4-24: Solution to the orange juice-blending problem\\nYour new objective based on the bossman’s grunts appears to be ﬁ nding the solution \\nthat degrades quality the least for 1.17 million dollars. And the way to implement that is to \\nstick a decision variable in the model that loosens up the quality constraints.\\nGo ahead and copy the Optimization Model tab into a new sheet, called Relaxed Quality. \\nYou don’t have to change a whole lot to make this work.\\nTake a moment and think about how you might change things around to accommo-\\ndate the new relaxed quality objective and cost constraint. Don’t peak ahead until your \\nhead hurts!\\nAll right.\\nThe ﬁ rst thing you do is pop $1,117,000 as the cost limit in cell B2 right next to the old \\nobjective. Also, copy and paste values of the old minima and maxima for taste and color \\ninto columns H and I, respectively. And in column G on rows 27 through 30, add a new \\ndecision variable called % Relaxed.\\nNow consider how you might use the Brix/Acid relaxation decision in cell G27 to relax \\nthe lower bound of 11.5. Currently, the allowable band of Brix/Acid is 11.5 to 12.5, which'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 149, 'page_label': '128'}, page_content='128 Data Smart\\nis a width of 1. So a 10 percent broadening at the bottom of the constraint would make \\nthe minimum 11.4.\\nFollowing this approach, replace the minimum in B27 with this formula: \\n=H27-G27*(I27-H27)\\nThis takes the old minimum, now in H27, and subtracts from it the percent relaxation \\ntimes the distance of the old maximum from the old minimum (I27 minus H27). You can \\ncopy this formula down through row 30. Similarly, implement the relaxed maximum in \\ncolumn F.\\nFor the objective, take the average of the relaxation decisions in G27:G30. Placing this \\ncalculation in cell D2, the new sheet now looks like Figure 4-25.\\nFigure 4-25: Relaxed quality model\\nOpen Solver and change the objective to minimize the average relaxation of the quality \\nbounds calculated in cell D2. You also need to add G27:G30 to the list of decision variables \\nand set the cost in A2 as less than or equal to the limit in B2. This new formulation is \\npictured in Figure 4-26.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 150, 'page_label': '129'}, page_content='129Optimization Modeling\\nTo recap then, you’ve transformed your previous cost objective into a constraint with \\nan upper bound. You’ve also transformed your hard constraints on quality into soft con-\\nstraints that can be relaxed by altering G27:G30. Your objective in D2 is to minimize the \\naverage amount you must degrade quality across your specs. Press Solve.\\nFigure 4-26: Solver implementation of the relaxed quality model\\nExcel ﬁ nds that with an average relaxation of 35 percent on each end of the bounds, a \\nsolution can be achieved that meets the cost constraint, as shown in Figure 4-27.\\nNow that you have the model set up, one thing you can do is provide more information \\nto Mr. Landingsly than he asked for. You know that for $1.23 million you get a quality \\ndegradation of 0 percent, so why not step down the cost in increments of 20 grand or so \\nand see what quality degradation results? At $1.21 million it’s 5 percent, at $1.19 million \\nit’s 17 percent, and so forth, including 35 percent, 54 percent, 84 percent, and 170 percent. \\nIf you try to dip below $1.1 million the model becomes infeasible.\\nCreating a new tab called Frontier, you can paste all these solutions and graph them \\nto illustrate the trade-off  between cost and quality (see Figure 4-28). To insert a graph \\nlike the one pictured in Figure 4-28, simply highlight the two columns of data on the \\nFrontier sheet and insert a Smoothed Line Scatter plot from the Scatter selection in Excel \\n(see Chapter 1 for more on inserting charts).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 151, 'page_label': '130'}, page_content='130 Data Smart\\nFigure 4-27: Solution to the relaxed quality model\\nFigure 4-28: Graphing the trade-off between cost and quality'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 152, 'page_label': '131'}, page_content='131Optimization Modeling\\nDead Squirrel Removal: The Minimax Formulation\\nIf you look at the relaxed quality solution for a cost bound of $1.17 million, there’s a poten-\\ntial problem. Sure, the average relaxation across the taste and color bounds is 35 percent, \\nbut for color it’s 80 percent and for Brix/Acid ratio it’s 51 percent. The average hides this \\nvariability.\\nWhat you’d rather do in this situation is minimize the maximum relaxation across the four \\nquality bounds. This problem is commonly called a “minimax” problem because you’re \\nminimizing a maximum, and it’s fun to say really fast. Minimax, minimax, minimax.\\nBut how can you do that? If you make your objective function \\nMAX(G27:G30), you’ll be \\nnon-linear. You could try that with the evolutionary solver, but it’ll take forever to solve. \\nIt turns out there’s a way to model this non-linear problem in a linear way.\\nFirst, copy the relaxed model to a new tab called Minimax Relaxed Quality. \\nNow, how many of you have had to pick up and get rid of a dead animal? Last summer \\nI had a squirrel die in my blisteringly hot attic here in Atlanta, and the smell knocked \\nmany brave men and women to their knees.\\nHow did I get rid of that squirrel? \\nI refused to touch it or deal with it directly. \\nInstead, I scooped it from below with a shovel and pressed down on it from above with \\na broom handle. It was like picking it up with giant salad tongs or chopsticks. Ultimately, \\nthis pincer move had the same eff ect as grabbing the squirrel with my bare hands, but it \\nwas less gross.\\nYou can handle the calculation \\nMAX(G27:G30)  in the same way I handled that dead \\nsquirrel. Since you’re no longer computing the average of G27:G30, you can clear out the \\nobjective in D2. That’s where you would compute the \\nMAX() function, but you can leave \\nthe cell blank. It needs to be lifted up to the max somehow without being touched directly.\\nHere’s how you can do it:\\n 1. Set the objective, D2, to be a decision variable, so that the algorithm can move it as \\nneeded. Keep in mind that since you’ve set the model to be a minimization, Simplex \\nis going to try to send this cell down as far as it can go.\\n 2. Set G27:G30 to be less than or equal to D2 using the Add Constraint window. \\nD2 must go in the right side of the Add Constraint dialogue for Excel to allow an \\nunequal number of cells (4 cells in a range on the left side and 1 upper bound on \\nthe right side). Unlike elsewhere in this chapter where you couldn’t use two dif-\\nferent sized ranges in a constraint, this works because Excel has been designed to \\nunderstand the case where the right side of the constraint is a single cell.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 153, 'page_label': '132'}, page_content='132 Data Smart\\nOkay, so what did you just do?\\nWell, as the objective function of the model, simplex will try to force D2 down to 0, \\nwhile the taste and color constraints will force it up to maintain a workable blend. Where \\nwill cell D2 land? The lowest it can go will be the maximum of the four relaxation per-\\ncentages in G27 through G30.\\nOnce the objective strikes that maximum, the only way the Solver can make progress \\nis by forcing that maximum down. Just like with the squirrel, the constraints are the \\nshovel under the squirrel and the minimization objective is the mop handle pressing \\ndown. Hence, you get the term “minimax.” Pretty cool, ain’t it? Or gross...depending on \\nhow you feel about dead squirrels.\\nNow that you’ve cleared out the formula in D2, the implementation in Solver (making \\nD2 a variable and adding G27:G30 ≤ D2) looks like Figure 4-29.\\nFigure 4-29: Solver setup for minimax quality reduction\\nSolving this setup yields a quality reduction of 58.7 percent, which, while greater than \\nthe average 34.8 percent from the previous model, is a vast improvement over the worst-\\ncase color relaxation of 84 percent.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 154, 'page_label': '133'}, page_content='133Optimization Modeling\\nIf-Then and the “Big M” Constraint\\nNow that you have a feel for vanilla linear modeling, you can add some integers. Mr. \\nLandingsly III eventually signs off   on your original procurement plan, but when you \\ndeliver it to the supply chain team, their eyes start twitching uncontrollably.\\nThey refuse to procure juice in any given month from more than four suppliers. Too \\nmuch paperwork, apparently.\\nOkay, so how do you handle this within the model? \\nTake a minute and think about what model modiﬁ  cations might be required before \\nmoving on.\\nStart by copying the original Optimization Model sheet to a new tab called Optimization \\nModel (Limit 4).\\nNow, regardless of how much juice you buy from a supplier, whether it’s 1,000 gallons \\nor 1,000,000 gallons, that counts as an order from one supplier. In other words, you need \\nto ﬁ nd a way to ﬂ ick a switch the moment you order a drop of juice from a supplier.\\nIn integer programming, a “switch” is a binary decision variable, which is merely a cell \\nthat Solver can set to 0 or 1 only.\\nSo what you want to do is deﬁ ne a range the same size as your order variables only it’ll \\nhold 0s and 1s, where a 1 is set when an order gets placed.\\nYou can place these variables in range C34:E44. Now, assuming they’re going to be \\nset to 1 when you place an order from the supplier, you can sum up each column in row \\n45 and make sure the sum is less than the limit of 4, which you can toss in row 46. The \\nresulting spreadsheet is pictured in Figure 4-30.\\nHere’s the tricky part though. You can’t use an \\nIF formula that sets the indicator to 1 \\nwhen the order quantity above is nonzero. That would be non-linear, which would force \\nyou to use the much slower evolutionary algorithm. For truly large problems with if-then \\nconstraints, the slower non-linear algorithms become useless. So you’ll need to “turn on” \\nthe indicator using linear constraints instead.\\nBut say you add a constraint to have the Brazilian Hamlin indicator variable turn on \\nwhen you place an order by using the constraint C34 ≥ C6. \\nIf C34 is supposed to be binary, then that’s going to limit C6 to a max of 1 (that is, \\n1,000 gallons ordered).\\nThus, you have to model this if-then statement, “if we order, then turn on the binary \\nvariable,” using something colloquially called a “Big M” constraint. “Big M” is just a num-\\nber, a big number, called M. In the case of C34, M should be big enough that you’d never'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 155, 'page_label': '134'}, page_content='134 Data Smart\\norder more Brazilian Hamlin than M. Well, you’ll never order more juice than is available, \\nright? For Hamlin, the available quantity is 672 thousand gallons. So make that M.\\nFigure 4-30:  Adding indicator variables to the spreadsheet\\nThen you can set a constraint where 672*C34 ≥ C6. When C6 is 0, C34 is allowed to \\nbe zero. And when C6 is greater than zero, C34 is forced to ﬂ ip to 1 in order to raise the \\nupper bound from 0 to 672.\\nTo implement this in the spreadsheet, you set up a new range of cells in F34:H44 where \\nyou’ll multiply the indicators to the left times their respective available quantities in range \\nG6:G16. The result is pictured in Figure 4-31.\\nIn Solver, you need to add C34:E44 to the range of decision variables. You also need \\nto make them binary, which you accomplish by putting a \\nbin constraint on the range.\\nTo put the “Big M” constraint in eff ect, you set C6:E16 ≤ F34:H44. You can then check \\nthe supplier counts and make sure they’re under four by setting C45:E45 ≤ C46:E46. The \\nresulting spreadsheet is pictured in Figure 4-32.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 156, 'page_label': '135'}, page_content='135Optimization Modeling\\nFigure 4-31: Setting up our “Big M” constraint values\\nFigure 4-32: Initializing Solver'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 157, 'page_label': '136'}, page_content='136 Data Smart\\nPress Solve. You’ll notice that the problem takes longer to solve with the addition of the \\nbinary variables. When using integer and binary variables in your formulation, Solver will \\ndisplay the best “incumbent” solution it ﬁ nds in the status bar. If for some reason Solver \\nis taking too long, you can always press the Escape key and keep the best incumbent it’s \\nfound so far.\\nAs shown in Figure 4-33, the optimal solution of the model restricted to four suppliers \\nper month is $1.24 million, about $16,000 more than the original optimum. Armed with \\nthis plan, you can return to the supply chain team and ask them if their reduced paper-\\nwork is worth an extra $16,000. \\nQuantifying the introduction of new business rules and constraints in this way is \\none of the hallmarks of employing optimization modeling in a business. You can place \\na dollar ﬁ gure to a business practice and make an informed decision to the question, “Is \\nit worth it?”\\nFigure 4-33: Optimal solution limited to four suppliers per period'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 158, 'page_label': '137'}, page_content='137Optimization Modeling\\nThat’s how “Big M” constraints are set up; you’ll encounter them again in the graph \\nclustering problem in Chapter 5.\\nMultiplying Variables: Cranking Up the Volume to 11\\nOPENSOLVER NEEDED FOR EXCEL 2010 AND EXCEL 2013\\nThat last bit was tough, but it was child’s play compared to this next business rule \\nyou’re going to model.\\nFor this next problem, please keep the worked spreadsheet available for download \\nwith you for reference. This is a tough one but worth learning if your business is con-\\nfronted with complex optimization problems. Also, nothing in the book is dependent \\non you learning this section, so if it gets too hard, just skip ahead. That said, I urge you \\nto dig deep and give it a shot.\\nIf you’re working in Excel 2010 or Excel 2013, you’ll want to have OpenSolver \\ninstalled and loaded (see Chapter 1 for an explanation). If you don’t use OpenSolver to \\nsolve the problem in those versions of Excel, you’ll get an error saying the optimization \\nmodel is too large. To use OpenSolver in this chapter, set up the problem normally as \\nshown in this section, but when it comes time to solve, use OpenSolver’s Solve button \\non the ribbon.\\nBefore you implement the limited supplier plan, you’re informed that the new “acid-\\nreducers” have been hooked up in the blending facility. Using ion exchange with a bed \\nof calcium citrate, the technology is able to neutralize 20 percent of the acid in the juice \\nthat’s run through it. This not only reduces acid percent by 20 but also increases the Brix/\\nAcid ratio by 25 percent.\\nBut the power and raw materials needed to run the reducer cost $20 per 1,000 gal-\\nlons of juice put through it. Not all orders from suppliers need to be put through the de-\\nacidiﬁ cation process; however, if an order is processed through the ion exchanger, the \\nentire order must be pumped through.\\nCan you create a new optimal plan that tries to use ion exchange to reduce the optimal \\ncost? Think about how you might set this one up. You now have to make a new set of \\ndecisions regarding when and when not to reduce the acid. How might those decisions \\ninteract with order quantities?\\nStart by copying the Optimization Model (Limit 4) tab to a new tab. Call it Optimization \\nModel Integer Acid.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 159, 'page_label': '138'}, page_content='138 Data Smart\\nThe problem with this business rule is that the natural way to model it is non-linear, \\nand that would force you to use a slow optimization algorithm. You could have a binary \\nvariable that you “turn on” when you want to de-acidify an order, but that means that the \\ncost of that de-acidifcation is: \\nDe-acid indicator * Amount purchased * $20\\nYou can’t multiply two variables together unless you want to switch to using the non-\\nlinear solver, but that thing is never gonna ﬁ gure out the complexities of this model. There \\nhas to be a better way to do this. Keep this in mind when doing linear programming: There \\nare very few things that cannot be linearized through the judicious use of new variables \\nmanipulated by additional constraints and the objective function like a pair of salad tongs.\\nThe ﬁ rst thing you’re going to need is a set of new binary variables that get “turned \\non” when you choose to de-acidify a batch of juice. You can insert a new chunk of them \\nin a rectangle between the Valencia orders and the quality constraints (cells C26:E36).\\nFurthermore, you can’t use the product of \\nDe-acid indicator * Amount purchased, \\nso instead you’ll create a new grid of variables below the indicators that you’re going to \\nforce to equal this amount without expressly touching them (a la dead squirrel). Insert \\nthese empty cells in C38:E48.\\nThe spreadsheet now has two empty grids of variables—the indicators and the total \\namount of juice being fed through acid reduction—as shown in Figure 4-34.\\nNow, if you want to multiply a de-acidiﬁ  cation binary variable times the amount of \\njuice you’ve ordered, what are the values that product can take on? There are a number \\nof distinct possibilities:\\n• If both the indicator and the product purchase amount are 0, their product is 0. \\n• If you order some juice but decide to not reduce the acid, the product is still 0. \\n• If you choose to reduce, the product is merely the amount of juice ordered.\\nIn every case, the total possible juice that can be de-acidiﬁ  ed is limited by the de-\\nacidiﬁ cation indicator variable times the total juice available to purchase. If you don’t \\nreduce the acid, this upper bound goes to zero. If you choose to reduce, the upper bound \\npops up to the max available for purchase. This is a “Big M” constraint just like in the \\nlast section.\\nFor Brazilian Hamlin then, this “Big M” constraint could be calculated as the indicator \\nin cell C26 times the amount available for purchase, 672,000 gallons, in cell G6. Adding \\nthis calculation next to the indicator variables in cell G26, you can copy it to the remain-\\ning months and varietals.\\nThis yields the worksheet shown in Figure 4-35.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 160, 'page_label': '139'}, page_content='139Optimization Modeling\\nFigure 4-34:  Indicator and amount variables added for the de-acidiﬁ  cation decision\\nOn the ﬂ  ip side, the total possible juice that can be de-acidiﬁ  ed is limited by the \\namount you decide to purchase, given in C6:E16. So now you have two upper bounds on \\nthis product:\\n• De-acid indicator * Amount available for purchase\\n• Amount purchased\\nThat’s one upper bound per variable in the original non-linear product.\\nBut you can’t stop there. If you decide to de-acidify a batch, you need to send the whole \\nbatch through. That means you have to add a lower bound to the two upper bounds to \\nhelp “scoop up” the de-acidiﬁ ed amount in C38:E48.\\nSo how about just using the purchase amount as the lower bound? In the case where \\nyou decide to de-acidify, that works perfectly. You’ll have a lower bound of the purchase'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 161, 'page_label': '140'}, page_content='140 Data Smart\\namount, an upper bound of the purchase amount, and an upper bound of the total amount \\navailable for purchase times a de-acidiﬁ cation indicator set to 1. These upper and lower \\nbounds force the amount going through de-acidiﬁ cation to be the whole shipment, which \\nis what you want. \\nFigure 4-35: Calculation added for upper bound on how much juice can be de-acidiﬁ  ed\\nBut what if you choose not to de-acidify a batch? Then one of the upper bounds becomes \\nan indicator of 0 times the amount available to purchase, whereas the lower bound is still \\nthe amount purchased. In that case, a non-zero purchase amount that’s not de-acidiﬁ ed \\nbecomes impossible.\\nHmmm. \\nSo you need a way to “turn off ” this lower bound in the situation where you choose \\nnot to de-acidify the juice.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 162, 'page_label': '141'}, page_content='141Optimization Modeling\\nInstead of making the lower bound the amount you ordered, why not make it the \\nfollowing: \\nAmount purchased - Amount available for purchase * (1 – de-acid \\nindicator)\\nIn the case where you choose to de-acidify, this lower bound bounces up to the amount \\nyou purchased. In the case where you don’t de-acidify, this value becomes less than or \\nequal to 0. The constraint still exists, but it’s for all intents worthless.\\nIt’s a bit janky, I know.\\nTry working it through an example. You buy 40,000 gallons of the Brazilian Hamlin \\njuice. Furthermore, you decide to de-acidify. \\nThe upper bounds on the amount you’re de-acidifying are the amount purchased of 40 \\nand the de-acid indicator times the amount available of 672.\\nThe lower bound on the amount you’re de-acidifying is 40 – 672 * (1-1) = 40. In other \\nwords, you have upper and lower bounds of 40, so you’ve sandwiched the amount you’re \\nde-acidifying right into \\nDe-acid indicator * Amount purchased without ever calculat-\\ning this quantity.\\nIf I choose not to de-acidify the Hamlin, the indicator is set to 0. In that case you have \\nupper bounds of 40 and 672*0 = 0. You have a lower bound of 40 – 672 * (1-0) = -632. \\nAnd since you’ve checked the box making all the variables be non-negative, that means \\nthat the amount of Hamlin you’re de-acidifying is sandwiched between 0 and 0. \\nPerfect! \\nAll right, so let’s add this lower bound in a grid to the right of the upper bound calcula-\\ntion. In cell K26 you’d type:\\n=C6-$G6*(1-C26)\\nAnd you can copy that formula to each varietal and month, giving you the spreadsheet \\nin Figure 4-36.\\nNext to the Total Reduced section, subtract that value from the total purchases in \\nC6:E16 to get the remaining Not Reduced quantities of juice. For example, in cell G38, \\nyou place:\\n=C6 – C38\\nYou can drag this across and down to the remaining cells in the grid (see Figure 4-37).\\nWrapping up the formulation, you need to alter the cost, Brix/Acid, and Acid % calcula-\\ntions. For cost, you can just add $20 times the sum of the month’s Total Reduced values \\ninto the Price cell. For example, January’s Price calculation would become:\\n=SUMPRODUCT(C6:C16,$L6:$L16)+20*SUM(C38:C48)\\nwhich you can then drag across to February and March.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 163, 'page_label': '142'}, page_content='142 Data Smart\\nFigure 4-36: Adding in a lower bound on de-acidiﬁ  cation\\nThe Brix/Acid and Acid % calculations will now be calculated off  of the split quantities \\nin the Total Reduced and Not Reduced sections of the spreadsheet. Not Reduced values will \\nbe put through a \\nSUMPRODUCT with their original specs, whereas the same SUMPRODUCT using \\nthe reduced acid juice will be scaled by 1.25 and 0.8, respectively, for BAR and Acid and \\nadded to the total in the monthly averages.\\nFor example, Brix/Acid for January in C51 can be calculated as:\\n=(SUMPRODUCT(G38:G48,$H6:$H16)+SUMPRODUCT(C38:C48,$H6:$H16)*1.25)/C21'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 164, 'page_label': '143'}, page_content='143Optimization Modeling\\nNow you need to modify the model in Solver. The objective function remains the same \\n(sum of price and shipping), but the decision variables now include the de-acid indicators \\nand amounts to be reduced located in C26:E36 and C38:E48.\\nAs for the constraints, you need to indicate that C26:E36 is \\nbin. Also, C38:C48 is less \\nthan or equal to the two upper bounds in C6:E16 and G26:I36. Also, you need a lower \\nbound constraint where C38:E48 is greater than or equal to K26:M36.\\nThis all yields the new model pictured in Figure 4-38.\\nFigure 4-37: Adding a “Not Reduced” calculation'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 165, 'page_label': '144'}, page_content='144 Data Smart\\nFigure 4-38:  Solver formulation for the de-acidiﬁ  cation problem\\nPress Solve and let the Branch and Bound do its thing. You’ll end up with an optimal \\nsolution that’s about $4,000 lower than in the previous formulation. Examining the new \\ndecision variables, you ﬁ nd that two batches—one from Arizona and one from Texas—are \\ngoing through the de-acidiﬁ cation process. The lower and upper bounds for those two \\nbatches match precisely to force the product of the variables into place (see Figure 4-39).\\nModeling Risk\\nThat last business rule was a toughie, but it illustrates how a modeler can linearize most \\nbusiness problems by adding more constraints and variables. However, no matter how \\neasy or hard the previous problems were, they all had one thing in common—they treat \\nthe input data as gospel.\\nThis doesn’t always conform to the reality many businesses ﬁ nd themselves in. Parts are \\nnot all to spec, shipments don’t always arrive on time, demand doesn’t match the forecast, \\nand so on. In other words, there’s variability and risk in the data.\\nSo how do you take that risk and model it within an optimization model?'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 166, 'page_label': '145'}, page_content='145Optimization Modeling\\nFigure 4-39: Solved de-acidiﬁ  cation model\\nNormally Distributed Data\\nIn the orange juice problem, you’re trying to blend juices to take out variability, so is it \\nreasonable to expect that the product you’re getting from your suppliers won’t have vari-\\nable specs?\\nChances are that shipment of Biondo Commune orange juice you’re getting from Egypt \\nwon’t have an exact 13 Brix/Acid ratio. That may be the expected number, but there’s \\nprobably some give around it. And oftentimes, that wiggle room can be characterized \\nusing a probability distribution.\\nA probability distribution, loosely speaking, gives a likelihood to each possible outcome \\nof some situation, and all the probabilities add up to 1. Perhaps the most famous and \\nwidely used distribution is the normal distribution, otherwise known as the “bell curve.” \\nThe reason why the bell curve crops up a lot is because when you have a bunch of inde-\\npendent, complex, real-world factors added together that produce randomly distributed'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 167, 'page_label': '146'}, page_content='146 Data Smart\\ndata, that data will often be distributed in a normal or bell-like way. This is called the \\ncentral limit theorem.\\nTo see this, let’s do a little experiment. Pull out your cell phone and grab the last four \\ndigits of each of your saved contacts’ phone numbers. Digit one will probably be uniformly \\ndistributed between 0 and 9, meaning each of those digits will show up roughly the same \\namount. Same goes for digits 2, 3, and 4. \\nNow, let’s take these four “random variables” and sum them. The lowest number you \\ncould get is 0 (0 + 0 + 0 + 0). The highest is 36 (9 + 9 + 9 + 9). There’s only one way to get \\n0 and 36. There are four ways to get 1 and four ways to get 35, but there’s a ton of ways to \\nget 20. So if you did this to enough phone numbers and graphed a bar chart of the various \\nsums, you’d have a bell curve that looks like Figure 4-40 (I used 1,000 phone numbers to \\nget the ﬁ gure, because I’m just that popular).  \\n1\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n3 5 7 9 11 13 15 17 19 21 23\\nSum of 4 digits\\nSumming the last 4 digits of the numbers\\nin your cell phone’s contact list\\nCount of instances\\n25 27 29 31 33\\nFigure 4-40:  Combining independent random variables to illustrate how they gather into a bell curve\\nThe Cumulative Distribution Function\\nThere’s another way of drawing this distribution that’s going to be super helpful, and it’s \\ncalled the cumulative distribution  function (CDF). The cumulative distribution function \\ngives the probability of an outcome that’s less than or equal to a particular value.\\nIn the case of the cell phone data, only 12 percent of the cases are less than or equal to \\n10, whereas 100 percent of the cases are less than or equal to 36 (since that’s the largest \\npossible value). This cumulative distribution is pictured in Figure 4-41.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 168, 'page_label': '147'}, page_content='147Optimization Modeling\\n1\\n0%\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\n70%\\n80%\\n90%\\n100%\\n3579 1 1 1 3 1 5 1 7 1 9 2 1 2 3\\nDigit sum\\nCumulative distribution function for summing last 4\\nphone number digits\\n% that’s LESS THAN OR EQUAL TO digit sum\\n25 27 29 31 33\\nFigure 4-41:  The cumulative distribution function for the cell phone contact sums \\nAnd here’s the cool thing about the cumulative distribution function— you can read it \\nbackward to generate samples from the distribution. \\nFor example, if you wanted to generate a random value from this contact list four-digit \\nsum distribution, you could generate a random number between 0 and 100 percent. Say \\nyou come up with 61 percent as your random value. Looking that up on the vertical axis \\nof the CDF, 61 percent lines up with 19 on the horizontal axis. And you could do this \\nover and over to generate a lot of samples from the distribution.\\nNow, a normal CDF can be described completely by two numbers: a mean and a standard \\ndeviation . The mean is nothing more than the center of the distribution. The standard \\ndeviation measures the variability or spread of the bell curve around the mean. \\nSay in the case of the juice you order from Egypt, it has a Brix/Acid mean of 13 and a \\nstandard deviation of 0.9. That means that 13 is the center of the probability distribution \\nand 68 percent of orders are going to be within +/-0.9 of 13, 95 percent will be within two \\nstandard deviations (+/-1.8), and 99.7 percent will be within three standard deviations \\n(+/-2.7). This is sometimes called the “68-95-99.7” rule.\\nIn other words, it’s pretty likely you’ll receive a 13.5 Brix/Acid batch from Egypt, but \\nit’s very unlikely you’ll receive a 10 Brix/Acid batch.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 169, 'page_label': '148'}, page_content='148 Data Smart\\nCALCULATING THE SAMPLE MEAN AND STANDARD DEVIATION\\nFor those of you who haven’t calculated standard deviation before and are interested \\nto know how it’s done, it’s super easy.\\nFigure 4-42 shows the past 11 orders of the Biondo Commune orange juice from \\nEgypt and their respective Brix/Acid measurements in column B. The sample mean of \\nthose measures is 13, as given in the original specs spreadsheet. \\nThe sample estimate of the standard deviation is just the square root of the mean \\nsquared error. By “error,” I just mean the deviation of each order from the expected \\nvalue of 13.\\nIn column C of Figure 4-42, you can see the error calculation, and the squared error \\ncalculation is in column D. The mean squared error is \\nAVERAGE(D2:D12), which comes \\nout to 0.77. The square root of the mean squared error is then 0.88. Easy enough!\\nIn practice however, when calculating the sample standard deviation for a small \\nnumber of orders, you get a better estimate if you sum the squared error and divide \\nthrough by 1 less than your total orders (in this case 10 instead of 11).\\nIf you make this adjustment, the standard deviation becomes 0.92, as shown in \\nFigure 4-42.\\nFigure 4-42:  An example of the sample standard deviation calculation\\nGenerating Scenarios from Standard Deviations in the Blending Problem\\nNOTE\\nJust as in the previous section, those using Excel 2010 and Excel 2013 will need to \\nemploy OpenSolver. Just set the problem up normally and use the OpenSolver Solve \\nbutton on the ribbon when the time comes. See Chapter 1 for more detail on OpenSolver.\\nImagine instead of receiving the Specs tab, you received standard deviations along with \\nyour speciﬁ cations in a tab titled Specs Variability, as shown in Figure 4-43. The goal is'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 170, 'page_label': '149'}, page_content='149Optimization Modeling\\nto ﬁ nd a blending plan that’s less than $1.25 million dollars that best meets the quality \\nexpectations in light of supplier variability.\\nYou can create a copy of the original Minimax Relaxed Quality tab called the Robust \\nOptimization Model, where the new standard deviations will go in N6:Q16 adjacent to \\nthe old speciﬁ cations.\\nOnce they’re in there, what do you do with them? \\nYou’re going to use the mean and standard deviation for the specs to take a Monte \\nCarlo simulation approach to solving this problem. The Monte Carlo method means that \\ninstead of somehow incorporating the distribution directly into the model, you sample \\nthe distribution, creating scenarios or instantiations from each set of samples, and then \\ninclude those samples in the model.\\nA scenario is one possible answer to the question, “If these are the distributions for \\nmy stats, what would an actual order look like?” To draw a scenario, you read the nor-\\nmal CDF—characterized by the mean and standard deviation—backward, as discussed \\npreviously with Figure 4-41.\\nFigure 4-43:  Speciﬁ  cations with standard deviation added\\nThe formula in Excel for reading the normal CDF backward (or “inverted” if you like) \\nis NORMINV.\\nSo generate a scenario in column B, starting at row 33 below everything that’s in the \\nworksheet already. You can call this Scenario 1.\\nIn B34:B44 you’ll generate an actual scenario of Brix/Acid values for all the suppliers. \\nIn B34 generate a random value for Brazilian Hamlin where its mean Brix/Acid is 10.5 \\n(H6) and its standard deviation is 2 (N6) using the \\nNORMINV formula:\\n=NORMINV(RAND(),$H6,$N6)'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 171, 'page_label': '150'}, page_content='150 Data Smart\\nYou’re feeding a random number between 0 and 100 percent into NORMINV along with \\nthe mean and standard deviation, and out pops a random Brix/Acid value. Let’s drag that \\nformula down to B44. \\nStarting at B45, you can do the same thing for Acid, then Astringency, then Color. The \\nrange B34:B77 now contains a single scenario, randomly drawn from the distributions. \\nDragging this scenario across the columns all the way to CW (note the absolute refer-\\nences that allow for this), you can generate 100 such random spec scenarios. Solver can’t \\nunderstand them if they remain non-linear formulas, so go ahead and copy and paste the \\nscenarios on top of themselves as values only. Now the scenarios are ﬁ xed data.\\nThis mound of scenario data in B34:CW77 is pictured in Figure 4-44.\\nFigure 4-44:  100 generated juice spec scenarios'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 172, 'page_label': '151'}, page_content='151Optimization Modeling\\nSetting Up the Scenario Constraints\\nOkay, so what you want to do is ﬁ nd a solution that relaxes the quality bounds the least \\nin order to meet them in each and every scenario you’ve generated. Just ﬁ  nd a solution \\nthat protects the product.\\nSo under the ﬁ rst scenario in cell B79 calculate the BAR for January as:\\n=SUMPRODUCT($C$6:$C$16,B34:B44)/$C$21\\nYou can do the same for February and March on rows 80 and 81 and then drag the \\nentire calculation right through column CW to get a Brix/Acid for each scenario.\\nDoing the same for the other specs, you end up with calculations on each scenario, as \\nshown in Figure 4-45.\\nFigure 4-45:  Spec calculations for each scenario\\nSetting up the model isn’t all that diffi  cult. You put a cost upper bound of $1.25 mil-\\nlion in B2. You’re still minimizing D2, the quality relaxation, in a minimax setup. All you \\nneed to do is place the quality bounds around all of the scenarios rather than just the \\nexpected quality values.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 173, 'page_label': '152'}, page_content='152 Data Smart\\nThus, for BAR, you add that B79:CW81 ≥ B27 and ≤ F27 and similarly for Acid, Astringency, \\nand Color, yielding the formulation shown in Figure 4-46.\\nFigure 4-46:  Solver setup for robust optimization\\nPress Solve. You’ll get a solution rather quickly. Now, if you generated the random \\nscenarios yourself rather than keeping the ones provided in the spreadsheet available for \\ndownload, the solution you get will be diff erent. For my 100 scenarios, the best quality I \\ncould get is a 133 percent relaxation while keeping cost under $1.25 million.\\nFor giggles, you can up the cost upper bound to $1.5 million and solve again. You get a \\n114 percent relaxation without the cost even going to the upper bound but rather staying \\nat about $1.3 million. It seems that upping the cost higher than that doesn’t give you any \\nmore leeway to improve quality (see the solution in Figure 4-47). \\nAnd that’s it! You now have a balance of cost and quality that meets constraints even \\nin random, real-world situations.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 174, 'page_label': '153'}, page_content='153Optimization Modeling\\nFigure 4-47: Solution to the robust optimization model\\nAN EXERCISE FOR THE READER\\nIf you’re a glutton for pain, I’d like to off er one more formulation to work through. \\nIn the previous problem, you minimized the percent you had to lower and raise the \\nquality bounds such that every constraint was satisﬁ  ed. But what if you cared only \\nthat 95 percent of the scenarios were satisﬁ ed? \\nYou would still minimize the quality relaxation percentage, but you’d need to stick \\nan indicator variable on each scenario and use constraints to set it to 1 when the sce-\\nnario’s quality constraints were violated. The sum of these indicators could then be set \\n≤5 as a constraint.\\nGive it a shot. See if you can work it.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 175, 'page_label': '154'}, page_content='154 Data Smart\\nWrapping Up\\nIf you stuck with me on those last couple of models, then bravo. Those suckers weren’t toy \\nproblems. In fact, this may be the hardest chapter in this book. It’s all downhill from here! \\nHere’s a little recap of what you just learned:\\n• Simple linear programming\\n• The minimax formulation\\n• Adding integer variables and constraints\\n• Modeling if-then logic using a “Big M” constraint\\n• Modeling the product of decision variables in a linear way\\n• The normal distribution, central limit theorem, cumulative distribution functions, \\nand the Monte Carlo method\\n• Using the Monte Carlo method to model risk within a linear program\\nYour head is probably spinning with all sorts of applications of this stuff  to your busi-\\nness right now. Or you’ve just downed a stiff  drink and never want to deal with linear \\nprogramming again. I hope it’s the former, because the truth is, you can get arbitrarily \\ncreative and complex with linear programming. In many business contexts you’ll often \\nﬁ nd models with tens of millions of decision variables.\\nPRACTICE, PRACTICE, PRACTICE! AND READ SOME MORE\\nModeling linear programs, especially when you have to execute funky “squirrel \\nremoval” tricks, can be rather non-intuitive. The best way to get good at it is to ﬁ nd \\nsome opportunities in your own line of work that could use modeling and have at it. \\nYou can’t memorize this stuff ; you have to get a feel for how to address certain mod-\\neling peculiarities. And that comes with practice.\\nIf you want some additional linear programming literature to supplement your prac-\\ntice, here are some free online resources that I highly recommend:\\n• The AIMMS optimization modeling book available at http://www.aimms.com/\\ndownloads/manuals/optimization-modeling  is an incredible resource. Don’t \\nskip their two Tips and Tricks chapters; those things are awesome.\\n• “Formulating Integer Linear Programs: A Rogue’s Gallery” from Brown and Dell \\nof the Naval Postgraduate School: \\nhttp://faculty.nps.edu/gbrown/docs/\\nBrown_Dell_INFORMS_Transactions_on_Education_January2 0 07.pdf.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 176, 'page_label': '155'}, page_content='5\\nT\\nhis chapter continues the discussion on cluster identification and analysis using the \\nwholesale wine dataset from Chapter 2. Although it’s perfectly fine to jump around \\nin this book, in this case I recommend at least skimming Chapter 2 before reading this \\nchapter, because I don’t repeat the data preparation steps, and you’re going to be using \\ncosine similarity, which was discussed at the end of Chapter 2.\\nAlso, the techniques used here rely on the “Big M” constraint optimization techniques \\nintroduced in Chapter 4, so some familiarity with that will be helpful.\\nThis chapter continues addressing the problem of detecting interesting groups of cus-\\ntomers based on their purchases, but it approaches the problem from a fundamentally \\ndiff erent direction. \\nRather than thinking about customers huddling around ﬂ ags planted on the dance ﬂ oor \\nto assign them to groups, as you did with k-means clustering (Chapter 2), you’re going to \\nlook at your customers in a more relational way. Customers buy similar things, and in that \\nway, they’re related to each other. Some are more “friendly” than others, in that they’re \\ninterested in the same stuff . So by thinking about how related or not related each customer \\nis to the others, you can identify communities of customers without needing to plant a set \\nnumber of ﬂ ags in the data that get moved around until people feel at home.\\nThe key concept that allows you to approach customer clustering in this relational way \\nis called a network graph. A network graph, as you’ll see in the next section, is a simple \\nway to store and visualize entities (such as customers) that are connected (by purchase \\ndata for instance).\\nThese days, network visualization and analysis are all the rage, and the techniques used \\nto mine insights from network graphs often work better than traditional techniques (such \\nas k-means clustering in Chapter 2), so it’s important that a modern analyst understand \\nand be able to leverage network graphs in their work.\\nWhen doing cluster analysis on a network, people often use the term community detection \\ninstead, which makes sense because many network graphs are social in nature and their \\nCluster Analysis \\nPart II: Network Graphs \\nand Community \\nDetection'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 177, 'page_label': '156'}, page_content='156 Data Smart\\nclusters do indeed make up communities. This chapter focuses on a particular community \\ndetection algorithm called modularity maximization.\\nAt a high level, modularity maximization rewards you every time you place two good \\nfriends in a cluster together and penalizes you every time you shove some strangers together. \\nBy grabbing all the rewards you can and avoiding as many penalties as possible, the tech-\\nnique leads to a natural clustering of customers. And here’s the cool part, which you’ll see \\nlater—unlike the k-means clustering approach, you don’t need to choose k. The algorithm \\ndoes it for you! In this way, the clustering technique used here takes unsupervised machine \\nlearning to a whole new level of knowledge discovery.\\nAlso, from a mathematical-sex-appeal perspective, k-means clustering, while rad, has \\nbeen around for over half a century. The techniques you’ll use in this chapter were devel-\\noped in just the past several years. This is cutting edge stuff .\\nWhat Is a Network Graph?\\nA network graph is a collection of things called nodes that are connected by relationships \\ncalled edges. Social networks like Facebook provide a lot of network-graphable data, such \\nas friends who are connected to you and possibly to each other. Hence, the term “the social \\ngraph” has come up a lot in recent years.\\nThe nodes in a network graph don’t have to be people of course, and the edges that \\nrepresent relationships don’t have to be interpersonal relationships. For instance, you \\ncould have nodes that are Facebook users and other nodes that are product pages they \\nlike. Those “likes” comprise the edges of the graph. Similarly, you could create a network \\ngraph of all the stops on your city’s transportation system. Or all the destinations and \\nroutes on Delta’s ﬂ ight map (in fact, if you look at the route map on any airline’s website, \\nyou’ll see it’s a canonical network graph).\\nOr you could get all spy-like and graph anyone who has called anyone on a GPS sat \\nphone within al-Qaeda in the Islamic Magreb. With the release of material on the NSA’s \\nspying eff orts by Edward Snowden, this last type of network graph has been getting a \\nlot of attention in the media. One example is the congressional discussion around NSA’s \\nability to perform a “three-hop” query—that is go into their network graph of phone call \\ndata and ﬁ nd people three hops from a known terrorist (nodes connected to a terrorist \\nby a three edge path in the graph).\\nWhatever your business is, I guarantee you have a graph hiding in your data. One \\nof my favorite network graphing projects is called DocGraph (\\nhttp://notonlydev.com/\\ndocgraph/). Some intrepid folks have used a Freedom of Information Act request to cre-\\nate a graph of all kinds of Medicare referral data. Doctors get connected to other doctors \\nvia referrals, and the graph can be used to identify communities, inﬂ  uential providers \\n(the doctor everyone goes to for the ﬁ nal opinion on a tricky diagnosis), and even cases \\nof fraud and abuse.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 178, 'page_label': '157'}, page_content='157Cluster Analysis Part II: Network Graphs and Community Detection  \\nNetwork graphs are a rare contradiction in the analytics world. They are aesthetically \\nbeautiful and yet extremely utilitarian in the way they store and enable certain analyses. \\nThese graphs allow analysts to discover all sorts of insights both visually and algorithmi-\\ncally, such as clusters, outliers, local inﬂ uencers, and bridges between diff erent groups.\\nIn the next section, you’ll visualize some network data to get a feel for how these things \\nwork. \\nVisualizing a Simple Graph\\nThe TV show Friends was one of the most popular sitcoms of the 1990s and early 2000s. \\nThe show centered around six friends: Ross, Rachel, Joey, Chandler, Monica, and Phoebe. \\nIf you’ve never heard of the show or these characters, you’re either super young or trapped \\nin a cave.\\nThese six characters become involved in a lot of romances with each other of various \\ntypes: real romances, fantasy romances that never amount to anything, play romances \\nbased on some dare or competition, and so on.\\nThink of these characters as six nodes or vertices on the graph. The relationships \\nbetween them are edges. Off  the top of my head, I can think of these edges:\\n• Ross and Rachel, obviously\\n• Monica and Chandler end up married.\\n• Joey and Rachel have a little romance going but ultimately decide it’s too weird.\\n• Chandler and Rachel meet each other in a ﬂ ashback episode over a pool table mis-\\nhap, and Rachel imagines what it’d be like to be with Chandler.\\n• Chandler and Phoebe play at a relationship and end up having to kiss, because \\nChandler refuses to admit he’s with Monica.\\nThese six characters and their ﬁ ve edges can be visualized as shown in Figure 5-1.\\nRoss Rachel\\nMonicaJoey\\nChandler Phoebe\\nFigure 5-1: Diagram of ro(faux)mances on Friends'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 179, 'page_label': '158'}, page_content='158 Data Smart\\nPretty simple, right? Nodes and edges. That’s all a network graph is. And note how net-\\nwork graphs have nothing whatsoever to do with the graphs you may be familiar with, such \\nas dot plots, line charts, and bar charts. No, these graphs are a diff erent animal entirely.\\nFigure 5-1 is what’s called an undirected network graph, because the relationships are \\nmutual by deﬁ nition. Something like Twitter data on the other hand is directed, that is, \\nI can follow you, but you don’t have to follow me. When visualizing a directed graph, the \\nedges are usually directional arrows.\\nNow, one of the drawbacks about using Excel to work on network graphs is that, unlike \\nother graphing and charting capabilities, Excel does not provide tools for visualizing \\nnetwork graphs.\\nSo for this chapter, I’m going to break my own ground rules for this book and use an \\nexternal tool called Gephi for some visualization and computation, which is discussed \\nmore in the next section. That said, you can ignore all the Gephi aspects of this chapter if \\nyou want to. All the actual data mining on network data can be done without visualizing \\nthe network in Gephi; you’re just doing that part for fun.\\nBut visualization aside, if you want to work on this type of graph, you need a numerical \\nrepresentation of the data. One intuitive representation is called an adjacency matrix. An \\nadjacency matrix is just a node-by-node grid of 0s and 1s, where a 1 in a particular cell \\nmeans “put an edge here” and a 0 means “these nodes are unconnected.”\\nYou can create an adjacency matrix out of the Friends data, as shown in Figure 5-2 (the \\nmatrix looks a bit like a Galaga-style lobster to me). The friends’ names line the columns \\nand rows, and relationships between them are shown with 1s. Notice how the graph is \\nsymmetric along the diagonal, because the graph is undirected. If Joey has an edge with \\nRachel, then the converse is true, and the adjacency matrix shows this. If relationships \\nwere one-sided, you could have a matrix without this symmetry.\\nAlthough the edges here are represented with 1s, they don’t have to be. You can \\nadd weights to the edges, such as capacities—think of diff  erent planes with diff  erent \\nNODEXL\\nIf you’re in Excel 2007 or 2010, the Social Media Research Foundation has released a \\ntemplate that allows network visualization in Excel called NodeXL. It’s not covered \\nin this book because it’s still early days for the software, and LibreOffi  ce and Excel \\n2011 for Mac users wouldn’t be able to follow along. If you’re interested, you can \\ncheck out NodeXL for yourself at \\nhttp://www.smrfoundation.org/nodexl/.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 180, 'page_label': '159'}, page_content='159Cluster Analysis Part II: Network Graphs and Community Detection  \\ncapacities ﬂ ying routes or varying bandwidths available on diff erent links of an IT network. \\nA weighted adjacency matrix is also called an affi  nity matrix.\\nFigure 5-2: An adjacency matrix for the Friends data\\nBrief Introduction to Gephi\\nLet’s go ahead and get Gephi running so you can import and visualize the Friends dataset. \\nThen you’ll know your way around later when things get real all up in here. \\nGephi is an open source network visualization tool written in Java, and it’s the main \\nculprit behind many of the network visualization graphics you see in the media today. \\nIt’s easy to produce striking pictures, and people seem to have taken to it for graphing \\ntweets like bunnies to carrots.\\nThe reason why I’ve waived my usual hesitancy to stay in Excel is that Gephi ﬁ  lls in \\nthe network visualization gap in Excel, it’s free, and it works on Windows, Mac OS, and \\nLinux, so no matter what computer you’re using, you can follow along.\\nYou don’t have to do these visualization steps. If you just want to follow along in the \\nﬁ gures feel free, but I recommend getting your hands dirty. It’s fun. Keep in mind, though, \\nthat this book is not about Gephi. If you want to get really crazy with this tool, check out \\nthe resources at \\nwiki.gephi.org for deeper instruction.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 181, 'page_label': '160'}, page_content='160 Data Smart\\nGephi Installation and File Preparation\\nTo download Gephi, navigate to gephi.org in your browser, and then download and install \\nthe package following the instructions for your OS at http://gephi.org/users/install/. \\nIf you want a general tutorial on Gephi, check out the quick start guide at https://\\ngephi.org/users/quick-start/. Also, inside the application, Gephi has a Help selection \\nin the menu bar if you need it.\\nOnce Gephi is installed, you need to prep the adjacency matrix for importing into the \\nvisualization tool.\\nNow, I ﬁ nd that importing an adjacency matrix into Gephi takes one step more than it \\nshould. Why? Because Gephi doesn’t accept comma-separated adjacency matrices. Each \\nvalue has to be separated by a semicolon.\\nAlthough Kurt Vonnegut said in A Man Without A Country, “Do not use semicolons. \\nThey are transvestite hermaphrodites representing absolutely nothing. All they do is show \\nyou’ve been to college,” Gephi has ignored his sound advice. My apologies. So follow along, \\nand I’ll take you through the import process.\\nI’ve made the FriendsGraph.xlsx spreadsheet available with the book (download at the \\nbook’s website at \\nwww.wiley.com/go/datasmart), or if you like, you can just hand-jam in \\nthe small dataset from the adjacency matrix pictured in Figure 5-2.\\nThe ﬁ rst thing you’re going to do to import this graph into Gephi is save it as a CSV, \\nwhich is a plain-text, comma-separated ﬁ le format. To do so, go to Save As in Excel and \\nchoose CSV from the format list. The ﬁ lename will end up as FriendsGraph.csv, and when \\nyou save it, Excel may bark some warnings at you, which I give you permission to ignore.\\nOnce you’ve exported the ﬁ le, you need to replace all the commas in it with semico-\\nlons. To do this, open the ﬁ le in a text editor (such as Notepad on Windows or TextEdit \\non Mac OS) and ﬁ nd and replace the commas with semicolons. Save the ﬁ  le. Figure 5-3 \\nshows this process in Mac OS TextEdit.\\nFigure 5-3: Replacing commas with semicolons in the Friends graph CSV'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 182, 'page_label': '161'}, page_content='161Cluster Analysis Part II: Network Graphs and Community Detection  \\nOnce that’s completed, open your freshly installed copy of Gephi, and using the Open \\nGraph File option on the Welcome screen (see Figure 5-4), select the FriendsGraph.csv \\nﬁ le you just edited.\\nFigure 5-4:  Open the FriendsGraph.csv ﬁ  le in Gephi.\\nWhen you attempt to open the ﬁ le, an Import Report window will pop up. Note that six \\nnodes and ten edges have been detected. The reason why ten edges are listed is because \\nthe adjacency matrix is symmetric, so each relationship is duplicated. To resolve this \\nduplication, change the Graph Type from directed to undirected in the import window \\n(see Figure 5-5). Press OK.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 183, 'page_label': '162'}, page_content='162 Data Smart\\nFigure 5-5:  Importing the Friends graph\\nLaying Out the Graph\\nMake sure the Overview tab is selected in the top left of the Gephi window. If it is selected, \\nyour Gephi window should look something like Figure 5-6. The nodes and edges are laid \\nout haphazardly in space. The zoom is all out of whack so the graph is barely visible. Your \\ninitial layout will likely appear diff erent.\\nLet’s make this graph a little prettier. A couple of navigational items you should be \\naware of—you can zoom in with the scroll wheel on your mouse, and you can move the \\ncanvas around by right-clicking in the space and dragging the graph until it’s centered.\\nBy clicking the T button at the foot of the overview window, you can add labels to the \\ngraph nodes so you know which character is which node. After zooming in, adjusting, \\nand adding labels, the graph now looks as shown in Figure 5-7.\\nYou need to lay this graph out in a nicer fashion. And luckily, Gephi has a bunch of \\nalgorithms for automating this process. Many of them use forces such as gravity between \\nconnected nodes and repulsion between unconnected nodes to settle things into place. \\nThe layout section of Gephi is in the bottom-left window of the overview panel. Feel free \\nto select things haphazardly from the menu to try them out.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 184, 'page_label': '163'}, page_content='163Cluster Analysis Part II: Network Graphs and Community Detection  \\nFigure 5-6:  Initial layout of the Friends graph\\nNOTE\\nBe warned that some of the layout algorithms are going to shrink or expand the \\ngraph such that you’ll have to zoom in or out to see the graph again. Also, the sizes \\nof your labels are going to get out of whack, but there’s a Label Adjust selection \\nunder the Layout drop-down menu to ﬁ x that.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 185, 'page_label': '164'}, page_content='164 Data Smart\\nFigure 5-7: The Friends graph is decipherable but messy.\\nTo get my preferred layout, the ﬁ rst thing I’m going to do is select ForceAtlas 2 from the \\nlayout menu and press the Run button. This is going to move my nodes around to better \\npositions. But the labels are now huge (see Figure 5-8).\\nSelect Label Adjust from the menu and press Run. You’ll get something that looks much \\nbetter. I can see that Rachel and Chandler are really the most well-connected in the graph. \\nObviously, Monica and Ross are distant because they’re brother and sister, and so on.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 186, 'page_label': '165'}, page_content='165Cluster Analysis Part II: Network Graphs and Community Detection  \\nNode Degree\\nOne concept in network graphing that’s going to be important in this chapter is that of \\ndegree. The degree of a node is simply the count of edges connected to it. So Chandler \\nhas a degree of 3, whereas Phoebe has a degree of 1. You can use these degrees in Gephi \\nto resize nodes.\\nFigure 5-8:  After running ForceAtlas 2 on the Friends graph'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 187, 'page_label': '166'}, page_content='166 Data Smart\\nTo get a sense of the average degree of the graph and who has what degree, press the \\nAverage Degree button on the right side of Gephi in the Statistics section. This will pop \\nup a window like the one shown in Figure 5-9, where the average degree of the graph is \\n1.6667 with four nodes of degree 1 and two nodes of degree 3 (Rachel and Chandler).\\nClose this window and navigate to the Ranking section of the Overview window in \\nthe top left box. Select the Nodes section and the red gemstone label that indicates node \\nresizing. Select Degree from the drop-down and toggle the minimum and maximum sizes \\nfor nodes. When you press Apply, Gephi will resize the nodes using degree as a proxy for \\nimportance. I’ve called out this section of the Overview window in Figure 5-10.\\nPretty Printing\\nAlthough these pictures look okay, you’re not going to hang them on your wall. To prepare \\nthe graph for printing an image, click the Preview pane at the top of Gephi.\\nINDEGREE, OUTDEGREE, IMPORTANCE, AND BAD BEHAVIOR\\nIn a directed graph, the count of edges going into a node is called the indegree. The \\ncount of outbound edges is the outdegree. Indegree in a social network is a simple \\nway to gauge the prestige of a node. This is often the ﬁ rst value people look at on \\nFacebook or Twitter to gauge importance. “Oh, they have a lot of followers…they \\nmust be a big deal.”\\nNow, this metric can certainly be gamed. Who exactly are these followers whose \\nedges ﬂ ow into your node? Maybe they’re all fake users you signed up for to heighten \\nyour own prestige.\\nGoogle uses indegree (in search engine speak this is a backlink  count) in their \\nPageRank algorithm. When someone fakes inbound links to their website to heighten \\nits prestige and move up the search results, that’s called link spam. In contexts such as an \\nInternet search where rankings mean big business, more complex measures of prestige, \\ninﬂ uence, and centrality have evolved to account for such bad behavior.\\nAs you’ll see in Chapter 9, these network graph concepts are useful in outlier  \\ndetection. Rather than ﬁ nding who is central in a graph, you can use indegree to ﬁ  nd \\nwho’s on the periphery.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 188, 'page_label': '167'}, page_content='167Cluster Analysis Part II: Network Graphs and Community Detection  \\nUnder the Preview Settings tab, select the Black Background preset from the Presets \\ndrop-down (because you have hacker delusions), and click the Refresh button at the bot-\\ntom left of the window.\\nGephi will paint the graph with stunning, curvy beauty (see Figure 5-11). Note how the \\nlabels are resized with the nodes, which is awesome. I ﬁ nd the edges of this graph a little \\non the thin side, so I bumped the edge thickness up from 1 to 3 on the left settings pane.\\nFigure 5-9: Calculating the average degree of a graph'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 189, 'page_label': '168'}, page_content='168 Data Smart\\nFigure 5-10:  Resizing the graph according to node degree\\nIf you want to export this image to a graphics ﬁ  le (for example, a .png ﬁ le), press the \\nExport button in the bottom left of the preview settings section. You can then distribute \\nthe graph on a website, in a PowerPoint presentation, or even in a book on data science.\\nTouching the Graph Data\\nBefore you move back to Excel to confront the wholesale wine problem from Chapter 2, I \\nwant to take you through the Data Laboratory section of Gephi. Click Data Laboratory at \\nthe top of Gephi to see the underlying data that you’ve imported into the graph.\\nNote that there are two sections of data: Nodes and Edges. In the Nodes section, you \\nsee the six characters. And because you went through the Average Degree calculation \\nearlier, a column for Degree has been added to the node dataset. If you want to, you can \\nexport this column back to Excel by pressing the Export Table button on the menu bar. \\nSee Figure 5-12.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 190, 'page_label': '169'}, page_content='169Cluster Analysis Part II: Network Graphs and Community Detection  \\nFigure 5-11: A prettier Friends graph\\nFigure 5-12: Node information with degree count in the Data Laboratory'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 191, 'page_label': '170'}, page_content='170 Data Smart\\nClicking the edges section, the ﬁ ve edges with their endpoints are laid out. Each edge \\nwas a weight of 1, because you imported an adjacency matrix with all 1s. If you had \\nchanged some of those values to be higher in the case of, say, an actual marriage then \\nthose higher weights would be reﬂ  ected in this column (they also would have aff  ected \\nthe ForceAtlas 2 layout).\\nAll right! So there’s your 30,000-foot tour of Gephi. Let’s get back to clustering the \\nwholesale wine data, and you’ll return to Gephi later to do some more visualizations and \\ncomputations.\\nBuilding a Graph from the Wholesale Wine Data\\nNOTE\\nThe Excel workbook used in this chapter, “WineNetwork.xlsx,” is available for \\ndownload at the book’s website at www.wiley.com/go/datasmart. This workbook \\nincludes all the initial data if you want to work from that. Or you can just read along \\nusing the sheets I’ve already put together in the workbook.\\nIn this chapter, I want to demonstrate how to detect clusters within your customer pur-\\nchase data by representing that data as a graph. Some businesses have data that’s already \\ngraphable, such as the Medicare referral data discussed earlier. \\nBut in this case, the wine purchase matrix from Chapter 2 does not represent customer-\\nto-customer relationships out of the box.\\nTo start, you should ﬁ gure out how to graph the wholesale wine dataset as a network. \\nAnd that means constructing an adjacency matrix similar to the Friends adjacency matrix \\nshown in Figure 5-2. From there you’ll be able to visualize and compute whatever you \\nwant on the graph.\\nI’ll pick up the analysis using the Matrix tab in the WineNetwork.xlsx workbook \\n(available for download with this book). If you remember, this is the same Matrix tab \\nyou created at the beginning of Chapter 2 from the wine sale transactional data and the \\nwholesale deal metadata. \\nPictured in Figure 5-13, the rows of the Matrix tab give details of the 32 wine deals \\noff ered by Joey Bag O’ Donuts Wine Emporium last year. In the columns of the sheet are'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 192, 'page_label': '171'}, page_content='171Cluster Analysis Part II: Network Graphs and Community Detection  \\ncustomer names, and each (deal, customer) cell has a value of 1 if that customer purchased \\nthat deal.\\nFigure 5-13: The Matrix tab showing who bought what \\nSo you need to turn this data from Chapter 2 into something similar to the Friends \\nadjacency matrix, but how do you go about doing that? \\nIf you created the Distances matrix for the k-means silhouette in Chapter 2, you’ve \\nalready seen something similar. For that calculation, you created a matrix of distances \\nbetween each customer based on the deals they took (shown in Figure 5-14).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 193, 'page_label': '172'}, page_content='172 Data Smart\\nFigure 5-14: The customer distances tab from Chapter 2\\nThis dataset was oriented in a customer-to-customer fashion just like the Friends data-\\nset. Connections between customers were characterized by how their purchases aligned.\\nBut there are a couple of problems with this customer-to-customer distance matrix \\ncreated in Chapter 2:\\n• At the end of Chapter 2 you discovered that asymmetric similarity and distance \\nmeasures between customers work much better than Euclidean distance in the case \\nof purchase data. You care about purchases, not “non-purchases.” \\n• If you want to draw edges between two customers, you want to do so because the \\ntwo customers are similar not because they are distant, so this calculation needs to \\nbe reversed. This closeness of purchases is captured via cosine similarity, so you \\nneed to create a similarity matrix in contrast to Chapter 2’s distance matrix.\\nCreating a Cosine Similarity Matrix\\nIn this section, you’ll take the Matrix tab in your notebook and construct from it a cus-\\ntomer-to-customer graph using cosine similarity. The process for doing this in Excel, \\nusing numbered rows and columns together with the \\nOFFSET formula, is identical to that \\nused in Chapter 2 for the Euclidean distances sheet. For more on OFFSET, see Chapter 1.\\nYou’ll start by creating a tab called Similarity in which you will paste a customer-by-\\ncustomer grid, whereby each customer is numbered in each direction. Remember that'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 194, 'page_label': '173'}, page_content='173Cluster Analysis Part II: Network Graphs and Community Detection  \\ncopying and pasting customers from the Matrix tab down the rows requires using the \\nPaste Special feature in Excel with the Transpose box checked. \\nThis empty grid is shown in Figure 5-15.\\nFigure 5-15: The empty grid for the cosine similarity matrix\\nStart by computing the cosine similarity between Adams and himself (which should \\nbe 1). As a refresher, recall the deﬁ  nition of cosine similarity between two customers’ \\nbinary purchase vectors that you read in Chapter 2:\\nThe count of matched purchases in the two vectors divided by the product of the square root \\nof the number of purchases in the ﬁ rst vector times the square root of the number of purchases \\nin the second vector. \\nAdams’ purchase vector is Matrix!$H$2:$H$33;  so in order to compute the cosine \\nsimilarity of Adams to himself, you use the following formula in cell C3:\\n=SUMPRODUCT(Matrix!$H$2:$H$33,Matrix!$H$2:$H$33)/\\n  (SQRT(SUM(Matrix!$H$2:$H$33))*SQRT(SUM(Matrix!$H$2:$H$33)))\\nIn the top of the formula you take the SUMPRODUCT  of the purchase vectors you care \\nabout to count matched purchases. In the denominator, you take the square roots of the \\nnumber of purchases for each customer and multiply them.\\nNow, this computation works for Adams, but you want to drag it around the \\nsheet so you don’t have to type each formula individually. And to make that \\nhappen, you use the \\nOFFSET  formula. By replacing Matrix!$H$2:$H$33  with \\nOFFSET(Matrix!$H$2:$H$33,0,Similarity!C$1)  for the columns and, similarly using'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 195, 'page_label': '174'}, page_content='174 Data Smart\\nOFFSET(Matrix!$H$2:$H$33,0,Similarity!$A3) for the rows, you get a formula that uses \\nthe customer numbers in column A and row 1 to shift the purchase vectors being used \\nin the similarity calculation.\\nThis leads to a slightly more ugly (sorry!) formula for cell C3:\\n=SUMPRODUCT(OFFSET(Matrix!$H$2:$H$33,0,Similarity!C$1),\\n   OFFSET(Matrix!$H$2:$H$33,0,Similarity!$A3))/\\n   (SQRT(SUM(OFFSET(Matrix!$H$2:$H$33,0,Similarity!C$1)))\\n   *SQRT(SUM(OFFSET(Matrix!$H$2:$H$33,0,Similarity!$A3))))\\nThis formula locks down Matrix!$H$2:$H$33 by the absolute references, so as you drag \\nthe formula around the sheet, it stays the same. Similarity!C$1 will change columns but \\nwill stay on row 1 where you want it, and Similarity!$A3 will stay in column A.\\nBut you’re not quite done. You’re interested in creating a graph of customers who are \\nsimilar to each other, but honestly, you don’t care about the diagonal of the matrix. Yes, \\nAdams is identical to himself and has a cosine similarity of 1, but you’re not interested \\nin drawing a graph with edges that loop back to point where they start, so you need to \\nmake all those entries 0 instead.\\nThis just means wrapping the cosine similarity calculation in an \\nIF statement to check \\nwhether the customer on the row equals the one in the column. Thus, you get the ﬁ  nal \\nformula of:\\nIF(C$1=$A3,0,SUMPRODUCT(OFFSET(Matrix!$H$2:$H$33,0,Similarity!C$1),\\n   OFFSET(Matrix!$H$2:$H$33,0,Similarity!$A3))/\\n   (SQRT(SUM(OFFSET(Matrix!$H$2:$H$33,0,Similarity!C$1)))\\n   *SQRT(SUM(OFFSET(Matrix!$H$2:$H$33,0,Similarity!$A3)))))\\nNow that you have a formula that you can drag around, grab the bottom-right corner \\nof C3, drag it across the sheet to CX3, and drag it down to CX102.\\nYou now have a cosine similarity matrix that shows which customers match each other. \\nPlacing some conditional formatting on the grid, you get what’s pictured in Figure 5-16.\\nProducing an r-Neighborhood Graph\\nThe Similarity tab is a weighted graph. Each pair of customers either has a 0 between \\nthem or some non-zero cosine similarity value that shows how strong their edge should \\nbe. As it is, this similarity matrix is an affi  nity matrix.\\nSo why not just dump this affi  nity matrix out and peek at it in Gephi? Maybe you’re \\nall set to do the analysis on the graph as is.\\nSure, exporting the CSV and importing it into Gephi is possible at this step. But let \\nme save you the heartache and just throw up an image (Figure 5-17) of the graph after \\nit’s been laid out in Gephi. It’s a huge mess of edges going every which way. Too many \\nconnections prevent the layout algorithms from properly moving nodes away from each \\nother, so in the end you have an oblong chunk of noise.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 196, 'page_label': '175'}, page_content='175Cluster Analysis Part II: Network Graphs and Community Detection  \\nFigure 5-16: The completed customer cosine similarity matrix\\nFigure 5-17: The mess of a cosine similarity customer-to-customer graph'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 197, 'page_label': '176'}, page_content='176 Data Smart\\nYou’ve taken about 300 purchases and turned them into thousands of edges in the \\ngraph. Some of these edges you can probably chalk up to randomness. Yeah, maybe you \\nand I lined up on 1 of our 10 wine purchases, and you have a teeny tiny cosine similarity, \\nbut is that edge worth drawing on the graph?\\nIn order to make sense of the data, it’s best if you prune edges from the graph that \\nreally don’t matter all that much, and keep only the strongest relationships on there—the \\nrelationships that don’t just come from one lucky shared purchase.\\nOkay, so which edges should you drop? \\nThere are two popular techniques for pruning edges from network graphs. You can \\ntake the affi  nity matrix and build one of the following:\\n• An r-neighborhood graph:  In an r-neighborhood graph, you keep only the edges \\nthat are of a certain strength. For instance, in the affi  nity matrix, edge weights range \\nfrom 0 to 1. Maybe you should drop all edges below 0.5. That’d be an example of \\nan r-neighborhood graph where r is 0.5.\\n• A k nearest neighbors  (kNN) graph: In a kNN graph, you keep a set number of \\nedges (k) going out of each node. For instance, if you set k to 5, you’d keep the ﬁ ve \\nedges coming out of each node that have the highest affi  nities.\\nNeither graph is superior to the other. It depends on the situation.\\nThis chapter focuses on the ﬁ rst option, an r-neighborhood graph. I leave it as an exer-\\ncise for you to go back and work the problem with a kNN graph. It’s pretty easy to imple-\\nment in Excel using the \\nLARGE formula (see Chapter 1 for more on LARGE). In Chapter 9, \\nwe’ll use a kNN graph for outlier detection.\\nAll right. So how do you take the Similarity tab and turn it into an r-neighborhood \\nadjacency matrix? Well, ﬁ rst you need to settle on what r should be.\\nIn the white space below the similarity matrix, count how many edges (non-zero simi-\\nlarity values) you have in the affi  nity matrix using the formula in cell C104:\\n=COUNTIF(C3:CX102,\">0\")\\nThis returns 2,950 edges made from the original 324 sales. What if you kept only the \\ntop 20 percent of them? What would the value of r have to be to make that happen? Well, \\nbecause you have 2,950 edges, the 80th percentile similarity value would be whatever the \\n590th edge has. So below the edge count in C105, you can use the \\nLARGE formula to get \\nthe 590th largest edge weight (see Figure 5-18):\\n=LARGE(C3:CX102,590)\\nThis returns a value of 0.5. So you can keep the top 20 percent of edges by throwing \\naway everything with a cosine similarity of less than 0.5.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 198, 'page_label': '177'}, page_content='177Cluster Analysis Part II: Network Graphs and Community Detection  \\nFigure 5-18:  Calculating the 80th percentile of edge weights \\nNow that you have the cutoff for the r-neighborhood graph, construction of \\nthe adjacency matrix is super easy. First create a new tab in the workbook called \\nr-NeighborhoodAdj, and paste the customer names in column A and row 1 to create a grid.\\nIn any cell in the grid, you put a 1 if the similarity value on the previous Similarity tab \\nis greater than 0.5. So, for example, in cell B2, you can use the following formula:\\n=IF(Similarity!C3>=Similarity!$C$105,1,0)\\nThe IF formula simply checks the appropriate similarity value against the cutoff   in \\nSimilarity$C$105 (0.5) and assigns a 1 if it’s large enough. Because Similarity$C$105 \\nis locked down with absolute references, you can drag this formula across the columns \\nand down the rows to ﬁ ll in the whole adjacency matrix, as shown in Figure 5-19 (I’ve \\nused some conditional formatting for the beneﬁ t of the ﬁ gure).\\nYou now have the r-neighborhood graph of the customer purchase data. You’ve trans-\\nformed the purchase data into customer relationships and then whittled those down to \\na set of meaningful ones.\\nIf you were to now export the r-neighborhood adjacency matrix to Gephi and lay it out, \\nyou would get something much improved over Figure 5-17. Export the graph yourself, do \\nthe semicolon two-step, and take a peek along with me.\\nAs shown in Figure 5-20, there are at least two tightly knit communities in the graph \\nthat kinda look like tumors. One of them is well-separated from the rest of the herd, \\nwhich is awesome, because it means their interests separate them from other customers.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 199, 'page_label': '178'}, page_content='178 Data Smart\\nFigure 5-19: The 0.5-neighborhood adjacency matrix\\nFigure 5-20: Gephi visualization of the r-neighborhood graph'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 200, 'page_label': '179'}, page_content='179Cluster Analysis Part II: Network Graphs and Community Detection  \\nAnd then there’s poor old Parker, the one customer who didn’t end up with any edges \\ngreater than or equal to 0.5 cosine similarity. So he’s by himself, crying in his tea. I hon-\\nestly feel bad for the guy, because the layout algorithms are going to try to toss him as far \\nas possible from the connected part of the graph.\\nAll right! So now you have a graph that you can eyeball. And in fact, just laying a graph \\nout and eyeballing it—separating it into communities by inspection—isn’t half bad. You’ve \\ntaken high-dimensional data and distilled it into something ﬂ  at like the middle school \\ndance ﬂ oor from Chapter 2. But if you had thousands of customers instead of a hundred, \\nyour eyeballs wouldn’t be terribly helpful. Indeed, even now, there’s a mesh of custom-\\ners in the graph who are hard to group together. Are they in one community or several?\\nThis is where modularity maximization comes into play. The algorithm uses these \\nrelationships in the graph to make community assignment decisions even when your \\neyeballs might have trouble.\\nHow Much Is an Edge Worth? Points and Penalties in \\nGraph Modularity\\nPretend that I’m a customer hanging out in my graph, and I want to know who belongs \\nin a community with me.\\nHow about that lady who’s connected to me by an edge? Maybe. Probably. We are con-\\nnected after all.\\nHow about the guy on the other side of the graph who shares no edge with me? Hmmm, \\nit’s much less likely.\\nGraph modularity quantiﬁ es this gut feeling that communities are deﬁ ned by connections. \\nThe technique assigns scores to each pair of nodes. If two nodes aren’t connected, I need \\nto be penalized for putting them in a community. If two nodes are connected, I need to be \\nrewarded. Whatever community assignment I make, the modularity of the graph is driven \\nby the sum of those scores for each pair of nodes that ends up in a community together.\\nUsing an optimization algorithm (you knew Solver was coming!), you can “try out” dif-\\nferent community assignments on the graph and see which one rakes in the most points \\nwith the fewest penalties. This will get you a winning modularity score.\\nWhat’s a Point and What’s a Penalty?\\nIn modularity maximization you give yourself one point every time you cluster two nodes \\nthat share an edge in the adjacency matrix. You get zero points every time you cluster \\nthose who don’t.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 201, 'page_label': '180'}, page_content='180 Data Smart\\nEasy.\\nWhat about penalties?\\nThis is where the modularity maximization algorithm really gets creative. Consider \\nagain the Friends graph, originally pictured in Figure 5-1.\\nModularity maximization bases its penalties for putting two nodes together on one \\nquestion:\\nIf you had this graph and you erased the middle of each edge and “rewired” it a bunch of \\ntimes at random, what is the expected number of edges you’d get between two nodes? \\nThat expected number of edges is the penalty.\\nWhy is the expected number of edges between two nodes the penalty? Well, you don’t \\nwant to reward the model as much for clustering people based on a relationship that was \\nlikely to happen anyway because both parties are extremely social. \\nI want to know how much of that graph is intentional  relationship and connection, \\nand how much of it is just because, “Yeah, well, Chandler’s connected to a lot of people, \\nso odds are Phoebe would be one of them.” This means that edges between two highly \\nselective individuals are “less random” and worth more than edges between two socialites.\\nTo understand this more clearly, look at a version of the Friends graph in which I’ve \\nerased the middle of each edge. These half-edges are called stubs. See Figure 5-21.\\nRoss Rachel\\nMonicaJoey\\nChandler Phoebe\\nFigure 5-21: Stubby Friends graph\\nNow, think about wiring the graph up randomly. In Figure 5-22, I’ve drawn an ugly \\nrandom rewiring. And yes, in a random rewiring it’s totally possible to connect someone \\nto him or herself if they have multiple stubs coming out of them. Trippy.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 202, 'page_label': '181'}, page_content='181Cluster Analysis Part II: Network Graphs and Community Detection  \\nRoss Rachel\\nMonicaJoey\\nChandler Phoebe\\nFigure 5-22: A rewiring of the Friends graph\\nFigure 5-22 is just one way to wire it up, right? There are tons of possibilities even \\nwith a graph with just ﬁ ve edges. Notice that Ross and Rachel were chosen. What were \\nthe odds of that happening? Based on that probability, what is the expected number of \\nedges between the two if you rewired the graph randomly over and over and over again?\\nWell, when drawing a random edge, you need to select two stubs at random. So what’s \\nthe probability that a node’s stubs will be selected?\\nIn the case of Rachel, she has three stubs out of a total of ten (two times the number \\nof edges) on the graph. Ross has one stub. So the probability that you’d select Rachel for \\nany edge is 30 percent, and the probability that you’d select Ross’s stub for any edge is 10 \\npercent. The node selection probabilities are shown in Figure 5-23.\\nRoss Rachel\\nMonicaJoey\\nChandler Phoebe\\n1\\n10\\n1\\n10\\n3\\n10\\n3\\n10\\n1\\n10\\n1\\n10\\nFigure 5-23: Node selection probabilities on the Friends graph\\nSo if you were randomly selecting nodes to link up, you could select Ross and then \\nRachel or Rachel and then Ross. That’s roughly 10 percent times 30 percent or 30 percent \\ntimes 10 percent, which is 2 times 0.3 times 0.1. That comes out to 6 percent.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 203, 'page_label': '182'}, page_content='182 Data Smart\\nBut you’re not drawing just one edge, are you? You need to draw a random graph with \\nﬁ ve edges, so you get ﬁ ve tries to pick that combo. The expected number of edges between \\nRoss and Rachel then is roughly 6 percent times 5, or 0.3 edges. Yes, that’s right, expected \\nedges can be fractional. \\nDid I just blow your mind Inception-style? Think of it like this. If I ﬂ ip a Sacagawea dol-\\nlar coin, which you get to keep if it lands on heads but not tails, then ﬁ fty percent of the \\ntime you’re going to get a dollar and ﬁ fty percent of the time you get nothing. Your expected \\npayoff  is 0.5 * $1 = $0.50, even though you’ll never actually win ﬁ fty cents in a game.\\nSimilarly here, you’ll only ever encounter graphs where Ross and Rachel are or are not \\nconnected, but their expected edge value is nevertheless 0.3.\\nFigure 5-24 shows these calculations in detail.\\nRoss Rachel\\nProbability of getting Ross-Rachel:\\nExpected number of Ross-Rachel connections:MonicaJoey\\nChandler Phoebe\\n1\\n10\\n1\\n10\\n3\\n10\\n3\\n10\\n1\\n10\\n1\\n10\\n2 # Ross Stubs\\n2 ∗ # Edges\\n# Rachel Stubs\\n2 ∗ # Edges\\n= 2 1\\n10\\n3\\n10\\n# Ross Stubs ∗ # Rachel Stubs\\n2 ∗ # Edges\\n= 3\\n10\\n2 ∗ Edges # Ross Stubs\\n2 ∗ # Edges\\n# Rachel Stubs\\n2 ∗ # Edges\\n= \\nFigure 5-24: The expected number of edges between Ross and Rachel\\nBringing the points and penalties together, things should now become clear.\\nIf you put Ross and Rachel in a community together, you don’t get a full 1 point. This is \\nbecause you get penalized 0.3 points since that’s the expected number of edges a random \\ngraph would have anyway. That leaves you with a score of 0.7.\\nIf you didn’t cluster Ross and Rachel, then you would receive 0 rather than 0.7 points.\\nOn the other hand, Rachel and Phoebe aren’t connected. They have the same expected \\nedge value of 0.3 though. That means that if you put them in a community together, you’d \\nstill get the penalty but you’d receive no points, so the score would be adjusted by −0.3.\\nWhy? Because the fact that there’s no edge between Rachel and Phoebe means some-\\nthing! The expected number of edges was 0.3 and yet this graph doesn’t have one, so the \\nscore should account for that possibly intentional separation.\\nIf you didn’t put Rachel and Phoebe in a community together, then they’d receive no \\nscore at all, so all things being equal, you’re best separating them into diff erent clusters.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 204, 'page_label': '183'}, page_content=\"183Cluster Analysis Part II: Network Graphs and Community Detection  \\nTo sum it all up then, the points and penalties capture the amount that the graph’s \\nstructure deviates from the expected  graph structure. You need to assign communities \\nthat account for these deviations.\\nThe modularity of a community assignment is just the sum of these points and penalties \\nfor pairs of nodes placed in community together, divided by the total number of stubs in \\nthe graph. You divide by the number of stubs so that whatever the size of the graph, the \\nmaximum modularity score is 1, which facilitates comparisons across graphs.\\nSetting Up the Score Sheet\\nEnough talk! Let’s actually calculate these scores for each pair of customers in the graph.\\nTo start, let’s count how many stubs are coming out of each customer and how many \\ntotal stubs there are in the graph. Note that the stub count of a customer is just the degree \\nof the node.\\nSo on the r- NeighborhoodAdj tab you can count the degree of a node simply by sum-\\nming down a column or across a row. If there’s a 1, that’s an edge, hence a stub, hence it’s \\ncounted. So, for example, how many stubs does Adams have? In cell B102, you can just \\nplace the following formula to count them:\\n=SUM(B2:B101)\\nYou get 14. Similarly, you could sum across row 2 by placing in CX2 the formula:\\n=SUM(B2:CW2)\\nYou get 14 in that case as well, which is what you’d expect since the graph is undirected.\\nCopying these formulas across and down respectively, you can count the stubs for each \\nnode. And by simply summing column CX in row 102, you get the total number of stubs \\nfor the graph. As shown in Figure 5-25, the graph has a total of 858 stubs.\\nNow that you have the stub counts, you can create a Scores tab in your workbook \\nwhere you place the customers’ names across row 1 and down column A, just as in the \\nr-NeighborhoodAdj tab.\\nConsider cell B2, which is the score for Adams connecting with himself. Does this \\nget one point or none? Well, you can read in the value from the adjacency matrix, \\n'r-NeighborhoodAdj'!B2, and you’re done. If the adjacency matrix is a 1, it’s copied in. \\nSimple.\\nAs for the expected edge calculation that you need to tack on as a penalty, you can \\ncalculate it the same way that was shown in Figure 5-24:\\n# stubs customer A * # stubs customer B / Total stubs \\nBy bringing these points and penalties together in cell B2, you end up with this formula:\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 205, 'page_label': '184'}, page_content=\"184 Data Smart\\n='r-NeighborhoodAdj'!B2 – \\n(('r-NeighborhoodAdj'!$CX2*'r-NeighborhoodAdj'!B$102)/\\n'r-NeighborhoodAdj'!$CX$102)\\nFigure 5-25: Counting edge stubs on the r-Neighborhood graph\\nYou have the 0/1 adjacency score minus the expected count.\\nNote that the formula uses absolute cell references on the stub values so that when you \\ndrag the formula, everything changes appropriately. Thus, dragging the formula across \\nand down the Scores tab, you end up with the values shown in Figure 5-26.\\nFigure 5-26: The Scores tab\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 206, 'page_label': '185'}, page_content=\"185Cluster Analysis Part II: Network Graphs and Community Detection  \\nTo drive this score home, check out cell K2. This is the score for an Adams/Brown \\nclustering. It’s 0.755.\\nAdams and Brown share an edge on the adjacency matrix so you get 1 point for cluster-\\ning them ('r-NeighborhoodAdj'!K2 in the formula), but Adams has a stub count of 14 \\nand Brown is a 15, so their expected edge count is 14 * 15 / 858. That second part of the \\nformula looks like this:\\n(('r-NeighborhoodAdj'!$CX2*'r-NeighborhoodAdj'!K$102)/\\n'r-NeighborhoodAdj'!$CX$102) \\nwhich comes out to 0.245. Bringing it all together, you get 1 - 0.245 = 0.755 for the score.\\nLet’s Get Clustering!\\nYou now have the scores you need. All you need to do now is set up an optimization model \\nto ﬁ nd optimal community assignments.\\nNow, I’m going to be honest with you up front. Finding optimal communities using \\ngraph modularity is a more intense optimization setup than what you encountered in \\nChapter 2. This problem is often solved with complex heuristics such as the popular \\n“Louvain” method (see \\nhttp://perso.uclouvain.be/vincent.blondel/research/\\nlouvain.html for more info), but this is a code-free zone, so you’re going to make do with \\nSolver.\\nTo make this possible, you’re going to attack the problem using an approach called \\ndivisive clustering or hierarchical partitioning. All that means is that you’re going to set up \\nthe problem to ﬁ nd the best way to split the graph into two communities. Then you’re \\ngoing to split those two into four, and on and on until Solver decides that the best way to \\nmaximize modularity is to stop dividing the communities.\\nNOTE\\nDivisive clustering is the opposite of another often-used approach called \\nagglomerative clustering. In agglomerative clustering, each customer starts in their \\nown cluster, and you recursively glom together the two closest clusters until you \\nreach a stopping point.\\nSplit Number 1\\nAll right. So you start this divisive clustering process by dividing the graph into two com-\\nmunities so the modularity score is maximized.\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 207, 'page_label': '186'}, page_content='186 Data Smart\\nFirst create a new sheet called Split1 and paste customers down column A. Each cus-\\ntomer’s community assignment will go in column B, which you should label Community. \\nSince you’re splitting the graph in half, have the Community column be a binary decision \\nvariable in Solver, where the 0/1 value will denote whether you’re in community 0 or \\ncommunity 1. Neither community is better than the other. There’s no shame in being a 0.\\nScoring Each Customer’s Community Assignment\\nIn column C, you’re going to calculate the scores you get by placing each customer in \\ntheir respective community. By that, I mean if you place Adams in community 1, you’ll \\ncalculate his piece of the total modularity score by summing all the values from his row \\nin the Scores tab whose customer columns also landed in community 1.\\nConsider how you’d add these scores in a formula. If Adams is in community 1, you need \\nto sum all values from the Scores tab on row 2 where the corresponding customer in the \\noptimization model is also assigned a 1. Because assignment values are 0/1, you can use \\nSUMPRODUCT to multiply the community vector by the score vector and then sum the result.\\nAlthough the score values go across the Scores tab, in the optimization model, the \\nassignments go top to bottom, so you need to TRANSPOSE the score values in order to make \\nthis work (and using TRANSPOSE means making this an array formula): \\n{=SUMPRODUCT(B$2:B$101,TRANSPOSE(Scores!B2:CW2))}  \\nThe formula simply multiplies the Scores values for Adams times the community assign-\\nments. Only scores matching community assignment 1 stay, whereas the others get set \\nto 0. The \\nSUMPRODUCT just sums everything.\\nBut what if Adams were assigned to community 0? You need only ﬂ ip the community \\nassignments by subtracting them from 1 in order to make the sum of scores work.\\n{=SUMPRODUCT(1-(B$2:B$101),TRANSPOSE(Scores!B2:CW2))}\\nIn an ideal world, you could put these two together with an IF formula that checks \\nAdams’ community assignment and then uses one of these two formulas to sum up the \\ncorrect neighbors’ scores. But in order to use an \\nIF formula, you need to use the non-linear \\nsolver (see Chapter 4 for details), and in this particular case, maximizing modularity is too \\nhard for the non-linear solver to handle effi  ciently. You need to make the problem linear.\\nMaking the Score Calculation into a Linear Model\\nIf you read Chapter 4, you’ll recall a method for modeling the IF formula using linear \\nconstraints, called a “Big M” constraint. You’re going to use this tool here.\\nBoth of the previous two formulas are linear; so what if you just set a score variable \\nfor Adams to be less than both of them? You’re trying to maximize the total modularity \\nscores, so Adams’ score will want to rise until it bumps up against the lowest of these two \\nconstraining formulas.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 208, 'page_label': '187'}, page_content='187Cluster Analysis Part II: Network Graphs and Community Detection  \\nBut how do you know which score calculation corresponding to Adams’ actual com-\\nmunity assignment is the lowest? You don’t.\\nTo ﬁ  x that, you need to deactivate  whichever of those two formulas isn’t in play. If \\nAdams is assigned a 1, the ﬁ rst formula becomes an upper bound and the second formula \\nis turned off . If Adams is a zero, you have the opposite.\\nHow do you turn off  one of the two upper bounds? Add a “Big M” to it— just big enough \\nthat its bound is meaningless, because the legit bound is lower.\\nConsider this modiﬁ cation to the ﬁ rst formula:\\n{=SUMPRODUCT(B$2:B$101,TRANSPOSE(Scores!B2:CW2))+ \\n(1-B2)*SUM(ABS(Scores!B2:CW2))}\\nIf Adams is assigned to community 1, the addition you made at the end of the formula \\nturns to 0 (because you’re multiplying by 1-B2). In this way, the formula becomes identical \\nto the ﬁ rst one you examined. But if Adams gets assigned to community 0, this formula \\nno longer applies and needs to be turned off  . So the \\n(1-B2)*SUM(ABS(Scores!B2:CW2)  \\npiece of the formula adds one times the sum of all the absolute values of the scores Adams \\ncould possibly get, which guarantees the formula is higher than its ﬂ ipped version that’s \\nnow in play:\\n{=SUMPRODUCT(1-(B$2:B$101),TRANSPOSE(Scores!B2:CW2))+\\nB2*SUM(ABS(Scores!B2:CW2))} \\nAll you’re doing is setting Adams’ score to be less than or equal to the correct calcula-\\ntion and removing the other formula from consideration by making it larger. It’s a ghetto-\\nhacked \\nIF statement.\\nThus, in column C you can create a score column that will be a decision variable, \\nwhereas in columns D and E in the spreadsheet you can place these two formulas as upper \\nbounds on the score (see Figure 5-27).\\nFigure 5-27: Adding two upper bounds to each customer’s score variable'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 209, 'page_label': '188'}, page_content=\"188 Data Smart\\nNote that in the formula absolute references are used on the community assignment \\nrange, so that as you drag the formulas down, nothing shifts.\\nSumming the scores in cell G2 for each eventual community assignment in col-\\numn C, you get the total score, which you can normalize by the total stub count in \\n'r-NeighborhoodAdj'!CX102 in order to get the modularity calculation:\\n=SUM(C2:C101)/'r-NeighborhoodAdj'!CX102\\nThis gives the sheet shown in Figure 5-28.\\nFigure 5-28: Filled out Split1 tab, ready for optimization \\nSetting Up the Linear Program\\nNow everything is set up for optimizing. Open the Solver window and specify that you’re \\nmaximizing the graph modularity score in cell G2. The decision variables are the com-\\nmunity assignments in B2:B101 and their modularity scores are in C2:C101.\\nYou need to add a constraint forcing the community assignments in B2:B101 to be \\nbinary. Also, you need to make the customer score variables in column C less than both \\nthe upper bounds in columns D and E.\\nAs shown in Figure 5-29, you can then set all the variables to be non-negative with the \\ncheckbox and select Simplex LP as the optimization algorithm.\\nBut wait. There’s more!\\nOne of the problems with using a “Big M” constraint is that Solver often has trouble \\nconﬁ rming it’s actually found the optimal solution. So it’ll just sit there and spin its wheels \\neven though it’s got a great solution in its back pocket. To prevent that from happening, \\npress the Options button in Solver and set the Max Subproblems value to 15,000. That \\nensures that Solver quits after about 20 minutes on my laptop. \\nGo ahead and press Solve—regardless of whether you’re using Solver or OpenSolver \\n(see the nearby sidebar) when the algorithm terminates due to a user-deﬁ  ned limit, it\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 210, 'page_label': '189'}, page_content='189Cluster Analysis Part II: Network Graphs and Community Detection  \\nmay tell you that while it found a feasible solution, it didn’t solve to optimality. This just \\nmeans that the algorithm didn’t prove optimality (similar to how non-linear solvers are \\nunable to prove optimality), but in this case, your solution should be strong nonetheless.\\nFigure 5-29: The LP formulation for the ﬁ  rst split\\nOnce you have a solution, the Split1 tab should appear as in Figure 5-30.\\nEXCEL 2010 AND 2013 MUST USE OPENSOLVER\\nIf you’re in Excel 2010 or 2013 on Windows, this problem is too hard for the Solver \\nprovided you, and you’ll need to use OpenSolver, as discussed in Chapters 1 and 4.\\nIf you use OpenSolver, set up the problem with regular Solver, but before solving, \\nopen the OpenSolver plugin to beef up your system. OpenSolver has the same diffi  culty \\nwith “Big M” constraints, so before running the model, click the OpenSolver options \\nbutton and set the time limit to 300 seconds. If you don’t do this, the default run time \\non OpenSolver is really high, and it may just spin its wheels, forcing you to kill Excel.\\nIf you’re in Excel 2007 or Excel 2011 for Mac, you’re good to go with vanilla \\nSolver, although if you’d like to use OpenSolver with Excel 2007, you can. If you’re in \\nLibreOffi  ce, you should be just ﬁ ne.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 211, 'page_label': '190'}, page_content='190 Data Smart\\nFigure 5-30:  Optimal solution for the ﬁ  rst split\\nMy Solver run came up with 0.464 for the modularity; your solution may be better if \\nyou use OpenSolver. Running down column B, you can see who ended up in community \\n0 and who’s in community 1. The question then is, are you done? Are there only two com-\\nmunities or are there more?\\nIn order to answer that question, you need to try to split these two communities up. \\nIf you’re done, Solver won’t have any of it. But if making three or four communities from \\nthese two improves modularity, well, then Solver is going to do it.\\nSplit 2: Electric Boogaloo\\nAll right. Split these communities up like you’re doing cell division. You start by making \\na copy of the Split1 tab and calling it Split2.\\nThe ﬁ rst thing you need to do is insert a new column after the community values in \\ncolumn B. Label this new column C Last Run and copy the values over from B into C. \\nThis gives the sheet pictured in Figure 5-31.\\nIn this model, the decisions are the same—customers are given a 1 or a 0. But you need \\nto keep in mind that if two customers are given 1s this time around they’re not necessarily \\nin the same community. If one of them was in community 0 on the ﬁ rst run and the other \\nwas in community 1, they’re in two diff erent communities.\\nIn other words, the only scores Adams might get for being in, say, community 1-0 \\nare from those customers who were also placed in community 0 on the ﬁ  rst split and in \\ncommunity 1 on the second. Thus, you need to change the upper bounds on the score'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 212, 'page_label': '191'}, page_content='191Cluster Analysis Part II: Network Graphs and Community Detection  \\ncalculation. The score calculation for column E (here you show E2) then requires a check \\nagainst the previous run in column C:\\n{=SUMPRODUCT(B$2:B$101,IF(C$2:C$101=C2,1,0),TRANSPOSE(Scores!B2:CW2))}\\nFigure 5-31: The Split2 tab with previous run values \\nThe IF statement IF(C$2:C$101=C2,1,0) prevents Adams from getting points unless \\nhis neighbors are with him on the ﬁ rst split.\\nYou can use an IF statement here, because column C isn’t a decision variable this time \\naround. That split was ﬁ xed on the last run, so there’s nothing non-linear about this. You \\ncan add the same IF statement into the “Big M” part of the formula to make the ﬁ  nal \\ncalculation in column E:\\n=SUMPRODUCT(B$2:B$101,IF(C$2:C$101=C2,1,0),TRANSPOSE(Scores!B2:CW2))+\\n(1-B2)*SUMPRODUCT(IF(C$2:C$101=C2,1,0),TRANSPOSE(ABS(Scores!B2:CW2)))\\nSimilarly, you can add the same IF statements into the second upper bound in column F:\\n=SUMPRODUCT(1-(B$2:B$101),IF(C$2:C$101=C2,1,0),TRANSPOSE(Scores!B2:CW2))\\n+B2*SUMPRODUCT(IF(C$2:C$101=C2,1,0),TRANSPOSE(ABS(Scores!B2:CW2)))\\nAll you’ve done is silo-ed the problem—those who were split into community 0 the \\nﬁ rst time around have their own little world of scores to play with and the same goes for \\nthose who ended up in 1 the ﬁ rst time.\\nAnd here’s the cool part—you don’t have to change the Solver formulation at all! Same \\nformulation, same options! If you’re using OpenSolver, it may not have saved your maxi-\\nmum time limit options from the previous tab. Reset the option to three hundred seconds. \\nSolve again.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 213, 'page_label': '192'}, page_content='192 Data Smart\\nIn my run on Split2, I ended up with a ﬁ nal modularity of 0.546 (see Figure 5-32), which \\nis a substantial improvement over 0.464. That means that splitting was a good idea. (Your \\nsolution may end up diff erent and possibly better.)\\nFigure 5-32: The optimal solution for Split2\\nAnd…Split 3: Split with a Vengeance\\nOkay, so should you stop here or should you keep going? The way to tell is to split again, \\nand if Solver can’t do better than 0.546, you’re through.\\nStart by creating a Split3 tab, renaming Last Run to Last Run 2, and then inserting a \\nnew Last Run in column C. Then copy the values from column B into C.\\nAdd more IF statements to the upper bounds to check for community assignments in \\nthe previous run. For example, F2 becomes:\\n=SUMPRODUCT(B$2:B$101,\\nIF(D$2:D$101=D2,1,0),IF(C$2:C$101=C2,1,0),\\nTRANSPOSE(Scores!B2:CW2))+\\n(1-B2)*SUMPRODUCT(\\nIF(C$2:C$101=C2,1,0),IF(D$2:D$101=D2,1,0),\\nTRANSPOSE(ABS(Scores!B2:CW2)))\\nOnce again, the Solver formulation doesn’t change. Reset your maximum solving time \\nif need be, press Solve, and let the model run its course. In the case of my model, I saw \\nno improvement in modularity (see Figure 5-33).\\nSplitting again added nothing, so this means that modularity was eff ectively maximized \\non Split2. Let’s take the cluster assignments from that tab and investigate.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 214, 'page_label': '193'}, page_content='193Cluster Analysis Part II: Network Graphs and Community Detection  \\nEncoding and Analyzing the Communities\\nIn order to investigate these community assignments, the ﬁ rst thing you should do is take \\nthis binary tree that’s been created by the successive splits and turn those columns into \\nsingle cluster labels.\\nCreate a tab called Communities and paste the customer name, community, and last \\nrun values from the Split2 tab. You can rename the two binary columns Split2 and Split1. \\nTo turn their binary values into single numbers, Excel provides a nifty binary-to-decimal \\nformula called \\nBIN2DEC. So in column D, starting at D2, you can add:\\n=BIN2DEC(CONCATENATE(B2,C2))\\nFigure 5-33: No modularity improvement in Split 3\\nCopying that formula down, you get the community assignments shown in Figure 5-34 \\n(your assignments may vary depending on Solver).\\nFigure 5-34:  Final community labels for modularity maximization'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 215, 'page_label': '194'}, page_content='194 Data Smart\\nYou get four clusters with labels 0 to 3 out of the decimal encoding. So what are these \\nfour optimal clusters? Well, you can ﬁ nd out in the same way you delved into clusters in \\nChapter 2—by investigating the most popular purchases of their members.\\nTo begin, just as in Chapter 2, create a tab called TopDealsByCluster and paste the deal \\ninformation from columns A through G on the Matrix tab. Next to the matrix, place the \\ncluster labels 0, 1, 2, and 3 in columns H through K. This gives you the sheet pictured in \\nFigure 5-35.\\nFigure 5-35: The initial TopDealsByCluster tab\\nFor label 0 in column H, you now want to look up all customers on the Communities \\ntab who have been assigned to community 0 and sum how many of them took each deal. \\nJust as in Chapter 2 and in the previous Split tabs, you use \\nSUMPRODUCT with an IF state-\\nment to achieve this:\\n{=SUMPRODUCT(IF(Communities!$D$2:$D$101=TopDealsByCluster!H$1,1,0),\\n    TRANSPOSE(Matrix!$H2:$DC2))} \\nIn this formula you check which customers match the 0 in the column label at H1, and \\nwhen they do match, you sum whether or not they took the ﬁ rst deal by checking H2:DC2 \\non the Matrix tab. Note that you use TRANSPOSE in order to orient everything vertically. \\nThis means you have to make the calculation an array formula.\\nNote that you’ve used absolute references on the customer community assignments, \\nthe header rows, and the purchase matrix columns. This allows you to drag the formula \\nto the right and down, giving you a full picture of the popular purchases for each cluster \\n(see Figure 5-36).\\nJust as in Chapter 2, you need to apply ﬁ  ltering to the sheet and sort by descending \\ndeal count on community 0 in column H. This gives you Figure 5-37, the low-volume'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 216, 'page_label': '195'}, page_content='195Cluster Analysis Part II: Network Graphs and Community Detection  \\ncustomer community (your clusters may vary in their order and composition depending \\non the solution Solver terminated with at each step).\\nFigure 5-36: TopDealsByCluster with completed purchase counts\\nFigure 5-37: Top deals for community 0\\nSorting by community 1, you get what appears to be the high-volume French Champagne \\ncluster (see Figure 5-38). Fascinating.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 217, 'page_label': '196'}, page_content='196 Data Smart\\nFigure 5-38:  Poppin’ bottles in community 1\\nAs for community 2, it looks similar to community 0, except that the March Espumante \\ndeal is the main driver (see Figure 5-39).\\nFigure 5-39: People who liked the March Espumante deal\\nAnd for community 3, it’s the Pinot Noir folks. Haven’t you ever heard of Cabernet \\nSauvignon, people!? Admittedly, I have a terrible palate for wine. See Figure 5-40.\\nThat’s it! You have four clusters, and honestly, three of them make perfect sense, \\nalthough I suppose it’s possible that you have a group of people who really just love \\nEspumante in March. And you may get that in your work—some indecipherable outlier \\nclusters.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 218, 'page_label': '197'}, page_content='197Cluster Analysis Part II: Network Graphs and Community Detection  \\nFigure 5-40:  Pinot peeps\\nNote how similar this solution is to the clusters found in Chapter 2, however. In \\nChapter 2, you used a whole diff erent methodology by keeping each customer’s deal vector \\nin the mix and using it to measure their distances from a cluster center. Here, there’s no \\nconcept of a center and even which deals a customer has purchased have been obfuscated. \\nWhat’s important is the distance to other customers.\\nThere and Back Again: A Gephi Tale\\nNow that you’ve gone through the entire clustering process, I’d like to show you that same \\nprocess in Gephi. In Figure 5-20, you examined a laid out export of the r-Neighborhood \\ngraph into Gephi, which I return to in this section.\\nThis next step is going to make you envious, but here it goes. In Excel you had to solve \\nfor the optimal graph modularity using divisive clustering. In Gephi, there’s a Modularity \\nbutton. You’ll ﬁ nd it on the right side of the window in the Network Overview section of \\nthe Statistics tab. \\nWhen you press the Modularity button, a settings window opens. You needn’t use edge \\nweights since you exported an adjacency matrix (see Figure 5-41 for the Gephi modular-\\nity settings window).\\nPress OK. The modularity optimization will run using an approximation algorithm \\nthat’s blindingly fast. A report is then displayed with a total modularity score of 0.549 \\nas well as the size of each detected cluster (see Figure 5-42). Note that if you run this in \\nGephi, the solution may come out diff erent since the calculation is randomized.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 219, 'page_label': '198'}, page_content='198 Data Smart\\nFigure 5-41:  Gephi modularity settings\\nFigure 5-42:  Modularity score from Gephi'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 220, 'page_label': '199'}, page_content='199Cluster Analysis Part II: Network Graphs and Community Detection  \\nOnce you have your clusters from Gephi, you can do a few things with them. \\nFirst, you can recolor the graph using the modularity. Just as you resized the Friends \\ngraph using node degree, you can navigate to the Ranking window in the upper left of \\nwindow in Gephi and go into the Nodes section. From there, you can select Modularity \\nClass from the drop-down menu, pick any color scheme you want, and press Apply to \\nrecolor the graph (see Figure 5-43).\\nFigure 5-43:  Customer graph recolored to show modularity clusters\\nCool! You can now see that the two “tumor-esque” parts of the graph are indeed com-\\nmunities. The spread-out middle section of the graph was divided into three clusters. And \\npoor Parker was placed in his own cluster, unconnected to anyone. How lonely and sad.\\nThe second thing you can do with the modularity information is export it back into \\nExcel to examine it, just as you did with your own clusters. To accomplish this, go into \\nthe Data Laboratory tab you visited earlier in Gephi. You’ll notice that the modularity \\nclasses have already been populated as a column in the Nodes data table. Pressing the \\nExport Table button, you can select the label and modularity class columns to dump to \\na CSV ﬁ le (see Figure 5-44).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 221, 'page_label': '200'}, page_content=\"200 Data Smart\\nFigure 5-44:  Exporting modularity classes back to Excel\\nPress OK on the export window to export your modularity classes to a CSV wherever \\nyou like and then open that ﬁ  le in Excel. From there, you can create a new tab in the \\nmain workbook called CommunitiesGephi, where you can paste the classes Gephi has \\nfound for you (see Figure 5-45). You’ll need to use the ﬁ lter capability in Excel to sort your \\ncustomers by name just as they are in the rest of the workbook.\\nJust for kicks, let’s conﬁ rm that this clustering really does beat the original score in \\ncolumn C. You’re not bound by linear modeling constraints anymore, so you can total \\neach customer’s modularity scores using the following formula (shown here using our \\nfavorite customer, Adams, in cell C2):\\n{=SUMPRODUCT(IF($B$2:$B$101=B2,1,0),TRANSPOSE(Scores!B2:CW2))} \\nThe formula merely checks for customers in the same cluster using an IF statement, \\ngives those customers 1s and all else 0s, and then uses a SUMPRODUCT to sum their modu-\\nlarity scores.\\nYou can double-click this formula to send it down column C. Summing the column in \\ncell E2 and dividing through by the total stub count from 'r-NeighborhoodAdj'!CX102,\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 222, 'page_label': '201'}, page_content='201Cluster Analysis Part II: Network Graphs and Community Detection  \\nyou do indeed get a modularity score of 0.549 (see Figure 5-46). So Gephi’s heuristic has \\nbeat out the divisive clustering heuristic by 0.003. Oh well! Pretty close. (If you used \\nOpenSolver, you may actually be able to beat Gephi.)\\nFigure 5-45:  Gephi modularity classes back in Excel\\nFigure 5-46:  Reproducing the modularity score for the communities detected by Gephi\\nLet’s see which clusters Gephi actually came up with. To start, let’s make a copy of the \\nTopDealsByCluster tab, which you should rename TopDealsByClusterGephi. Once you’ve \\nmade a copy, sort the deals back in order by column A and drop the ﬁ  ltering placed on \\nthe table. Now, in Gephi’s clustering, you have six clusters with labels 0 through 5 (your \\nresults may be diff erent since Gephi uses a randomized algorithm), so let’s add 4 and 5 \\nto the mix in columns L and M.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 223, 'page_label': '202'}, page_content='202 Data Smart\\nThe formula in cell H2 need only be modified to reference column B on the  \\nCommunitiesGephi tab instead of column D on the Communities tab. You can then drag \\nthis formula to the rest of the sheet, yielding Figure 5-47.\\nFigure 5-47: Top purchases per cluster from Gephi\\nIf you sort once again by column, you see the all too familiar clusters—low volume, \\nsparkling wine, Francophiles, Pinot people, high volume, and last but not least, Parker \\nby himself.\\nWrapping Up\\nIn Chapter 2, you looked at k-means clustering. Using the same data in this chapter, you \\ntackled network graphs and clustering via modularity maximization. You should feel \\npretty good about your data mining chops by now. In more detail, here are some items \\nyou learned:\\n• How network graphs are visually represented as well as how they’re represented \\nnumerically using adjacency and affi  nity matrices\\n• How to load a network graph into Gephi to augment Excel’s visualization deﬁ ciencies\\n• How to prune edges from network graphs via the r-neighborhood graph. You also \\nlearned the concept of a kNN graph, which I recommend you go back and tinker \\nwith.\\n• The deﬁ nitions of node degree and graph modularity and how to calculate modular-\\nity scores for grouping two nodes together'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 224, 'page_label': '203'}, page_content='203Cluster Analysis Part II: Network Graphs and Community Detection  \\n• How to maximize graph modularity using a linear optimization model and divisive \\nclustering\\n• How to maximize graph modularity in Gephi and export the results\\nNow, you may be wondering, “John, why in the world did you take me through that \\ngraph modularity maximization process when Gephi does it for me?”\\nRemember, the point of this book is not to press buttons blindly, without understanding \\nwhat they do. Now you know how to construct and prep graph data for cluster detection. \\nAnd you know how community detection on graph data works. You’ve done it. So next \\ntime you do this, even if you’re just pushing a button, you’ll know what’s going on behind \\nthe scenes, and that level of understanding and conﬁ dence in the process is invaluable.\\nAlthough Gephi is one of the best places to do this analysis, if you’re looking for a place \\nto code with graph data, the igraph library, which has hooks in R and Python, is excellent \\nfor working with network graphs. \\nAlso worth mentioning are the Neo4J and Titan graph databases. These databases are \\ndesigned to store graph data for querying later, whether that query is something as simple \\nas “get John’s friends’ favorite ﬁ lms” or as complex as “ﬁ nd the shortest path on Facebook \\nbetween John and Kevin Bacon.”\\nSo that’s it. Go forth, graph, and ﬁ nd commu nities!'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 225, 'page_label': '204'}, page_content=''),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 226, 'page_label': '205'}, page_content='6\\nWait, What? You’re Pregnant?\\nI\\nn a recent Forbes article, it was reported that Target had created an artificial intelligence \\n(AI) model that could predict when a customer was pregnant and use that information \\nto start targeting them with pregnancy-related marketing and offers. New parents blow a \\nlot of money on the accouterments of child rearing, and what better time to turn them into \\nloyal customers than before the baby even shows up? They’ll be buying the store brand \\ndiapers for years!\\nThis story about Target is just one of many that have peppered the press recently. Watson \\nwon Jeopardy!. Netﬂ ix off ered a million dollar prize to improve its recommendation system. \\nThe Obama re-election campaign used artiﬁ cial intelligence to help direct ground, online, \\nand on the air media and fundraising operations. And then there’s Kaggle.com, where \\ncompetitions are popping up to predict everything from whether a driver is getting sleepy \\nto how much a grocery shopper will spend on groceries. \\nBut those are only the headline-catching applications. AI is useful across nearly any \\nindustry you can think of. Your credit card company uses it to identify odd transactions \\non your account. The enemy in your shoot-em-up Xbox game runs on AI. There’s e-mail \\nspam ﬁ ltering, tax fraud detection, spelling auto-correction, and friend recommendation \\non social networks. \\nQuite simply, a good AI model can help a business make better decisions, market better, \\nincrease revenue, and decrease costs. An AI model can help your sales and support staff   \\nprioritize leads and support calls. AI can help predict what off  ers will bring a customer \\nback to your brick and mortar store. AI can identify applicants who lie on their online \\ndating proﬁ le or are going to have a coronary in the next year. You name it; if there’s good \\nhistorical data, a trained AI model can help.\\nThe Granddaddy of \\nSupervised Artiﬁ cial \\nIntelligence—\\nRegression'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 227, 'page_label': '206'}, page_content='Data Smart206\\nDon’t Kid Yourself\\nFolks who don’t know how AI models work often experience some combination of awe and \\ncreepiness when hearing about how these models can predict the future. But to paraphrase \\nthe great 1992 ﬁ lm Sneakers, “Don’t kid yourself. It’s not that [intelligent].”\\nWhy? Because AI models are no smarter than the sum of their parts. At a simplistic level, you \\nfeed a supervised AI algorithm some historical data, purchases at Target for example, and \\nyou tell the algorithm, “Hey, these purchases were from pregnant people, and these other \\npurchases were from not-so-pregnant people.” The algorithm munches on the data and \\nout pops a model. In the future, you feed the model a customer’s purchases and ask, “Is \\nthis person pregnant?” and the model answers, “No, that’s a 26-year-old dude living in \\nhis mom’s basement.”\\nThat’s extremely helpful, but the model isn’t a magician. It just cleverly turns past data \\ninto a formula or set of rules that it uses to predict a future case. As we saw in the case \\nof naïve Bayes in Chapter 3, it’s the AI model’s ability to recall this data and associated \\ndecision rules, probabilities, or coeffi  cients that make it so eff ective.\\nWe do this all the time in our own non-artiﬁ cially intelligent lives. For example, using \\npersonal historical data, my brain knows that when I eat a sub sandwich with brown-\\nlooking alfalfa sprouts on it, there’s a good chance I may be ill in a few hours. I’ve taken \\npast data (I got sick) and trained my brain on it, so now I have a rule, formula, model, \\nwhatever you’d like to call it: brown sprouts = gastrointestinal nightmare.\\nIn this chapter, we’re going to implement two diff erent regression models just to see how \\nstraightforward AI can be. Regression is the granddaddy of supervised predictive modeling \\nwith research being done on it as early as the turn of the 19\\nth century. It’s an oldie, but its \\npedigree contributes to its power—regression has had time to build up all sorts of rigor \\naround it in ways that some newer AI techniques have not. In contrast to the MacGyver \\nfeel of naïve Bayes in Chapter 3, you’ll feel the weight of the statistical rigor of regression \\nin this chapter, particularly when we investigate signiﬁ cance testing.\\nSimilarly to how we used the naïve Bayes model in Chapter 3, we’ll use these models \\nfor classiﬁ cation. However as you’ll see, the problem at hand is very diff  erent from the \\nbag-of-words document classiﬁ cation problem we encountered earlier.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 228, 'page_label': '207'}, page_content='207The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\\nPredicting Pregnant Customers at RetailMart Using \\nLinear Regression\\nNOTE\\nThe Excel workbook used in this chapter, “RetailMart.xlsx,” is available for download \\nat the book’s website at www.wiley.com/go/datasmart.This workbook includes all the \\ninitial data if you want to work from that. Or you can just read along using the sheets \\nI’ve already put together in the workbook.\\nPretend you’re a marketing manager at RetailMart’s corporate headquarters in charge of \\ninfant merchandise. Your job is to help sell more diapers, formula, onesies, cribs, strollers, \\npaciﬁ ers, etc. to new parents, but you have a problem.\\nYou know from focus groups that new parents get into habits with baby products. \\nThey ﬁ nd diaper brands they like early on and stores that have the best prices on their \\nbrands. They ﬁ nd the paciﬁ er that works with their baby, and they know where to go \\nto get the cheap two-pack. You want RetailMart to be the ﬁ  rst store these new parents \\nbuy diapers at. You want to maximize RetailMart’s chances of being a parent’s go-to for \\nbaby purchases.\\nBut to do that, you need to market to these parents before they buy their ﬁ rst package \\nof diapers somewhere else. You need to market to the parents before the baby shows up. \\nThat way, when the baby arrives, the parents have already received and possibly already \\nused that coupon they got in the mail for diapers and ointment.\\nQuite simply, you need a predictive model to help identify potential pregnant custom-\\ners for targeted direct marking. \\nThe Feature Set\\nYou have a secret weapon at your disposal for building this model: customer account \\ndata. You don’t have this data for every customer; no, you’re up the creek for the guy who \\nlives in the woods and only pays cash. But for those who use a store credit card or have \\nan online account tied to their major credit card, you can tie purchases not necessarily to \\nan individual but at least to a household.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 229, 'page_label': '208'}, page_content='Data Smart208\\nHowever, you can’t just feed an entire purchase history, unstructured, into an AI model \\nand expect things to happen. You have to be smart about pulling relevant predictors out of \\nthe dataset. So the question you should ask yourself is which past purchases are predictive \\nfor or against a household being pregnant?\\nThe ﬁ rst purchase that comes to mind is a pregnancy test. If a customer buys a preg-\\nnancy test, they’re more likely to be pregnant than the average customer. These predictors \\nare often called model features or independent variables , while the thing we’re trying to \\npredict “Pregnant (yes/no)?” would be the dependent variable  in the sense that its value is \\ndependent on the independent variable data we’re pushing into the model.\\nPause a moment, and jot down your thoughts on possible features for the AI model. \\nWhat purchase history should RetailMart consider?\\nHere’s a list of example features that could be generated from a customer’s purchase \\nrecords and associated account information:\\n• Account holder is Male/Female/Unknown by matching surname to census data.\\n• Account holder address is a home, apartment, or PO box.\\n• Recently purchased a pregnancy test\\n• Recently purchased birth control\\n• Recently purchased feminine hygiene products\\n• Recently purchased folic acid supplements\\n• Recently purchased prenatal vitamins\\n• Recently purchased prenatal yoga DVD\\n• Recently purchased body pillow\\n• Recently purchased ginger ale\\n• Recently purchased Sea-Bands\\n• Bought cigarettes regularly until recently, then stopped\\n• Recently purchased cigarettes\\n• Recently purchased smoking cessation products (gum, patch, etc.)\\n• Bought wine regularly until recently, then stopped\\n• Recently purchased wine\\n• Recently purchased maternity clothing\\nNone of these predictors are perfect. Customers don’t buy everything at RetailMart; \\na customer might choose to buy their pregnancy test at the local drug store instead of \\nRetailMart or their prenatal supplements might be prescription. Even if the customer did \\nbuy everything at RetailMart, pregnant households can still have a smoker or a drinker. \\nMaternity clothing is often worn by non-pregnant folks, especially when the Empire waist \\nis in style—thank goodness RetailMart doesn’t exist in a Jane Austen novel. Ginger ale \\nmay help nausea, but it’s also great with bourbon. You get the picture.\\nNone of these predictors are going to cut it, but the hope is that with their powers com-\\nbined Captain-Planet-style, the model will be able to classify customers reasonably well.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 230, 'page_label': '209'}, page_content='209The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\\nAssembling the Training Data\\nSix percent of RetailMart’s customer households are pregnant at any given time according \\nto surveys the company has conducted. You need to grab some examples of this group \\nfrom the RetailMart database and assemble your modeling features on their purchase \\nhistory before they gave birth. Likewise, you need to assemble these features for a sample \\nof customers who aren’t pregnant.\\nOnce you assemble these features for a bunch of pregnant and non-pregnant households, \\nyou can use these known examples to train an AI model. \\nBut how should you go about identifying past pregnant households in the data? \\nSurveying customers to build a training set is always an option. You’re just building a \\nprototype, so perhaps approximating households who just had a baby by looking at buying \\nhabits is good enough. For customers who suddenly began buying newborn diapers and \\ncontinued to buy diapers of increasing size on and off  for at least a year, you can reason-\\nably assume the customer’s household has a new baby.\\nSo by looking at the purchase history for the customer before the diaper-buying event, \\nyou can assemble the features listed previously for a pregnant household. Imagine you \\npull 500 examples of pregnant households and assemble their feature data from the \\nRetailMart database.\\nAs for non-pregnant customers, you can assemble purchase history from a random selec-\\ntion of customers in RetailMart’s database that don’t meet the “ongoing diaper purchasing” \\ncriteria. Sure, one or two pregnant people might slip into the not-pregnant category, but \\nbecause pregnant households only make up a small percentage of the RetailMart popu-\\nlation (and that’s before excluding diaper-buyers), this random sample should be clean \\nenough. Imagine you grab another 500 examples of these non-pregnant customers.\\nIf you plopped the 1,000 rows (500 preggers, 500 not) into a spreadsheet it’d look like \\nFigure 6-1.\\nFigure 6-1: Raw training data'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 231, 'page_label': '210'}, page_content='Data Smart210\\nRESOLVING CLASS IMBALANCE\\nNow, you know that only 6 percent of our customer population in the wild is preg-\\nnant at any given time, but the training set you’ve assembled is 50/50. This is called \\nover-sampling. Pregnancy would be the “minority” or rare class in the data, and by \\nbalancing the sample, the classiﬁ er you’re going to train won’t become overwhelmed \\nby non-pregnant customers. After all, if you left the sample at a natural 6/94 split, \\nthen just labeling everyone as not pregnant leads to a 94 percent accuracy rate. \\nThat’s dangerous since pregnancy, while in the minority, is actually the class you \\ncare about marketing to.\\nThis rebalancing of the training data will introduce a bias to the model—it’ll think \\npregnancy is more common than it really is. But that’s ﬁ ne, because you don’t need to \\nget actual probabilities of being pregnant out of the model. As you’ll see later in this \\nchapter, you just need to ﬁ nd the sweet spot for pregnancy scores coming out of the \\nmodel that balances the true positives and false positives.\\nIn the ﬁ rst two columns of the training dataset, you have categorical data for gender \\nand address type. The rest of the features are binary where a 1 means TRUE. So for \\nexample, if you look at the ﬁ rst row in the spreadsheet, you can see that this customer \\nwas conﬁ rmed pregnant (column S). That’s the column you’re going to train the model to \\npredict. And if you look at this customer’s past purchasing history, you can see that they \\npurchased a pregnancy test and some prenatal vitamins. Also, they have not purchased \\ncigarettes or wine recently.\\nIf you scroll through the data, you’ll see all types of customers, some with lots of indi-\\ncators and some with little. Just as expected, pregnant households will occasionally buy \\ncigarettes and wine, while non-pregnant households will buy products associated with \\npregnancy.\\nCreating Dummy Variables\\nYou can think of an AI model as nothing more than a formula that takes numbers in, \\nchews on them a bit, and spits out a prediction that should look something like the 1s \\n(pregnant) and 0s (not) in column S of the spreadsheet. \\nBut the problem with this data is that the ﬁ  rst two columns aren’t numbers, now are \\nthey? They’re letters standing for categories, like male and female. \\nThis issue, handling categorical data, that is, data that’s grouped by a ﬁ nite number of \\nlabels without inherent numeric equivalents, is one that constantly nips at data miners’'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 232, 'page_label': '211'}, page_content='211The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\\nheels. If you send out a survey to your customers and they have to report back what line \\nof work they’re in, their marital status, the country they live in, the breed of dog they \\nown, or their favorite episode of Gilmore Girls, then you’re going to be stuck dealing with \\ncategorical data. \\nThis is in contrast to quantitative data , which is already numeric and ready to be \\ndevoured by data mining techniques.\\nSo what do you do to handle categorical data? Well, in short you need to make it \\nquantitative.\\nSometimes, your categorical data may have a natural ordering that you can use to \\nassign each category a value. For instance, if you had a variable in your dataset where \\nfolks reported whether they drove a Scion, a Toyota, or a Lexus, maybe you could just \\nmake those responses 1, 2, and 3. Voila, numbers.\\nBut more frequently, there is no ordering, such as with gender. For example, male, \\nfemale, and unknown are distinct labels without a notion of ordering. In this case, it’s \\ncommon to use a technique called dummy coding  to convert your categorical data to \\nquantitative data.\\nDummy coding works by taking a single categorical column (consider the Implied \\nGender column) and turning it into multiple binary columns. You could take the Implied \\nGender column and instead have one column for male, another for female, and another \\nfor unknown gender. If a value in the original column were “M,” that instead could be \\ncoded as a 1 in the male column, a 0 in the female column, and a 0 in the unknown \\ngender column.\\nThis is actually overkill, because if the male and female columns were both 0, then the \\nunknown gender is already implied. You don’t need a third column.\\nIn this way, when dummy coding a categorical variable, you always need one less \\ncolumn than you have category values—the last category is always implied by the other \\nvalues. In stats-speak, you’d say that the gender categorical variable has only two degrees \\nof freedom, because the degrees of freedom are always one less than the possible values \\nthe variable can take.\\nIn this particular example, start by creating a copy of the Training Data sheet called \\nTraining Data w Dummy Vars . You’re going to split the ﬁ rst two predictors into two \\ncolumns each, so go ahead and clear out column A and B and insert another two blank \\ncolumns to the left of column A.\\nLabel these four empty columns Male, Female, Home, and Apt (unknown gender and \\nPO box become implied). As shown in Figure 6-2, you should now have four empty col-\\numns to house the dummy coding of your two categorical variables.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 233, 'page_label': '212'}, page_content='Data Smart212\\nFigure 6-2: Training Data w Dummy Vars tab with new columns for the dummy variables\\nConsider the ﬁ rst row of training data. To turn the “M” in the gender column into \\ndummy encoded data, you place a 1 in the Male column and a 0 in the Female column. \\n(The 1 in the Male column naturally implies that the gender is not Unknown.)\\nIn cell A2 on the Training Data w Dummy Vars tab, check the old category on the \\nTraining Data tab and set a 1 if the category was set to “M”:\\n=IF(‘Training Data’!A2=”M”,1,0)\\nSame goes for values “F” in the Female column, “H” in the Home column, and “A” in \\nthe Apt column. To copy these four formulas down through all the rows of the training \\ndata, you can either drag them, or better yet, as explained in Chapter 1, highlight all four \\nformulas and then double-click the bottom right corner of D2. That’ll ﬁ  ll in the sheet \\nwith the converted values through D1001. Once you’ve converted these two categorical \\ncolumns into four binary dummy variables (see Figure 6-3), you’re ready to get modeling.\\nFigure 6-3: Training data with dummy variables populated'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 234, 'page_label': '213'}, page_content='213The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\\nLet’s Bake Our Own Linear Regression\\nEvery time I say this, a statistician loses its wings, but I’m going to say it anyway—If \\nyou’re ever shoved a trendline through a cloud of points on a scatter plot, then you’ve \\nbuilt an AI model.\\nYou’re probably thinking, “But there’s no way! I would’ve known had I created a robot \\nthat could travel back in time to stop John Conner!”\\nThe Simplest of Linear Models\\nLet me explain by showing some simple data in Figure 6-4.\\nFigure 6-4:  Cat ownership versus me sneezing\\nIn the pictured table, you have the number of cats in a house in the ﬁ rst column and the \\nlikelihood that I’ll sneeze inside that house in the second column. No cats? Three percent \\nof the time I sneeze any way just because I know a Platonic cat exists somewhere. Five \\ncats? Well, then my sneezing is just about guaranteed. Now, we can scatter plot this data \\nin Excel and look at it as shown in Figure 6-5 (For more on inserting plots and charts \\nsee Chapter 1).\\n0\\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\n12\\n# of cats\\nLikelihood I’ll sneeze in your home\\nLikelihood of sneezing\\n345\\nFigure 6-5:  Scatter plot of cats versus sneezing'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 235, 'page_label': '214'}, page_content='Data Smart214\\nBy right-clicking on the data points in the graph (you have to right-click an actual data \\npoint, not just the graph itself) and selecting Add Trendline from the menu, you can select \\na linear regression model to add to the graph. Under the “Options” section of the “Format \\nTrendline” window, you can select to “Display equation on chart.” Pressing OK, you can \\nnow see the trendline and formula for the line (Figure 6-6).\\n0\\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\ny = 0.1529x + 0.0362\\n12\\n# of cats\\nLikelihood I’ll sneeze in your home\\nLikelihood of sneezing\\n345\\nFigure 6-6:  Linear model displayed on the graph\\nThe trendline in the graph rightly shows the relationship between cats and sneezing \\nwith a formula of:\\nY = 0.1529x + 0.0362 \\nIn other words, when x is 0, the linear model thinks I’ve got about a 3-4 percent chance \\nof sneezing, and the model gives me an extra 15 percent chance per cat.\\nThat baseline of 3-4 percent is called the intercept of the model, and the 15 percent per \\ncat is called a coeffi  cient for the cats variable. Making a prediction with a linear model \\nlike this requires nothing more than taking my future data and combining it with the \\ncoeffi  cients and the intercept of the model.\\nIn fact, you can copy the formula \\n=0.1529x+0.0362 out of the graph if you like and paste \\nit in a cell to make predictions by replacing the x with an actual number., For example, \\nif in the future I went into a home with three and a half cats (poor Timmy lost his hind \\npaws in a boating accident), then I’d take a “linear combination” of the coeffi  cients and \\nmy data, add in the intercept, and get my prediction:\\n0.1529*3.5 cats + 0.0362 = 0.57'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 236, 'page_label': '215'}, page_content='215The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\\nA 57 percent chance of sneezing! This is an AI model in the sense that we’ve taken an \\nindependent variable (cats) and a dependent variable (sneezing) and asked the computer \\nto describe their relationship as a formula that best ﬁ ts our historical data.\\nNow, you might wonder how the computer ﬁ gured this trendline out from the data. \\nIt looks good, but how’d it know where to put it? Basically, the computer looked for a \\ntrendline that best ﬁ t the data, where by best ﬁ t I mean the trendline that minimizes the \\nsum of squared error with the training data.\\nTo get a handle on what the sum of squared error means, if you evaluate the trendline \\nfor one cat you get:\\n0.1529*1 cat + 0.0362 = 0.1891\\nBut the training data gives a likelihood of 20 percent, not 18.91 percent. So then your \\nerror at this point on the trendline is 1.09 percent. This error value is squared to make \\nsure it a positive value, regardless of whether the trendline is above or below the data \\npoint. 1.09 percent squared is 0.012 percent. Now if you summed each of these squared \\nerror values for the points in our training data, you’d get the sum of the squared error \\n(often just called the sum of squares). And that’s what Excel minimized when ﬁ tting the \\ntrendline to the sneeze graph.\\nAlthough your RetailMart data has way too many dimensions to toss into a scatter \\nplot, in these next sections, you’ll ﬁ t the exact same type of line to the data from scratch.\\nBack to the RetailMart Data\\nOK, so it’s time to build a linear model like the Kitty Sneeze model on the RetailMart \\ndataset. First, create a new tab called Linear Model, and paste the values from the Training \\nData w Dummy Vars tab, except when you paste it, start in column B to save room for \\nsome row labels in column A and on row 7 to leave space at the top of the sheet for the \\nlinear model’s coeffi  cients and other evaluative data you’ll be tracking.\\nPaste the header row for your dependent variables again on row 1 to stay organized. \\nAnd in column U, add the label Intercept because your linear model will need a baseline \\njust like in the previous example. Furthermore, to incorporate the intercept into the model \\neasier, ﬁ ll in your intercept column (U8:U1007) with 1s. This will allow you to evaluate \\nthe model by taking a \\nSUMPRODUCT of the coeffi  cient row with a data row that will incor-\\nporate the intercept value.\\nAll the coeffi  cients for this model are going to go on row 2 of the spreadsheet, so label \\nrow 2 as Model Coeffi  cients and place a starting value of 1 in each cell. You can also lay'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 237, 'page_label': '216'}, page_content='Data Smart216\\non some conditional formatting on the coeffi  cient row so you can see diff erences in them \\nonce they’re set. \\nYour dataset now looks like Figure 6-7.\\nFigure 6-7: Linear modeling setup\\nOnce the coeffi  cients in row 2 are set, you can take a linear combination (formula \\nSUMPRODUCT ) of the coefficients with a row of customer data and get a pregnancy \\nprediction.\\nYou have too many columns here, to build a linear model by graphing it the way I did \\nwith the cats, so instead you’re going to train the model yourself. The ﬁ rst step is to add \\na column to the spreadsheet with a prediction on one of the rows of data.\\nIn column W, next to the customer data, add the column label Linear Combination  \\n(Prediction) to row 7 and below it take a linear combination of coeffi  cients and customer \\ndata (intercept column included). The formula you plug into row 8 to do this for your \\nﬁ rst customer is:\\n=SUMPRODUCT(B$2:U$2,B8:U8)\\nThe absolute reference should be placed on row 2, so that you can drag this formula \\ndown to all the other customers without the coeffi  cient row changing.\\nTIP\\nAlso, you may want to highlight column W, right-click, select “Format Cells…,” and \\nformat the values as a number with two decimal places just to keep your eyes from \\nbleeding at the sight of so many decimals.\\nOnce you’ve added this column, your data will look like Figure 6-8.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 238, 'page_label': '217'}, page_content='217The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\\nFigure 6-8:  The prediction column for a linear model\\nIdeally, the prediction column (column W) would look identical to what we know to be \\nthe truth (column V), but using coeffi  cients of 1 for every variable, it’s easy to see you’re \\nway off . The ﬁ rst customer gets a prediction of 5 even though pregnancy is indicated with \\na 1 and non-pregnancy with a 0. What’s a 5? Really, really pregnant?\\nAdding in an Error Calculation\\nYou need to get the computer to set these model coeffi  cients for you, but in order for it to \\nknow how to do that, you need to let the machine know when a prediction is right and \\nwhen it’s wrong.\\nTo that end, add an error calculation in column X. Use squared error, which is just the square \\nof the distance of the value of PREGNANT (column V) from the predicted value (column W).\\nSquaring the error allows each error calculation to be positive, so that you can sum \\nthem together to get a sense of overall error of the model. You don’t want positive and \\nnegative errors canceling each other out. So for the ﬁ rst customer in the sheet, you’d have \\nthe following formula:\\n=(V8-W8)^2\\nYou can drag that cell down the rest of the column to give each prediction its own \\nerror calculation.\\nNow, add a cell above the predictions in cell X1 (labeled in W1 as Sum Squared Error) \\nwhere you’ll sum the squared error column using the formula:\\n=SUM(X8:X1007)'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 239, 'page_label': '218'}, page_content='Data Smart218\\nYour spreadsheet looks like Figure 6-9:\\nFigure 6-9: Predictions and sum of squared error\\nTraining with Solver\\nNow you’re ready to train your linear model. You want to set the coeffi  cients for each vari-\\nable such that the sum of squared error is as low as it can be. If this sounds like a job for \\nSolver to you, you’re right. Just as you did in Chapters 2, 4, and 5, you’re going to open up \\nSolver and get the computer to ﬁ nd the best coeffi  cients for you.\\nThe objective function will be the Sum Squared Error value from cell X1, which you’ll \\nwant to minimize “by changing variable cells” B2 through U2, which are your model \\ncoeffi  cients.\\nNow, squared error is a quadratic function of your decision variables, the coeffi  cients, \\nso you can’t use Simplex-LP as the solving method like you used extensively in Chapter \\n4. Simplex is super-fast and guarantees ﬁ  nding the best answer, but it requires that the \\nmodel only consider linear combinations of the decisions. You’ll need to use the evolu-\\ntionary algorithm in Solver.\\nREFERENCE\\nFor more on non-linear optimization models and the inner workings of the evolution-\\nary optimization algorithm, see Chapter 4. If you like, you can also play with the other \\nnon-linear optimization algorithm Excel off ers called GRG.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 240, 'page_label': '219'}, page_content='219The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\\nBasically, Solver is going to sniff   around for coeffi  cient values that make the sum of \\nsquares fall until it feels like it’s found a really good solution. But in order to use the evo-\\nlutionary algorithm eff ectively, you need to set upper and lower bounds on each of the \\ncoeffi  cients you’re trying to set.\\nI urge you to play around with these upper and lower bounds. The tighter they are \\n(without getting too tight!), the better the algorithm works. For this model, I’ve set them \\nto be between -1 and 1.\\nOnce you’ve completed these items, your Solver setup should look like Figure 6-10.\\nFigure 6-10:  Solver setup for linear model\\nPress the Solve button and wait! As the Evolutionary Solver tries out various coeffi  cients \\nfor the model, you’ll see the values change. The conditional formatting on the cells will \\ngive you a sense of magnitude. Furthermore, the sum of the squared error should bounce \\naround but generally decrease over time. Once Solver ﬁ nishes, it will tell you the problem \\nis optimized. Click OK, and you’ll have your model back.\\nIn Figure 6-11, you’ll see that the Solver run ﬁ  nished with a 135.52 sum of squared \\nerror. If you’re following along and would like to run Solver yourself, be aware that two \\nruns of the evolutionary algorithm don’t have to end up in the same place—your sum of \\nsquares might end up being higher or lower than the book’s, with slightly diff erent ﬁ nal \\nmodel coeffi  cients. The optimized linear model is pictured in Figure 6-11.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 241, 'page_label': '220'}, page_content='Data Smart220\\nFigure 6-11: Optimized linear model\\nUSING THE LINEST() FORMULA FOR LINEAR REGRESSION\\nSome readers may be aware that Excel has its own linear regression formula called \\nLINEST(). In one stroke, this formula can, indeed, do what you just did by hand. It \\ncraps out at 64 features, however, so for truly large regressions, you’ll need to roll \\nyour own anyway.\\nFeel free to try it out on this dataset. But beware! Read the Excel help documentation \\non the formula. In order to get all your coeffi  cients out of it, you’ll need to use it as an \\narray formula (see Chapter 1). Also, it spits the coeffi  cients out in reverse order (Male \\nwill be the ﬁ nal coeffi  cient before the intercept), which is truly annoying.\\nWhere LINEST() comes in super handy is that it automatically computes many of \\nthe values needed for performing statistical testing on your linear model, such as the \\ndreaded coeffi  cient standard error calculation that you’ll see in the next section.\\nBut in this chapter, you’re going to do everything by hand so that you’ll know a great \\ndeal about what LINEST() (and other software packages’ linear modeling functions) is \\ndoing and will feel comfortable leaning on it in the future. Also, doing things by hand \\nwill aid the transition into logistic regression, which Excel does not support.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 242, 'page_label': '221'}, page_content='221The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\\nUSING MEDIAN REGRESSION TO BETTER HANDLE OUTLIERS\\nIn median regression, you minimize the sum of the absolute values of the errors instead \\nof the sum of the squared errors. That’s the only change from linear regression.\\nWhat does it get you?\\nIn linear regression, outliers (values that are markedly distant from the rest of the data) \\nin your training set have more pull and can throw off  the model ﬁ tting process. When \\nan outlier’s error values are large, the linear regression will chase them more, striking \\na diff erent balance between a large error and a bunch of other normal points’ smaller \\nerrors than the balance that is struck in median regression. In median regression, the \\nline that’s ﬁ t to the data will stay close to the typical, inlying data points rather than \\nchase the outliers so much.\\nWhile I won’t work through median regression in this chapter, it’s not hard to try \\nout on your own. Just swap the squared error term for the absolute value (Excel has the \\nABS function) and you’re off  and running.\\nThat said, if you’re on Windows and have OpenSolver installed (see Chapter 1), then \\nhere’s a huge bonus problem!\\nSince in median regression, you’re minimizing error, and since an absolute value can \\nalso be thought of as a max function (the max of a value and -1 times that value), try to \\nlinearize the median regression as a minimax-esque optimization model (see Chapter 4 \\nfor more on minimax optimization models). Hint: You’ll need to create one variable per \\nrow of training data, which is why you need OpenSolver—regular Solver can’t handle \\na thousand decisions and two thousand constraints.\\nGood luck!\\nLinear Regression Statistics: R-Squared, F Tests, t Tests\\nNOTE\\nThis next section is the heaviest statistical section in the whole book. Indeed, this section \\narguably houses the most complex calculation in this entire book—the calculation of \\nmodel coeffi  cient standard error. I’ve tried to describe everything as intuitively as pos-\\nsible, but some of the calculations defy explanation at a level appropriate for the text. \\nAnd I don’t want to get sidetracked teaching a linear algebra course here.\\nTry to understand these concepts as best you can. Practice them. And if you want to \\nknow more, grab an intro level stats textbook (for example, Statistics in Plain English  \\nby Timothy C. Urdan [Routledge, 2010]).\\nIf you get bogged down, know that this section is self-contained. Skip it and come \\nback if you need to.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 243, 'page_label': '222'}, page_content='Data Smart222\\nYou have a linear model now that you ﬁ t by minimizing the sum of squares. Glancing \\nat the predictions in Column Y, they look all right to the eye. For example, the pregnant \\ncustomer on row 27 who bought a pregnancy test, prenatal vitamins, and maternity clothes \\ngets a score of 1.07 while the customer on row 996 who’s only ever bought wine gets a \\nscore of 0.15. That said, questions remain:\\n• How well does the regression actually ﬁ t the data from a quantitative, non-eyeball \\nperspective?\\n• Is this overall ﬁ t by chance or is it statistically signiﬁ cant?\\n• How useful are each of the features to the model?\\nTo answer these questions for a linear regression, you can compute the R-squared, an \\noverall F test, and t tests for each of your coeffi  cients.\\nR-Squared—Assessing Goodness of Fit\\nIf you knew nothing about a customer in the training set (columns B through T were \\nmissing) but you were forced to make a prediction on pregnancy anyway, the best way to \\nminimize the sum of squared error in that case would be to just put the average of column \\nV in the sheet for each prediction. In this case the average is 0.5 given the 500/500 split in \\nthe training data. And since each actual value is either a 0 or 1, each error would be 0.5, \\nmaking each squared error 0.25. At 1000 predictions then, this strategy of predicting the \\naverage, would give a sum of squares of 250.\\nThis value is called the total sum of squares. It’s the sum of squared deviations of each \\nvalue in column V from the average of column V. And Excel off  ers a nifty formula for \\ncalculating it in one step, \\nDEVSQ.\\nIn X2, you can calculate the total sum of squares as:\\n=DEVSQ(V8:V1007)\\nBut while putting the mean for every prediction would yield a sum of squared error of \\n250, the sum of the squared error given by the linear model you ﬁ t earlier is far less than \\nthat. Only 135.52. \\nThat means 135.52 out of the total 250 sum of squares remains unexplained after you \\nﬁ t your regression (in this context, the sum of squared error is often called the residual \\nsum of squares). \\nFlipping this value around, the explained sum of squares (which is exactly what it says—\\nthe amount you explained with your model) is 250 – 135.52. Put this in X3 as:\\n=X2–X1\\nThis gives 114.48 for the explained sum of squares (if you didn’t obtain a sum of squared \\nerror of 135.52 when you ﬁ t your regression, then your results might vary slightly).\\nSo how good of a ﬁ t is this?'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 244, 'page_label': '223'}, page_content='223The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\\nGenerally, this is answered by looking at the ratio of the explained sum of squares to the \\ntotal sum of squares. This value is called the R-squared. We can calculate the ratio in X4:\\n=X3/X2\\nAs shown in Figure 6-12, this gives an R-squared of 0.46. If the model ﬁ  t perfectly, \\nyou’d have 0 squared error, the explained sum of squares would equal the total, and the \\nR-squared would be a perfect 1. If the model didn’t ﬁ t at all, the R-squared would be closer \\nto 0. So then in the case of this model, given the training data’s inputs, the model can do \\nan okay-but-not-perfect job of replicating the training data’s independent variable (the \\nPregnancy column).\\nFigure 6-12: R-squared of 0.46 for the linear regression\\nNow, keep in mind that the R-squared calculation only works in ﬁ nding linear relation-\\nships between data. If you have a funky, non-linear relationship (maybe a V or U shape) \\nbetween a dependent and independent variable in a model, the R-squared value could not \\ncapture that relationship.\\nThe F Test—Is the Fit Statistically Signiﬁ cant?\\nOftentimes, people stop at R-squared when analyzing the ﬁ t of a regression. \\n“Hey, the ﬁ t looks good! I’m done.”\\nDon’t do that.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 245, 'page_label': '224'}, page_content='Data Smart224\\nThe R-squared only tells you how well the model ﬁ ts the data. What it doesn’t tell you \\nis whether this ﬁ t is statistically signiﬁ cant. \\nIt is easy, especially with sparse datasets (only a few observations), to get a model that \\nﬁ ts quite well but whose ﬁ  t is statistically insigniﬁ  cant, meaning that the relationship \\nbetween the features and the independent variable may not actually be real. \\nIs your model’s ﬁ t due to chance? Some stroke of luck? For a model to be statistically \\nsigniﬁ cant, you must reject this ﬁ t-by-ﬂ uke hypothesis. So assume for a moment, that your \\nmodel’s ﬁ t is a complete ﬂ uke. That the entire ﬁ t is due to luck of the draw on the random \\n1,000 observations you pulled from the RetailMart database. This devil’s advocate assump-\\ntion is called the null hypothesis.\\nThe standard practice is to reject the null hypothesis if given it were true, the prob-\\nability of obtaining a ﬁ t at least this good is less than 5 percent. This probability is often \\ncalled a p value.\\nTo calculate that probability, we perform an F test. An F test takes three pieces of \\ninformation about our model and runs them through a probability distribution called the \\nF distribution (for an explanation of the term probability distribution, see Chapter 4’s \\ndiscussion of the normal distribution). Those three pieces of information are:\\n• Number of model coeffi  cients—This is 20 in our case (19 features plus an intercept).\\n• Degrees of freedom—This is the number of training data observations minus the \\nnumber of model coeffi  cients.\\n• The F statistic—The F statistic is the ratio of explained to unexplained squared error \\n(X3/X1 in the sheet) times the ratio of degrees of freedom to dependent variables.\\nThe larger the F statistic, the lower the null hypothesis probability is. And given the \\nexplanation of the F statistic above, how do you make it larger? Make one of the two ratios \\nin the calculation larger. You can either explain more of the data (i.e., get a better ﬁ  t) or \\nyou can get more data for the same number of variables (i.e., make sure your ﬁ t holds in \\na larger sample).\\nReturning then to the sheet, we need to count up the number of observations and the \\nnumber of model coeffi  cients we have. \\nLabel Y1 as Observation Count and in Z1 count up all the pregnancy values in column V:\\n=COUNT(V8:V1007)\\nYou should, as you’d expect, get 1,000 observations.\\nIn Z2, get the Model Coeffi  cient Count by counting them on row 2:\\n=COUNT(B2:U2)'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 246, 'page_label': '225'}, page_content='225The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\\nYou should get 20 counting the intercept. You can then calculate the Degrees of Freedom \\nin Z3 by subtracting the model coeffi  cient count from the observation count:\\n=Z1-Z2\\nYou’ll get a value of 980 degrees of freedom. \\nNow for the F statistic in Z4. As noted above, this is just the ratio of explained to \\nunexplained squared error (X3/X1) times the ratio of degrees of freedom to dependent \\nvariables (Z3/(Z2-1)):\\n=(X3/X1)*(Z3/(Z2-1))\\nWe can then plug these values into the F distribution in Z5 using the Excel function \\nFDIST. Label the cell F Test P Value. FDIST takes the F statistic, the number of dependent \\nvariables in the model, and the degrees of freedom:\\n=FDIST(Z4,Z2-1,Z3)\\nAs shown in Figure 6-13, the probability of getting a ﬁ t like this given the null hypoth-\\nesis is eff ectively 0. Thus, you may reject the null hypothesis and conclude that the ﬁ t is \\nstatistically signiﬁ cant.\\nFigure 6-13: The result of the F test'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 247, 'page_label': '226'}, page_content='Data Smart226\\nCoefﬁ cient t Tests—Which Variables Are Signiﬁ cant?\\nWARNING: MATRIX MATH AHEAD!\\nWhile the previous two statistics weren’t hard to compute, performing a t test on a \\nmultiple linear regression requires matrix multiplication and inversion. If you don’t \\nremember how these operations work from high school or intro college math, check \\nout a linear algebra or calculus book. Or just read up on Wikipedia. And use the \\nworkbook that’s available for download with this chapter to make sure your math is \\ncorrect.\\nIn Excel, matrix multiplication uses the \\nMMULT  function while inversion uses the \\nMINVERSE function. Since a matrix is nothing more than a rectangular array of numbers, \\nthese formulas are array formulas (see Chapter 1 for using array formulas in Excel).\\nWhile the F test veriﬁ ed that the entire regression was signiﬁ cant, you can also check \\nthe signiﬁ cance of individual variables. By testing the signiﬁ cance of single features, you \\ncan gain insight into what’s driving your model’s results. Statistically insigniﬁ cant vari-\\nables might be able to be eliminated, or if you’re sure in your gut that the insigniﬁ  cant \\nvariable should matter, then you might investigate if there are data cleanliness issues in \\nyour training set.\\nThis test for model coeffi  cient signiﬁ cance is called a t test. When performing a t test, \\nmuch like an F test, you assume that the model coeffi  cient you’re testing is worthless and \\nshould be 0. Given that assumption, the t test calculates the probability of obtaining a \\ncoeffi  cient as far from 0 as what you actually obtained from your sample.\\nWhen performing a t test on a dependent variable, the ﬁ rst value you should calculate is \\nthe prediction standard error. This is the sample standard deviation of the prediction error \\n(see Chapter 4 for more on standard deviation), meaning that it’s a measure of variability \\nin the model’s prediction errors.\\nYou can calculate the prediction standard error in X5 as the square root of the sum of \\nsquared error (X1) divided by the degrees of freedom (Z3):\\n=SQRT(X1/Z3)\\nThis gives us the sheet shown in Figure 6-14.\\nUsing this value, you can then calculate the model’s coeffi  cient standard errors. Think \\nof the standard error of a coeffi  cient as the standard deviation of that coeffi  cient if you \\nkept drawing new thousand-customer samples from the RetailMart database and ﬁ  tting \\nnew linear regressions to those training sets. You wouldn’t get the same coeffi  cients each \\ntime; they’d vary a bit. And the coeffi  cient standard error quantiﬁ es the variability you’d \\nexpect to see.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 248, 'page_label': '227'}, page_content='227The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\\nFigure 6-14: The prediction standard error for the linear regression\\nTo start this calculation, create a new tab in the workbook called \\nModelCoeffi  cientStandardError. Now, the thing that makes computing the standard error \\nso diffi  cult is that we need to understand both how the training data for a coeffi  cient var-\\nies by itself and in concert  with the other variables. The ﬁ  rst step in nailing that down \\nis multiplying the training set as one gigantic matrix (often called the design matrix  in \\nlinear regression) by itself.\\nThis product of the design matrix (B8:U1007) with itself forms what’s called a sum of \\nsquares and cross products (SSCP) matrix. To see what this looks like, ﬁ rst paste the row \\nheaders for the training data in the ModelCoeffi  cientStdError tab in B1:U1 and transposed \\ndown the rows in A2:A21. This includes the Intercept header.\\nTo multiply the design matrix times itself, you feed it into the Excel’s MMULT function, \\nﬁ rst transposed, then right-side up:\\n{=MMULT(TRANSPOSE(‘Linear Model’!B8:U1007),’Linear Model’!B8:U1007)}\\nSince this function returns a variables-by-variables sized matrix, you actually have to \\nhighlight the entire range of B2:U21 on the ModelCoeffi  cientStdError tab and execute the \\nfunction as an array formula (see Chapter 1 for more on array formulas). \\nThis yields the tab shown in Figure 6-15.\\nNote the values in the SSCP matrix. Along the diagonal, you’re counting matches of \\neach variable with itself—the same as just summing up the 1s in each column of the \\ndesign matrix. The intercept gets 1000, for example, in cell U21, because in the original \\ntraining data, that column is made up of 1000 ones.\\nIn the off -diagonal cells, you end up with counts of the matches between diff  erent \\npredictors. While Male and Female obviously never match by design, Pregnancy Test and \\nBirth Control appear together in six customer rows in the training data.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 249, 'page_label': '228'}, page_content='Data Smart228\\nFigure 6-15: The SSCP matrix\\nThe SSCP matrix then gives you a glimpse into the magnitudes of each variable and \\nhow much they overlap and move with each other. \\nThe coeffi  cient standard error calculation uses the inverse of the SSCP matrix. To obtain \\nthe inverse, paste the variable headers again below the SSCP matrix in B24:U24 and in \\nA25:A44. The inverse of the SSCP matrix in B2:U21 is then calculated by highlighting \\nB25:U44 and employing the \\nMINVERSE function as an array formula:\\n{=MINVERSE(B2:U21)}\\nThis yields the sheet shown in Figure 6-16.\\nThe values required in the coeffi   cient standard error calculation are those on the \\ndiagonal of the SSCP inverse matrix. Each coeffi  cient standard error is calculated as the \\nprediction standard error for the entire model (calculated as 0.37 on the Linear Model \\ntab earlier in cell X5) scaled by the square root of the appropriate value from the SSCP \\ninverse diagonal.\\nFor example, the coeffi  cient standard error for Male would be the square root of its \\nMale-to-Male entry in the inverse SSCP matrix (square root of 0.0122) times the predic-\\ntion standard error.\\nTo calculate this for all variables, number each variable starting with 1 in B46 through 20 \\nin U46. The appropriate diagonal value can then be read for each predictor using the \\nINDEX \\nformula. For example, INDEX(ModelCoefficientStdError!B25:B44,ModelCoefficientSt\\ndError!B46) returns the Male-to-Male diagonal entry (see more on the INDEX formula in \\nChapter 1).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 250, 'page_label': '229'}, page_content='229The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\\nFigure 6-16: The inverse of the SSCP matrix\\nTaking the square root of this value and multiplying it times the prediction standard \\nerror, the Male coeffi  cient standard error is calculated in cell B47 as:\\n=’Linear Model’!$X5*SQRT(INDEX(ModelCoefficientStdError!B25:B44,\\nModelCoefficientStdError!B46))\\nThis comes out to 0.04 for the model ﬁ t in the book.\\nDrag this formula through column U to obtain all the coeffi  cient standard error values \\nas shown in Figure 6-17.\\nFigure 6-17: The standard error of each model coefﬁ  cient'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 251, 'page_label': '230'}, page_content='Data Smart230\\nOn the Linear Model tab, label A3 as Coeffi  cient Standard Error. Copy the coeffi  cient \\nstandard errors, and paste their values back on the Linear Model tab in row 3 (B3:U3).\\nPhew! It’s downhill from here. No more matrix math for the rest of the book. I swear.\\nNow you have everything you need to calculate each coeffi  cient’s t statistic (similar to \\nthe entire model’s F statistic from the previous section). You will be performing what’s \\ncalled a two-tailed t test, meaning that you’ll be calculating the probability of obtaining \\na coeffi  cient at least as large in either the positive or negative direction if, in reality, there’s \\nno relationship between the feature and the dependent variable. \\nThe t statistic for the test can be calculated in row 4 as the absolute value of the coef-\\nﬁ cient normalized by the coeffi  cient’s standard error. For the Male feature this is:\\n=ABS(B2/B3)\\nCopy this through column U to all the variables.\\nThe t test can then be called by evaluating the t distribution (another statistical distri-\\nbution like the normal distribution introduced in Chapter 4) at the value of the t statistic \\nfor your particular degrees of freedom value. Label row 5 then as t Test p Value, and in \\nB5 use the formula TDIST to calculate the probability of a coeffi   cient at least this large \\ngiven the null hypothesis:\\n=TDIST(B4,$Z3,2)\\nThe two in the formula indicates you’re performing the two-tailed t test. Copying this \\nformula across to all variables and applying conditional formatting to cells over 0.05 \\n(5percent probability), you can see which features are not statistically signiﬁ cant. While \\nyour results may vary based on the ﬁ  t of your model, in the workbook shown in Figure \\n6-18, the Female, Home, and Apt columns are shown to be insigniﬁ cant.\\nFigure 6-18:  Female, Home, and Apt are insigniﬁ  cant predictors according to the test\\nYou could remove these columns from your model in future training runs.\\nNow that you’ve learned how to evaluate the model using statistical tests, let’s change gears \\nand look at measuring the model’s performance by making actual predictions on a test set.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 252, 'page_label': '231'}, page_content='231The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\\nMaking Predictions on Some New Data and Measuring \\nPerformance\\nThat last section was all statistics. Lab work you could say. It’s not the most fun you’ve \\never had, but validating goodness of ﬁ t and signiﬁ cance are important skills to have. But \\nnow it’s time to take this model to the racetrack and have some fun!\\nHow do you know your linear model actually will predict well in the real world? After \\nall, your training set does not encapsulate every possible customer record, and your coef-\\nﬁ cients have been purpose built to ﬁ  t the training set (although if you’ve done your job \\nright, the training set, very nearly, resembles the world at large).\\nTo get a better sense of how the model will perform in the real world, you should run some \\ncustomers through the model that were not used in the training process. You’ll see this sepa-\\nrate set of examples used for testing a model often called a validation set, test set, or holdout set. \\nTo assemble your test set, you can just return to the customer database and select \\nanother set of data from random customers (paying special attention to not pull the same \\ncustomers used in training). Now, as noted earlier, 6 percent of RetailMart’s customers are \\npregnant, so if you randomly selected a thousand customers from the database, roughly \\n60 of them would be pregnant.\\nWhile you oversampled the pregnant class in training the model, for testing you’ll leave \\nthe ratio of pregnant households at 6 percent so that our measurements of the precision \\nof the model are accurate for how the model would perform in a live setting.\\nIn the RetailMart spreadsheet available for download that accompanies this chapter, \\nyou’ll ﬁ nd a tab called Test Set, which is populated with a thousand rows of data identi-\\ncal to the training data. The ﬁ rst 60 customers are pregnant, while the other 940 are not \\n(see Figure 6-19).\\nFigure 6-19: Test set data\\nJust as you did on the Linear Model tab, run this new data through the model by tak-\\ning a linear combination of customer data and coeffi  cients and adding in the intercept.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 253, 'page_label': '232'}, page_content='Data Smart232\\nPlacing this prediction in column V, you have the following formula for the ﬁ rst customer \\non row 2 (since the test set doesn’t have an Intercept column, you add it in separately): \\n=SUMPRODUCT(‘Linear Model’!B$2:T$2,’Test Set’!A2:S2)+’Linear Model’!U$2\\nCopy this calculation down to all the customers. The resulting spreadsheet looks as \\nshown in Figure 6-20.\\nFigure 6-20: Predictions on the test set\\nYou can see in Figure 6-20 that the model has identiﬁ ed many of the pregnant house-\\nholds with predictions closer to 1 than they are to 0. The highest prediction values are \\nfor households that bought a product clearly related to pregnancy, such as folic acid or \\nprenatal vitamins.\\nOn the other hand, out of the 60 pregnant households, there are some who never bought \\nanything to indicate they were pregnant. Of course, they didn’t buy alcohol or tobacco, but \\nas their low pregnancy scores indicate, not buying something doesn’t mean a whole lot.\\nConversely, if you look at the predictions for non-pregnant folks there are some misses. \\nFor instance if you’re following along in the workbook, on row 154 a non-pregnant cus-\\ntomer bought maternity clothing and stopped buying cigarettes, and the model gave them \\na score of 0.76.\\nIt’s clear then that if you are going to use these predictions in real marketing eff  orts, \\nyou need to set a score threshold for when you can assume someone is pregnant and reach'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 254, 'page_label': '233'}, page_content='233The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\\nout to that person with marketing materials. Perhaps you only send someone marketing \\nmaterials if they’re scored at  0.8 or above. Perhaps that cutoff   should be 0.95, so that \\nyou’re extra sure.\\nIn order to set this classiﬁ  cation threshold, you need to look at trade-off  s in model \\nperformance metrics. Most predictive model performance metrics are based on counts \\nand ratios of four values that come from the predictions on our test set:\\n• True positives—Labeling a pregnant customer as pregnant\\n• True negatives—Labeling a not pregnant customer as not pregnant\\n• False positives (also called type I error)—Calling a not-so-pregnant customer preg-\\nnant. In my experience, this speciﬁ c false positive is very insulting face-to-face. Do \\nnot try this at home.\\n• False negatives (also called type II error)—Failing to identify a pregnant customer \\nas such. This is not nearly as insulting in my experience. \\nAs you’ll see, while there are lots of diff erent performance metrics for a predictive model, \\nthey all feel a bit like Tex Mex food—they’re all basically combinations of the same four \\ningredients listed above.\\nSetting Up Cutoff Values\\nCreate a new sheet called Performance. The lowest value that could practically be used as \\na cutoff  between pregnant and not pregnant is the lowest prediction value from the test \\nset. Label A1 as Min Prediction and in A2, you can calculate this as:\\n=MIN(‘Test Set’!V2:V1001)\\nSimilarly, the highest cutoff  value would be the max prediction from the test set. Label \\nA4 as Max Prediction, and in A5, you can calculate this as:\\n=MAX(‘Test Set’!V2:V1001)\\nThe values given back are -0.35 and 1.25 respectively. Keep in mind that your linear \\nregression can make predictions below 0 and above 1 because it’s not actually returning \\nclass probabilities (we’ll address this with another model later).\\nIn column B, then, add the header Probability Cutoff  for Pregnant Classiﬁ cation and \\nbelow that specify a range of cutoff  values starting with -0.35. In the sheet shown in Figure \\n6-21, the cutoff  values have been chosen to increase in increments of 0.05 all the way to \\nthe max of 1.25 (just enter the ﬁ rst three by hand, highlight them, and drag down to ﬁ  ll \\nin the rest).\\nAlternatively, you could specify every single prediction value from the test set as a cutoff  \\nif you wanted to be thorough. No more than that would be needed.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 255, 'page_label': '234'}, page_content='Data Smart234\\nPrecision (Positive Predictive Value)\\nLet’s now ﬁ ll in some model performance metrics for each of these cutoff  values using the \\nTest Set data predictions starting with precision, also known as positive predictive value.\\nPrecision is the measure of how many pregnant households we correctly identify out \\nof all the households the model says are pregnant. In business-speak, precision is the \\npercent of ﬁ sh in your net that are tuna and not dolphins.\\nLabel column C as Precision. Consider the cutoff  score in B2 of -0.35. What’s the preci-\\nsion of our model if we consider anyone scoring at least a -0.35 to be pregnant? \\nTo calculate that, we can go to the “Test Set” tab and count the number of cases where \\na pregnant household scored greater than or equal to -0.35 divided by the number of total \\nrows with a score over -0.35. Using the \\nCOUNTIFS formula to check actuals and predictions, \\nthe formula in cell C2 would look as follows:\\n=COUNTIFS(‘Test Set’!$V$2:$V$1001,”>=” & B2,\\n‘Test Set’!$U$2:$U$1001,”=1”)/COUNTIF(‘Test Set’!$V$2:$V$1001,”>=” & B2)\\nFigure 6-21: Cutoff values for the pregnancy classiﬁ  cation'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 256, 'page_label': '235'}, page_content='235The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\\nThe ﬁ rst COUNTIFS  statement in the formula matches both on actual pregnancy and \\nmodel prediction, while the COUNTIF in the denominator just cares about only those who \\nscored higher that -0.35 regardless of pregnancy. You can copy this formula to all the \\nthresholds you’re evaluating.\\nAs seen in Figure 6-22, the precision of the model increases with the cutoff  value, and \\nat a cutoff  value of 1, the model becomes completely precise. A completely precise model \\nidentiﬁ es only pregnant customers as pregnant.\\nFigure 6-22: Precision calculations on the test set\\nSpeciﬁ city (True Negative Rate)\\nAnother performance metric that increases with the cutoff   value is called Speciﬁ city. \\nSpeciﬁ city, also called the Tr ue Negative Rate is a count of how many not pregnant cus-\\ntomers are correctly predicted as such (true negatives) divided by the total number of not \\npregnant cases.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 257, 'page_label': '236'}, page_content='Data Smart236\\nLabeling column D as Speciﬁ city/True Negative Rate , you can calculate it in D2 by \\nusing COUNTIFS in the numerator to count true negatives, and COUNTIF in the denominator \\nto count total customers who aren’t pregnant:\\n=COUNTIFS(‘Test Set’!$V$2:$V$1001,”<” & B2,\\n‘Test Set’!$U$2:$U$1001,”=0”)/COUNTIF(‘Test Set’!$U$2:$U$1001,”=0”) \\nCopying this calculation down through the other cutoff   values, you should see it \\nincrease (see Figure 6-23). Once a cutoff   value of 0.85 is reached, 100 percent of not \\npregnant customers in the test set are appropriately predicted.\\nFigure 6-23: Speciﬁ  city calculations on the test set\\nFalse Positive Rate\\nThe false positive rate is a common metric looked at to understand model performance. \\nAnd since you already have the true negative rate, this can quickly be calculated as one \\nminus the true negative rate. Label column E as False Positive Rate/(1 – Speciﬁ city) and \\nﬁ ll in the cells as one minus the value in the adjacent cell in D. For E2, that’s written as:\\n=1-D2'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 258, 'page_label': '237'}, page_content='237The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\\nCopying this formula down, you can see that as the cutoff  value increases, you get less \\nfalse positives. In other words, you’re committing fewer type I errors (calling customers \\npregnant who aren’t).\\nTrue Positive Rate/Recall/Sensitivity\\nThe ﬁ nal metric you can calculate on your model’s performance is call true positive rate. \\nAnd recall. And sensitivity. Geez. They should just pick one name and stick with it.\\nThe true positive rate is the ratio of correctly identiﬁ ed pregnant women divided by the \\ntotal of actual pregnant women in the test set. Label column F as True Positive Rate/Recall/\\nSensitivity. In F2 then you can calculate the true positive rate of a cutoff  value of -0.35 as:\\n=COUNTIFS(‘Test Set’!$V$2:$V$1001,”>=” & B2,\\n‘Test Set’!$U$2:$U$1001,”=1”)/COUNTIF(‘Test Set’!$U$2:$U$1001,”=1”)\\nLooking back at the true negative rate column, this calculation is exactly the same \\nexcept “<” becomes “>=” and 0s become 1s.\\nCopying this metric down, you can see that as the cutoff  increases, some of the pregnant \\nwomen cease to be identiﬁ ed as such (these are type II errors) and the true positive rate \\nfalls. Figure 6-24 shows the false and true positive rates in columns E and F.\\nFigure 6-24: The false positive rate and the true positive rate'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 259, 'page_label': '238'}, page_content='Data Smart238\\nEvaluating Metric Trade-Offs and the Receiver Operating Characteristic Curve\\nWhen choosing a threshold value for a binary classiﬁ  er, it’s important to select the best \\nbalance of these performance metrics. The higher the cutoff , the more precise the model \\nbut the lower the recall, for example. One of the most common visualizations used to \\nassess these performance trade-off s is the receiver operating characteristic (ROC) curve. \\nThe ROC curve is just a plot of the False Positive Rate versus the True Positive Rate (col-\\numns E and F in the Performance sheet). \\nWHY IS IT CALLED THE RECEIVER OPERATING CHARACTERISTIC?\\nThe reason why such a simple graph has such a complex name is that it was devel-\\noped during World War II by radar engineers rather than by marketers predicting \\nwhen customers are pregnant.\\nThese folks were using signals to detect enemies and their equipment in the battle-\\nﬁ eld, and they wanted to better visualize the trade-off  between correctly and incorrectly \\nidentifying something as a foe.\\nTo insert this graph, simply highlight the data in columns E and F and select the straight \\nlined scatter plot in Excel (see Chapter 1 for more on inserting charts and graphs). With a \\nlittle formatting (setting the axes between 0 and 1, bumping up the font), the ROC curve \\nlooks as shown in Figure 6-25.\\n0%\\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90%100%\\nTrue Positive Rate\\nFalse Positive Rate\\nROC Curve\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\n70%\\n80%\\n90%\\n100%\\nFigure 6-25: The ROC curve for the linear regression'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 260, 'page_label': '239'}, page_content='239The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\\nThis curve allows you to quickly assess the false positive rate that’s associated with a \\ntrue positive rate in order to understand your options. For example, in Figure 6-25, you \\ncan see that the model is capable of identifying 40 percent of pregnant customers using \\na cutoff  of 0.85 without a single fal se positive. Nice! \\nAnd if you were okay with occasionally sending a not pregnant household some \\npregnancy-related coupons, the model could achieve a 75 percent true positive rate with \\nonly a 9 percent false positive rate.\\nWhere you decide to set the threshold for acting on someone’s pregnancy score is a \\nbusiness decision , not purely an analytic one. If there were little downside to predicting \\nsomeone was pregnant, then a low precision might be a ﬁ ne trade-off  for a high true posi-\\ntive rate. But if you’re predicting likelihood of default for loan applications, you’re going \\nto want speciﬁ city and precision to be a bit higher, right? On the extreme end, if a model \\nlike this were being used to validate the legitimacy of overseas threats based on a body of \\nintelligence, then you’d hope that the operator of the model would want a very high level \\nof precision before calling in a drone strike.\\nSo whether we’re talking sending coupons in the mail, approving loans, or dropping \\nbombs, the balance you strike between these performance metrics is a strategic decision.\\nCOMPARING ONE MODEL TO ANOTHER\\nAs we’ll see a bit later, the ROC curve is also good for choosing one predictive model \\nover another. Ideally, the ROC curve would jump straight up to 1 on the y-axis as \\nfast as possible and stay there all the way across the graph. So the model that looks \\nmost like that (also said to have the highest area under the curve or AUC) is often \\nconsidered superior.\\nAll right! So now you’ve run the model on some test data, made some predictions, \\ncomputed its performance on the test set for diff erent cutoff  values, and visualized that \\nperformance with the ROC curve.\\nBut in order to compare model performance, you need another model to race against.\\nPredicting Pregnant Customers at RetailMart Using \\nLogistic Regression\\nIf you look at the predicted values coming out your linear regression, it’s clear that while \\nthe model is useful for classiﬁ cation, the prediction values themselves are certainly in no \\nway class probabilities. You can’t be pregnant with 125 percent probability or -35 percent \\nprobability.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 261, 'page_label': '240'}, page_content='Data Smart240\\nSo is there a model whose predictions are actually class probabilities? Once such model \\nthat we can build is called a logistic regression.\\nFirst You Need a Link Function\\nThink about the predictions currently coming out of your linear model. Is there a formula \\nyou can shove these numbers through that will make them stay between 0 and 1? It turns \\nout, this kind of function is called a link function, and there’s a great one for doing just that:\\nexp(x)/(1 + exp(x))\\nIn this formula, x is our linear combination from column W on the Linear Model tab, \\nand exp is the exponential function. The exponential function exp(x) is just the math-\\nematical constant e (2.71828…it’s like pi, but a little lower) raised to the power of x.\\nLook at a graph of the function pictured in Figure 6-26.\\n–5 –4 –3 –2 –10\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\n1234\\nAny value can go in\\nLink function for pregnant/not pregnant\\nValues between 0 and 1 come out\\n5\\nFigure 6-26: The link function\\nThis link function looks like a really wide S. It takes in any values given from multiply-\\ning the model coeffi  cients times a row of customer data, and it outputs a number between \\n0 and 1. But why does this odd function look like this?\\nWell, just round e to 2.7 real quick and think about the case where the input to this \\nfunction is pretty big, say 10. Then the link function is:\\nexp(x)/(1 + exp(x)) = 2.7^10 / (1+ 2.7^10) = 20589/20590\\nWell, that’s basically 1, so we can see that as x gets larger, that 1 in the denominator \\njust doesn’t matter much. But as x goes negative? Look at -10:\\nexp(x)/(1 + exp(x)) = 2.7^-10 / (1+ 2.7^-10) = 0.00005/1.00005'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 262, 'page_label': '241'}, page_content='241The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\\nWell, that’s just 0 for the most part. In this case the 1 in the denominator means every-\\nthing and the teeny numbers are more or less 0s.\\nIsn’t that handy? In fact, this link function has been so useful that someone gave it a \\nname along the way. It’s called the “logistic” function.\\nHooking Up the Logistic Function and Reoptimizing\\nNow create a copy of the Linear Model tab in the spreadsheet and call it Logistic Link \\nModel. Delete all of the statistical testing data from the sheet since that was primarily \\napplicable to linear regression. Speciﬁ  cally, highlight and delete rows 3 through 5, and \\nclear out all the values at the top of columns W through Z except for the Sum Squared \\nError placeholder. Also, clear out the squared error column and rename it Prediction (after \\nLink Function). See Figure 6-27 to see what the sheet should look like.\\nFigure 6-27: The initial logistic model sheet\\nYou’re going use column X to suck in the linear combination of coeffi  cients and data \\nfrom column W and put it through your logistic function. For example, the ﬁ  rst row of \\nmodeled customer data would be sent through the logistic function by putting this for-\\nmula in cell X5:\\n=EXP(W5)/(1+EXP(W5))\\nIf you copy this formula down the column, you can see that the new values are all \\nbetween 0 and 1 (see Figure 6-28).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 263, 'page_label': '242'}, page_content='Data Smart242\\nNOTE\\nYour sheet might have slightly diff erent values in columns W and X to start since the \\nmodel coeffi  cients are coming from the evolutionary algorithm run on the previous tab.\\nFigure 6-28: Values through the logistic function\\nHowever, most of the predictions appear to be middling, between 0.4 and 0.7. Well, \\nthat’s because we didn’t optimize our coeffi  cients in the “Linear Model” tab for this new \\nkind of model. We need to optimize again.\\nSo add back in a squared error column to column Y, although this time, the error cal-\\nculation will use the predictions coming out of the link function in column X:\\n=(V5-X5)^2\\nWhich you’ll again sum up just as in the linear model in cell X1 as:\\n=SUM(Y5:Y1004)\\nYou can then minimize the sum of squares in this new model using the exact same \\nSolver setup (see Figure 6-29) as in the linear model, except if you experiment with the \\nvariable bounds, you’ll ﬁ nd it’s best to broaden them a bit for a logistic model. In Figure \\n6-29, the bounds have been set to keep each coeffi  cient between -5 and 5.\\nOnce you’ve reoptimized for the new link function, you can see that your predictions \\non the training data now all fall between 0 and 1 with many predictions conﬁ dently being'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 264, 'page_label': '243'}, page_content='243The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\\ncommitted to either a 0 or a 1. As you can see in Figure 6-30, from an aesthetic perspec-\\ntive, these predictions feel nicer than those from the linear regression.\\nFigure 6-29: Identical Solver setup for logistic model\\nFigure 6-30:  Fitted logistic model'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 265, 'page_label': '244'}, page_content='Data Smart244\\nBaking an Actual Logistic Regression\\nThe truth is that in order to do an actual logistic regression that gives accurate, unbiased \\nclass probabilities, you can’t, for reasons outside the scope of this book, minimize the \\nsum of squared error.\\nInstead, you ﬁ t the model by ﬁ nding the model coeffi  cients that maximize the joint \\nprobability (see Chapter 3 for more on joint probability) of you having pulled this training \\nset from the RetailMart database given that the model accurately explains reality. \\nSo what is the likelihood of a training row given a set of logistic model parameters? For \\na given row in the training set, let p stand in for the class probability your logistic model \\nis giving in column X. Let y stand for the actual pregnancy value housed in column V. \\nThe likelihood of that training row, given the model parameters is:\\npy(1-p)(1-y)\\nFor a pregnant customer (column V is 1) with a prediction of 1 (column X has a 1 in \\nit), this likelihood calculation is, likewise, 1. But if the prediction were 0 for a pregnant \\ncustomer, then the above calculation would be 0 (plug in the numbers and check it). Thus, \\nthe likelihood of each row is maximized when the predictions and actuals all line up. \\nAssuming each row of data is independent (see Chapter 3 for more on independence) as \\nis the case in any good random pull from a database, then you can calculate the log of the \\njoint probability of the data by taking the log of each of these likelihoods and summing \\nthem up. The log of the above equation, using the same rules you saw in the ﬂ oating-point \\nunderﬂ ow section in Chapter 3, is:\\ny*ln(p)+(1-y)*ln(1-p)\\nThe log likelihood is near 0 when the previous formula is near 1 (i.e., when the model \\nﬁ ts well).\\nRather than minimize the sum of the squared error then, you can calculate this \\nlog-likelihood value on each prediction and sum them up instead. The model coeffi  cients \\nthat maximize the joint likelihood of the data will be the best ones.\\nTo start, make a copy of the Logistic Link Model tab and call it Logistic Regression. In \\ncolumn Y, change the squared error column to read Log Likelihood. In cell Y5, the ﬁ  rst \\nlog likelihood can be calculated as:\\n=IFERROR(V5*LN(X5)+(1-V5)*LN(1-X5),0)\\nThe entire log likelihood calculation is wrapped in an IFERROR formula, because when \\nthe model coeffi  cients generate a prediction very, very near the actual 0/1 class value, you \\ncan get numerical instability. In that case, it’s fair just to set the log-likelihood to a perfect \\nmatch score of 0.\\nCopy this formula down column Y, and in X1, sum the log likelihoods. Optimizing, \\nyou get a set of coeffi  cients that look similar to the sum of squares coeffi  cients with some \\nsmall shifts here and there. See Figure 6-31.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 266, 'page_label': '245'}, page_content='245The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\\nFigure 6-31: The Logistic Regression sheet\\nIf you check the sum of squared error associated with your actual logistic regression, \\nit’s nearly optimal for that metric anyway.\\nSTATISTICAL TESTS ON A LOGISTIC REGRESSION\\nAnalogous statistical concepts to the R-squared, F test, and t test are available in \\nlogistic regression. Computations such as pseudo R-squared, model deviance, and \\nthe Wald statistic lend logistic regression much of the same rigor as linear regres-\\nsion. For more information, see Applied Logistic Regression by David W. Hosmer, Jr., \\nStanley Lemeshow, and Rodney X. Sturdivant (John Wiley & Sons, 2013).\\nModel Selection—Comparing the Performance of the Linear \\nand Logistic Regressions\\nNow that you have a second model, you can run it on the test set and compare its perfor-\\nmance to that of your linear regression. Predictions using the logistic regression are made in \\nexactly the same way they were modeled in the Logistic Regression tab in columns W and X. \\nIn cell W2 on the Test Set tab, take the linear combination of model coeffi  cients and \\ntest data as:\\n=SUMPRODUCT(‘Logistic Regression’!B$2:T$2,’Test Set’!A2:S2)+\\n‘Logistic Regression’!U$2\\nIn X2, run this through the link function to get your class probability:\\n=EXP(W2)/(1+EXP(W2))\\nCopy these cells down through the test set to obtain the sheet shown in Figure 6-32.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 267, 'page_label': '246'}, page_content='Data Smart246\\nFigure 6-32: Logistic regression predictions on the test set\\nTo see how the predictions stack up, make a copy of the Performance tab and call it \\nPerformance Logistic . Changing the minimum and maximum prediction formulas to \\npoint to column X from the Test Set tab, the values come back as 0 and 1, just as you’d \\nexpect now that your model is giving actual class probabilities unlike the linear regression.\\nNOTE\\nWhile the logistic regression returns class probabilities (actual predictions between 0 \\nand 1), these probabilities are based on the 50/50 split of pregnant and not pregnant \\ncustomers in the rebalanced training set. \\nThis is ﬁ ne if all you care about is binary classiﬁ  cation at some cutoff  value rather \\nthan using the actual probabilities.  \\nChoose cutoff  values from 0 to 1 in 0.05 increments (actually, you may need to make \\n1 a 0.999 or so to keep the precision formula from dividing by 0). Everything below row \\n22 can be cleared, and the performance metrics need only be changed to check column X \\non the Test Set tab instead of V. This yields the sheet shown in Figure 6-33.\\nYou can set the ROC curve up in exactly the same way as before, however, in order \\nto compare the logistic regression to the linear regression, add in a data series for each \\nmodel’s performance metrics (right-click the chart and choose Select Data to add another \\nseries). In Figure 6-34, it’s apparent that the ROC curves for the two models are almost \\nexactly on top of each other.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 268, 'page_label': '247'}, page_content='247The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\\nFigure 6-33:  The Performance Logistic tab\\n0%\\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90%100%\\nTrue Positive Rate\\nFalse Positive Rate\\nROC Curve\\n10%\\n20%\\n30%\\n40%\\nLogistic Regression\\nLinear Regression\\n50%\\n60%\\n70%\\n80%\\n90%\\n100%\\nFigure 6-34:  The linear and logistic regression ROC curves graphed together'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 269, 'page_label': '248'}, page_content='Data Smart248\\nGiven that the models’ performances are nearly identical, you might consider using \\nthe logistic regression if for no other reason than the practicality of getting actual class \\nprobabilities bounded between 0 and 1 from the model. It’s prettier if nothing else.\\nA WORD OF CAUTION\\nYou may hear a lot about model selection out there in the real world. Folks may ask, \\n“Why didn’t you use support vector machines or neural nets or random forests or \\nboosted trees?” There are numerous types of AI models, all with their strengths and \\nweaknesses. And I would encourage you to read about them, and if in your work you \\nhappen to use an AI model, then you should try some of these models head-to-head.\\nBut. \\nTrying diff erent AI models is not the most important part of an AI modeling project. \\nIt’s the last step, the icing on the cake. This is where sites like Kaggle.com (an AI model-\\ning competition website) have it all wrong.\\nYou get more bang for your buck spending your time on selecting good data and \\nfeatures than models. For example, in the problem I outlined in this chapter, you’d be \\nbetter served testing out possible new features like “customer ceased to buy lunch meat \\nfor fear of listeriosis” and making sure your training data was perfect than you would \\nbe testing out a neural net on your old training data.\\nWhy? Because the phrase “garbage in, garbage out” has never been more applicable \\nto any ﬁ eld than AI. No AI model is a miracle worker; it can’t take terrible data and \\nmagically know how to use that data. So do your AI model a favor and give it the best \\nand most creative features you can ﬁ nd.\\nFor More Information\\nIf you just love supervised AI, and this chapter wasn’t enough for you, then let me make \\nsome reading suggestions:\\n• Data Mining with R  by Luis Torgo (Chapman & Hall/CRC, 2010) is a great next \\nstep. The book covers machine learning in the programming language, R. R is a \\nprogramming language beloved by statisticians everywhere, and it’s not hard to \\npick up for AI modeling purposes. In fact, if you were going to productionalize \\nsomething like the model in this chapter, R would be a great place to train up and \\nrun that production model.\\n• The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome \\nFriedman (Springer, 2009) takes an academic look at various AI models. At times'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 270, 'page_label': '249'}, page_content='249The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\\na slog, the book can really up your intellectual game. A free copy can be found on \\nHastie’s Stanford website.\\nFor discussion with other practitioners, I usually head to the CrossValidated forum at \\nStackExchange (stats.stackexchange.com). Oftentimes, someone has already asked your \\nquestion for you, so this forum makes for an excellent knowledge base.\\nWrapping Up\\nCongratulations! You just built a classiﬁ  cation model in a spreadsheet. Two of them \\nactually. Maybe even two and a half. And if you took me up on my median regression \\nchallenge, then you’re a beast.\\nLet’s recap some of the things we covered:\\n• Feature selection and assembling training data, including creating dummy variables \\nout of categorical predictors\\n• Training a linear regression model by minimizing the sum of squared error\\n• Calculating R-squared, showing a model is statistically signiﬁ cant using an F test, \\nand showing model coeffi  cients are individually signiﬁ cant using a t test\\n• Evaluating model performance on a holdout set at various classiﬁ cation cutoff  values \\nby calculating precision, speciﬁ city, false positive rate, and recall\\n• Graphing a ROC curve\\n• Adding a logistic link function to a general linear model and reoptimizing\\n• Maximizing likelihood in a logistic regression\\n• Comparing models with the ROC curve\\nAnd while I’ll be the ﬁ rst to admit that the data in this chapter is fabricated from whole \\ncloth, let me assure you that the power of such a logistic model is not to be scoff  ed at. \\nYou could use something like it in a production decision support or automated marketing \\nsystem for your business. \\nIf you’d like to keep going with AI, in the next chapter, I’m going to introduce a diff er-\\nent approach to AI called the ensemble model.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 271, 'page_label': '250'}, page_content=''),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 272, 'page_label': '251'}, page_content='7\\nO\\nn the American version of the popular TV show The Office, the boss, Michael Scott, \\nbuys pizza for his employees. Everyone groans when they learn that he has unfortu-\\nnately bought pizza from Pizza by Alfredo instead of Alfredo’s Pizza. Although it’s cheaper, \\napparently pizza from Pizza by Alfredo is awful.\\nIn response to their protests, Michael asks his employees a question: is it better to have \\na small amount of really good pizza or a lot of really bad pizza?\\nFor many practical artiﬁ cial intelligence implementations, the answer is arguably the \\nlatter. In the previous chapter, you built a single, good model for predicting pregnant \\nhouseholds shopping at RetailMart. What if instead, you got democratic? What if you \\nbuilt a bunch of admittedly crappy models and let them vote on whether a customer was \\npregnant? The vote tally would then be used as a single prediction.\\nThis type of approach is called ensemble  modeling, and as you’ll see, it turns simple \\nobservations into gold. \\nYou’ll be going over a type of ensemble model called bagged decision stumps, which is \\nvery close to an approach used constantly in industry called the random forest model. In \\nfact, it’s very nearly the approach I use daily in my own life here at MailChimp.com to \\npredict when a user is about to send some spam.\\nAfter bagging, you’ll investigate another awesome technique called boosting. Both of \\nthese techniques ﬁ nd creative ways to use the training data over and over and over again \\nto train up an entire ensemble of classiﬁ ers. There’s an intuitive feel to these approaches \\nthat’s reminiscent of naïve Bayes—a stupidity that, in aggregate, is smart.\\nEnsemble Models: A \\nWhole Lot of Bad Pizza'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 273, 'page_label': '252'}, page_content='Data Smart252\\nUsing the Data from Chapter 6\\nNOTE\\nThe Excel workbook used in this chapter, “Ensemble.xlsm,” is available for down-\\nload at the book’s website at www.wiley.com/go/datasmart. This workbook includes \\nall the initial data if you want to work from that. Or you can just read along using \\nthe sheets I’ve already put together in the workbook.\\nThis chapter’s gonna move quickly, because you’ll use the RetailMart data from \\nChapter 6. Using the same data will give you a sense of the diff erences in these two mod-\\nels’ implementations from the regression models in the previous chapter. The modeling \\ntechniques demonstrated in this chapter were invented more recently. They’re somewhat \\nmore intuitive, and yet, are some of the most powerful off   the shelf AI technologies we \\nhave today.\\nAlso, we’ll be building ROC curves identical to those from Chapter 6, so I won’t be \\nspending much time explaining performance metric calculations. See Chapter 6 if you \\nreally want to understand concepts like precision and recall.\\nStarting off , the workbook available for download has a sheet called TD which includes \\nthe training data from Chapter 6 with the dummy variables already set up properly (for \\nmore on this see Chapter 6). Also, the features have been numbered 0 to 18 in row 2. This \\nwill come in handy with recordkeeping later (see Figure 7-1). \\nThe workbook also includes the Test Set tab from Chapter 6.\\nFigure 7-1: The TD tab houses the data from Chapter 6.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 274, 'page_label': '253'}, page_content='253Ensemble Models: A Whole Lot of Bad Pizza \\nYou will try to do exactly what you did in Chapter 6 with this data—predict the values \\nin the PREGNANT column using the data to the left of it. Then you’ll verify the accuracy on \\nthe holdout set.\\nMISSING VALUE IMPUTATION\\nIn the RetailMart example introduced in Chapter 6 and continued here, you’re work-\\ning with a dataset that doesn’t have holes in it. For many models built off  of trans-\\nactional business data, this is often the case. But there will be situations in which \\nelements are missing from some of the rows in a dataset.\\nFor example, if you were building a recommendation AI model for a dating site and \\nyou asked users in their proﬁ le questionnaire if they listened to the symphonic heavy \\nmetal band Evanescence, you might expect that question to be left blank on occasion.\\nSo how do you train a model if some of the folks in your training set leave the \\nEvanescence question blank?\\nThere are all sorts of ways around this issue, but really quickly I’ll list some places \\nto start:\\n• Just drop the rows with missing values. If the missing values are more or less \\nrandom, losing some rows of training data isn’t going to kill you. In the dating \\nsite example, these blanks are more likely intentional than random, so dropping \\nthe rows could cause the training data to get a skewed view of reality.\\n• If the column is numeric, ﬁ  ll in the missing value with the median of those \\nrecords that have values. Filling in missing values is often called imputation. If \\nthe column is categorical, use the most common category value. Once again, in \\nthe case of ashamed Evanescence fans, the most common value is probably No, so \\nﬁ lling in with the most common value can be the wrong way to go when people \\nare censoring themselves.\\n• On top of the previous option, you can add another indicator column that has a \\n0 in it unless you had a missing value in your original column and a 1 otherwise. \\nThat way, you’ve ﬁ lled in the missing value as best you could, but you’ve told \\nthe model not to quite trust it.\\n• Instead of just using the median, you can train a model like the general linear \\nmodel presented in Chapter 6 to predict the missing value using the data from \\nthe other columns. This is a fair bit of work, but it’s worth it if you have a small \\ndataset and can’t aff ord to lose accuracy or throw away rows. \\ncontinues'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 275, 'page_label': '254'}, page_content='254 Data Smart\\nBagging: Randomize, Train, Repeat\\nBagging is a technique used to train multiple classiﬁ ers (an ensemble if you will) without \\nthem all being trained on the exact same set of training data. Because if you trained the \\nclassiﬁ ers on the same data, they’d look identical; you want a variety of models, not a \\nbunch of copies of the same model. Bagging lets you introduce some variety in a set of \\nclassiﬁ ers where there otherwise wouldn’t be.\\nDecision Stump Is an Unsexy Term for a Stupid Predictor\\nIn the bagging model you’ll be building, the individual classiﬁ ers will be decision stumps. A \\ndecision stump is nothing more than a single question you ask about the data. Depending \\non the answer, you say that the household is either pregnant or not. A simple classiﬁ  er \\nsuch as this is often called a weak learner.\\nFor example, in the training data, if you count the number of times a pregnant house-\\nhold purchased folic acid by highlighting H3:H502 and summing with the summary bar, \\nyou’d ﬁ nd that 104 pregnant households made the purchase before giving birth. On the \\nother hand, only two not-pregnant customers bought folic acid.\\nSo there’s a relationship between buying folic acid supplements and being pregnant. \\nYou can use that simple relationship to construct the following weak learner: \\nDid the household buy folic acid? If yes, then assume they’re pregnant. If no, then assume \\nthey’re not pregnant.\\n(continued)\\n• Unfortunately, this last approach (like all others mentioned in this note) feels a \\nbit overly conﬁ dent. It treats the imputed data point as if it’s a ﬁ rst-class citizen \\nonce it’s predicted from the regression line. To get around this, statisticians will \\noften use statistical models to generate multiple regression lines. The empty data \\nwill be ﬁ lled in multiple times using these regression models, each creating a \\nnew imputed dataset. Any analysis will be run on each of the imputed datasets \\nand any results will be combined at the end of the analysis. This is called multiple \\nimputation.\\n• Another approach worth trying is called k nearest neighbors imputation. Using \\ndistance (see Chapter 2) or affi  nity matrices (Chapter 5), calculate the k nearest \\nneighbors to an entry with missing data. Take a weighted average by distance \\n(or the most common value if you prefer) of the neighbors’ values to impute the \\nmissing data.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 276, 'page_label': '255'}, page_content='255Ensemble Models: A Whole Lot of Bad Pizza \\nThis predictor is visualized in Figure 7-2. \\nCustomer Data\\nTRUE\\nPurchased\\nFolic Acid?\\nNot\\nPregnant Pregnant\\nFALSE\\nFigure 7-2: The folic acid decision stump\\nDoesn’t Seem So Stupid to Me!\\nThe stump in Figure 7-2 divides the set of training records into two subsets. Now, you \\nmight be thinking that that decision stump makes perfect sense, and you’re right, it does. \\nBut it ain’t perfect. After all, there are nearly 400 pregnant households in the training data \\nthat didn’t buy folic acid but who would be classiﬁ ed incorrectly by the stump.\\nIt’s still better than not having a model at all, right? \\nUndoubtedly. But the question is how much better is the stump than not having a model. \\nOne way to evaluate that is through a measurement called node impurity.\\nNode impurity measures how often a chosen customer record would be incorrectly \\nlabeled as pregnant or not-pregnant if it were assigned a label randomly, according to the \\ndistribution of customers in its decision stump subset.\\nFor instance, you could start by shoving all 1,000 training records into the same subset, \\nwhich is to say, start without a model.\\nThe probability that you’ll pull a pregnant person from the heap is 50 percent. And \\nif you label them randomly according to the 50/50 distribution, you have a 50 percent \\nchance of guessing the label correctly.\\nThus, you have a 50%*50% = 25 percent chance of pulling a pregnant customer and \\nappropriately guessing they’re pregnant. Similarly, you have a 25 percent chance of pulling'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 277, 'page_label': '256'}, page_content='256 Data Smart\\na not-pregnant customer and guessing they’re not pregnant. Everything that’s not those \\ntwo cases is just some version of an incorrect guess.\\nThat means I have a 100% – 25% – 25% = 50 percent chance of incorrectly labeling a \\ncustomer. So you would say that the impurity of my single starting node is 50 percent.\\nThe folic acid stump splits this set of 1,000 cases into two groups—894 folks who didn’t \\nbuy folic acid and 106 folks who did. Each of those subsets will have its own impurity, \\nso if you average the impurities of those two subsets (adjusting for their size diff erence), \\nyou can tell how much the decision stump has improved your situation.\\nFor those 894 customers placed into the not-pregnant bucket, 44 percent of them are \\npregnant and 56 percent are not. This gives an impurity calculation of 100% – 44%^2 – \\n56%^2 = 49 percent. Not a whole lot of improvement.\\nBut for the 106 customers placed in the pregnant category, 98 percent of them are \\npregnant and 2 percent are not. This gives an impurity calculation of 100% – 98%^2 – \\n2%^2 = 4 percent. Very nice. Averaging those together, you ﬁ nd that the impurity for the \\nentire stump is 44 percent. That’s better than a coin ﬂ ip!\\nFigure 7-3 shows the impurity calculation.\\nSPLITTING A FEATURE WITH MORE THAN TWO VALUES\\nIn the RetailMart example, all the independent variables are binary. You never have \\nto decide how to split the training data when you create a decision tree—the 1s go \\none way and the 0s go the other. But what if you have a feature that has all kinds \\nof values?\\nFor example, at MailChimp one of the things we predict is whether an e-mail address \\nis alive and can receive mail. One of the metrics we use to do this is how many days have \\nelapsed since someone sent an e-mail to that address. (We send about 7 billion e-mails \\na month, so we pretty much have data on everyone ...)\\nThis feature isn’t anywhere close to being binary! So when we train a decision tree \\nthat uses this feature, how do we determine what value to split it on so that some of the \\ntraining data can go one direction and the rest the other direction?\\nIt’s actually really easy.\\nThere’s only a ﬁ nite number of values you can split on. At max, it’s one unique value \\nper record in your training set. And there’s probably some addresses in your training \\nset that have the exact same number of days since you last sent to them. \\nYou need to consider only these values. If you have four unique values to split on \\nfrom your training records (say 10 days, 20 days, 30 days, and 40 days), splitting on 35 \\nis no diff erent than splitting on 30. So you just check the impurity scores you get if you \\nchose each value to split on, and you pick the one that gives you the least impurity. Done!'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 278, 'page_label': '257'}, page_content='257Ensemble Models: A Whole Lot of Bad Pizza \\nCustomer Data\\nTRUE\\nPurchased\\nFolic Acid?\\nNot\\nPregnant Pregnant\\nFALSE\\n500 Pregnant/500 Not\\nImpurity = 100% – (50%2) – (50%2) = 50%\\n396 Pregnant/498 Not (89% of rows)\\nImpurity = 100% – (44%2) – (56%2) = 49%\\n104 Pregnant/2 Not (11% of rows)\\nImpurity = 100% – (98%2) – (2%2) = 4%\\nAverage Impurity:\\n.89*.49 + .11*.04 = 44%\\nFigure 7-3: Node impurity for the folic acid stump\\nYou Need More Power!\\nA single decision stump isn’t enough. What if you had scads of them, each trained on \\ndiff erent pieces of data and each with an impurity slightly lower than 50 percent? Then \\nyou could allow them to vote. Based on the percentage of stumps that vote pregnant, you \\ncould decide to call a customer pregnant.\\nBut you need more stumps.\\nWell, you’ve trained one on the Folic Acid column. Why not just do the same thing on \\nevery other feature?\\nYou have only 19 features, and frankly, some of those features, like whether the cus-\\ntomer’s address is an apartment, are pretty terrible. So you’d be stuck with 19 stumps of \\ndubious quality.\\nIt turns out that through bagging, you can make as many decision stumps as you like. \\nBagging will go something like this:\\n 1. First, bite a chunk out of the dataset. Common practice is to take roughly the square \\nroot of the feature count (four random columns in our case) and a random two \\nthirds of the rows.\\n 2. Build a decision stump for each of those four features you chose using only the \\nrandom two thirds of the data you picked.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 279, 'page_label': '258'}, page_content='258 Data Smart\\n 3. Out of those four stumps, single out the purest stump. Keep it. Toss everything back \\ninto the big pot and train a new stump.\\n 4. Once you have a load of stumps, grab them all, make them vote, and call them a \\nsingle model.\\nLet’s Train It\\nYou need to be able to select a random set of rows and columns from the training data. \\nAnd the easiest way to do that is to shuffl  e the rows and columns like a deck of cards and \\nthen select what you need from the top left of the table.\\nTo start, copy A2:U1002 from the TD tab into the top of a new tab called TD_BAG \\n(you won’t need the feature names, just their index values from row 2). The easiest way to \\nshuffl  e TD_BAG will be to add an extra column and an extra row next to the data ﬁ  lled \\nwith random numbers (using the \\nRAND() formula). Sorting by the random values from top \\nto bottom and left to right and then skimming the amount you want off  the upper left of \\nthe table gives you a random sample of rows and features.\\nGetting the Random Sample\\nInsert a row above the feature indexes and add the RAND() formula to row 1 (A1:S1) and \\nto column V (V3:V1002). The resulting spreadsheet then looks like Figure 7-4. Note that \\nI’ve titled column V as RANDOM.\\nFigure 7-4: Adding random numbers to the top and side of the data \\nSort the columns and rows randomly. Start with the columns, because side-to-side \\nsorting is kind of funky. To shuffl  e the columns, highlight columns A through S. Don’t \\nhighlight the PREGNANT column, because that’s not a feature; it’s the dependent variable.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 280, 'page_label': '259'}, page_content='259Ensemble Models: A Whole Lot of Bad Pizza \\nOpen the custom sort window (see Chapter 1 for a discussion on custom sorting). \\nFrom the Sort window (Figure 7-5), press the Options button and select to sort left to \\nright in order to sort the columns. Make sure Row 1, which is the row with the random \\nnumbers, is selected as the row to sort by. Also, conﬁ rm that the My List Has Headers box \\nis unchecked since you have no headers in the horizontal direction. \\nFigure 7-5: Sorting from left to right\\nPress OK. You’ll see the columns on the sheet reorder themselves. \\nNow you need to do the same thing to the rows. This time around, select the range \\nA2:V1002, including the PREGNANT column so that it remains tied to its data while \\nexcluding the random numbers at the top of the sheet. \\nAccess the Custom Sort window again, and under the Options section, select to sort \\nfrom top to bottom this time.\\nMake sure the My List Has Headers box is checked this time around, and then select \\nthe RANDOM column from the drop-down. The Sort window should look like Figure 7-6.\\nFigure 7-6: Sorting from top to bottom'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 281, 'page_label': '260'}, page_content='260 Data Smart\\nNow that you’ve sorted your training data randomly, the ﬁ rst four columns and the ﬁ rst \\n666 rows form a rectangular random sample that you can grab. Create a new tab called \\nRandomSelection. To pull out the random sample, you point the cell in A1 to the following:\\n=TD_BAG!A2\\nAnd then copy that formula through D667.\\nYou can get the PREGNANT  values next to the sample, by mapping them straight into \\ncolumn E. E1 points to cell U2 from the previous tab:\\n=TD_BAG!U2\\nJust double-click that formula to send it down the sheet. Once you complete this, you’re \\nleft with nothing but the random sample from the data (see Figure 7-7). Note that since \\nthe data is sorted randomly, you’ll likely end up with four diff erent feature columns.\\nAnd what’s cool is that if you go back to the TD_BAG tab and sort again, this sample \\nwill automatically update!\\nFigure 7-7: Four random columns and a random two-thirds of the rows\\nGetting a Decision Stump Out of the Sample\\nWhen looking at any one of these four features, there are only four things that can happen \\nbetween a single feature and the dependent PREGNANT variable:\\n• The feature can be 0 and PREGNANT can be 1.\\n• The feature can be 0 and PREGNANT can be 0.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 282, 'page_label': '261'}, page_content='261Ensemble Models: A Whole Lot of Bad Pizza \\n• The feature can be 1 and PREGNANT can be 1.\\n• The feature can be 1 and PREGNANT can be 0.\\nYou need to get a count of the number of training rows that fall into each of these cases \\nin order to build a stump on the feature similar to that pictured in Figure 7-2. To do this, \\nenumerate the four combinations of 0s and 1s in G2:H5. Set I1:L1 to equal the column \\nindexes from A1:D1.\\nThe spreadsheet then looks like Figure 7-8.\\nFigure 7-8: Four possibilities for the training data\\nOnce you’ve set up this small table, you need to ﬁ ll it in by getting counts of the train-\\ning rows whose values match the combination of predictor and pregnant values speciﬁ ed \\nto the left. For the upper-left corner of the table (the ﬁ  rst feature in my random sample \\nended up being number 15), you can count the number of training rows where feature 15 \\nis a 0 and the PREGNANT column is a 1 using the following formula:\\n=COUNTIFS(A$2:A$667,$G2,$E$2:$E$667,$H2)\\nThe COUNTIFS() formula allows you to count rows that match multiple criteria, hence \\nthe S at the end of IFS. The ﬁ rst criterion looks at the feature number 15 range (A2:A667) \\nand checks for rows that are identical to the value in G2 (0), whereas the second criterion \\nlooks at the \\nPREGNANT range (E2:E667) and checks for rows that are identical to the value \\nin H2 (1). \\nCopy this formula into the rest of the cells in the table to get counts for each case (see \\nFigure 7-9).\\nIf you were going to treat each of these features as a decision stump, which value for \\nthe feature would indicate pregnancy? It’d be the value with the highest concentration of \\npregnant customers in the sample.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 283, 'page_label': '262'}, page_content='262 Data Smart\\nSo in row 6 below the count values you can compare these two ratios. In I6 place the \\nformula:\\n=IF(I2/(I2+I3)>I4/(I4+I5),0,1)\\nFigure 7-9: Feature/response pairings for each of the features in the random sample\\nIf the ratio of pregnant customers associated with the 0 value for the feature ( I2/\\n(I2+I3)) is larger than that associated with 1 (I4/(I4+I5)), then 0 is predictive of preg-\\nnancy in this stump. Otherwise, 1 is. Copy this formula across through column L. This \\ngives the sheet shown in Figure 7-10.\\nFigure 7-10: Calculating which feature value is associated with pregnancy\\nUsing the counts in rows 2 through 5, you can calculate the impurity values for the \\nnodes of each decision stump should you choose to split on that feature.\\nLet’s insert the impurity calculations on row 8 below the case counts. Just as in Figure \\n7-3, you need to calculate an impurity value for the training cases that had a feature value \\nof 0 and average it with those that had a value of 1.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 284, 'page_label': '263'}, page_content='263Ensemble Models: A Whole Lot of Bad Pizza \\nIf you use the ﬁ rst feature (number 15 for me), 299 pregnant folks and 330 not-pregnant \\nfolks ended up in the 0 node, so the impurity is 100% – (299/629)^2 – (330/629)^2, which \\ncan be entered in the sheet in cell I8 as follows:\\n=1-(I2/(I2+I3))^2-(I3/(I2+I3))^2\\nLikewise, the impurity for the 1 node can be written as follows:\\n=1-(I4/(I4+I5))^2-(I5/(I4+I5))^2\\nThey are combined in a weighted average by multiplying each impurity times the \\nnumber of training cases in its node, summing them, and dividing by the total number \\nof training cases, 666:\\n=(I8*(I2+I3)+I9*(I4+I5))/666\\nYou can then drag these impurity calculations across all four features yielding com-\\nbined impurity values for each of the possible decision stumps, as shown in Figure 7-11.\\nFigure 7-11: Combined impurity values for four decision stumps\\nLooking over the impurity values, for my workbook (yours will likely be diff erent due \\nto the random sort), the winning feature is number 8 (looking back at the TD sheet, this \\nis Prenatal Vitamins) with an impurity of 0.450.\\nRecording the Winner\\nAll right, so prenatals won on this sample for me. You probably got a diff  erent winner, \\nwhich you should record somewhere.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 285, 'page_label': '264'}, page_content='264 Data Smart\\nLabel cells N1 and N2 as Winner and Pregnant Is. You’ll save the winning stump in \\ncolumn O. Start with saving the winning column number in cell O1. This would be the \\nvalue in I1:L1 that has the lowest impurity (in my case that’s 8). You can combine the \\nMATCH and INDEX formulas to do this lookup (see Chapter 1 for more on these formulas):\\n=INDEX(I1:L1,0,MATCH(MIN(I10:L10),I10:L10,0))\\nMATCH(MIN(I10:L10),I10:L10,0) ﬁ nds which column has the minimum impurity on \\nrow 10 and hands it to INDEX. INDEX locates the appropriate winning feature label.\\nSimilarly, in O2 you can put whether 0 or 1 is associated with pregnancy by ﬁ  nding \\nthe value on row 6 from the column with the minimum impurity:\\n=INDEX(I6:L6,0,MATCH(MIN(I10:L10),I10:L10,0))\\nThe winning decision stump and its pregnancy-associated node are then called out, as \\npictured in Figure 7-12.\\nFigure 7-12: The winner’s circle for the four decision stumps\\nShake Me Up, Judy!\\nPhew! I know that was a lot of little steps to create one stump. But now that all the formulas \\nare in place, creating the next couple hundred will be a lot easier.\\nYou can create a second one real quick. But before you do, save the stump you just \\nmade. To do that, just copy and paste the values in O1:O2 over to the right into P1:P2.\\nThen to create a new stump, ﬂ  ip back to the TD_BAG tab and shuffl  e the rows and \\ncolumns again.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 286, 'page_label': '265'}, page_content='265Ensemble Models: A Whole Lot of Bad Pizza \\nClick back on the RandomSelection tab. Voila! The winner has changed. In my case, \\nit’s folic acid, and the value associated with pregnancy is 1 (see Figure 7-13). The previous \\nstump is saved over to the right.\\nFigure 7-13: Reshufﬂ  ing the data yields a new stump.\\nTo save this second stump, right-click column P and select Insert to shift the ﬁ  rst \\nstump to the right. Then paste the new stump’s values in column P. The ensemble now \\nlooks like Figure 7-14.\\nFigure 7-14: And then there were two.\\nWell, that second one sure took less time than the ﬁ rst. So here’s the thing ...\\nLet’s say you want to shoot for 200 stumps in the ensemble model. All you have to do \\nis repeat these steps another 198 times. Not impossible, but annoying.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 287, 'page_label': '266'}, page_content='266 Data Smart\\nWhy don’t you just record a macro of yourself doing it and then play the macro back? \\nAs it turns out, this shuffl  ing operation is perfect for a macro.\\nFor those of you who have never recorded a macro, it’s nothing more than recording \\na series of repetitive button presses so you can play them back later instead of giving \\nyourself carpal tunnel syndrome.\\nSo hop on up to View ➪ Macros (Tools ➪ Macro in Mac OS) and select Record New \\nMacro.\\nPressing Record will open a window where you can name your macro something like \\nGetBaggedStump. And for convenience sake, let’s associate a shortcut key with the macro. \\nI’m on a Mac so my shortcut keys begin with Option+Cmd, and I’m going to throw in a \\nz into the shortcut box, because that’s the kind of mood I’m in today (see Figure 7-15).\\nFigure 7-15: Getting ready to record a macro\\nPress OK to get recording. Here are the steps that’ll record a full decision stump:\\n 1. Click the TD_BAG tab.\\n 2. Highlight columns A through S.\\n 3. Custom-sort the columns.\\n 4. Highlight rows 2 through 1002.\\n 5. Custom-sort the rows.\\n 6. Click over to the RandomSelection tab.\\n 7. Right-click column P and insert a new blank column.\\n 8. Select and copy the winning stump in O1:O2.\\n 9. Paste Special the values into P1:P2.\\nGo to View ➪ Macro ➪ Stop Recording (Tools ➪ Macro ➪ Stop Recording in Excel \\n2011 for Mac) to end the recording.\\nYou should now be able to generate a new decision stump with a single shortcut key \\npress to activate the macro. Hold on while I go click this thing about 198 hundred times . . .'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 288, 'page_label': '267'}, page_content='267Ensemble Models: A Whole Lot of Bad Pizza \\nEvaluating the Bagged Model\\nThat’s bagging! All you do is shuffl  e the data, grab a subset, train a simple classiﬁ  er, and \\ngo again. And once you have a bunch of classiﬁ  ers in your ensemble, you’re ready to \\nmake predictions.\\nOnce you’ve run the decision stump macro a couple hundred times, the RandomSelection \\nsheet should look like Figure 7-16 (your stumps will likely diff er).\\nFigure 7-16: The 200 decision stumps\\nPredictions on the Test Set\\nNow that you have your stumps, it’s time to send your test set data through the model. \\nCreate a copy of the Test Set tab and name it TestBag.\\nMoving over to the TestBag tab, insert two blank rows at the top of the sheet to make \\nroom for your stumps.\\nPaste the stump values from the RandomSelection tab (P1:HG2 if you’ve got 200 of them) \\nonto the TestBag tab starting in column W. This gives the sheet shown in Figure 7-17.\\nFigure 7-17: Stumps added to the TestBag tab'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 289, 'page_label': '268'}, page_content='268 Data Smart\\nYou can run each row in the Test Set through each stump. Start by running the ﬁ rst row \\nof data (row 4) through the ﬁ rst stump in column W. You can use the OFFSET formula \\nto look up the value from the stump column listed in W1, and if that value equals the \\none in W2, then the stump predicts a pregnant customer. Otherwise, the stump predicts \\nnon-pregnancy. The formula looks like this:\\n=IF(OFFSET($A4,0,W$1)=W$2,1,0)\\nThis formula can be copied across all stumps and down the sheet (note the absolute \\nreferences). This gives the sheet shown in Figure 7-18.\\nFigure 7-18: Stumps evaluated on the TestBag set\\nIn column V, take the average of the rows to the left in order to obtain a class probability \\nfor pregnancy. For example, in V4 if you have 200 stumps, you’d use:\\n=AVERAGE(W4:HN4) \\nCopy this down column V to get predictions for each row in the test set as shown in \\nFigure 7-19.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 290, 'page_label': '269'}, page_content='269Ensemble Models: A Whole Lot of Bad Pizza \\nFigure 7-19: Predictions for each row\\nPerformance\\nYou can evaluate these predictions using the same performance measures used in \\nChapter 6. I won’t dwell on these calculations since the technique is exactly the same as \\nthat in Chapter 6. First, create a new tab called PerformanceBag. In the ﬁ rst column, just \\nas in Chapter 6, calculate the maximum and minimum predictions. For my 200 stumps, \\nthat range comes out to 0.02 to 0.75.\\nIn column B, place a range of cutoff  values from the minimum to the maximum (in my \\ncase, I incremented by 0.02). Precision, speciﬁ city, false positive rate, and recall can all then \\nbe calculated in the same way as Chapter 6 (ﬂ ip back to Chapter 6 for the precise details).\\nThis gives the sheet shown in Figure 7-20. \\nNote that for a prediction cutoff  of 0.5, that is, with half of the stumps voting pregnant, \\nyou can identify 33 percent of pregnant customers with only a 1 percent false positive \\nrate (your mileage may vary due to the random nature of the algorithm). Pretty sweet for \\nsome simple stumps!\\nYou can also insert a ROC curve using the false positive rate and true positive rate \\n(columns E and F) just as you did in Chapter 6. For my 200 stumps, I got Figure 7-21.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 291, 'page_label': '270'}, page_content='270 Data Smart\\nFigure 7-20: Performance metrics for bagging\\n0%\\n0% 10%20%30%40%50% 60%70%80%90%100%\\nTrue Positive Rate\\nFalse Positive Rate\\nROC Curve\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\n70%\\n80%\\n90%\\n100%\\nFigure 7-21: The ROC Curve for Bagged Stumps'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 292, 'page_label': '271'}, page_content='271Ensemble Models: A Whole Lot of Bad Pizza \\nBeyond Performance\\nWhile this bagged stumps model is supported by industry standard packages like R’s \\nrandomForest package, it’s important to call out two diff erences between this and typical \\nrandom forest modeling settings:\\n• Vanilla random forests usually sample with replacement, meaning that the same row \\nfrom the training data can be pulled into the random sample more than once. When \\nyou sample with replacement, you can sample the same number of records as the \\nactual training set rather than limiting it to two thirds. In practice, while sampling \\nwith replacement has nicer statistical properties, if you’re working with a large \\nenough dataset, there’s virtually no diff erence between the two sampling methods.\\n• Random forests by default grow full classiﬁ cation trees rather than stumps. A full tree \\nis one where once you’ve split the data into two nodes, you pick some new features \\nto split those nodes apart, and on and on until you hit some stopping criteria. Full \\nclassiﬁ cation trees are better than stumps when there are interactions between the \\nfeatures that can be modeled.\\nMoving the conversation beyond model accuracy, here are some advantages to the \\nbagging approach:\\n• Bagging is resistant to outliers and tends not to overﬁ t the data. Overﬁ tting occurs \\nwhen the model ﬁ ts more than just the signal in your data and actually ﬁ  ts the \\nnoise as well.\\n• The training process can be parallelized since training an individual weak learner \\nis not dependent on the training of a previous weak learner.\\n• This type of model can handle tons of decision variables.\\nThe models we use at MailChimp for predicting spam and abuse are random forest \\nmodels, which we train in parallel using around 10 billion rows of raw data. That’s not \\ngoing to ﬁ t in Excel, and I sure as heck wouldn’t use a macro to do it! \\nNo, I use the R programming language with the \\nrandomForest package, which I would \\nhighly recommend learning about as a next step if you want to take one of these models \\ninto production at your organization. Indeed, the model in this chapter can be achieved \\nby the \\nrandomForest package merely by turning off  sampling with replacement and set-\\nting the maximum nodes in the decision trees to 2 (see Chapter 10).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 293, 'page_label': '272'}, page_content='272 Data Smart\\nBoosting: If You Get It Wrong, Just Boost and \\nTry Again\\nWhat was the reason behind doing bagging, again? \\nIf you trained up a bunch of decision stumps on the whole dataset over and over again, \\nthey’d be identical. By taking random selections of the dataset, you introduce some variety \\nto your stumps and end up capturing nuances in the training data that a single stump \\nnever could.\\nWell, what bagging does with random selections, boosting does with weights. Boosting \\ndoesn’t take random portions of the dataset. It uses the whole dataset on each training \\niteration. Instead, with each iteration, boosting focuses on training a decision stump that \\nresolves some of the sins committed by the previous decision stumps. It works like this:\\n• At ﬁ rst, each row of training data counts exactly the same. They all have the same \\nweight. In your case, you have 1000 rows of training data, so they all start with a \\nweight of 0.001. This means the weights sum up to 1.\\n• Evaluate each feature on the entire dataset to pick the best decision stump. Except \\nwhen it comes to boosting instead of bagging, the winning stump will be the one that \\nhas the lowest weighted error. Each wrong prediction for a possible stump is given a \\npenalty equal to that row’s weight. The sum of those penalties is the weighted error. \\nChoose the decision stump that gives the lowest weighted error.\\n• The weights are adjusted. If the chosen decision stump accurately predicts a row, \\nthen that row’s weight decreases. If the chosen decision stump messes up on a row, \\nthen that row’s weight increases.\\n• A new stump is trained using these new weights. In this way, as the algorithm rolls \\non, it concentrates more on the rows in the training data that previous stumps \\nhaven’t gotten right. Stumps are trained until the weighted error exceeds a threshold.\\nSome of this may seem a bit vague, but the process will become abundantly clear in a \\nspreadsheet. Off  to the data!\\nTraining the Model—Every Feature Gets a Shot\\nIn boosting, each feature is a possible stump on every iteration. You won’t be selecting \\nfrom four features this time.\\nTo start, create a tab called BoostStumps. And on it, paste the possible feature/response \\nvalue combinations from G1:H5 of the RandomSelection tab.\\nNext to those values, paste the feature index values (0–18) in row 1. This gives the \\nsheet shown in Figure 7-22.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 294, 'page_label': '273'}, page_content='273Ensemble Models: A Whole Lot of Bad Pizza \\nFigure 7-22: The initial portions of the BoostStumps tab\\nBelow each index, just as in the bagging process, you must sum up the number of train-\\ning set rows that fall into each of the four combinations of feature value and independent \\nvariable value listed in columns A and B.\\nStart in cell C2 (feature index 0) by summing the number of training rows that \\nhave a 0 for the feature value and also are pregnant. This can be counted using the \\nCOUNTIFS formula:\\n=COUNTIFS(TD!A$3:A$1002,$A2,TD!$U$3:$U$1002,$B2)\\nThe use of absolute references allows you to copy this formula through U5. This gives \\nthe sheet shown in Figure 7-23.\\nFigure 7-23: Counting up how each feature splits the training data\\nAnd just as in the case of bagging, in C6 you can ﬁ  nd the value associated with preg-\\nnancy for feature index 0 by looking at the pregnancy ratios associated with a feature \\nvalue of 0 and a feature value of 1:\\n=IF(C2/(C2+C3)>C4/(C4+C5),0,1)\\nThis too may be copied through column U.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 295, 'page_label': '274'}, page_content='274 Data Smart\\nNow, in column B enter in the weights for each data point. Begin in B9 with the label \\nCurrent Weights, and below that through B1009 put in a 0.001 for each of the thousand \\ntraining rows. Across row 9, paste the feature names from the TD sheet, just to keep track \\nof each feature.\\nThis gives the sheet shown in Figure 7-24.\\nFor each of these possible decision stumps, you need to calculate its weighted error \\nrate. This is done by locating the training rows that are miscategorized and penalizing \\neach according to its weight.\\nFor instance in C10, you can look back at the ﬁ rst training row’s data for feature index 0 \\n(A3 on the TD tab), and if it matches the pregnancy indicator in C6, then you get a penalty \\n(the weight in cell B10) if the row is not pregnant. If the feature value does not match C6, \\nthen you get a penalty if the row is pregnant. This gives the following two \\nIF statements:\\n=IF(AND(TD!A3=C$6,TD!$U3=0),$B10,0)+IF(AND(TD!A3<>C$6,TD!$U3=1),$B10,0)\\nThe absolute references allow you to copy this formula through U1009. The weighted \\nerror for each possible decision stump may then be calculated in row 7. For cell C7 the \\ncalculation of the weighted error is:\\n=SUM(C10:C1009)\\nFigure 7-24: Weights for each training data row'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 296, 'page_label': '275'}, page_content='275Ensemble Models: A Whole Lot of Bad Pizza \\nCopy this across row 7 to get the weighted error of each decision stump (see \\nFigure 7-25).\\nFigure 7-25: The weighted error calculation for each stump\\nTallying Up the Winner\\nLabel cell W1 as the Winning Error, and in X1, ﬁ nd the minimum of the weighted error \\nvalues:\\n=MIN(C7:U7)\\nJust as in the bagging section, in X2 combine the INDEX and MATCH formulas to grab the \\nfeature index of the winning stump:\\n=INDEX(C1:U1,0,MATCH(X1,C7:U7,0))\\nAnd in X3, you can likewise grab the value associated with pregnancy for the stump \\nusing INDEX and MATCH:\\n=INDEX(C6:U6,0,MATCH(X1,C7:U7,0))'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 297, 'page_label': '276'}, page_content='276 Data Smart\\nThis gives the sheet shown in Figure 7-26. Starting with equal weights for each data \\npoint, feature index 5 with a value of 0 indicating pregnancy is chosen as the top stump. \\nFlipping back to the TD tab, you can see that this is the Birth Control feature.\\nFigure 7-26: The ﬁ rst winning boosted stump\\nCalculating the Alpha Value for the Stump\\nBoosting works by giving weight to training rows that were misclassiﬁ  ed by previous \\nstumps. Stumps at the beginning of the boosting process are then more generally eff ective, \\nwhile the stumps at the end of the training process are more specialized—the weights \\nhave been altered to concentrate on a few annoying points in the training data.\\nThese stumps with specialized weights help ﬁ t the model to the strange points in the \\ndataset. However in doing so, their weighted error will be larger than that of the initial \\nstumps in the boosting process. As their weighted error rises, the overall improvement \\nthey contribute to the model falls. In boosting, this relationship is quantiﬁ ed with a value \\ncalled alpha:\\nalpha = 0.5 * ln((1 – total weighted error for the stump)/total weighted error for the stump)\\nAs the total weighted error of a stump climbs, the fraction inside the natural log func-\\ntion grows smaller and closer to 1. Since the natural log of 1 is 0, the alpha value gets \\ntinier and tinier. Take a look at it in the context of the sheet.\\nLabel cell W4 as Alpha and in X4 send the weighted error from call X1 through the \\nalpha calculation:\\n=0.5*LN((1-X1)/X1)'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 298, 'page_label': '277'}, page_content='277Ensemble Models: A Whole Lot of Bad Pizza \\nFor this ﬁ rst stump, you end up with an alpha value of 0.207 (see Figure 7-27). \\nFigure 7-27: Alpha value for the ﬁ  rst boosting iteration\\nHow exactly are these alpha values used? In bagging, each stump gave a 0/1 vote when \\npredicting. When it comes time to predict with your boosted stumps, each classiﬁ er will \\ninstead give alpha if it thinks the row is pregnant and –alpha if not. So for this ﬁ rst stump, \\nwhen used on the test set, it would give 0.207 points to any customer who had not bought \\nbirth control and -0.207 points to any customer who had. The ﬁ  nal prediction of the \\nensemble model is the sum of all these positive and negative alpha values.\\nAs you’ll see later on, to determine the overall pregnancy prediction coming from \\nthe model, a cutoff  is set for the sum of the individual stump scores. Since each stump \\nreturns either a positive or negative alpha value for its contribution to the prediction, it \\nis customary to use 0 as the classiﬁ  cation threshold for pregnancy, however this can be \\ntuned to suit your precision needs.\\nReweighting\\nNow that you’ve completed one stump, it’s time to reweight the training data. And to \\ndo that, you need to know which rows of data this stump gets right and which rows it \\ngets wrong. \\nSo in column V label V9 as Wrong. In V10, you can use the \\nOFFSET formula in combi-\\nnation with the winning stump’s column index (cell X2) to look up the weighted error \\nfor the training row. If the error is nonzero, then the stump is incorrect for that row, and \\nWrong is set to 1:\\n=IF(OFFSET($C10,0,$X$2)>0,1,0)'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 299, 'page_label': '278'}, page_content='278 Data Smart\\nThis formula can be copied down to all training rows (note the absolute references).\\nNow, the original weights for this stump are in column B. To adjust the weights accord-\\ning to which rows are set to 1 in the Wrong column, boosting multiplies the original weight \\ntimes exp(alpha * Wrong) (where exp is the exponential function you encountered when \\ndoing logistic regression in Chapter 6). \\nIf the value in the Wrong column is 0, then exp(alpha * Wrong) becomes 1, and the \\nweight stays put. \\nIf Wrong is set to 1, then exp(alpha * Wrong) is a value larger than 1, so the entire \\nweight is scaled up. Label column W as Scale by Alpha, and in W10, you can calculate \\nthis new weight as:\\n=$B10*EXP($V10*$X$4)\\nCopy this down through the dataset.\\nUnfortunately, these new weights don’t sum up to one like your old weights. They need \\nto be normalized (adjusted so that they sum to one). So label X9 as Normalize and in X10, \\ndivide the new, scaled weight by the sum of all the new weights:\\n=W10/SUM(W$10:W$1009)\\nThis ensures that your new weights sum to one. Copy the formula down. This gives \\nthe sheet shown in Figure 7-28.\\nFigure 7-28: The new weight calculation'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 300, 'page_label': '279'}, page_content='279Ensemble Models: A Whole Lot of Bad Pizza \\nDo That Again... and Again...\\nNow you’re ready to build a second stump. First, copy the winning stump data from the \\nprevious iteration over from X1:X4 to Y1:Y4.\\nNext, copy the new weight values from column X over to column B. The entire sheet \\nwill update to select the stump that’s best for the new set of weights. As shown in \\nFigure 7-29, the second winning stump is index 7 (Folic Acid) where a 1 indicates \\npregnancy.\\nYou can train 200 of these stumps in much the same way as you did in the bagging \\nprocess. Simply record a macro that inserts a new column Y, copies the values from X1:X4 \\ninto Y1:Y4, and pastes the weights over from column X to column B. \\nAfter 200 iterations, your weighted error rate will have climbed very near to 0.5 while \\nyour alpha value will have fallen to 0.005 (see Figure 7-30). Consider that your ﬁ rst stump \\nhad an alpha value of 0.2. That means that these ﬁ nal stumps are 40 times less powerful \\nin the voting process than your ﬁ rst stump.\\nFigure 7-29: The second stump'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 301, 'page_label': '280'}, page_content='280 Data Smart\\nFigure 7-30: The 200 th stump\\nEvaluating the Boosted Model\\nThat’s it! You’ve now trained an entire boosted decision stumps model. You can compare \\nit to the bagged model by looking at its performance metrics. To make that happen, you \\nmust ﬁ rst make predictions using the model on the test set data.\\nPredictions on the Test Set\\nFirst make a copy of the Test Set called TestBoost and insert four blank rows at the top \\nof it to make room for your winning decision stumps. Beginning in column W on the \\nTestBoost tab, paste your stumps (all 200 in my case) at the top of the sheet. This gives \\nthe sheet shown in Figure 7-31.\\nFigure 7-31: Decision stumps pasted to TestBoost'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 302, 'page_label': '281'}, page_content='281Ensemble Models: A Whole Lot of Bad Pizza \\nIn W6, you can then evaluate the ﬁ rst stump on the ﬁ rst row of test data using OFFSET \\njust as you did with the bagged model. Except this time, a pregnancy prediction returns \\nthe stump’s alpha value (cell W4) and a non-pregnancy prediction returns –alpha:\\n=IF(OFFSET($A6,0,W$2)=W$3,W$4,-W$4)\\nCopy this formula across to all the stumps and down through all the test rows (see \\nFigure 7-32). To make a prediction for a row, you sum these values across all its individual \\nstump predictions.\\nFigure 7-32: Predictions on each row of test data from each stump\\nLabel V5 as Score. The score then for V6 is just the sum of the predictions to the right:\\n=SUM(W6:HN6)\\nCopy this sum down. You get the sheet shown in Figure 7-33. A score in column V \\nabove 0 means that more alpha-weighted predictions went in the pregnant direction than \\nin the not pregnant direction (see Figure 7-33).\\nCalculating Performance\\nTo measure the performance of the boosted model on the test set, simply create a copy of \\nthe PerformanceBag tab called PerformanceBoost, point the formulas at column V on the \\nTestBoost tab, and set the cutoff  values to range from the minimum score to the maxi-\\nmum score produced by the boosted model. In my case, I incremented the cutoff   values \\nby 0.25 between a minimum prediction score of -8 and a maximum of 4.5. This gives the \\nperformance tab shown in Figure 7-34.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 303, 'page_label': '282'}, page_content='282 Data Smart\\nFigure 7-33: Final predictions from the boosted model\\nWith this model, you can see that a score cutoff   of 0 produces a true positive rate \\n85 percent with only a 27 percent false positive rate. Not bad for 200 stupid stumps.\\nAdd the boosted model’s ROC curve to the bagged model’s ROC curve to compare the \\ntwo just as you did in Chapter 6. As seen in Figure 7-35, at 200 stumps each, the boosted \\nmodel outperforms the bagged model for many points on the graph.\\nFigure 7-34: The performance metrics for boosted stumps'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 304, 'page_label': '283'}, page_content='283Ensemble Models: A Whole Lot of Bad Pizza \\n0%\\n0% 10% 20% 30% 40% 50% 60% 70% 80% 90%100%\\nTrue Positive Rate\\nFalse Positive Rate\\nROC Curve\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\n70%\\nBoosted\\nBagged80%\\n90%\\n100%\\nFigure 7-35: The ROC curves for the boosted and bagged models\\nBeyond Performance\\nIn general, boosting requires fewer trees than bagging to produce a good model. It’s not \\nas popular in practice as bagging, because there is a slightly higher risk of overﬁ tting the \\ndata. Since each reweighting of the training data is based on the misclassiﬁ  ed points in \\nthe previous iteration, you can end up in a situation where you’re training classiﬁ  ers to \\nbe overly-sensitive to a few noisy points in the data.\\nAlso, the iterative reweighting of the data means that boosting, unlike bagging, cannot \\nbe parallelized across multiple computers or CPU cores.\\nThat said, in a neck and neck contest between a well ﬁ  t boosted model and a well ﬁ t \\nbagged model, it’s hard for the bagged model to win.\\nWrapping Up\\nYou’ve just seen how a bunch of simple models can be combined via bagging or boosting \\nto form an ensemble model. These approaches were unheard of until about the mid-1990s, \\nbut today, they stand as two of the most popular modeling techniques used in business.\\nAnd you can boost or bag any model that you want to use as a weak learner. These \\nmodels don’t have to be decision stumps or trees. For example, there’s been a lot of talk \\nrecently about boosting naïve Bayes models like the one you encountered in Chapter 3.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 305, 'page_label': '284'}, page_content='284 Data Smart\\nIn Chapter 10, you’ll implement some of what you’ve encountered in this chapter using \\nthe R programming language.\\nIf you’d like to learn more about these algorithms, I’d recommend reading about them \\nin The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome \\nFriedman (Springer, 2 009).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 306, 'page_label': '285'}, page_content='8\\nA\\ns you saw in Chapters 3, 6 and 7, supervised machine learning is about predicting a \\nvalue or classifying an observation using a model trained on past data. Forecasting \\nis similar. Sure, you can forecast without data (astrology, anyone?). But in quantitative \\nforecasting, past data is used to predict a future outcome. Indeed, some of the same tech-\\nniques, such as multiple regression (introduced in Chapter 6), are used in both disciplines.\\nBut where forecasting and supervised machine learning diff er greatly is in their canoni-\\ncal problem spaces. Typical forecasting problems are about taking some data point over \\ntime (sales, demand, supply, GDP, carbon emissions, or population, for example) and \\nprojecting that data into the future. And in the presence of trends, cycles, and the occa-\\nsional act of God, the future data can be wildly outside the bounds of the observed past.\\nAnd that’s the problem with forecasting: unlike in Chapters 6 and 7 where pregnant \\nwomen more or less keep buying the same stuff , forecasting is used in contexts where the \\nfuture often looks nothing like the past. \\nJust when you think you have a good projection for housing demand, the housing bubble \\nbursts and your forecast is in the toilet. Just when you think you have a good demand \\nforecast, a ﬂ ood disrupts your supply chain, limiting your supply, forcing you to raise \\nprices, and throwing your sales completely out of whack. Future time series data can and \\nwill look diff erent than the data you’ve observed before.\\nThe only guarantee with forecasting is that your forecast is wrong. You hear that a lot in \\nthe world of forecasting. But that doesn’t mean you don’t try. When it comes to planning \\nyour business, you often need some projection. At MailChimp, we might continue to grow \\nlike gangbusters, or a hole might open up under Atlanta and swallow us. But we make an \\neff ort to forecast growth as best we can so that we can plan our infrastructure and HR \\npipelines. You don’t always want to be playing catch-up.\\nAnd as you’ll see in this chapter, you can try forecasting the future, but you can also \\nquantify the uncertainty around the forecast. And quantifying the forecast uncertainty \\nby creating prediction intervals is invaluable and often ignored in the forecasting world.\\nForecasting: Breathe \\nEasy; You Can’t Win'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 307, 'page_label': '286'}, page_content='Data Smart286\\nAs one wise forecaster said, “A good forecaster is not smarter than everyone else; they \\nmerely have their ignorance better organized.” \\nSo without further ado, let’s go organize some ignorance.\\nThe Sword Trade Is Hopping\\nImagine with me that you’re a rabid Lord of the Rings fan. Years ago when the ﬁ rst of the \\nfeature ﬁ lms came out, you strapped on some prosthetic hobbit feet and waited in line for \\nhours to see the ﬁ rst midnight showing. Soon you were attending conventions and arguing \\non message boards about whether Frodo could have just ridden an eagle to Mount Doom.\\nOne day, you decided to give something back. You took a course at the local commu-\\nnity college on metalwork and began handcrafting your own swords. Your favorite sword \\nfrom the book was Anduril, the Flame of the West. You became an expert at hammering \\nout those beefy broadswords in your homemade forge, and you started selling them on \\nAmazon, eBay, and Etsy. These days, your replicas are the go-to swords for the discerning \\nnerd; business is booming.\\nIn the past, you’ve found yourself scrambling to meet demand with the materials on \\nhand. And so you’ve decided to forecast your future demand. So you dump your past sales \\ndata in a spreadsheet. But how do you take that past data and project it out?\\nThis chapter looks at a set of forecasting techniques called exponential smoothing meth-\\nods. They’re some of the simplest and most widely used techniques in business today. \\nIndeed, I know a few Fortune 500s just off   the top of my head that forecast with these \\ntechniques, because they’ve proven the most accurate for their data.\\nThis accuracy stems in part from the techniques’ simplicity—they resist over-ﬁ tting the \\noften-sparse historical data used in forecasting. Furthermore, with these techniques, it’s \\nrelatively easy to compute prediction intervals around exponential smoothing forecasts, \\nso you’re going to do a bit of that too.\\nGetting Acquainted with Time Series Data\\nNOTE\\nThe Excel workbook used in this chapter, “SwordForecasting.xlsm,” is available for \\ndownload at the book’s website at www.wiley.com/go/datasmart . This workbook \\nincludes all the initial data if you want to work from that. Or you can just read along \\nusing the sheets I’ve already put together in the workbook.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 308, 'page_label': '287'}, page_content='287Forecasting: Breathe Easy; You Can’t Win \\nThe workbook for this chapter includes the last 36 months of sword demand starting from \\nJanuary three years ago. The data is shown in the Timeseries tab in Figure 8-1. As men-\\ntioned earlier in this chapter, data like this—observations over regular time intervals—is \\ncalled time series data. The time interval can be whatever is appropriate for the problem \\nat hand, whether that’s yearly population ﬁ gures or daily gas prices.\\nFigure 8-1: Time series data\\nIn this case, you have monthly sword demand data, and the ﬁ rst thing you should do \\nwith it is plot it, as shown in Figure 8-2. To insert a plot like this, just highlight columns \\nA and B in Excel and pick Scatter from the charts section of the Excel ribbon (Charts tab \\non Mac, Insert Tab on Windows). You can adjust the range of your axes by right-clicking \\nthem and selecting the Format option. \\nSo what do you see in Figure 8-2? The data ranges from the 140s three years ago to \\n304 last month. That’s a doubling of demand in three years—so maybe there’s an upward \\ntrend? You’ll come back to this thought in a bit.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 309, 'page_label': '288'}, page_content='288 Data Smart\\nFigure 8-2: Scatter plot of time series data\\nThere are a few ups and downs that may be indicative of some seasonal pattern. For \\ninstance, months 12, 24, and 36, which are all Decembers, are the highest demand months \\nfor each of their years. But that could just be chance or due to the trend. Let’s ﬁ nd out.\\nStarting Slow with Simple Exponential Smoothing\\nExponential smoothing techniques base a future forecast off  of past data where the most \\nrecent observations are weighted more than older observations. This weighting is done \\nthrough smoothing constants . The ﬁ rst exponential smoothing method you’re going to \\ntackle is called simple exponential  smoothing (SES), and it uses only one smoothing con-\\nstant, as you’ll see.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 310, 'page_label': '289'}, page_content='289Forecasting: Breathe Easy; You Can’t Win \\nSimple exponential smoothing assumes that your time series data is made up of two \\ncomponents: a level (or mean) and some error around that level. There’s no trend, no \\nseasonality, just a level around which the demand hovers with little error jitters here \\nand there. By preferring recent observations, SES can account for shifts in this level. In \\nformula-speak then, you have:\\n  Demand  at time t = level + random error around the level at time t\\nAnd the most current estimate of the level serves as a forecast for future time periods. \\nIf you’re at month 36, what’s a good estimate of demand at time period 38? The most \\nrecent level estimate. And time 40? The level. Simple—hence the name simple exponen-\\ntial smoothing.\\nSo how do you get an estimate of the level? \\nIf you assume that all your historical values are of equal importance, you just take a straight \\naverage. \\nThis mean would give you a level, and you’d forecast the future by just saying, “Demand \\nin the future is the average of the past demand.” And there are companies that do this. I’ve \\nseen monthly forecasts at companies where future months were equal to the average of \\nthose same months over the past few years. Plus a “fudge factor” for kicks. Yes, forecast-\\ning is often done so hand-wavily that even at huge, public companies words like “fudge \\nfactor” are still used. Eek.\\nBut when the level shifts over time, you don’t want to give equal weight to each histori-\\ncal point in the way that an average does. Should 2008 through 2013 all carry the same \\nweight when forecasting 2014? Maybe, but for most businesses, probably not. So you want \\na level estimate that gives more weight to your recent demand observations.\\nSo let’s think about calculating the level, instead, by rolling over the data points in \\norder, updating the level calculation as you go. To start, say the initial estimate of the level \\nis the average of some of the earliest data points. In this case, pick the ﬁ  rst year’s worth \\nof data. Call this initial estimate of the level, level\\n0:\\n  level 0 = average of the ﬁ rst year’s demand (months 1 – 12)\\nThat’s 163 for the sword demand.\\nNow, the way exponential smoothing works is that even though you know demand for \\nmonths 1 through 36, you’re going to take your most recent forecast components and use \\nthem to forecast one month ahead through the entire series.\\nSo you use level\\n0 (163) as the forecast for demand in month 1. \\nNow that you’ve forecasted period 1, you take a step forward in time from period 0 to \\nperiod 1. The actual demand was 165, so you were off  by two swords. You should update'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 311, 'page_label': '290'}, page_content='290 Data Smart\\nthe estimate of the level then to account for this error. Simple exponential smoothing \\nuses this equation:\\n  level 1 = level0 + some percentage * (demand1 – level0)\\nNote that (demand1 - level0) is the error you get when you forecast period one with the \\ninitial level estimate. Rolling forward:\\n  level 2 = level1 + some percentage * (demand2 – level1)\\nAnd again:\\n  level 3 = level2 + some percentage * (demand3 – level2)\\nNow, the percentage of the error you want to fold back into the level is the smoothing \\nconstant, and for the level, it’s historically been called alpha. It can be any value between \\n0 and 100 percent (0 and 1). \\nIf you set alpha to 1, you’re accounting for all the error, which just means the level of \\nthe current period is the demand of the current period. \\nIf you set alpha to 0, you conduct absolutely no error correction on that ﬁ  rst level \\nestimate.\\nYou’ll likely want something in between those two extremes, but you’ll learn how to \\npick the best alpha value later.\\nSo you can roll this calculation forward through time:\\n  level current period = levelprevious period + alpha * (demandcurrent period – levelprevious period)\\nEventually you end up with a ﬁ nal level estimate, level36, where the last demand obser-\\nvations count for more because their error adjustments haven’t been multiplied by alpha \\na zillion times:\\n  level 36 = level35 + alpha * (demand36 – level35)\\nThis ﬁ nal estimate of the level is what you’ll use as the forecast of future months. The \\ndemand for month 37? Well, that’s just level36. And the demand for month 40? level36. \\nMonth 45? level36. You get the picture. The ﬁ nal level estimate is the best one you have \\nfor the future, so that’s what you use.\\nLet’s take a look at it in a spreadsheet.\\nSetting Up the Simple Exponential Smoothing Forecast\\nThe ﬁ rst thing you’ll do is create a new worksheet in the workbook called SES. Paste the \\ntime series data in columns A and B starting at row 4 to leave some room at the top of the'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 312, 'page_label': '291'}, page_content='291Forecasting: Breathe Easy; You Can’t Win \\nsheet for an alpha value. You can put the number of months you have in your data (36) \\nin cell A2, and an initial swag at the alpha value in C2. I’m going with 0.5, because it’s in \\nbetween 0 and 1, and that’s just how I roll.\\nNow, in column C, you place the level calculations. You’ll need to insert a new row 5 \\ninto the time series data at the top for the initial level estimate at time 0. In C5, use the \\nfollowing calculation:\\n=AVERAGE(B6:B17)\\nThis averages the ﬁ rst year’s worth of data to give the initial level. The spreadsheet then \\nlooks as shown in Figure 8-3.\\nFigure 8-3: Initial level estimate for simple exponential smoothing\\n Adding in the One-Step Forecast and Error\\nNow that you’ve added the ﬁ rst level value into the sheet, you can roll forward in time \\nusing the SES formula laid out in the previous section. To do this, you’ll need to add two \\ncolumns: a one-step forecast column (D) and a forecast error column (E). The one-step'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 313, 'page_label': '292'}, page_content='292 Data Smart\\nforecast for time period 1 is just level0 (cell C5), and the error calculation is then the actual \\ndemand minus the forecast:\\n=B6-D6\\nThe level estimate then for period 1 is the previous level adjusted by alpha times the \\nerror, which is:\\n=C5+C$2*E6\\nNote that I’ve placed a $ in front of the alpha value so that when you drag the formula \\ndown the sheet, the absolute row reference leaves alpha be. This yields the sheet shown \\nin Figure 8-4.\\nFigure 8-4:  Generating the one-step forecast, error, and level calculation for period 1\\nDrag That Stuff Down!\\nHumorously enough, you’re pretty much done here. Just drag C6:E6 down through all 36 \\nmonths, and voila, you have level36.\\nLet’s add months 37–48 to column A. The forecast for these next 12 months is just \\nlevel36. So in B42, you can just add: \\n=C$41\\nas the forecast and drag it down for the next year.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 314, 'page_label': '293'}, page_content='293Forecasting: Breathe Easy; You Can’t Win \\nThis gives you a forecast of 272, as shown in Figure 8-5. \\nFigure 8-5: Simple exponential smoothing forecast with alpha of 0.5\\nBut is that the best you can do? Well, the way you optimize this forecast is by setting \\nalpha—the larger alpha is, the less you care about the old demand points.\\nOptimizing for One-Step Error\\nSimilar to how you minimized the sum of squared error when ﬁ  tting the regression in \\nChapter 6, you can ﬁ nd the best smoothing constant for the forecast by minimizing the \\nsum of the squared error for the one-step ahead forecasts.\\nLet’s add a squared error calculation into column F that’s just the value from column \\nE squared, drag that calculation through all 36 months, and sum it in cell E2 as the sum \\nof squared error (SSE). This yields the sheet shown in Figure 8-6.\\nAlso, you’re going to add the standard error to the spreadsheet in cell F2. The standard \\nerror is just the square root of the SSE divided by 35 (36 months minus the number of \\nsmoothing parameters in the model, which for simple exponential smoothing is 1).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 315, 'page_label': '294'}, page_content='294 Data Smart\\nFigure 8-6:  The sum of squared error for simple exponential smoothing\\nThe standard error is an estimate of the standard deviation of the one-step ahead error. \\nYou saw the standard deviation ﬁ rst in Chapter 4. It’s just a measure of the spread of the \\nerror.\\nIf you have a nicely ﬁ tting forecast model, its error will have a mean of 0. This is to \\nsay the forecast is unbiased. It over-estimates demand as often as it underestimates. The \\nstandard error quantiﬁ es the spread around 0 when the forecast is unbiased.\\nSo in cell F2, you can calculate the standard error as:\\n=SQRT(E2/(36-1))\\nFor an alpha value of 0.5, it comes out to 20.94 (see Figure 8-7). And if you’ll recall the \\n68-95-99.7 rule from the normal distribution discussed in Chapter 4, this is saying that 68 \\npercent of the one-step forecast errors should be less than 20.94 and greater than -20.94.\\nNow, what you want to do is shrink that spread down as low as you can by ﬁ nding the \\nappropriate alpha value. You could just try a bunch of diff erent values of alpha. But you’re \\ngoing to use Solver for the umpteenth time in this book.\\nThe Solver setup for this is super easy. Just open Solver, set the objective to the standard \\nerror in F2, set the decision variable to alpha in C2, add a constraint that C2 be less than \\n1, and check the box that the decision be non-negative. The recursive level calculations \\nthat go into making each forecast error are highly non-linear, so you’ll need to use the \\nevolutionary algorithm to optimize alpha.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 316, 'page_label': '295'}, page_content='295Forecasting: Breathe Easy; You Can’t Win \\nFigure 8-7: The standard error calculation\\nThe Solver formulation should look like what’s shown in Figure 8-8. Pressing Solve, \\nyou get an alpha value of 0.73, which gives a new standard error of 20.39. Not a ton of \\nimprovement.\\nFigure 8-8:  Solver formulation for optimizing alpha'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 317, 'page_label': '296'}, page_content='296 Data Smart\\nLet’s Graph It \\nThe best way to “gut check” a forecast is to graph it alongside your historical demand \\nand see how the predicted demand takes off  from the past. You can select the historical \\ndemand data and the forecast and plot them. I like the look of Excel’s straight-lined scat-\\nter. To start, select A6:B41, which is just the historical data, and choose the straight-line \\nscatter plot from Excel’s chart section.\\nOnce you’ve added that chart, right-click the center of the chart, choose Select Data, \\nand add a new series to the chart with just the forecasted values of A42:B53. You can also \\nadd some labels to the axes if you like, after which you should have something similar \\nto Figure 8-9.\\nFigure 8-9: Graphing the ﬁ  nal simple exponential smoothing forecast\\nYou Might Have a Trend\\nJust looking at that graph, a few things stand out. First, simple exponential smoothing is \\njust a ﬂ at line—the level. But when you look at the demand data from the past 36 months, \\nit’s on the rise. There appears to be a trend upward, especially at the end.\\nNot to denigrate the human eyeball, but how do you prove it?\\nYou prove it by ﬁ tting a linear regression to the demand data and performing a t test \\non the slope of that trendline, just as you did in Chapter 6.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 318, 'page_label': '297'}, page_content='297Forecasting: Breathe Easy; You Can’t Win \\nIf the slope of the line is nonzero and statistically signiﬁ  cant (has a p value less than \\n0.05 in the t test), you can be conﬁ  dent that the data has a trend. If that last sentence \\nmakes absolutely no sense to you, check out the statistical testing section in Chapter 6.\\nFlip back to the Timeseries tab in the workbook to perform the trend test.\\nNow, in Chapter 6 you proved your mettle by performing both an F test and a t test by \\nhand. No one wants to subject you to that again.\\nIn this chapter, you’ll use Excel’s built-in \\nLINEST function to ﬁ t a linear regression, pull \\nthe slope, standard error of the slope coeffi  cient, and degrees of freedom (see Chapter 6 \\nto understand these terms). Then you can calculate your t statistic and run it through the \\nTDIST function just as in Chapter 6. \\nIf you’ve never used \\nLINEST before, Excel’s help documentation on the function is very \\ngood. You provide LINEST with the dependent variable data (demand in column B) and \\nthe independent variable data (you only have one independent variable and it’s time in \\ncolumn A). \\nYou also have to provide a ﬂ ag of \\nTRUE to let the function know to ﬁ t an intercept as part \\nof the regression line, and you have to provide a second ﬂ ag of TRUE to get back detailed \\nstats like standard error and R-squared. For the Timeseries tab data then, a linear regres-\\nsion can be run as:\\n=LINEST(B2:B37,A2:A37,TRUE,TRUE)\\nThis call will only return the slope of the regression line however, because LINEST  \\nis an array formula. LINEST returns back all the regression stats in an array, so you can \\neither run LINEST as an array formula to dump everything out into a selected range in a \\nsheet, or you can run LINEST through the INDEX formula and pull off  just the values you \\ncare about one by one.\\nFor instance, the ﬁ rst components of a regression line that LINEST gives are the regres-\\nsion coeffi  cients, so you can pull the slope for the regression in cell B39 on the Timeseries \\ntab by feeding LINEST through INDEX:\\n=INDEX(LINEST(B2:B37,A2:A37,TRUE,TRUE),1,1)\\nYou get back a slope of 2.54, meaning the regression line is showing an upward trend \\nof 2.54 additional demanded swords per month. So there is a slope. But is it statistically \\nsigniﬁ cant?\\nTo run a t test on the slope, you need to pull the standard error for the slope and the \\ndegrees of freedom for the regression. \\nLINEST parks the standard error value in row 2, \\ncolumn 1 of its array of results. So in B40, you can pull it as:\\n=INDEX(LINEST(B2:B37,A2:A37,TRUE,TRUE),2,1)'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 319, 'page_label': '298'}, page_content='298 Data Smart\\nThe only change from pulling the slope is that in the INDEX formula you pull row 2, \\ncolumn 1 for the standard error instead of row 1 column 1 for the slope. \\nThe standard error of the slope is given as 0.34. This gives the sheet shown in \\nFigure 8-10.\\nFigure 8-10:  The slope and standard error for a regression line ﬁ  tted to the historical demand\\nSimilarly, Excel’s LINEST documentation notes that degrees of freedom for the regres-\\nsion are returned at the fourth row and second column value in the result array. So in B41 \\nyou can pull it as follows:\\n=INDEX(LINEST(B2:B37,A2:A37,TRUE,TRUE),4,2)\\nYou should get 34 for the degrees of freedom (as noted in Chapter 6, this is calculated \\nas 36 data points minus 2 coeffi  cients from the linear regression).\\nYou now have the three values you need to perform a t test on the statistical signiﬁ  -\\ncance of your ﬁ tted trend. Just as in Chapter 6, you can calculate the test statistic as the \\nabsolute value of the slope divided by the standard error for the slope. You can pull the p \\nvalue for this statistic from the t distribution with 34 degrees of freedom using the TDIST \\nfunction in B42:\\n=TDIST(ABS(B39/B40),B41,2)\\nThis returns a p value near 0 implying that if the trend were nonexistent in reality \\n(slope of 0), there’s no chance we would have gotten a slope so extreme from our regres-\\nsion. This is shown in Figure 8-11.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 320, 'page_label': '299'}, page_content='299Forecasting: Breathe Easy; You Can’t Win \\nFigure 8-11: Your trend is legit\\nAll right! So you have a trend. Now you just need to incorporate it into your forecast.\\n Holt’s Trend-Corrected Exponential Smoothing\\nHolt’s Trend-Corrected Exponential  Smoothing expands simple exponential smoothing to \\ncreate a forecast from data that has a linear trend. It’s often called double exponential  \\nsmoothing, because unlike SES, which has one smoothing parameter alpha and one non-\\nerror component, double exponential smoothing has two. \\nIf the time series has a linear trend, you can write it as:\\n  Demand  at time t = level + t*trend + random error around the level at time t\\nThe most current estimates of the level and trend (times the number of periods out) \\nserve as a forecast for future time periods. If you’re at month 36, what’s a good estimate of \\ndemand at time period 38? The most recent level estimate plus two months of the trend. \\nAnd time 40? The level plus four months of the trend. Not as simple as SES but pretty close.\\nNow, just as in simple exponential smoothing, you need to get some initial estimates \\nof the level and trend values, called level\\n0 and trend0. One common way to get them is \\njust to plot the ﬁ rst half of your demand data and send a trendline through it (just like \\nyou did in Chapter 6 in the cat allergy example). The slope of the line is trend0 and the \\ny-intercept is level0.\\nHolt’s Trend-Corrected Smoothing has two update equations, one for the level as you roll \\nthrough time and one for the trend. The level equation still uses a smoothing parameter'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 321, 'page_label': '300'}, page_content='300 Data Smart\\ncalled alpha, whereas the trend equation uses a parameter often called gamma. They’re \\nexactly the same—just values between 0 and 1 that regulate how much one-step forecast-\\ning error is incorporated back into the estimates.\\nSo, here’s the new level update equation:\\n  level\\n1 = level0 + trend0 + alpha *  (demand1 – (level0 + trend0) )\\nNote that (level0 + trend0) is just the one-step ahead forecast from the initial values to \\nmonth 1, so (demand1 – (level0 + trend0) ) is the one-step ahead error. This equation looks \\nidentical to the level equation from SES except you account for one time period’s worth \\nof trend whenever you count forward a slot. Thus, the general equation for the level esti-\\nmate is:\\n  level\\ncurrent  period  = levelprevious  period  + trendprevious  period  + alpha * ( demand current  period  – \\n(levelprevious period + trendprevious period) )\\nUnder this new smoothing technique, you also need a trend update equation. For the \\nﬁ rst time slot it’s:\\n  trend 1 = trend0 +gamma * alpha * (demand1 – (level0 + trend0) )\\nSo the trend equation is similar to the level update equation. You take the previous \\ntrend estimate and adjust it by gamma times the amount of error incorporated into the \\naccompanying level update (which makes intuitive sense because only some of the error \\nyou’re using to adjust the level would be attributable to poor or shifting trend estimation).\\nThus, the general equation for the trend estimate is:\\n  trend\\ncurrent  period  = trend previous  period  + gamma  * alpha  * ( demand current  period  – \\n(levelprevious period + trendprevious period) )\\nSetting Up Holt’s Trend-Corrected Smoothing in a Spreadsheet\\nTo start, create a new tab called Holt’sTrend-Corrected. On this tab, just as with the simple \\nexponential smoothing tab, paste the time series data on row 4 and insert an empty row \\n5 for the initial estimates.\\nColumn C will once again contain the level estimates, and you’ll put the trend estimates \\nin column D. So at the top of those two columns you’ll put the alpha and gamma values. \\nYou’re going to be optimizing them with Solver in a second, but for now, just toss in some \\n0.5s. This gives the sheet shown in Figure 8-12.\\nFor the initial values of level and trend that go in C5 and D5, let’s scatter plot the ﬁ rst \\n18 months of data and add a trendline to it with the equation (if you don’t know how to \\nadd a trendline to a scatterplot, see Chapter 6 for an example). This gives an initial trend \\nof 0.8369 and an initial level (intercept of the trendline) of 155.88.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 322, 'page_label': '301'}, page_content='301Forecasting: Breathe Easy; You Can’t Win \\nFigure 8-12: Starting with smoothing parameters set to 0.5\\nAdding these to D5 and C5 respectively, you get the sheet shown in Figure 8-13.\\nFigure 8-13: The initial level and trend values'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 323, 'page_label': '302'}, page_content='302 Data Smart\\nNow in Columns E and F, add the one-step ahead forecast and forecast error columns. \\nIf you look at row 6, the one-step ahead forecast is merely the previous level plus one \\nmonth’s trend using the previous estimate—that’s C5+D5. And the forecast error is the \\nsame as in simple exponential smoothing; F6 is just actual demand minus the one-step \\nforecast—B6-E6.\\nYou can then update the level in cell C6 as the previous level plus the previous trend \\nplus alpha times the error:\\n=C5+D5+C$2*F6\\nThe trend in D6 is updated as the previous trend plus gamma times alpha times the \\nerror:\\n=D5+D$2*C$2*F6\\nNote that you need to use absolute references on both alpha and gamma in order to \\ndrag the formulas down. You’ll do that now—drag C6:F6 down through month 36. This \\nis shown in Figure 8-14.\\nFigure 8-14: Dragging down the level, trend, forecast, and error calculations'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 324, 'page_label': '303'}, page_content='303Forecasting: Breathe Easy; You Can’t Win \\nForecasting Future Periods\\nTo forecast out from month 36, you add the ﬁ nal level (which for an alpha and gamma of \\n0.5 is 281) to the number of months out you’re forecasting times the ﬁ nal trend estimate. \\nYou can calculate the number of months between month 36 and the month you care about \\nby subtracting one month in column A from the other.\\nFor example, forecasting month 37 in cell B42, you’d use:\\n=C$41+(A42-A$41)*D$41\\nBy using absolute references for month 36, the ﬁ nal trend, and the ﬁ nal level, you can \\ndrag the forecast down through month 48, giving the sheet shown in Figure 8-15.\\nFigure 8-15: Forecasting future months with Holt’s Trend-Corrected Exponential Smoothing'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 325, 'page_label': '304'}, page_content='304 Data Smart\\nJust as on the simple exponential smoothing tab, you can graph the historical demand \\nand the forecast as two series on a straight-line scatter plot, as shown in Figure 8-16.\\nWith an alpha and gamma of 0.5, that forecast sure looks a bit nutty, doesn’t it? It’s taking \\noff  where the ﬁ nal month ends and increasing at a rather rapid rate from there. Perhaps \\nyou should optimize the smoothing parameters.\\nFigure 8-16: Graph of the forecast with default alpha and gamma values\\nOptimizing for One-Step Error\\nAs you did for simple exponential smoothing, add the squared forecast error in column \\nG. In F2 and G2, you can calculate the sum of the squared error and the standard error \\nfor the one-step forecast exactly as earlier. Except, this time the model has two smoothing \\nparameters so you’ll divide the SSE by 36 – 2 before taking the square root:\\n=SQRT(F2/(36-2))\\nThis gives you the sheet shown in Figure 8-17.\\nThe optimization setup is identical to simple exponential smoothing except this time \\naround you’re optimizing both alpha and gamma together, as shown in Figure 8-18.\\nWhen you solve, you get an optimal alpha value of 0.66 and an optimal gamma value of \\n0.05. The optimal forecast is shown in the straight-line scatter in Figure 8-19.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 326, 'page_label': '305'}, page_content='305Forecasting: Breathe Easy; You Can’t Win \\nFigure 8-17: Calculating the SSE and standard error\\nFigure 8-18: Optimization setup for Holt’s Trend-Corrected Exponential Smoothing'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 327, 'page_label': '306'}, page_content='306 Data Smart\\nFigure 8-19: Graph of optimal Holt’s forecast\\nThe trend you’re using from the forecast is an additional ﬁ ve swords sold per month. \\nThe reason why this trend is double the one you found using the trendline on the previ-\\nous tab is because trend-corrected smoothing favors recent points more, and in this case, \\nthe most recent demand points have been very “trendy.”\\nNote how this forecast starts very near the SES forecast for month 37 – 290 versus \\n292. But pretty quickly the trend-corrected forecast begins to grow just like you’d expect \\nwith a trend.\\nSo Are You Done? Looking at Autocorrelations\\nAll right. Is this the best you can do? Have you accounted for everything?\\nWell, one way to check if you have a good model for the forecast is to check the one-\\nstep ahead errors. If those errors are random, you’ve done your job. But if there’s a pattern \\nhidden in the error—some kind of repeated behavior at a regular interval—there may be \\nsomething seasonal in the demand data that is unaccounted for.\\nAnd by a “pattern in the error,” I mean that if you took the error and lined it up \\nwith itself shifted by a month or two months or twelve months, would it move in sync? \\nThis concept of the error being correlated with the time-shifted version of itself is called \\nautocorrelation (auto means “self” in Greek. It’s also a good preﬁ x for ditching vowels in \\nScrabble). \\nSo to start, create a new tab called Holt’s Autocorrelation. And in that tab, paste months \\n1 through 36 along with their one-step errors from the Holt’s forecast into columns A and B. \\nUnderneath the errors in B38, calculate the average error. This gives the sheet shown \\nin Figure 8-20.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 328, 'page_label': '307'}, page_content='307Forecasting: Breathe Easy; You Can’t Win \\nFigure 8-20: Months and associated one-step forecast errors\\nIn column C, calculate the deviations of each error in column B from the average in \\nB38. These deviations in the one-step error from the average are where patterns are going \\nto rear their ugly head. For instance, maybe every December the forecast error is sub-\\nstantially above average—that type of seasonal pattern would show up in these numbers. \\nIn cell C2, then, the deviation of the error in B2 from the mean would be:\\n=B2-B$38\\nYou can then drag this formula down to give all the mean deviations. In cell C38, cal-\\nculate the sum of squared deviations as:\\n=SUMPRODUCT($C2:$C37,C2:C37)\\nThis gives you the sheet shown in Figure 8-21.\\nNow, in column D “lag” the error deviations by one month. Label column D with a 1. \\nYou can leave cell D2 blank and set cell D3 to:\\n=C2\\nAnd then just drag the formula down until D37 equals C36. This gives you Figure 8-22.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 329, 'page_label': '308'}, page_content='308 Data Smart\\nFigure 8-21: Sum of squared mean deviations of Holt’s forecast errors\\nFigure 8-22: One month lagged error deviations \\nTo lag by two months, just select D1:D37 and drag it into column E. Similarly, to lag up \\nto 12 months, just drag the selection through column O. Easy! This gives you a cascading \\nmatrix of lagged error deviations, as shown in Figure 8-23.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 330, 'page_label': '309'}, page_content='309Forecasting: Breathe Easy; You Can’t Win \\nFigure 8-23: A beautiful cascading matrix of lagged error deviations ﬁ  t for a king\\nNow that you have these lags, think about what it means for one of these columns to \\n“move in sync” with column C. For instance, take the one-month lag in column D. If these \\ntwo columns were in sync then when one goes negative, the other should. And when one \\nis positive, the other should be positive. That means that the product of the two columns \\nwould result in a lot of positive numbers (a negative times a negative or a positive times \\na positive results in a positive number). \\nYou can sum these products, and the closer this \\nSUMPRODUCT of the lagged column with \\nthe original deviations gets to the sum of squared deviations in C38, the more in sync, \\nthe more correlated, the lagged errors are with the originals. \\nYou can also get negative autocorrelation where the lagged deviations go negative \\nwhenever the originals are positive and vice versa. The \\nSUMPRODUCT in this case will be a \\nlarger negative number.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 331, 'page_label': '310'}, page_content='310 Data Smart\\nTo start, drag the SUMPRODUCT($C2:$C37,C2:C37) in cell C38 across through column O. \\nNote how the absolute reference to column C will keep the column in place, so you get the \\nSUMPRODUCT of each lag column with the original, as shown in Figure 8-24.\\nFigure 8-24: SUMPRODUCT of lagged deviations with originals\\nYou calculate the autocorrelation for a given month lag as the SUMPRODUCT  of lagged \\ndeviations times original deviations divided by the sum of squared deviations in C38.\\nFor example, you can calculate the autocorrelation of a one-month lag in cell D40 as:\\n=D38/$C38\\nAnd dragging this across, you can get the autocorrelations for each lag.\\nHighlighting D40:O40, you can insert a bar chart into the sheet as shown in Figure \\n8-25 (Right-click and format the series’ ﬁ ll to be slightly transparent if you want to read \\nthe month labels under the negative values). This bar chart is called a correlogram, and it \\nshows the autocorrelations for each month lag up to a year. (As a personal note, I think \\nthe word correlogram is really cool.)\\nAll right. So which autocorrelations matter? Well, the convention is that you only worry \\nabout the autocorrelations larger than 2/sqrt(number of data points), which in this case \\nis 2/sqrt(36) = 0.333. You should also care about ones with a negative autocorrelation less \\nthan -0.333. \\nYou can just eyeball your chart for autocorrelations that are above or below these \\ncritical values. But it’s typical in forecasting to plot some dashed lines at these critical values \\non the correlogram. For the sake of a pretty picture, I’ll show you how to do that here.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 332, 'page_label': '311'}, page_content='311Forecasting: Breathe Easy; You Can’t Win \\nFigure 8-25: This is my correlogram; there are many like it but this is mine.\\nIn D42, add =2/SQRT(36) and drag it across through O. Do the same in D43 only with \\nthe negative value =-2/SQRT(36) and drag that across through O. This gives you the criti-\\ncal points for the autocorrelations, as shown in Figure 8-26.\\nFigure 8-26: Critical points for the autocorrelations'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 333, 'page_label': '312'}, page_content='312 Data Smart\\nRight-click the correlation bar chart and choose Select Data. From the window that \\nappears, press the Add button to create a new series. \\nFor one series select the range D42:O42 as the y-values. Add a third series using \\nD43:O43. This will add two more sets of bars to the graph. \\nRight-clicking each of these new bar series, you can select Change Series Chart Type \\nand select the Line chart to turn it into a solid line instead of bars. Right-click these lines \\nand select Format Data Series. Then navigate to the Line (Line Style in some Excel ver-\\nsions) option in the window. In this section, you can set the line to dashed, as shown in \\nFigure 8-27.\\nFigure 8-27: Changing the critical values for bars into a dashed line\\nThis yields a correlogram with plotted critical values, as shown in Figure 8-28.\\nAnd what do you see? \\nThere’s exactly one autocorrelation that’s above the critical value, and that’s at 12 \\nmonths. \\nThe error shifted by a year is correlated with itself. That indicates a 12-month seasonal cycle. \\nThis shouldn’t be too surprising. If you look at the plot of the demand on the Timeseries \\ntab, it’s apparent that there are spikes each Christmas and dips around April/May.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 334, 'page_label': '313'}, page_content='313Forecasting: Breathe Easy; You Can’t Win \\nYou need a forecasting technique that can account for seasonality. And wouldn’t you \\nknow it—there’s an exponential smoothing technique for that.\\nFigure 8-28: Correlogram with critical values\\nMultiplicative Holt-Winters Exponential Smoothing\\nMultiplicative Holt-Winters Smoothing is the logical extension of Holt’s Trend-Corrected \\nSmoothing. It accounts for a level, a trend, and the need to adjust the demand up or down \\non a regular basis due to seasonal ﬂ uctuations. Note that the seasonal ﬂ uctuation needn’t \\nbe every 12 months like in this example. In the case of MailChimp, we have periodic \\ndemand ﬂ uctuations every Thursday (people seem to think Thursday is a good day to send \\nmarketing e-mail). Using Holt-Winters, we could account for this 7-day cycle.\\nNow, in most situations you can’t just add or subtract a ﬁ xed amount of seasonal demand \\nto adjust the forecast. If your business grows from selling 200 to 2,000 swords each month, \\nyou wouldn’t adjust the Christmas demand in both those contexts by adding 20 swords. \\nNo, seasonal adjustments usually need to be multipliers. Instead of adding 20 swords \\nmaybe it’s multiplying the forecast by 120 percent. That’s why it’s called Multiplicative  \\nHolt-Winters. Here’s how this forecast conceives of demand:\\n  Demand  at time t = (level +t*trend) * seasonal adjustment for time t * whatever irregular \\nadjustments are left we can’t account for\\nSo you still have the identical level and trend structure you had in Holt’s Trend-\\nCorrected Smoothing, but the demand is adjusted for seasonality. And since you can’t \\naccount for irregular variations in the demand, such as acts of God, you’re not going to.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 335, 'page_label': '314'}, page_content='314 Data Smart\\nHolt-Winters is also called triple exponential  smoothing, because, you guessed it, there \\nare three smoothing parameters this time around. There are still alpha and gamma param-\\neters, but this time you have a seasonal adjustment factor with an update equation and \\na factor called delta.\\nNow, the three error adjustment equations are slightly more complex than what you’ve \\nseen so far, but you’ll recognize bits. \\nBefore you get started, I want to make one thing clear—so far you’ve used levels and \\ntrends from the previous period to forecast the next and adjust. But with seasonal adjust-\\nments, you don’t look at the previous period. Instead, you look at the previous estimate \\nof the adjustment factor for that point in the cycle. In this case, that’s 12 periods prior \\nrather than one. \\nThat means that if you’re at month 36 and you’re forecasting three months forward to \\n39, that forecast is going to look like:\\n  Forecast  for month 39 = (level\\n36+3*trend36)*seasonality27\\nYep, you’re seeing that seasonality27 correctly. It’s the most recent estimate for the March \\nseasonal adjustment. You can’t use seasonality36, because that’s for December.\\nAll right, so that’s how the future forecast works. Let’s dig into the update equations, \\nstarting with the level. You need only an initial level0 and trend0, but you actually need \\ntwelve initial seasonality factors, seasonality-11 through seasonality0. \\nFor example, the update equation for level1 relies on an initial estimate of the January \\nseasonality adjustment: \\n  level 1 = level 0 + trend 0 + alpha  * ( demand 1 – ( level 0 + trend 0)*seasonality -11)/\\nseasonality-11\\nYou have lots of familiar components here in this level calculation. The current level is \\nthe previous level plus the previous trend (just as in double exponential smoothing) plus \\nalpha times the one-step ahead forecast error (demand\\n1 – (level0 + trend0)*seasonality-11), \\nwhere the error gets a seasonal adjustment by being divided by seasonality-11.\\nAnd so as you walk forward in time, the next month would be:\\n  level 2 = level 1 + trend 1 + alpha  * ( demand 2 – ( level 1 + trend 1)*seasonality -10)/\\nseasonality-10\\nSo in general then the level is calculated as:\\n  level current  period = levelprevious  period + trendprevious  period + alpha * ( demand current  period – \\n(levelprevious period + trendprevious period)*seasonalitylast relevant period)/seasonalitylast relevant period'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 336, 'page_label': '315'}, page_content='315Forecasting: Breathe Easy; You Can’t Win \\nThe trend is updated in relation to the level in exactly the same way as in double expo-\\nnential smoothing:\\n  trend current  period  = trend previous  period  + gamma  * alpha  * ( demand current  period  – \\n(levelprevious period + trendprevious period)*seasonalitylast relevant period)/seasonalitylast relevant period\\nJust as in double exponential smoothing, the current trend is the previous trend plus \\ngamma times the amount of error incorporated into the level update equation.\\nAnd now for the seasonal factor update equation. It’s a lot like the trend update equa-\\ntion, except that it adjusts the last relevant seasonal factor using delta times the error that \\nthe level and trend updates ignored:\\n  seasonality current  period  = seasonality last  relevant  period  + delta  * ( 1-alpha ) * \\n(demandcurrent period – (levelprevious period + trendprevious period)*seasonality last relevant period)/ \\n(levelprevious period + trendprevious period)\\nIn this case you’re updating the seasonality adjustment with the corresponding factor \\nfrom 12 months prior, but you’re folding in delta times whatever error was left on the cut-\\nting room ﬂ oor from the level update. Except, note that rather than seasonally adjusting \\nthe error here, you’re dividing through by the previous level and trend values. By “level \\nand trend adjusting” the one-step ahead error, you’re putting the error on the same mul-\\ntiplier scale as the seasonal factors.\\nSetting the Initial Values for Level, Trend, and Seasonality\\nSetting the initial values for SES and double exponential smoothing was a piece of cake. \\nBut now you have to tease out what’s trend and what’s seasonality from the time series. \\nAnd that means that setting the initial values for this forecast (one level, one trend, and \\n12 seasonal adjustment factors) is a little tough. There are simple (and wrong!) ways of \\ndoing this. I’m going to show you a good way to initialize Holt-Winters, assuming you \\nhave at least two seasonal cycles’ worth of historical data. In this case, you have three \\ncycles’ worth.\\nHere’s what you’re going to do:\\n 1. Smooth out the historical data using what’s called a 2 × 12 moving average.\\n 2. Compare a smoothed version of the time series to the original to estimate seasonality.\\n 3. Using the initial seasonal estimates, deseasonalize the historical data.\\n 4. Estimate the level and trend using a trendline on the deseasonalized data.\\nTo start, create a new tab called HoltWintersInitial and paste the time series data into \\nits ﬁ rst two columns. Now you need to smooth out some of the time series data using a'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 337, 'page_label': '316'}, page_content='316 Data Smart\\nmoving average. Because the seasonality is in 12-month cycles, it makes sense to use a \\n12-month moving average on the data. \\nWhat do I mean by a 12-month moving average?\\nFor a moving average, you take the demand for a particular month as well as the \\ndemand around that month in both directions and average them. This tamps down any \\nweird spikes in the series.\\nBut there’s a problem with a 12-month moving average. Twelve is an even number. If \\nyou’re smoothing out the demand for month 7, should you average it as the demand of \\nmonths 1 through 12 or the demand of months 2 through 13? Either way, month 7 isn’t \\nquite in the middle. There is no middle!\\nTo accommodate this, you’re going to smooth out the demand with a “2 × 12 moving \\naverage,” which is the average of both those possibilities—months 1 through 12 and 2 \\nthrough 13. (The same goes for any other even number of time periods in a cycle. If your \\ncycle has an odd number of periods, the “2x” part of the moving average is unnecessary \\nand you can just do a simple moving average.)\\nNow note for the ﬁ rst six months of data and the last six months of data, this isn’t even \\npossible. They don’t have six months of data on either side of them. You can only smooth \\nthe middle months of the dataset (in this case it’s months 7–30). This is why you need at \\nleast two years’ worth of data, so that you get one year of smoothed data.\\nSo starting with month 7, use the following formula:\\n=(AVERAGE(B3:B14)+AVERAGE(B2:B13))/2\\nThis is the average of month 7 with the 12 months around it, except that months 1 and \\n13 count for half of what the other months count for, which makes sense; since months \\n1 and 13 are in the same month if they were each counted twice then you’d have January \\nover-represented in the moving average.\\nDragging this formula down through month 30 and graphing both the original and \\nsmoothed data in a straight-line scatter plot, you get the sheet shown in Figure 8-29. In \\nmy chart I’ve labeled the two series smoothed and unsmoothed. It’s apparent looking at \\nthe smoothed line that any seasonal variation present in the data has, more or less, been \\nsmoothed out.\\nNow, in column D, you can divide the original value by the smoothed value to get an \\nestimate of the seasonal adjustment factor. Starting at month 7, you have for cell D8:\\nB8/C8'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 338, 'page_label': '317'}, page_content='317Forecasting: Breathe Easy; You Can’t Win \\nAnd you can drag this down through month 30. Note how in both months 12 and 24 \\n(December) you get spikes around 20 percent of normal, whereas you get dips in the spring.\\nFigure 8-29: The smoothed demand data\\nThis smoothing technique has given you two point estimates for each seasonality fac-\\ntor. In column E, let’s average these two points together into a single value that will be \\nthe initial seasonal factor used in Holt-Winters.\\nFor example, in E2, which is January, you average the two January points in column \\nD, which are D14 and D26. Since the smoothed data starts in the middle of the year in \\ncolumn D, you can’t drag this average down. In E8, which is July, you have to take the \\naverage of D8 and D20 for instance.\\nOnce you have these 12 adjustment factors in column E, you can subtract 1 from each \\nof them in column F and format the cells as percentages (highlight the range and right-\\nclick Format Cells) to see how these factors move the demand up or down each month. \\nYou can even insert a bar chart of these skews into the sheet, as shown in Figure 8-30.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 339, 'page_label': '318'}, page_content='318 Data Smart\\nFigure 8-30:  A bar chart of estimated seasonal variations\\nNow that you have these initial seasonal adjustments, you can use them to \\ndeseasonalize the time series data. Once the entire series is deseasonalized, you can toss \\na trendline through it and use the slope and intercept as the initial level and trend.\\nTo start, paste the appropriate seasonal adjustment values for each month in G2 through \\nG37. Essentially, you’re just pasting E2:E13 three times in a row down column G (make \\nsure to paste values only). In column H you can then divide the original series in column \\nB by the seasonal factors in G to remove the estimated seasonality present in the data. \\nThis sheet is shown in Figure 8-31.\\nNext, as you’ve done on previous tabs, insert a scatter plot of column H and toss a \\ntrendline through it. Displaying the trendline equation on the graph, you get an initial \\ntrend estimate of 2.29 additional sword sales per month and an initial level estimate of \\n144.42 (see Figure 8-32).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 340, 'page_label': '319'}, page_content='319Forecasting: Breathe Easy; You Can’t Win \\nFigure 8-31: The deseasonalized time series\\nGetting Rolling on the Forecast\\nNow that you have the initial values for all the parameters, create a new tab called \\nHoltWintersSeasonal, where you’ll start by pasting the time series data on row 4 just as \\nyou did for the previous two forecasting techniques.\\nIn columns C, D, and E next to the time series you’re going to put the level, trend, and \\nseasonal values, respectively. And in order to start, unlike on previous tabs where you only \\nneeded to insert one new blank row 5, this time around you need to insert blank rows 5 \\nthrough 16 and label them as time slots -11 through 0 in column A. You can then paste \\nthe initial values from the previous tab in their respective spots, as shown in Figure 8-33.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 341, 'page_label': '320'}, page_content='320 Data Smart\\nFigure 8-32: Initial level and trend estimates via a trendline on the deseasonalized series\\nFigure 8-33: All of the initial Holt-Winters values in one place'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 342, 'page_label': '321'}, page_content='321Forecasting: Breathe Easy; You Can’t Win \\nIn column F you’ll do a one-step ahead forecast. So for time period 1, it’s the previous \\nlevel in C16 plus the previous trend in D16. But both of those are adjusted by the appro-\\npriate January seasonality estimate 12 rows up in E5. Thus, F17 is written as:\\n=(C16+D16)*E5\\nThe forecast error in G17 may then be calculated as:\\n=B17-F17\\nNow you’re ready to get started with calculating the level, trend, and seasonality rolling \\nforward. So in cells C2:E2, put the alpha, gamma, and delta values (as always I’m going to \\nstart with 0.5). Figure 8-34 shows the worksheet.\\nFigure 8-34: Worksheet with smoothing parameters and ﬁ  rst one-step forecast and error \\nThe ﬁ rst item you’ll calculate as you roll through the time periods is a new level esti-\\nmate for period 1 in cell C17:\\n=C16+D16+C$2*G17/E5'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 343, 'page_label': '322'}, page_content='322 Data Smart\\nJust as you saw in the previous section, the new level equals the previous level plus \\nthe previous trend plus alpha times the deseasonalized forecast error. And the updated \\ntrend in D17 is quite similar:\\n=D16+D$2*C$2*G17/E5\\nYou have the previous trend plus gamma times the amount of deseasonalized error \\nincorporated into the level update.\\nAnd for the January seasonal factor update you have:\\n=E5+E$2*(1-C$2)*G17/(C16+D16) \\nThat’s the previous January factor adjusted by delta times the error ignored by the level \\ncorrection scaled like the seasonal factors by dividing through by the previous level and \\ntrend.\\nNote that in all three of these formulas alpha, gamma, and delta are referenced via \\nabsolute references, so that as you drag the calculations down they don’t move. Dragging \\nC17:G17 down through month 36, you get the sheet shown in Figure 8-35.\\nFigure 8-35: Taking the update equations through month 36'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 344, 'page_label': '323'}, page_content='323Forecasting: Breathe Easy; You Can’t Win \\nAnd now that you have your ﬁ nal level, trend, and seasonal estimates, you can forecast \\nthe next year’s worth of demand. Starting in month 37 in cell B53 you have:\\n=(C$52+(A53-A$52)*D$52)*E41\\nJust as in Holt’s Trend-Corrected Smoothing, you’re taking the last level estimate \\nand adding to it the trend times the number of elapsed months since the most recent \\ntrend estimate. The only diff  erence is you’re scaling the whole forecast by the most \\nup-to-date seasonal multiplier for January, which is in cell E41. And while the level in \\nC$52 and the trend in D$52 use absolute references so that they won’t shift as you drag \\nthe forecast down, the seasonal reference in E41 must move down as you drag the \\nforecast through the next 11 months. And so, dragging the calculation down, you get \\nthe forecast shown in Figure 8-36.\\nFigure 8-36: Getting the Holt-Winters forecast for future months\\nYou can graph this forecast using Excel’s straight-line scatter plot just as in the previous \\ntwo techniques (see Figure 8-37).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 345, 'page_label': '324'}, page_content='324 Data Smart\\nFigure 8-37: Graphing the Holt-Winters forecast\\nAnd...Optimize!\\nYou thought you were done, but no. Time to set those smoothing parameters. So just as in \\nthe previous two techniques, toss the SSE in cell G2, and place the standard error in H2.\\nThe only diff erence this time around is that you have three smoothing parameters, so \\nthe standard error is calculated as:\\n=SQRT(G2/(36-3))\\nThis gives the sheet shown in Figure 8-38.\\nAs for the Solver setup (shown in Figure 8-39), this time around you’re optimizing H2 \\nby varying the three smoothing parameters. You’re able to achieve a standard error almost \\nhalf that of previous techniques. The forecast plot (see Figure 8-40) looks good to the \\neye, doesn’t it? You’re tracking with the trend and the seasonal ﬂ uctuations. Very nice.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 346, 'page_label': '325'}, page_content='325Forecasting: Breathe Easy; You Can’t Win \\nFigure 8-38:  Adding SSE and standard error\\nFigure 8-39: The Solver setup for Holt-Winters'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 347, 'page_label': '326'}, page_content='326 Data Smart\\nFigure 8-40:  The optimized Holt-Winters forecast\\nPlease Tell Me We’re Done Now!!!\\nYou now need to check the autocorrelations on this forecast. Since you’ve already set up \\nthe autocorrelation sheet, this time around you just need to make a copy of it and paste \\nin the new error values. \\nMake a copy of the Holt’s Autocorrelation tab and call it HW Autocorrelation. Then \\nyou need only paste special the values from the error column G into the autocorrelation \\nsheet in column B. This gives the correlogram shown in Figure 8-41.\\nFigure 8-41:  Correlogram for the Holt-Winters model'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 348, 'page_label': '327'}, page_content='327Forecasting: Breathe Easy; You Can’t Win \\nBam! Since there are no autocorrelations above the critical value of 0.33, you know that \\nthe model is doing a nice job at capturing the structure in the demand values.\\nPutting a Prediction Interval around the Forecast\\nAll right, so you have a forecast that ﬁ  ts well. How do you put some lower and upper \\nbounds around it that you can use to set realistic expectations with the boss?\\nYou’re going to do this through Monte Carlo simulation, which you’ve already seen \\nin Chapter 4. Essentially, you’re going to generate future scenarios of what the demand \\nmight look like and determine the band that 95 percent of those scenarios fall into. The \\nquestion is how do you even begin to simulate future demand? It’s actually quite easy.\\nStart by making a copy of the HoltWintersSeasonal tab and calling it PredictionIntervals. \\nDelete all the graphs in the tab. They’re unnecessary. Furthermore, clear out the forecast \\nin cells B53:B64. You’ll be putting “actual” (but simulated) demand in those spots.\\nNow, like I said at the beginning of this chapter, the forecast is always wrong. There \\nwill always be error. But you know how this error will be distributed. You have a well-\\nﬁ tting forecast that you can assume has mean 0 one-step error (unbiased) with a standard \\ndeviation of 10.37, as calculated on the previous tab.\\nJust as in Chapter 4, you can generate a simulated error using the \\nNORMINV function. In \\nfuture months, you can just feed the NORMINV function the mean (0), the standard devia-\\ntion (10.37 in cell H$2), and a random number between 0 and 1, and it’ll pull an error \\nfrom the bell curve. (See the discussion on cumulative distribution functions in Chapter \\n4 for more on how this works.)\\nOkay, so toss a simulated one-step error into cell G53:\\n=NORMINV(RAND(),0,H$2)\\nDrag it down through G64 to get 12 months of simulated errors in the one-step fore-\\ncast. This gives you the sheet shown in Figure 8-42 (yours will have diff  erent simulated \\nvalues from these).\\nBut now that you have the forecast error, you have everything you need to update the \\nlevel, trend, and seasonality estimates going forward as well as the one-step forecast. So \\ngrab cells C52:F52 and drag them down through row 64.\\nHere’s where things get analytically badass. You now have a simulated forecast error \\nand a one-step ahead forecast. So if you add the error in G to the forecast in F, you can \\nactually back out a simulated demand for that time period.\\nThus, B53 would simply be:\\n=F53+G53'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 349, 'page_label': '328'}, page_content='328 Data Smart\\nFigure 8-42:  Simulated one-step errors\\nAnd you can drag that down through B64 to get all 12 months’ demand values (see \\nFigure 8-43).\\nOnce you have that one scenario, by simply refreshing the sheet, the demand values \\nchange. So you can generate multiple future demand scenarios merely by copy-pasting \\none of the scenarios elsewhere and watching the sheet refresh itself.\\nTo start then, label cell A69 as Simulated Demand  and label A70:L70 as months 37 \\nthrough 48. You can do this by copying A53:A64 and doing a paste special with trans-\\nposed values into A70:L70.\\nSimilarly, paste special the transposed values of the ﬁ rst demand scenario into A71:L71. \\nTo insert a second scenario, simply right-click row 71 and select Insert to insert a new \\nblank row 71. Then paste special some more simulated demand values (they should have \\nupdated when you pasted the last set).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 350, 'page_label': '329'}, page_content='329Forecasting: Breathe Easy; You Can’t Win \\nYou can just keep doing this operation to generate as many future demand scenarios \\nas you want. That’s tedious though. Instead, you can record a quick macro. \\nFigure 8-43:  Simulated future demand\\nJust as in Chapter 7, record the following steps into a macro:\\n 1. Insert a blank row 71.\\n 2. Copy B53:B64. \\n 3. Paste special transposed values into row 71.\\n 4. Press the Stop recording button.\\nOnce you’ve recorded those keystrokes, you can hammer on whatever macro shortcut \\nkey you selected (see Chapter 7) over and over until you get a ton of scenarios. You can \\neven hold the shortcut key down—1,000 scenarios should do it. (If the idea of holding a \\nbutton down is abhorrent to you, you can read up on how to put a loop around your macro \\ncode using Visual Basic for Applications. Just Google for it.)'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 351, 'page_label': '330'}, page_content='330 Data Smart\\nWhen it’s all said and done, your sheet should look like Figure 8-44. \\nFigure 8-44:  I have 1,000 demand scenarios\\nOnce you have your scenarios for each month, you can use the PERCENTILE function to \\nget the upper and lower bounds on the middle 95 percent of scenarios to create a predic-\\ntion interval.\\nFor instance, above month 37 in A66 you can place the formula:\\n=PERCENTILE(A71:A1070,0.975)\\nThis gives you the 97.5th percentile of demand for this month. In my sheet it comes out \\nto about 264. And in A67 you can get the 2.5th percentile as:\\n=PERCENTILE(A71:A1070,0.025)\\nNote that I’m using A71:A1070 because I have 1,000 simulated demand scenarios. You \\nmay have more or less depending on the dexterity of your index ﬁ nger. For me, this lower \\nbound comes out to around 224. \\nThat means that although the forecast for month 37 is 245, the 95 percent prediction \\ninterval is 224 to 264.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 352, 'page_label': '331'}, page_content='331Forecasting: Breathe Easy; You Can’t Win \\nYou can drag these percentile equations across through month 48 in column L to get \\nthe entire interval (see Figure 8-45). So now you can provide your superiors with a con-\\nservative range plus a forecast if you like! And feel free to swap out the 0.025 and 0.975 \\nwith 0.05 and 0.95 for a 90 percent interval or with 0.1 and 0.9 for an 80 percent interval, \\nand so on.\\nFigure 8-45:  The forecast interval for Holt-Winters\\nCreating a Fan Chart for Effect\\nNow, this last step isn’t necessary, but forecasts with prediction intervals are often shown \\nin something called a fan chart. You can create such a chart in Excel.\\nTo start, create a new tab called Fan Chart and in that tab, paste months 37 through 48 \\non row 1 and then paste the values of the upper bound of the prediction interval from row \\n66 of the PredictionIntervals tab on row 2. On row 3, paste special the transposed values \\nfor the actual forecast from the HoltWintersSeasonal tab. And on row 4, paste the values of \\nthe lower bound of the prediction interval from row 67 of the intervals sheet.\\nSo you have the months, the upper bound of the interval, the forecast, and the lower \\nbound all right in a row (see Figure 8-46).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 353, 'page_label': '332'}, page_content='332 Data Smart\\nFigure 8-46:  The forecast sandwiched by the prediction interval\\nBy highlighting A2:L4 and selecting Area Chart from the charts menu in Excel, you \\nget three solid area charts laid over each other. Right-click one of the series and choose \\nSelect Data. You can change the Category (X) axis labels for one of the series to be A1:L1 \\nin order to add in the correct monthly labels to the graph.\\nNow, right-click the lower bound series and format it to have a white ﬁ  ll. You should \\nalso remove grid lines from the graph for consistency’s sake. Feel free to add axis labels \\nand a title. This yields the fan chart shown in Figure 8-47.\\nFigure 8-47: The fan chart is a thing of beauty'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 354, 'page_label': '333'}, page_content='333Forecasting: Breathe Easy; You Can’t Win \\nThe cool thing about this fan chart is that it conveys both the forecast and the intervals \\nin one simple picture. Heck, you could actually layer on an 80 percent interval too if you \\nwanted more shades of gray. There are two interesting items that stand out in the chart:\\n• The error gets wider as time goes on. This makes sense. The uncertainty from month \\nto month gets compounded.\\n• Similarly, there is more error in absolute terms during periods of high seasonal \\ndemand. When demand dips in a trough, the error bounds tighten up. \\nWrapping Up\\nThis chapter covered a ton of content:\\n• Simple exponential smoothing (SES)\\n• Performing a t test on a linear regression to verify a linear trend in the time series\\n• Holt’s Trend-Corrected Exponential Smoothing\\n• Calculating autocorrelations and graphing a correlogram with critical values\\n• Initializing Holt-Winters Multiplicative Exponential Smoothing using a 2 x 12 mov-\\ning average\\n• Forecasting with Holt-Winters\\n• Creating prediction intervals around the forecast using Monte Carlo simulation\\n• Graphing the prediction intervals as a fan chart\\nIf you made it through the entire chapter, bravo. Seriously, that’s a lot of forecasting \\nfor one chapter.\\nNow if you want to go further with forecasting, there are some excellent textbooks out \\nthere. I really like Forecasting, Time Series, and Regression by Bowerman et al. (Cengage \\nLearning, 2004). Hyndman has a free forecasting textbook online at http://otexts.com/\\nfpp/, and his blog (awesomely called “Hyndsight”) is an excellent resource. For questions, \\nhttp://stats.stackexchange.com/ is the community to go to.\\nWhen it comes to forecasting in a production setting, there are countless products out \\nthere. For light jobs, feel free to stay in Excel. If you have tons of products or SKUs, using \\nsome code would be helpful.\\nSAS and R both have excellent packages for forecasting. The ones in R were written by \\nHyndman himself (see Chapter 10), who came up with the statistical underpinnings for \\nhow to do prediction intervals on the exponential smoothing techniques.\\nAnd that’s it! I hope you now feel empowered to go forth and “organize your ign orance!”'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 355, 'page_label': '334'}, page_content=''),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 356, 'page_label': '335'}, page_content='9\\nO\\nutliers are the odd points in a dataset—the ones that don’t fit somehow. Historically, \\nthat’s meant extreme values, meaning quantities that were either too large or small \\nto have come naturally from the same process as the other observations in the dataset. \\nThe only reason people used to care about outliers was because they wanted to get rid \\nof them. Statisticians a hundred years ago had a lot in common with the Borg: a data point \\nneeded to assimilate or die. However, this was done with good reason (in the case of the \\nstatistician)—outliers can move averages and mess with spread measurements in the data. \\nA good example of outlier removal is in gymnastics, where the highest and lowest judges’ \\nscores are always trimmed from the data before taking the average score.\\nOutliers have a knack for messing up machine learning models. For example, in \\nChapters 6 and 7 you looked at predicting pregnant customers based on their purchase \\ndata. What if a store miscoded some items on the shelves of the pharmacy and were \\nregistering multi-vitamin purchases as folic acid purchases? The customers with those \\nfaulty purchase vectors are outliers that shift the relationship of pregnancy-to-folic-acid-\\npurchasing in a way that harms the AI model’s understanding.\\nOnce upon a time when I consulted for the government, my company found a water \\nstorage facility that the United States had in Dubai that had been valued at billions and \\nbillions of dollars. The property value was an outlier that was throwing off  the results of \\nour analysis—turns out someone had typed it into the database with too many zeroes.\\nSo that’s one reason to care about outliers: to facilitate cleaner data analysis and modeling.\\nBut there’s another reason to care about outliers. They’re interesting for their own sake!\\nOutliers Are (Bad?) People, Too\\nConsider when your credit card company calls you after you make a transaction that is \\npotentially fraudulent. What’s your credit card company doing? They’re detecting that \\ntransaction as being an outlier based on your past behavior. Rather than ignoring the \\nOutlier Detection: Just \\nBecause They’re Odd \\nDoesn’t Mean They’re \\nUnimportant'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 357, 'page_label': '336'}, page_content='336 Data Smart\\ntransaction because it’s an outlier, they’re purposefully ﬂ agging the potential fraud and \\nacting on it.\\nAt MailChimp when we predict spammers before they send, we’re predicting outliers. \\nThese spammers are a small group of people whose behavior lies outside of what we as a \\ncompany consider normal. We use supervised models similar to those in Chapters 6 and \\n7 to predict based on past occurrences when a new user is going to send spam.\\nSo in the case of MailChimp, then, an outlier is no more than a small but understood \\nclass of data in the population that can be predicted using training data. But what about \\nthe cases when you don’t know what you’re looking for? Like those mislabeled folic acid \\nshoppers? Fraudsters often change their behavior so that the only thing you can expect \\nfrom them is something unexpected. If that error has never happened before, how do you \\nﬁ nd those odd points for the ﬁ rst time?\\nThis type of outlier detection is an example of unsupervised learning and data mining. \\nIt’s the intuitive ﬂ ip side of the analysis performed in Chapters 2 and 5 of this book where \\nyou detected clusters of points. In cluster analysis, you look for a data point’s group of \\nfriends and analyze that group. In outlier detection, you care about data points that diff er \\nfrom the groups. They’re odd or exceptional in some way.\\nThis chapter starts with a simple, standard way of calculating outliers in normal-like \\none-dimensional data. Then it moves on to using k nearest neighbor (kNN) graphs to \\ndetect outliers in multidimensional data, similar to how you used r-neighborhood graphs \\nto create clusters in Chapter 5.\\nThe Fascinating Case of Hadlum v. Hadlum\\nNOTE\\nThe Excel workbook used in this section, “Pregnancy Duration.xlsx,” is available for \\ndownload at the book’s website at www.wiley.com/go/datasmart. Later in this chap-\\nter, you’ll be diving into a larger spreadsheet, “SupportCenter.xlsx,” also available on \\nthe same website.\\nBack in the 1940s, a British guy named Mr. Hadlum went off  to war. Some days later, 349 \\nof them in fact, his wife Mrs. Hadlum gave birth. Now, the average pregnancy lasts about \\n266 days. That places Mrs. Hadlum almost 12 weeks past her due date. I can’t think of a \\nsingle woman who’d stand for that added discomfort these days, but back then, inducing \\npregnancy wasn’t as common.\\nNow, Mrs. Hadlum claimed she had nothing more than an exceptionally long preg-\\nnancy. Fair enough.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 358, 'page_label': '337'}, page_content='337Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \\nBut Mr. Hadlum concluded her pregnancy must have been the result of another man \\nwhile he’d been away—that a 349-day pregnancy was an anomaly that couldn’t be justi-\\nﬁ ed given the distribution of typical birth durations.\\nSo, if you had some pregnancy data, what’s a quick-and-dirty way to decide whether \\nMrs. Hadlum’s pregnancy should be considered an outlier? \\nWell, studies have found that gestation length is more or less a normally distributed \\nrandom variable with a mean of 266 days after conception, with a standard deviation of \\nabout 9. So you can evaluate the normal cumulative distribution function (CDF) intro-\\nduced in Chapter 4 to get the probability of a value less than 349 occurring. In Excel, this \\nis evaluated using the \\nNORMDIST function:\\n=NORMDIST(349,266,9,TRUE)\\nThe NORMDIST function is supplied with the value whose cumulative probability you \\nwant, the mean, the standard deviation, and a ﬂ ag set to TRUE, which sets the function to \\nprovide the cumulative value.\\nThis formula returns a value of 1.000 all the way out as far as Excel tracks decimals. \\nThis means that nearly all babies born from here to eternity are going to be born at or \\nunder 349 days. Subtracting this value from 1:\\n=1-NORMDIST(349,266,9,TRUE)\\nYou get 0.0000000 as far as the eye can see. In other words, it’s nearly impossible for \\na human baby to gestate this long.\\nWe’ll never know for sure, but I’d bet good money Mrs. Hadlum had a little something \\nelse going on. Funny thing is, the court ruled in her favor, stating that such a long preg-\\nnancy, although highly unlikely, was still possible.\\nTukey Fences\\nThis concept of outliers being unlikely points when sampled from the bell curve has led \\nto a rule of thumb for outlier detection called Tukey fences. Tukey fences are easy to check \\nand easy to code. They are used by statistical packages the world over for identifying and \\nremoving spurious data points from any set of data that ﬁ ts in a normal bell curve.\\nHere’s the Tukey fences technique in its entirety:\\n• Calculate the 25th and 75th percentiles in any dataset you’d like to ﬁ nd outliers in. \\nThese values are also called the ﬁ rst quartile and the third quartile. Excel calculates \\nvalues these using the PERCENTILE function.\\n• Subtract the ﬁ rst quartile from the third quartile to get a measure of the spread of \\nthe data, which is called the Interquartile Range (IQR). The IQR is cool because it’s'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 359, 'page_label': '338'}, page_content='338 Data Smart\\nrelatively robust against extreme values as a measure of spread, unlike the typical \\nstandard deviation calculation you’ve used to measure spread in previous chapters \\nof this book.\\n• Subtract 1.5*IQR from the ﬁ rst quartile to get the lower inner fence. Add 1.5*IQR \\nto the third quartile to get the upper inner fence.\\n• Likewise, subtract 3*IQR from the ﬁ rst quartile to get the lower outer fence. Add \\n3*IQR to the third quartile to get the upper outer fence.\\n• Any value less than a lower fence or greater than an upper fence is extreme. In \\nnormally distributed data, you’d see about 1 in every 100 points outside the inner \\nfence, but only 1 in every 500,000 points outside the outer fence.\\nApplying Tukey Fences in a Spreadsheet\\nI’ve included a sheet called PregnancyDuration.xlsx for download off  the book’s website \\nso that you can apply this technique to some actual data. If you open it, you’ll see a tab \\ncalled Pregnancies, with a sample of 1,000 durations in column A.\\nMrs. Hadlum’s gestation period of 349 days is in cell A2. In column D, place all of the \\nsummary statistics and fences. Start with the median (the middle value), which is a more \\nrobust statistic of centrality than the average value (averages can be skewed by outliers). \\nLabel C1 as Median and in D1, calculate the median as follows:\\n=PERCENTILE(A2:A1001,0.5)\\nThat would be the 50th percentile. Below the median, you can calculate the ﬁ  rst and \\nthird quartiles as:\\n=PERCENTILE(A2:A1001,0.25)\\n=PERCENTILE(A2:A1001,0.75)\\nAnd the interquartile range is the diff erence between them:\\n=D3-D2\\nTacking on 1.5 and 3 times the IQR to the ﬁ rst and third quartile respectively, you can \\nthen calculate all the fences:\\n=D2-1.5*D4\\n=D3+1.5*D4\\n=D2-3*D4\\n=D3+3*D4\\nIf you label all these values, you’ll get the sheet shown in Figure 9-1.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 360, 'page_label': '339'}, page_content='339Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \\nFigure 9-1: Tukey fences for some pregnancy durations\\nNow you can apply some conditional formatting to the sheet and see who falls outside \\nthese fences. Start with the inner fence. To highlight the extreme values, select Conditional \\nFormatting from the Home tab, choose Highlight Cells Rules, and select Less Than, as \\nshown in Figure 9-2. \\nFigure 9-2: Adding conditional formatting for outliers\\nSpecifying the lower inner fence, feel free to choose a highlight color that tickles your \\nfancy (I’m going to choose a yellow ﬁ ll for inner fences and a red for outer, because I like \\ntraffi  c lights). Similarly, add formatting for the other three fences (if you’re using Excel \\n2011 for the Mac you can use the Not Between rule to add the formatting with two rules \\nrather than four).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 361, 'page_label': '340'}, page_content='340 Data Smart\\nAs shown in Figure 9-3, Mrs. Hadlum turns red, meaning her pregnancy was radically \\nextreme. Scrolling down, you’ll ﬁ nd no other red pregnancies, but there are nine yellows. \\nThis matches up closely with the roughly 1 out of 100 points you’d expect to be ﬂ agged \\nin normal data by the rule.\\nFigure 9-3: Uh oh, Mrs. Hadlum. What say you to this conditional formatting?\\nThe Limitations of This Simple Approach\\nTukey fences work only when three things are true:\\n• The data is vaguely normally distributed. It doesn’t have to be perfect, but it should \\nbe Bell-curve shaped and hopefully symmetric without some long tail jutting out \\none side of it.\\n• The deﬁ nition of an outlier is an extreme value on the perimeter of a distribution.\\n• You’re looking at one-dimensional data.\\nLet’s look at an example of an outlier that violates the ﬁ rst two of these assumptions.\\nIn The Fellowship of the Ring, when the adventurers ﬁ nally form a single company (the \\nfellowship for which the book is named), they all stand in a little group as the leader of \\nthe elves, Elrond, pronounces who they are and what their mission is.\\nThis group contains four tall people: Gandalf, Aragorn, Legolas, and Boromir. There \\nare also four short people. The hobbits themselves: Frodo, Merry, Pippin, and Sam. \\nAnd in between them, there’s a single dwarf: Gimli. Gimli is shorter than the men by \\na couple heads and taller than the hobbits by about the same (see Figure 9-4).\\nIn the movie, when we see this group presented to us for the ﬁ  rst time, Gimli is the \\nclear outlier by height. He belongs to neither group.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 362, 'page_label': '341'}, page_content='341Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \\nGuess\\nI’m an\\noutlier\\nHobbits Gimli Tall Folks\\nFigure 9-4:  Gimli, son of Gloin, Dwarven outlier\\nBut how is he the outlier? His height is neither the least nor the greatest. In fact, his \\nheight is the closest to the average of the group’s.\\nYou see, this height distribution isn’t anywhere near normal. If anything, you could call \\nit “multi-modal” (a distribution with multiple peaks). And Gimli is an outlier not because \\nhis height is extreme, but because it’s between these two peaks. And these types of data \\npoints can be even harder to spot when you’re looking over several dimensions.\\nThis kind of outlier crops up in fraud pretty frequently. Someone who’s too ordinary \\nto actually be ordinary. Bernie Madoff  is a great example of this. Although most Ponzi \\nschemes off er outlier rates of return of 20-plus percent, Madoff   off ered reliably modest \\nreturns that blended into the noise each year—he wasn’t jumping any Tukey fences. But \\nacross years, his multiyear returns in their reliability became a multi-dimensional outlier. \\nSo how do you ﬁ nd outliers in the case of multi-model, multi-dimensional data (you \\njust as easily could call it “real-world data”)?\\nOne awesome way to approach this is to treat the data like a graph, just as you did in \\nChapter 5 to ﬁ nd clusters in the data. Think about it. What deﬁ  nes Gimli as an outlier \\nis his relationship to the other data points; his distance from them in relation to their \\ndistance from each other. \\nAll of those distances, each point from every other point, deﬁ  nes edges on a graph. \\nUsing this graph, you can tease out the isolated points. To do that, you start by creating \\na k nearest neighbor (kNN) graph and going from there.\\nTerrible at Nothing, Bad at Everything\\nFor this next section, imagine that you manage a large customer support call center. Each \\ncall, e-mail, or chat from a customer creates a ticket, and each member of the support team \\nis required to handle at least 140 tickets daily. At the end of each interaction, a customer'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 363, 'page_label': '342'}, page_content='342 Data Smart\\nis given the opportunity to rate the support employee on a ﬁ  ve-star scale. Support staff  \\nare required to keep an average rating above 2, or they are ﬁ red. \\nHigh standards, I know.\\nThe company keeps track of plenty of other metrics on each employee as well. How \\nmany times they’ve been tardy over the past year. How many graveyard and weekend \\nshifts they’ve taken for the team. How many sick days they’ve taken, and out of those, \\nhow many have been on Friday. The company even tracks how many hours the employee \\nuses to take internal training courses (they get up to 40 hours paid) and how many times \\nthey’ve put in a request for a shift swap or been a good Samaritan and fulﬁ  lled another \\nemployee’s request.\\nYou have all this data for all 400 call center employees in a spreadsheet. And the ques-\\ntion is which employees are outliers, and what do they teach you about being a call center \\nemployee? Are there some baddies slipping through who don’t get culled by the ticket \\nrequirements and minimum customer ratings? Perhaps the outliers will teach you how \\nto write better rules.\\nIf you open the spreadsheet for this section of the chapter (SupportCenter.xlsx available \\nfor download on the book’s website at \\nwww.wiley.com/go/datasmart), you’ll ﬁ nd all this \\ntracked performance data on the SupportPersonnel sheet (see Figure 9-5).\\nFigure 9-5: Multi-dimensional employee performance data\\nPreparing Data for Graphing\\nThere’s a problem with this performance data. You can’t measure the distance between \\nemployees in order to ﬁ gure out who’s “on the outside” when each column is scaled so \\ndiff erently. What does it mean to have a diff erence of 5 between two employees on their'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 364, 'page_label': '343'}, page_content='343Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \\naverage tickets versus a diff erence of 0.2 in customer rating? You need to standardize each \\ncolumn so that the values are closer to the same center and spread.\\nThe way that columns of data are usually standardized is:\\n 1. Subtract the mean of a column from each observation.\\n 2. Divide each observation by the standard deviation of the column.\\nFor normally distributed data, this centers the data at 0 (gives it a mean of 0) and gives \\nit a standard deviation of 1. Indeed, a normal distribution with mean 0 and standard \\ndeviation 1 is called the standard normal distribution. \\nSTANDARDIZING USING ROBUST MEASURES OF CENTRALITY AND SCALE\\nNot all data you’ll want to scale is normally distributed to begin with. Subtracting \\nout the mean and dividing through by the standard deviation tends to work well \\nanyway. But outliers can screw up mean and standard deviation calculations, so \\nsometimes folks like to standardize by subtracting more robust statistics of centrality \\n(the “middle” of the data) and dividing through by more robust measures of scale/\\nstatistical dispersion (the spread of the data).\\nHere are some centrality calculations that work better against one-dimensional outli-\\ners than the mean:\\n• Median—Yep, just the 50th percentile\\n• Midhinge—The average of the 25th and 75th percentiles\\n• Trimean—The average of the median and the midhinge. I like this one, because \\nit sounds intelligent.\\n• Trimmed/truncated mean—The mean, but you throw away the top and bottom \\nN points or top and bottom percentage of points. You see this one in sports a \\nlot (think gymnastics where they throw out the top and bottom scores). If you \\nthrow away the top and bottom 25 percent and average the middle 50 percent of \\nthe data, that has its own name: the interquartile mean (IQM).\\n• Winsorized mean—Like the trimmed mean, but instead of throwing away points \\nthat are too large or too small, you replace them with a limit.\\nAs for robust measures of scale, here are some others worth using instead of the \\nstandard deviation:\\n• Interquartile range—You saw this one earlier in the chapter. It’s just the 75th \\npercentile minus the 25th percentile in the data. You can use other n-tiles too. For \\nexample, if you use the 90th and 10th percentiles, you get the interdecile range.\\n• Median absolute deviation (MAD)—Take the median of the data. Then take the \\nabsolute value of the diff erence of each point from the median. The median of \\nthese deviations is the MAD. It’s kinda like the median’s answer to the standard \\ndeviation.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 365, 'page_label': '344'}, page_content='344 Data Smart\\nTo start then, calculate the mean and standard deviation of each column at the bot-\\ntom of the SupportPersonnel sheet. The ﬁ rst value you’ll want in B402 is the mean of the \\ntickets taken per day, which you can write as:\\n=AVERAGE(B2:B401)\\nAnd below that you take the standard deviation of the column as:\\n=STDEV(B2:B401)\\nCopying those two formulas through column K, you get the sheet shown in Figure 9-6.\\nFigure 9-6:  Mean and standard deviation for each column\\nCreate a new tab called Standardized and copy the column labels from row 1 as well \\nas the employee IDs from column A. You can start standardizing the values in cell B2 \\nusing Excel’s \\nSTANDARDIZE formula. This formula just takes the original value, a center, \\nand a spread measure and returns the value with the center subtracted out divided by the \\nspread. So in B2 you would have:\\n=STANDARDIZE(SupportPersonnel!B2,\\n             SupportPersonnel!B$402,SupportPersonnel!B$403)\\nNote that you’re using absolute references on the rows only for the mean and standard \\ndeviation, so that they stay put when you copy the formula down. However, when you \\ncopy the formula across, the column will change.\\nCopy and paste B2 across through K2, highlight the range, and then double-click it to \\nsend the calculations down through K401. This yields the standardized set of data shown \\nin Figure 9-7.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 366, 'page_label': '345'}, page_content='345Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \\nFigure 9-7: The standardized set of employee performance data\\nCreating a Graph\\nA graph is nothing more than some nodes and edges. In this case, each employee is a node, \\nand to start, you can just draw edges between everybody. The length of the edge is the \\nEuclidean distance between the two employees using their standardized performance data.\\nAs you saw in Chapter 2, the Euclidean (as-the-crow-ﬂ ies) distance between two points \\nis the square root of the sum of the squared diff erences of each column value for the two.\\nIn a new sheet called Distances, create an employee-by-employee distance matrix in \\nthe exact same way as in Chapter 2, by using the \\nOFFSET formula.\\nTo start, number the employees 0 through 399 starting at A3 going down and at C1 \\ngoing across. (Tip: Type 0, 1, and 2 in the ﬁ rst three cells and then highlight those cells \\nand drag down or across. Excel will ﬁ  ll in the rest for you, because it’s smart like that.) \\nNext to these off set values, paste the employee IDs (Paste Special values transposed for \\nthe columns). This creates the empty matrix shown in Figure 9-8.\\nTo ﬁ ll in this matrix, let’s start in the ﬁ rst distance cell C3. This is the distance between \\nemployee 144624 and themselves. \\nNow, for all these distance calculations, you’re going to use the OFFSET formula anchored \\non the ﬁ rst row of standardized employee data:\\nOFFSET(Standardized!$B$2:$K$2,Some number of rows, 0 columns)\\nIn the case of cell C3, Standardized!$B$2:$K$2 is the actual row you want for employee \\n144624, so you can take the diff erences between this employee and themselves using the \\noff set formula as:\\nOFFSET(Standardized!$B$2:$K$2,Distances!$A3,0)-\\nOFFSET(Standardized!$B$2:$K$2,Distances!C$1,0)'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 367, 'page_label': '346'}, page_content='346 Data Smart\\nFigure 9-8:  Empty employee distance matrix\\nIn the ﬁ rst off set formula, you’re moving rows using the value in $A3, while in the \\nsecond off set formula you use the value in C$1 to move the OFFSET formula to another \\nemployee. Absolute references are used on these values in the appropriate places so that \\nas you copy the formula around the sheet, you’re still reading row off  sets from column \\nA and row 1.\\nThis diff erence calculation needs to be squared, summed, and then square rooted to \\nget the full Euclidean distance:\\n{=SQRT(SUM((OFFSET(Standardized!$B$2:$K$2,Distances!$A3,0)\\n   -OFFSET(Standardized!$B$2:$K$2,Distances!C$1,0))^2))}\\nNote that this calculation is an array formula due to the diff erence of entire rows from \\neach other. So you have to press Ctrl+Shift+Enter (Command+Return on a Mac) to make \\nit work.\\nThe Euclidean distance of employee 144624 from his/herself is, naturally, 0. This for-\\nmula can be copy and pasted through OL2. Then highlight this range and double-click \\nthe bottom corner to send the calculation down through cell OL402. This gives you the \\nsheet shown in Figure 9-9.\\nAnd that’s it! Now you have an employee-by-employee graph. You could export it to \\nGephi like you did in Chapter 5 and take a peak at it, but since it has 16,000 edges and \\nonly 400 nodes, it would be a mess.\\nSimilarly to how in Chapter 5 you constructed an r-neighborhood graph out of the \\ndistance matrix, in this chapter you’re going to focus on only the nearest k neighbors of \\neach employee in order to ﬁ nd the outliers.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 368, 'page_label': '347'}, page_content='347Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \\nThe first step is ranking the distance of each employee in relation to each other \\nemployee. This ranking will yield the ﬁ rst and most basic method for highlighting outli-\\ners on the graph.\\nFigure 9-9: The employee distance matrix\\nGetting the k Nearest Neighbors\\nCreate a new tab called Rank. Paste the employee IDs starting down at A2 and across at \\nB1 to form a grid, as on the previous tab.\\nNow you need to rank each employee going across the top according to his or her dis-\\ntance to each employee in column A. Start the rankings at 0, just so that rank 1 will go \\nto an actual other employee, and all the 0s will stay on the diagonal of the graph (due to \\nself-distances always being the smallest). \\nStarting in B2, the ranking of employee 144624 in relation to him/herself is written \\nusing the \\nRANK formula:\\n=RANK(Distances!C3,Distances!$C3:$OL3,1)-1\\nThis -1 at the end of the formula gives this self-distance a rank of 0 instead of 1. Note \\nthat you lock down columns C through OL on the Distances tab with absolute references, \\nwhich allows you to copy this formula to the right.\\nCopying this formula one to the right, C2, you are now ranking employee 142619 in \\nrelationship to their distance from employee 144624:\\n=RANK(Distances!D3,Distances!$C3:$OL3,1)-1'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 369, 'page_label': '348'}, page_content='348 Data Smart\\nThis returns a rank of 194 out of 400, so these two folks aren’t exactly buds (see \\nFigure 9-10).\\nFigure 9-10:  Employee 142619 ranked by distance in relation to 144624\\nCopy this formula throughout the sheet. You’ll get the full ranking matrix pictured in \\nFigure 9-11.\\nFigure 9-11: Each employee on the column ranked in relation to each row\\nGraph Outlier Detection Method 1: Just Use the Indegree\\nIf you wanted to assemble a k nearest neighbors (kNN) graph using the Distances and \\nRank sheets, all you’d need to do is delete any edge in the Distances sheet (set its cell to'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 370, 'page_label': '349'}, page_content='349Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \\nblank) whose rank was greater than k. For k = 5, you’d drop all the distances with a rank \\non the Rank sheet that was 6 or over.\\nWhat would it mean to be an outlier in this context? Well, an outlier wouldn’t get picked \\nall that often as a “nearest neighbor,” now would it?\\nSay you created a 5NN graph, so you kept only those edges with a rank of 5 or less. If \\nyou scroll down a column, such as column B for employee 144624, how many times does \\nthis employee end up in the top-ﬁ ve ranks for all the other employees? That is, how many \\nemployees choose 144624 as one of their top ﬁ ve neighbors? Not many. I’m eyeballing none, \\nin fact, except for its self-distance on the diagonal with a rank of 0, which you can ignore. \\nHow about if you made a 10NN? Well, in that case employee 139071 on row 23 hap-\\npens to consider 144624 its ninth nearest neighbor. This means that in the 5NN graph \\nemployee 144624 has an indegree of 0, whereas in the 10NN graph employee 144624 has \\nan indegree of 1. \\nThe indegree is the count of the number of edges going into any node on a graph. The lower \\nthe indegree, the more of an outlier you are, because no one wants to be your neighbor.\\nAt the bottom of column B on the Rank sheet, count up the indegree for employee \\n144624 for the cases of 5, 10, and 20 nearest neighbor graphs. You can do this using a \\nsimple \\nCOUNTIF formula (subtracting out 1 for the self-distance on the diagonal which \\nyou’re ignoring). So, for example, to count up the indegree for employee 144624 in a 5NN \\ngraph, you’d use the following formula in cell B402:\\n=COUNTIF(B2:B401,”<=5”)–1\\nSimilarly below it, you could calculate the employee’s indegree if you made a 10NN \\ngraph:\\n=COUNTIF(B2:B401,”<=10”)-1\\nAnd below that for a 20NN:\\n=COUNTIF(B2:B401,”<=20”)-1\\nIndeed, you could pick any k you wanted between 1 and the number of employees \\nyou have. But you can stick with 5, 10, and 20 for now. Using the conditional formatting \\nmenu, you can highlight cells whose counts are 0 (which means there are no inbound \\nedges to the node for a graph of that size). This calculation on employee 144624 yields \\nthe tab shown in Figure 9-12.\\nHighlighting B402:B404, you can drag the calculations to the right through column \\nOK. Scrolling through the results, you can see that some employees may be considered \\noutliers at the 5NN mark but not necessarily at the 10NN mark (if you deﬁ  ne an outlier \\nas an employee with a 0 indegree—you could use another number if you liked).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 371, 'page_label': '350'}, page_content='350 Data Smart\\nFigure 9-12: The indegree counts for three different nearest neighbor graphs\\nThere are only two employees who even at the 20NN graph level still have no inbound \\nedges. No one considers them even in the top 20 closest of neighbors. That’s pretty distant!\\nThose two employee IDs are 137155 and 143406. Flipping back to the SupportPersonnel \\ntab, you can investigate. Employee 137155 is on row 300 (see Figure 9-13). They have a \\nhigh ticket average, high customer rating, and they appear to be a good Samaritan. They’ve \\ntaken lots of weekend shifts, graveyard shifts, and they’ve off ered on seven occasions to \\nswap shifts with an employee who needed it. Nice! This is someone who across multiple \\ndimensions is exceptional enough that they’re not even in the top 20 distances to any other \\nemployee. That’s pretty amazing. Maybe this employee deserves a pizza party or something.\\nFigure 9-13: The performance data for employee 137155'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 372, 'page_label': '351'}, page_content='351Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \\nWhat about the other employee—143406? They’re on row 375, and they’re an interesting \\ncontrast to the previous employee (see Figure 9-14). No metric by itself is enough to ﬁ re \\nthem, but that said, their ticket number is two standard deviations below the average, their \\ncustomer rating is likewise a couple of standard deviations down the distribution. Their \\ntardies are above average, and they’ve taken ﬁ ve out of six sick days on a Friday. Hmmm. \\nThis employee has taken plenty of employee development, which is a plus. But maybe \\nthat’s because they just enjoy getting out of taking tickets. Perhaps employee dev should \\nstart being graded. And they’ve requested four shift swaps without off ering to swap with \\nsomeone else. \\nThis employee feels like they’re working the system. While meeting the minimum \\nrequirements for employment (note they’re not jumping any Tukey fences here), they seem \\nto be skating by at the bad end of every distribution.\\nFigure 9-14: The performance data for employee 143406\\nGraph Outlier Detection Method 2: Getting Nuanced with \\nk-Distance\\nOne of the drawbacks of the previous method is that for a given kNN graph you either get \\nan inbound edge from someone or you don’t. And that means that you get large shifts in \\nwho’s an outlier and who’s not one, depending on the value of k you pick. This example \\nended up trying 5, 10, and 20 before you were left with just two employees. And of those \\ntwo employees, which one was the biggest outlier? Beats me! They both had an indegree \\nof 0 on the 20NN, so they were kinda tied, right?\\nWhat would be nice is to have a calculation that assigned an employee a continuous \\ndegree of outlying-ness. The next two methods you’ll look at attempt to do just that. First, \\nyou’ll look at ranking outliers using a quantity called the k-distance.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 373, 'page_label': '352'}, page_content='352 Data Smart\\nThe k-distance is the distance from an employee to their kth neighbor. \\nNice and simple, but since it’s giving back a distance rather than a count, you can get \\na nice ranking out of the value. Create a new tab in the workbook called K-Distance to \\ntake a look.\\nFor k, use 5, which means you’ll grab everyone’s distance to their ﬁ fth closest neighbor. \\nOne way to think of this is that if the neighborhood where I live has ﬁ  ve neighbors and \\nmyself, how much land does that neighborhood sit on? If I have to walk 30 minutes to \\nmake it to my ﬁ fth neighbor’s house, then maybe I live in the boonies. \\nSo label A1 as How many employees are in my neighborhood? and put a 5 in B1. This \\nis your k value.\\nStarting in A3, label the column Employee ID and paste the employee IDs down. Then \\nyou’ll start calculating the k-distance with that of employee 144624 in cell B4.\\nNow, how do you calculate the distance between 144624 and his ﬁ fth closest neighbor? \\nThe ﬁ fth closest employee will be ranked 5 on row 2 (144624’s row) of the Rank tab. So \\nyou can just use an \\nIF statement to set that value to 1 in a vector of all 0s, and then mul-\\ntiply that vector times the distances row for 144624 on the Distances tab. Finally, sum \\neverything up. \\nThus, in B4 you’d have:\\n{=SUM(IF(Rank!B2:OK2=$B$1,1,0)*Distances!C3:OL3)}\\nNote that the k value in cell B1 is locked down with absolute references, so you can \\ncopy the formula down. Also, this is an array formula since the IF statement is checking \\nan entire array of values.\\nDouble-click the formula to send it down the sheet and apply some conditional for-\\nmatting to highlight the large distances. Once again, the two outliers from the previous \\nsection rise to the top (see Figure 9-15).\\nFigure 9-15: Employee 143406 has a high 5-distance'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 374, 'page_label': '353'}, page_content='353Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \\nThis time around, you get a little more nuance. You can see in this single list that the \\nbad employee, 143406, is substantially more distant than 137155, and both of those values \\nare substantially larger than the next largest value of 3.53.\\nBut there’s a drawback to this approach, which is visualized in Figure 9-16. Merely using \\nk-distance gives you a sense of global outlying-ness, that is, you can highlight points that \\nare farther away from their neighbors than any other points. But when you look at Figure \\n9-16, the triangular point is clearly the outlier, and yet, its k-distance is going to be less \\nthan that of some of the diamond shape points.\\nAre those diamonds really weirder than that triangle? Not to my eyes!\\nThe issue here is that the triangle is not a global outlier, so much as it is a local outlier. \\nThe reason why your eyeballs pick it up as the odd point out is that it’s nearest to the tight \\ncluster of circles. If the triangle were among the spaced-out diamonds, it’d be ﬁ ne. But it’s \\nnot. Instead, it looks nothing like its circular neighbors.\\nThis leads to a cutting-edge technique called local outlier factors (LOF).\\nFigure 9-16: k-distance fails on local outliers\\nGraph Outlier Detection Method 3: Local Outlier Factors Are \\nWhere It’s At\\nJust like using k-distance, local outlier factors provide a single score for each point. The \\nlarger the score, the more of an outlier they are. But LOF gives you something a little \\ncooler than that: The closer the score is to 1, the more ordinary the point is locally. As'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 375, 'page_label': '354'}, page_content='354 Data Smart\\nthe score increases, the point should be considered less typical and more like an outlier. \\nAnd unlike k-distance, this “1 is typical” fact doesn’t change no matter the size or scale \\nof your graph, which is really cool.\\nAt a high level here’s how it works: You are an outlier if your k nearest neighbors consider \\nyou farther away than their neighbors consider them. The algorithm cares about a point’s \\nfriends and friends-of-friends. That’s how it deﬁ nes “local.”\\nLooking back at Figure 9-16 this is exactly what makes the triangle an outlier, isn’t it? \\nIt may not have the highest k-distance, but the ratio of the triangle’s distance to its nearest \\nneighbors over their distance to each other is quite high (see Figure 9-17). \\nFigure 9-17: The triangle is not nearly as reachable by its neighbors as the neighbors are by each other\\nStarting with Reach Distance\\nBefore you can put together your local outlier factors for each employee, you need to cal-\\nculate one more set of numbers, called reachability distances. \\nThe reachability distance of employee A with respect to employee B is just their ordinary \\ndistance, unless A is within B’s k-distance neighborhood, in which case the reachability distance \\nis just B’s k-distance.\\nIn other words, if A is inside B’s neighborhood, you round up A’s distance to B to the \\nsize of B’s neighborhood; otherwise, you leave it alone.\\nUsing reachability distance rather than ordinary distance for LOF helps stabilize the \\ncalculation a bit. \\nCreate a new tab called Reach-dist and replace the distances from the Distances tab \\nwith the new reach distances.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 376, 'page_label': '355'}, page_content='355Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \\nFirst thing you’ll want to do is Paste Special the transposed values from the K-Distance \\ntab across the top of the tab, and then paste the employee-by-employee grid, like on the \\nDistances tab starting in row 3. This gives you the empty sheet shown in Figure 9-18.\\nFigure 9-18: The skeleton of the reach distance tab\\nStarting in cell B4, you’re going to slide in the distance of 144624 to itself from the \\nDistances tab (Distances!C3) unless it’s less than the k-distance above in B1. It’s a simple \\nMAX formula:\\n=MAX(B$1,Distances!C3)\\nThe absolute reference on the k-distance allows you to copy the formula around the \\nsheet. Copying the formula through OK4, you can then highlight the calculations on row 4 \\nand double-click them to send them through row 403. This ﬁ lls in all the reach distances, \\nas shown in Figure 9-19.\\nPutting Together the Local Outlier Factors\\nNow you’re ready to calculate each employee’s local outlier factor. To start, create a new \\ntab called LOF and paste the employee IDs down column A.\\nAs stated earlier, local outlier factors gauge how a point is viewed by its neighbors versus \\nhow those neighbors are viewed by their neighbors. If I’m 30 miles outside of town, my \\nclosest neighbors may view me as a redneck, whereas they are viewed by their neighbors \\nas members of the community. That means that locally I’m viewed more as an outlier than \\nmy neighbors are. You want to capture that phenomenon.\\nThese values hinge on the average reachability of each employee with respect to his k \\nnearest neighbors.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 377, 'page_label': '356'}, page_content='356 Data Smart\\nFigure 9-19:  All reach distances\\nConsider employee 144624 on row 2. You’ve already set k to 5, so the question is, what \\nis the average reachability distance of 144624 with respect to that employee’s ﬁ ve nearest \\nneighbors? \\nTo calculate this, pull a vector of 1s from the Rank tab for the ﬁ ve employees closest to \\n144624 and 0s for everyone else (similar to what you did on the K-Distance tab). Such a \\nvector can be created using \\nIF formulas to grab the top-ranked neighbors while exclud-\\ning the actual employee:\\nIF(Rank!B2:OK2<=’K-Distance’!B$1,1,0)*IF(Rank!B2:OK2>0,1,0)\\nMultiply this indicator vector times 144624’s reach distances, sum up the product, and \\ndivide them by k=5. In cell B2, then, you have:\\n=SUM(IF(Rank!B2:OK2<=’K-Distance’!B$1,1,0)*\\nIF(Rank!B2:OK2>0,1,0)*\\n‘Reach-dist’!B4:OK4)/’K-Distance’!B$1}\\nJust as when you calculated k-distance, this is an array formula. You can send this \\nformula down the sheet by double-clicking it (see Figure 9-20).\\nSo this column indicates how the ﬁ ve nearest neighbors of each employee view them.\\nThe local outlier factor then for an employee is the average of the ratios of the employee’s \\naverage reachability distance divided by the average reachability distances of each of their k \\nneighbors.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 378, 'page_label': '357'}, page_content='357Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \\nFigure 9-20: Average reachability for each employee with respect to his neighbors\\nYou will tackle the LOF calculation for employee 144624 in cell C2 ﬁ  rst. Just as in \\nprevious calculations, the following IF statements give you a vector of 1s for 144624’s top \\nﬁ ve nearest neighbors:\\nIF(Rank!B2:OK2<=’K-Distance’!B$1,1,0)*IF(Rank!B2:OK2>0,1,0)\\nYou then multiply the ratio of 144624’s average reachability divided by each neighbor’s \\naverage reachability as:\\nIF(Rank!B2:OK2<=’K-Distance’!B$1,1,0)\\n  *IF(Rank!B2:OK2>0,1,0)*B2/TRANSPOSE(B$2:B$401)\\nNote that the neighbors’ reachability distances referenced in range B2:B401 on the \\nbottom of the ratio are transposed so that the column is turned into a row, just like the \\nvectors coming out of the \\nIF statements in the equation.\\nYou can average these ratios by summing them and dividing by k:\\n{=SUM(IF(Rank!B2:OK2<=\\n   ‘K-Distance’!B$1,1,0)\\n   *IF(Rank!B2:OK2>0,1,0)\\n   *B2/TRANSPOSE(B$2:B$401))/’K-Distance’!B$1}\\nNote the curly braces since this is an array formula. Press Control+Shift+Enter \\n(Command+Return on Mac) to get back the LOF factor for 144624.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 379, 'page_label': '358'}, page_content='358 Data Smart\\nIt’s 1.34, which is somewhat over a value of 1, meaning that this employee is a bit of a \\nlocal outlier.\\nYou can send this formula down the sheet by double-clicking and then check out \\nthe other employees. Conditional formatting is helpful to highlight the most signiﬁ  cant \\noutliers.\\nLo and behold, when you scroll down you ﬁ  nd that employee 143406, the resident \\nslacker, is the most outlying point with an LOF of 1.97 (see Figure 9-21). His neighbors \\nview him as twice as distant as they are viewed by their neighbors. That’s pretty far out-\\nside the community.\\nFigure 9-21: LOFs for the employees. Somebody is knocking on the door of 2.\\nAnd that’s it! You now have a single value assigned to each employee that ranks them \\nas a local outlier and is scaled the same no matter the size of the graph. Pretty ﬂ  ippin’ \\nawesome.\\nWrapping Up\\nBetween the graph modularity chapter and this chapter on outlier detection, you’ve been \\nexposed to the power of analyzing a dataset by “graphing” your data, that is, assigning \\ndistances and edges between your observations. \\nAlthough in the clustering chapters, you mined groups of related points for insights, \\nhere you mined the data for points outside of communities. You saw the power of some-\\nthing as simple as indegree to demonstrate who’s inﬂ uential and who’s isolated.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 380, 'page_label': '359'}, page_content='359Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \\nFor more on outlier detection, check out the 2010 survey put together by Kriegel, Kroger, \\nand Zimek at http://www.siam.org/meetings/sdm10/tutorial3.pdf for the 2010 SIAM \\nconference. All the techniques in this chapter show up there along with a number of others.\\nNote that these techniques don’t require any kind of arbitrarily long-running process \\nthe way optimization models might. There are a ﬁ nite number of steps to get LOFs, so this \\nkind of thing can be coded in production on top of a database quite easily. \\nIf you’re looking for a good programming language to do this stuff  in, R is the way to \\ngo. The bplot function in R provides box plots of data with Tukey fences built in. The \\nability to plot Tukey fences graphically is something so painful in Excel that I didn’t even \\nbother putting it in this book, so the bplot function is a huge plus for R.\\nAlso in R, the DMwR package (which accompanies the excellent Data Mining with R \\nbook by Torgo [Chapman and Hall, 2010]) includes an implementation of LOF in a func-\\ntion called lofactor. To construct and analyze the degree of nodes in a graph, the igraph \\npackage in Python and R is the way  to go.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 381, 'page_label': '360'}, page_content=''),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 382, 'page_label': '361'}, page_content='10\\nA\\nfter spending the previous nine chapters injecting Excel directly into your veins, I’m \\nnow going to tell you to drop it. Well, not for everything, but let’s be honest, Excel \\nis not ideal for all analytics tasks.\\nExcel is awesome for learning analytics, because you can touch and see your data in \\nevery state as an algorithm changes it from input into output. But you came, you saw, \\nyou learned. Do you really need to go through all those steps manually every time? For \\nexample, do you really need to bake up your own optimization formulation to ﬁ t your own \\nlogistic regressions? Do you need to input the deﬁ nitions of cosine similarity all yourself? \\nNow that you’ve learned it, you’re allowed to cheat and have someone else do that for \\nyou! Think of yourself as Wolfgang Puck. Does he cook everything at all his restaurants? \\nI sure hope not; otherwise, his skills vary wildly from airport to real world. Now that \\nyou’ve learned this stuff , you too should feel comfortable using other folks’ implementa-\\ntions of these algorithms.\\nAnd that, among many other things (for example, referencing a whole table of data using \\none word) is why moving from Excel into the analytics-focused programming language \\ncalled R is worth doing.\\nThis chapter runs some of the previous chapters’ analyses in R rather than Excel—same \\ndata, same algorithms, diff erent environment. You’ll see how easy this stuff  can be!\\nNow, just as a warning, this chapter is not an intro tutorial of R. I’m going to be \\nmoving at a thousand miles an hour to hit a few algorithms in a single chapter. If you \\nwant a more comprehensive introduction, check out the books I recommend at the end \\nof this chapter.\\nAnd if you haven’t read the previous chapters to this point, this isn’t going to make \\na lick of sense, because I’m going to assume that you are already familiar with the data, \\nproblems, and techniques from earlier chapters. This ain’t a “choose your own adventure” \\nnovel. Read everything else and come back!\\nMoving from \\nSpreadsheets into R'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 383, 'page_label': '362'}, page_content='362 Data Smart\\nGetting Up and Running with R\\nYou can download R from the R website at www.r-project.org. Just click the download \\nlink, pick a mirror nearest you, and download the installer for your OS.\\nRun through the installer (on Windows it’s nice to install the software as the admin-\\nistrator) and then open the application. On Windows and Mac, the R console is going to \\nload. It looks something like Figure 10-1.\\nFigure 10-1:  The R console on Mac OS\\nInside the R console, you type commands into the > prompt and press Return to get \\nthe system to do anything. Here’s a couple for you:\\n> print(\"No regrets. Texas forever.\")\\n[1] \"No regrets. Texas forever.\"\\n> 355/113\\n[1] 3.141593\\nYou can call the print  function to get the system to print out text. You can also \\ntype in arithmetic directly to make calculations. Now , my standard workflow for \\nusing R is:\\n 1. Bring data into an R.\\n 2. Do data-sciency things with data.\\n 3. Dump results out of R where some other person or process can use them.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 384, 'page_label': '363'}, page_content='363Moving from Spreadsheets into R \\nWhen it comes to the ﬁ rst step, bringing data in R, there are all sorts of options, but in \\norder to understand variables and datatypes, you’ll start simply by entering data manually.\\nSome Simple Hand-Jamming\\nThe simplest way to get data in R is the same way you get it into Excel. By typing it with \\nyour ﬁ ngers and storing those keystrokes somewhere. You can start by storing a single \\nvalue in a variable:\\n> almostpi <- 355/113\\n> almostpi\\n[1] 3.141593\\n> sqrt(almostpi)\\n[1] 1.772454\\nIn this little bit of code, you are storing 355/113 in a variable called almostpi. Then \\nby typing the variable back into the console and pressing Return, you can print its con-\\ntents. You can then act on that variable with a variety of functions (this example calls \\nthe square root). \\nFor a quick reference of many of the built-in functions R has (functions available without \\nloading packages ... something you’re building toward), check out the R reference card at \\nhttp://cran.r-project.org/doc/contrib/Short-refcard.pdf.\\nTo understand what a function does, just type a question mark before it when you put \\nit into the console:\\n> ?sqrt\\nThis will pop open a Help window on the function (see Figure 10-2 for the Help win-\\ndow on sqrt). \\nYou can also type two question marks in front of functions to do a search for informa-\\ntion, like the following:\\n> ??log\\nThe log search yields the results shown in Figure 10-3.\\nNOTE\\nThere are all sorts of great resources for ﬁ nding out what functions and packages are avail-\\nable to you in R besides the whole ?? rigmarole. For example, rseek.org is a great search \\nengine for R-related content. And you can post speciﬁ c questions to stackoverflow.com \\n(see http://stackoverflow.com/questions/tagged/r) and the R mailing list (see http://\\nwww.r-project.org/mail.html).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 385, 'page_label': '364'}, page_content='364 Data Smart\\nFigure 10-2: The Help window for the square root function\\nVector Math and Factoring\\nYou can insert a vector of numbers using the c() function (the c stands for “combine”). \\nToss some primes into a variable:\\n> someprimes <- c(1,2,3,5,7,11)\\n> someprimes\\n[1]  1  2  3  5  7 11'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 386, 'page_label': '365'}, page_content='365Moving from Spreadsheets into R \\nFigure 10-3:  Search results for the word log\\nUsing the Length() function, you can count the number of elements you have in your \\nvector:\\n> length(someprimes)\\n[1] 6\\nYou can also reference single values in the vector using bracket notation:\\n> someprimes[4]\\n[1] 5\\nThis gives back the fourth value in the vector, which happens to be 5. You can provide \\nvectors of indices using the c() function or a : character to specify a range:\\n> someprimes[c(4,5,6)]\\n[1]  5  7 11\\n> someprimes[4:6]\\n[1]  5  7 11'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 387, 'page_label': '366'}, page_content='366 Data Smart\\nIn both of these cases, you’re grabbing the fourth through sixth values of the vector. \\nYou can also use logical statements to pull out values. For instance, if you only wanted \\nprimes less than seven, you could use the \\nwhich() function to return their indices:\\n> which(someprimes<7)\\n[1] 1 2 3 4\\n> someprimes[which(someprimes<7)]\\n[1] 1 2 3 5\\nOnce you’ve placed your data in a variable, you can perform operations on the entire \\ndataset and store the results in a new variable. For example, you can multiply all the data \\nby two:\\n> primestimes2 <- someprimes*2\\n> primestimes2\\n[1]  2  4  6 10 14 22\\nThink about how you do this in Excel. You enter the formula in the adjacent column \\nand copy it down. R lets you name that column or row of data and operate on that variable \\nas a single entity, which is neat.\\nOne useful function for checking your data for wonky entries is the \\nsummary function:\\n> summary(someprimes)\\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \\n  1.000   2.250   4.000   4.833   6.500  11.000\\nAnd you can work with text data too:\\n> somecolors <- c(\"blue\",\"red\",\"green\",\"blue\",\\n\"green\",\"yellow\",\"red\",\"red\")\\n> somecolors\\n[1] \"blue\"   \"red\"    \"green\"  \"blue\"   \"green\"  \"yellow\" \"red\"    \"red\"\\nIf you summarize somecolors, all you get is a little bit of descriptive data:\\n> summary(somecolors)\\n   Length     Class      Mode \\n        8 character character\\nBut you can treat these colors as categories and make this vector into categorical data \\nby “factoring” it:\\n> somecolors <- factor(somecolors)\\n> somecolors\\n[1] blue   red    green  blue   green  yellow red    red   \\nLevels: blue green red yellow\\nNow when you summarize the data, you get back counts for each “level” (a level is \\nessentially a category):'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 388, 'page_label': '367'}, page_content='367Moving from Spreadsheets into R \\n> summary(somecolors)\\n  blue  green    red yellow \\n     2      2      3      1\\nTwo-Dimensional Matrices\\nThe vectors you’ve been playing with so far are one-dimensional. Something more akin \\nto a spreadsheet in R might be a matrix, which is a two-dimensional array of numbers. \\nYou can construct one with the \\nmatrix function:\\n> amatrix <- matrix(data=c(someprimes,primestimes2),nrow=2,ncol=6)\\n> amatrix\\n     [,1] [,2] [,3] [,4] [,5] [,6]\\n[1,]    1    3    7    2    6   14\\n[2,]    2    5   11    4   10   22\\nYou can count columns and rows:\\n> nrow(amatrix)\\n[1] 2\\n> ncol(amatrix)\\n[1] 6\\nIf you want to transpose the data (just as you did throughout the book using Excel’s \\nPaste Special transpose functionality), you use the t() function:\\n> t(amatrix)\\n     [,1] [,2]\\n[1,]    1    2\\n[2,]    3    5\\n[3,]    7   11\\n[4,]    2    4\\n[5,]    6   10\\n[6,]   14   22\\nTo grab individual records or ranges, you use the same bracket notation, except you \\nseparate column and row references with a comma:\\n> amatrix[1:2,3]\\n[1]  7 11\\nThis gives back rows 1 through 2 for column 3. But you need not reference row 1 and \\n2 since that’s all the rows you have—you can instead leave that portion of the bracket \\nblank and all the rows will be printed:\\n> amatrix[,3]\\n[1]  7 11'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 389, 'page_label': '368'}, page_content='368 Data Smart\\nUsing the rbind() and cbind() functions, you can smush new rows and columns of \\ndata into the matrix:\\n> primestimes3 <- someprimes*3\\n> amatrix <- rbind(amatrix,primestimes3)\\n> amatrix\\n             [,1] [,2] [,3] [,4] [,5] [,6]\\n                1    3    7    2    6   14\\n                2    5   11    4   10   22\\nprimestimes3    3    6    9   15   21   33\\nHere you’ve created a new row of data (primestimes3) and used rbind() on the amatrix \\nvariable to tack primestimes3 onto it and assign the result back into amatrix.\\nThe Best Datatype of Them All: The Dataframe\\nA dataframe is the ideal way to work with real world, database table-style data in R. A \\ndataframe in R is a speciﬁ c version of the “list” datatype. So what’s a list? A list is a col-\\nlection of objects in R that can be of diff erent types. For instance, here’s a list with some \\ninfo about yours truly:\\n> John <- list(gender=\"male\", age=\"ancient\", height = 72,\\n               spawn = 3, spawn_ages = c(.5,2,5))\\n> John\\n$gender\\n[1] \"male\"\\n$age\\n[1] \"ancient\"\\n$height\\n[1] 72\\n$spawn\\n[1] 3\\n$spawn_ages\\n[1] 0.5 2.0 5.0\\nA dataframe is a type of list that looks eerily similar to an Excel sheet. Essentially, it’s a \\ntwo-dimensional column-oriented sheet of data where columns can be treated as numeric \\nor categorical vectors. You can create a dataframe by calling the \\ndata.frame() function \\non arrays of imported or jammed-in data. The following example uses data from James \\nBond ﬁ lms to illustrate. First, create some vectors:\\n> bondnames <- c(\"connery\",\"lazenby\",\"moore\",\"dalton\",\"brosnan\",\"craig\")\\n> firstyear <- c(1962,1969,1973,1987,1995,2006)\\n> eyecolor <- c(\"brown\",\"brown\",\"blue\", \"green\", \"blue\", \"blue\")\\n> womenkissed <- c(17,3,20,4,12,4)\\n> countofbondjamesbonds <- c(3,2,10,2,5,1)'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 390, 'page_label': '369'}, page_content='369Moving from Spreadsheets into R \\nSo at this point you have ﬁ ve vectors—some text, some numeric—and all are the same \\nlength. You can combine them into a single dataframe called bonddata like so:\\n> bonddata <- data.frame(bondnames,firstyear,eyecolor,womenkissed,\\ncountofbondjamesbonds)\\n> bonddata\\n  bondnames firstyear eyecolor womenkissed countofbondjamesbonds\\n1   connery      1962    brown          17                     3\\n2   lazenby      1969    brown           3                     2\\n3     moore      1973     blue          20                    10\\n4    dalton      1987    green           4                     2\\n5   brosnan      1995     blue          12                     5\\n6     craig      2006     blue           4                     1\\nThe data.frame function is going to take care of recognizing which of these columns \\nare factors and which are numeric. You can see this diff  erence by calling the str() and \\nsummary() functions (the str stands for “structure”):\\n> str(bonddata)\\n\\'data.frame’: 6 obs. of  5 variables:\\n$ bondnames            : Factor w/ 6 levels \"brosnan\",\"connery\",..: \\n2 5 6 4 1 3\\n $ firstyear            : num  1962 1969 1973 1987 1995 ...\\n $ eyecolor             : Factor w/ 3 levels \"blue\",\"brown\",..: \\n2 2 1 3 1 1\\n $ womenkissed          : num  17 3 20 4 12 4\\n $ countofbondjamesbonds: num  3 2 10 2 5 1\\n> summary(bonddata)\\nbondnames  firstyear     eyecolor womenkissed    countofbondjamesbonds\\nbrosnan:1  Min.   :1962  blue :3  Min.   : 3.00  Min.   : 1.000       \\nconnery:1  1st Qu.:1970  brown:2  1st Qu.: 4.00  1st Qu.: 2.000       \\ncraig  :1  Median :1980  green:1  Median : 8.00  Median : 2.500       \\ndalton :1  Mean   :1982           Mean   :10.00  Mean   : 3.833       \\nlazenby:1  3rd Qu.:1993           3rd Qu.:15.75  3rd Qu.: 4.500       \\nmoore  :1  Max.   :2006           Max.   :20.00  Max.   :10.000 \\nNote that the year is being treated as a number. You could factorize this column using \\nthe factor() function if you wanted it treated categorically instead. \\nAnd one of the awesome things about dataframes is that you can reference each column \\nusing a $ character plus the column name, as shown:\\n> bonddata$firstyear <- factor(bonddata$firstyear)\\n> summary(bonddata)\\n   bondnames firstyear  eyecolor  womenkissed    countofbondjamesbonds\\n brosnan:1   1962:1    blue :3   Min.   : 3.00   Min.   : 1.000       \\n connery:1   1969:1    brown:2   1st Qu.: 4.00   1st Qu.: 2.000       \\n craig  :1   1973:1    green:1   Median : 8.00   Median : 2.500       \\n dalton :1   1987:1              Mean   :10.00   Mean   : 3.833       \\n lazenby:1   1995:1              3rd Qu.:15.75   3rd Qu.: 4.500       \\n moore  :1   2006:1              Max.   :20.00   Max.   :10.000'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 391, 'page_label': '370'}, page_content='370 Data Smart\\nThus, when you run the summary function, the years are rolled up by category counts \\ninstead of by distribution data. Also, keep in mind that whenever you transpose a \\ndataframe, the result is a good old two-dimensional matrix rather than another dataframe. \\nThis makes sense since the transposed version of the Bond data would not have consistent \\ndatatypes in each column.\\nReading Data into R\\nNOTE\\nThe CSV ﬁ le used in this section, “WineKMC.csv,” is available for download at the \\nbook’s website, www.wiley.com/go/datasmart.\\nOkay, so you’ve learned how to shove data into various datatypes by hand, but how \\ndo you read data in from ﬁ  les? The ﬁ rst thing you need to understand is the working \\ndirectory. The working directory is the folder in which you can put data so that the R \\nconsole can ﬁ nd it and read it in. The \\ngetwd() function displays the current working \\ndirectory:\\n> getwd()\\n[1] \"/Users/johnforeman/RHOME\"\\nIf you don’t like the present working directory, you can change it with the setwd() \\ncommand. Keep in mind, even on Windows machines R expects directory paths to be \\nspeciﬁ ed with forward slashes. For example:\\n> setwd(\"/Users/johnforeman/datasmartfiles\")\\nUse this command to set your working directory to a place where you’re happy to toss \\nsome data. You’ll start by placing the downloaded WineKMC.csv ﬁ  le in that directory. \\nThis comma-delimited ﬁ le has the data from the Matrix tab in the k-means clustering \\nworkbook from Chapter 2. Read it in and take a look.\\nTo read in data, you use the read.csv() function:\\n> winedata <- read.csv(\"WineKMC.csv\")\\nThis data should look exactly like the Matrix tab from Chapter 2, so when you print \\nthe ﬁ rst few columns (I’ve chosen nine to ﬁ t on this page) you see descriptive data about \\neach of the 32 off ers followed by some customers’ click vectors in columns:\\n> winedata[,1:9]\\n   Offer  Mth   Varietal MinQty Disc    Origin PastPeak Adams Allen\\n1      1  Jan     Malbec     72   56    France    FALSE    NA    NA\\n2      2  Jan Pinot Noir     72   17    France    FALSE    NA    NA'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 392, 'page_label': '371'}, page_content='371Moving from Spreadsheets into R \\n3      3  Feb  Espumante    144   32    Oregon     TRUE    NA    NA\\n4      4  Feb  Champagne     72   48    France     TRUE    NA    NA\\n5      5  Feb Cab. Sauv.    144   44        NZ     TRUE    NA    NA\\n6      6  Mar   Prosecco    144   86     Chile    FALSE    NA    NA\\n7      7  Mar   Prosecco      6   40 Australia     TRUE    NA    NA\\n8      8  Mar  Espumante      6   45 S. Africa    FALSE    NA    NA\\n9      9  Apr Chardonnay    144   57     Chile    FALSE    NA     1\\n10    10  Apr   Prosecco     72   52        CA    FALSE    NA    NA\\n11    11  May  Champagne     72   85    France    FALSE    NA    NA\\n12    12  May   Prosecco     72   83 Australia    FALSE    NA    NA\\n13    13  May     Merlot      6   43     Chile    FALSE    NA    NA\\n14    14  Jun     Merlot     72   64     Chile    FALSE    NA    NA\\n15    15  Jun Cab. Sauv.    144   19     Italy    FALSE    NA    NA\\n16    16  Jun     Merlot     72   88        CA    FALSE    NA    NA\\n17    17  Jul Pinot Noir     12   47   Germany    FALSE    NA    NA\\n18    18  Jul  Espumante      6   50    Oregon    FALSE     1    NA\\n19    19  Jul  Champagne     12   66   Germany    FALSE    NA    NA\\n20    20  Aug Cab. Sauv.     72   82     Italy    FALSE    NA    NA\\n21    21  Aug  Champagne     12   50        CA    FALSE    NA    NA\\n22    22  Aug  Champagne     72   63    France    FALSE    NA    NA\\n23    23 Sept Chardonnay    144   39 S. Africa    FALSE    NA    NA\\n24    24 Sept Pinot Noir      6   34     Italy    FALSE    NA    NA\\n25    25  Oct Cab. Sauv.     72   59    Oregon     TRUE    NA    NA\\n26    26  Oct Pinot Noir    144   83 Australia    FALSE    NA    NA\\n27    27  Oct  Champagne     72   88        NZ    FALSE    NA     1\\n28    28  Nov Cab. Sauv.     12   56    France     TRUE    NA    NA\\n29    29  Nov  P. Grigio      6   87    France    FALSE     1    NA\\n30    30  Dec     Malbec      6   54    France    FALSE     1    NA\\n31    31  Dec  Champagne     72   89    France    FALSE    NA    NA\\n32    32  Dec Cab. Sauv.     72   45   Germany     TRUE    NA    NA\\nIt’s all in! But you’ll notice that the blank spaces in purchase vectors (which Excel treats \\nas zeroes) have become NA values. You need to make those NA values 0, which you can do \\nusing the is.na() function inside of brackets:\\n> winedata[is.na(winedata)] <- 0\\n> winedata[1:10,8:17]\\n   Adams Allen Anders Bailey Baker Barnes Bell Bennett Brooks Brown\\n1      0     0      0      0     0      0    0       0      0     0\\n2      0     0      0      0     0      0    1       0      0     0\\n3      0     0      0      0     0      0    0       0      1     0\\n4      0     0      0      0     0      0    0       0      0     0\\n5      0     0      0      0     0      0    0       0      0     0\\n6      0     0      0      0     0      0    0       0      0     0\\n7      0     0      0      1     1      0    0       0      0     1\\n8      0     0      0      0     0      0    0       1      1     0\\n9      0     1      0      0     0      0    0       0      0     0\\n10     0     0      0      0     1      1    0       0      0     0\\nBam! NA becomes 0.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 393, 'page_label': '372'}, page_content='372 Data Smart\\nDoing Some Actual Data Science\\nAt this point you’ve learned how to work with variables and datatypes, hand-jam data, \\nand read it in from a CSV. But how do you actually use the algorithms you learned earlier \\nin this book? Since you already have the wine data loaded up, you’ll start with a little \\nk-means clustering.\\nSpherical K-Means on Wine Data in Just a Few Lines\\nIn this section, you’ll cluster based on cosine similarity (also called spherical k-means). \\nAnd in R, there’s a spherical k-means package you can load, called skmeans. But skmeans \\ndoesn’t come baked into R; it’s written by a third party as a package that you can load into \\nR and use. Essentially, these geniuses have done all the work for you, and you just have \\nto stand on their shoulders.\\nLike most R packages, you can read up on it and install it from the Comprehensive R \\nArchive Network (CRAN). CRAN is a repository of many of the useful packages that can \\nbe loaded into R to extend its functionality. A list of all the packages you can download \\nfrom CRAN is available here: \\nhttp://cran.r-project.org/web/packages/.\\nJust search for “spherical k means” in rseek.org and a PDF explaining the package \\ncomes up as the ﬁ rst result. There’s a function called skmeans() that you want.\\nR is initially set up to download packages from CRAN, so to get the skmeans package \\nyou need only use the install.packages()  function (R may ask to set up a personal \\nlibrary the ﬁ rst time you do this):\\n> install.packages(\"skmeans\",dependencies = TRUE)\\ntrying URL \\'http://mirrors.nics.utk.edu/cran/bin/macosx/leopard/\\n                   contrib/2.15/skmeans_0.2-3.tgz’\\nContent type \\'application/x-gzip’ length 224708 bytes (219 Kb)\\nopened URL\\n==================================================\\ndownloaded 219 Kb\\nThe downloaded binary packages are in\\n   /var/…/downloaded_packages\\nYou can see in the code that I set dependencies = TRUE  in the installation call. This \\nensures that if the skmeans package is dependent on any other packages, R downloads \\nthose packages as well. The call downloads the appropriate package for my R installation \\n(version 2.15 on Mac) from a mirror and puts it where it needs to go. \\nYou can then load the package using the \\nlibrary() function:\\n> library(skmeans)'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 394, 'page_label': '373'}, page_content='373Moving from Spreadsheets into R \\nYou can look up how to use the skmeans() function using the ? call. The documenta-\\ntion speciﬁ es that skmeans() accepts a matrix where each row corresponds to an object \\nto cluster. \\nYour data on the other hand is column-oriented with a bunch of deal descriptors at the \\nbeginning that the algorithm isn’t gonna want to see. So you need to transpose it (note \\nthat the transpose function coerces a matrix out of the dataframe). \\nUsing the \\nncol() function, you can see that the customer columns go out to column 107, \\nso you can isolate just the purchase vectors as rows for each customer by transposing the \\ndata from column 8 to 107 and shoving it in a new variable called \\nwinedata.transposed:\\n> ncol(winedata)\\n[1] 107\\n> winedata.transposed <- t(winedata[,8:107])\\n> winedata.transposed[1:10,1:10]\\n        [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\\nAdams      0    0    0    0    0    0    0    0    0     0\\nAllen      0    0    0    0    0    0    0    0    1     0\\nAnders     0    0    0    0    0    0    0    0    0     0\\nBailey     0    0    0    0    0    0    1    0    0     0\\nBaker      0    0    0    0    0    0    1    0    0     1\\nBarnes     0    0    0    0    0    0    0    0    0     1\\nBell       0    1    0    0    0    0    0    0    0     0\\nBennett    0    0    0    0    0    0    0    1    0     0\\nBrooks     0    0    1    0    0    0    0    1    0     0\\nBrown      0    0    0    0    0    0    1    0    0     0\\nThen you can call skmeans on the dataset, specifying ﬁ ve means and the use of a genetic \\nalgorithm (much like the algorithm you used in Excel). You’ll assign the results back to \\nan object called \\nwinedata.clusters:\\n> winedata.clusters <- skmeans(winedata.transposed, 5, method=\"genetic\")\\nTyping the object back into the console, you can get a summary of its contents (your \\nresults may vary due to the optimization algorithm):\\n> winedata.clusters\\nA hard spherical k-means partition of 100 objects into 5 classes.\\nClass sizes: 16, 17, 15, 29, 23\\nCall: skmeans(x = winedata.transposed, k = 5, method = \"genetic\")\\nCalling str() on the clusters object shows you that the actual cluster assignments are \\nstored within the “cluster” list of the object:\\n> str(winedata.clusters)\\nList of 7\\n $ prototypes: num [1:5, 1:32] 0.09 0.153 0 0.141 0 ...\\n  ..- attr(*, \"dimnames\")=List of 2\\n  .. ..$ : chr [1:5] \"1\" \"2\" \"3\" \"4\" ...'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 395, 'page_label': '374'}, page_content='374 Data Smart\\n  .. ..$ : NULL\\n $ membership: NULL\\n $ cluster   : int [1:100] 5 4 1 5 2 2 1 3 3 5 ...\\n $ family    :List of 7\\n  ..$ description: chr \"spherical k-means\"\\n  ..$ D          :function (x, prototypes)  \\n  ..$ C          :function (x, weights, control)  \\n  ..$ init       :function (x, k)  \\n  ..$ e          : num 1\\n  ..$ .modify    : NULL\\n  ..$ .subset    : NULL\\n  ..- attr(*, \"class\")= chr \"pclust_family\"\\n $ m         : num 1\\n $ value     : num 38\\n $ call      : language skmeans(x = winedata.transposed,\\n                                k = 5, method = \"genetic\")\\n - attr(*, \"class\")= chr [1:2] \"skmeans\" \"pclust\"\\nSo for instance, if you wanted to pull back the cluster assignment for row 4, you’d just \\nuse the matrix notation on the cluster vector:\\n> winedata.clusters$cluster[4]\\n[1] 5\\nNow, each row is labeled with a customer’s name (because they were labeled when you \\nread them in with the read.csv() function), so you can also pull assignments by name \\nusing the row.names() function combined with the which() function:\\n> winedata.clusters$cluster[\\nwhich(row.names(winedata.transposed)==\"Wright\")\\n]\\n[1] 4\\nCool! Furthermore, you can write out all these cluster assignments using the \\nwrite.csv()  function if you cared to. Use ? to learn how to use it. Spoiler: It’s like \\nread.csv() . \\nNow, the main way you understood the clusters in Excel was by understanding the \\npatterns in the descriptors of the deals that deﬁ ned them. You counted up the total deals \\ntaken in each cluster and sorted. How do you do something similar in R?\\nTo perform the counts, you just use the aggregate() function where in the “by” ﬁ eld \\nyou specify the cluster assignments—meaning “aggregate purchases by assignment.” And \\nyou also need to specify that the type of aggregation you want is a sum as opposed to a \\nmean, min, max, median, and so on:\\naggregate(winedata.transposed,by=list(winedata.clusters$cluster),sum)\\nYou’ll use transpose to store these counts back as ﬁ  ve columns (just as they were \\nin Excel) and you’ll lop off the first row of the aggregation, which just gives back'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 396, 'page_label': '375'}, page_content='375Moving from Spreadsheets into R \\nthe cluster assignment names. Then, store all this back as a variable called winedata\\n.clustercounts :\\n> winedata.clustercounts <-t(aggregate(winedata.transposed,by=list\\n           (winedata.clusters$cluster),sum)[,2:33])\\n> winedata.clustercounts\\n    [,1] [,2] [,3] [,4] [,5]\\nV1     2    5    0    3    0\\nV2     7    3    0    0    0\\nV3     0    2    3    0    1\\nV4     0    5    1    6    0\\nV5     0    0    0    4    0\\nV6     0    8    1    3    0\\nV7     0    3    1    0   15\\nV8     0    1   15    0    4\\nV9     0    2    0    8    0\\nV10    1    4    1    0    1\\nV11    0    7    1    4    1\\nV12    1    3    0    0    1\\nV13    0    0    2    0    4\\nV14    0    3    0    6    0\\nV15    0    3    0    3    0\\nV16    1    1    0    3    0\\nV17    7    0    0    0    0\\nV18    0    1    4    0    9\\nV19    0    4    1    0    0\\nV20    0    2    0    4    0\\nV21    0    1    1    1    1\\nV22    0   17    2    2    0\\nV23    1    1    0    3    0\\nV24   12    0    0    0    0\\nV25    0    3    0    3    0\\nV26   12    0    0    3    0\\nV27    1    4    1    3    0\\nV28    0    5    0    0    1\\nV29    0    1    4    0   12\\nV30    0    4    4    1   13\\nV31    0   16    1    0    0\\nV32    0    2    0    2    0\\nAll right, so there are your counts of deals by cluster. Let’s slap those seven columns of \\ndescriptive data back on to the deals using the column bind function cbind():\\n> winedata.desc.plus.counts <- \\ncbind(winedata[,1:7],winedata.clustercounts)\\n> winedata.desc.plus.counts\\n    Offer  Mth   Varietal MinQty Disc    Origin PastPeak  1  2  3 4  5\\nV1      1  Jan     Malbec     72   56    France    FALSE  2  5  0 3  0\\nV2      2  Jan Pinot Noir     72   17    France    FALSE  7  3  0 0  0\\nV3      3  Feb  Espumante    144   32    Oregon     TRUE  0  2  3 0  1'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 397, 'page_label': '376'}, page_content='376 Data Smart\\nV4      4  Feb  Champagne     72   48    France     TRUE  0  5  1 6  0\\nV5      5  Feb Cab. Sauv.    144   44        NZ     TRUE  0  0  0 4  0\\nV6      6  Mar   Prosecco    144   86     Chile    FALSE  0  8  1 3  0\\nV7      7  Mar   Prosecco      6   40 Australia     TRUE  0  3  1 0 15\\nV8      8  Mar  Espumante      6   45 S. Africa    FALSE  0  1 15 0  4\\nV9      9  Apr Chardonnay    144   57     Chile    FALSE  0  2  0 8  0\\nV10    10  Apr   Prosecco     72   52        CA    FALSE  1  4  1 0  1\\nV11    11  May  Champagne     72   85    France    FALSE  0  7  1 4  1\\nV12    12  May   Prosecco     72   83 Australia    FALSE  1  3  0 0  1\\nV13    13  May     Merlot      6   43     Chile    FALSE  0  0  2 0  4\\nV14    14  Jun     Merlot     72   64     Chile    FALSE  0  3  0 6  0\\nV15    15  Jun Cab. Sauv.    144   19     Italy    FALSE  0  3  0 3  0\\nV16    16  Jun     Merlot     72   88        CA    FALSE  1  1  0 3  0\\nV17    17  Jul Pinot Noir     12   47   Germany    FALSE  7  0  0 0  0\\nV18    18  Jul  Espumante      6   50    Oregon    FALSE  0  1  4 0  9\\nV19    19  Jul  Champagne     12   66   Germany    FALSE  0  4  1 0  0\\nV20    20  Aug Cab. Sauv.     72   82     Italy    FALSE  0  2  0 4  0\\nV21    21  Aug  Champagne     12   50        CA    FALSE  0  1  1 1  1\\nV22    22  Aug  Champagne     72   63    France    FALSE  0 17  2 2  0\\nV23    23 Sept Chardonnay    144   39 S. Africa    FALSE  1  1  0 3  0\\nV24    24 Sept Pinot Noir      6   34     Italy    FALSE 12  0  0 0  0\\nV25    25  Oct Cab. Sauv.     72   59    Oregon     TRUE  0  3  0 3  0\\nV26    26  Oct Pinot Noir    144   83 Australia    FALSE 12  0  0 3  0\\nV27    27  Oct  Champagne     72   88        NZ    FALSE  1  4  1 3  0\\nV28    28  Nov Cab. Sauv.     12   56    France     TRUE  0  5  0 0  1\\nV29    29  Nov  P. Grigio      6   87    France    FALSE  0  1  4 0 12\\nV30    30  Dec     Malbec      6   54    France    FALSE  0  4  4 1 13\\nV31    31  Dec  Champagne     72   89    France    FALSE  0 16  1 0  0\\nV32    32  Dec Cab. Sauv.     72   45   Germany     TRUE  0  2  0 2  0\\nAnd you can sort using the order()  function inside the brackets of the dataframe. \\nHere’s a sort to discover the most popular deals for cluster 1 (note that I put a minus sign \\nin front of the data to sort descending. Alternatively, you can set the \\ndecreasing=TRUE  \\nﬂ ag in the order() function.):\\n> winedata.desc.plus.counts[order(-winedata.desc.plus.counts[,8]),]\\n    Offer  Mth   Varietal MinQty Disc    Origin PastPeak  1  2  3 4  5\\nV24    24 Sept Pinot Noir      6   34     Italy    FALSE 12  0  0 0  0\\nV26    26  Oct Pinot Noir    144   83 Australia    FALSE 12  0  0 3  0\\nV2      2  Jan Pinot Noir     72   17    France    FALSE  7  3  0 0  0\\nV17    17  Jul Pinot Noir     12   47   Germany    FALSE  7  0  0 0  0\\nV1      1  Jan     Malbec     72   56    France    FALSE  2  5  0 3  0\\nV10    10  Apr   Prosecco     72   52        CA    FALSE  1  4  1 0  1\\nV12    12  May   Prosecco     72   83 Australia    FALSE  1  3  0 0  1\\nV16    16  Jun     Merlot     72   88        CA    FALSE  1  1  0 3  0\\nV23    23 Sept Chardonnay    144   39 S. Africa    FALSE  1  1  0 3  0'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 398, 'page_label': '377'}, page_content='377Moving from Spreadsheets into R \\nV27    27  Oct  Champagne     72   88        NZ    FALSE  1  4  1 3  0\\nV3      3  Feb  Espumante    144   32    Oregon     TRUE  0  2  3 0  1\\nV4      4  Feb  Champagne     72   48    France     TRUE  0  5  1 6  0\\nV5      5  Feb Cab. Sauv.    144   44        NZ     TRUE  0  0  0 4  0\\nV6      6  Mar   Prosecco    144   86     Chile    FALSE  0  8  1 3  0\\nV7      7  Mar   Prosecco      6   40 Australia     TRUE  0  3  1 0 15\\nV8      8  Mar  Espumante      6   45 S. Africa    FALSE  0  1 15 0  4\\nV9      9  Apr Chardonnay    144   57     Chile    FALSE  0  2  0 8  0\\nV11    11  May  Champagne     72   85    France    FALSE  0  7  1 4  1\\nV13    13  May     Merlot      6   43     Chile    FALSE  0  0  2 0  4\\nV14    14  Jun     Merlot     72   64     Chile    FALSE  0  3  0 6  0\\nV15    15  Jun Cab. Sauv.    144   19     Italy    FALSE  0  3  0 3  0\\nV18    18  Jul  Espumante      6   50    Oregon    FALSE  0  1  4 0  9\\nV19    19  Jul  Champagne     12   66   Germany    FALSE  0  4  1 0  0\\nV20    20  Aug Cab. Sauv.     72   82     Italy    FALSE  0  2  0 4  0\\nV21    21  Aug  Champagne     12   50        CA    FALSE  0  1  1 1  1\\nV22    22  Aug  Champagne     72   63    France    FALSE  0 17  2 2  0\\nV25    25  Oct Cab. Sauv.     72   59    Oregon     TRUE  0  3  0 3  0\\nV28    28  Nov Cab. Sauv.     12   56    France     TRUE  0  5  0 0  1\\nV29    29  Nov  P. Grigio      6   87    France    FALSE  0  1  4 0 12\\nV30    30  Dec     Malbec      6   54    France    FALSE  0  4  4 1 13\\nV31    31  Dec  Champagne     72   89    France    FALSE  0 16  1 0  0\\nV32    32  Dec Cab. Sauv.     72   45   Germany     TRUE  0  2  0 2  0\\nLooking at the top deals, it becomes clear that cluster 1 is the Pinot Noir cluster. (Your \\nmileage may vary. The genetic algorithm doesn’t give the same answer each time.)\\nSo just to reiterate then, if you strip away all my pontiﬁ  cation, the following R code \\nreplicates much of Chapter 2 of this book:\\n> setwd(\"/Users/johnforeman/datasmartfiles\")\\n> winedata <- read.csv(\"WineKMC.csv\")\\n> winedata[is.na(winedata)] <- 0\\n> install.packages(\"skmeans\",dependencies = TRUE)\\n> library(skmeans)\\n> winedata.transposed <- t(winedata[,8:107])\\n> winedata.clusters <- skmeans(winedata.transposed, 5, method=\"genetic\")\\n> winedata.clustercounts <-\\nt(aggregate(winedata.transposed,\\nby=list(winedata.clusters$cluster),sum)[,2:33])\\n> winedata.desc.plus.counts <- \\ncbind(winedata[,1:7],winedata.clustercounts)\\n> winedata.desc.plus.counts[order(-winedata.desc.plus.counts[,8]),]\\nThat’s it—from reading in the data all the way to analyzing the clusters. Pretty nuts! \\nAnd that’s because the call to skmeans() pretty much isolates all the complexity of this \\nmethod away from you. Terrible for learning, but awesome for working.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 399, 'page_label': '378'}, page_content='378 Data Smart\\nBuilding AI Models on the Pregnancy Data\\nNOTE\\nThe CSV ﬁ les used in this section, “Pregnancy.csv” and “Pregnancy_Test.csv,” are avail-\\nable for download at the book’s website, www.wiley.com/go/datasmart.\\nIn this section, you’re going to replicate some of the pregnancy prediction models you \\nbuilt in Chapters 6 and 7 of this book. Speciﬁ  cally, you’re going to build two classiﬁ ers \\nusing the glm() function (general linear model) with a logistic link function and using \\nthe randomForest() function (randomForest() bags trees, which may be anywhere from \\nsimple stumps to full decision trees).\\nThe training and test data are separated into two CSV ﬁ  les, called Pregnancy.csv and \\nPregnancy_Test.csv. Go ahead and save them into your working directory and then load \\nthem into a couple of dataframes:\\n> PregnancyData <- read.csv(\"Pregnancy.csv\")\\n> PregnancyData.Test <- read.csv(\"Pregnancy_Test.csv\")\\nYou can then run summary() and str() on the data to get a feel for it. It’s immediately \\napparent that the gender and address type data have been loaded as categorical data, but as \\nyou can see in the \\nstr() output, the response variable (1 for pregnant, 0 for not pregnant) \\nhas been treated as numeric instead of as two distinct classes:\\n> str(PregnancyData)\\n\\'data.frame’: 1000 obs. of  18 variables:\\n$ Implied.Gender        : Factor w/ 3 levels \"F\",\"M\",\"U\": 2 2 2 3 1...\\n$ Home.Apt..PO.Box      : Factor w/ 3 levels \"A\",\"H\",\"P\": 1 2 2 2 1...\\n$ Pregnancy.Test        : int  1 1 1 0 0 0 0 0 0 0 ...\\n$ Birth.Control         : int  0 0 0 0 0 0 1 0 0 0 ...\\n$ Feminine.Hygiene      : int  0 0 0 0 0 0 0 0 0 0 ...\\n$ Folic.Acid            : int  0 0 0 0 0 0 1 0 0 0 ...\\n$ Prenatal.Vitamins     : int  1 1 0 0 0 1 1 0 0 1 ...\\n$ Prenatal.Yoga         : int  0 0 0 0 1 0 0 0 0 0 ...\\n$ Body.Pillow           : int  0 0 0 0 0 0 0 0 0 0 ...\\n$ Ginger.Ale            : int  0 0 0 1 0 0 0 0 1 0 ...\\n$ Sea.Bands             : int  0 0 1 0 0 0 0 0 0 0 ...\\n$ Stopped.buying.ciggies: int  0 0 0 0 0 1 0 0 0 0 ...\\n$ Cigarettes            : int  0 0 0 0 0 0 0 0 0 0 ...\\n$ Smoking.Cessation     : int  0 0 0 0 0 0 0 0 0 0 ...\\n$ Stopped.buying.wine   : int  0 0 0 0 1 0 0 0 0 0 ...\\n$ Wine                  : int  0 0 0 0 0 0 0 0 0 0 ...\\n$ Maternity.Clothes     : int  0 0 0 0 0 0 0 1 0 1 ...\\n$ PREGNANT             : int  1 1 1 1 1 1 1 1 1 1 ...'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 400, 'page_label': '379'}, page_content='379Moving from Spreadsheets into R \\nIt’s best for randomForest()  that you actually factorize this response variable into \\ntwo classes (a 0 class and a 1 class) instead of treating the data as an integer. So you can \\nfactorize the data like so:\\nPregnancyData$PREGNANT <- factor(PregnancyData$PREGNANT)\\nPregnancyData.Test$PREGNANT <- factor(PregnancyData.Test$PREGNANT)\\nNow if you summarize the PREGNANT column, you merely get back class counts as if 0 \\nand 1 were categories:\\n> summary(PregnancyData$PREGNANT)\\n  0   1 \\n500 500\\nTo build a logistic regression, you need the glm() function, which is in the built-in stats \\npackage for R. But for the randomForest() function, you’ll need the randomForest pack-\\nage. Also, it’d be nice to build the ROC curves that you saw in Chapters 6 and 7. There’s a \\npackage speciﬁ cally built to give you those graphs, called \\nROCR. Go ahead and install and \\nload up those two real quick:\\n> install.packages(\"randomForest\",dependencies=TRUE)\\n> install.packages(\"ROCR\",dependencies=TRUE)\\n> library(randomForest)\\n> library(ROCR)\\nYou now have the data in and the packages loaded. It’s time to get model building! Start \\nwith a logistic regression:\\n> Pregnancy.lm <- glm(PREGNANT ~ .,\\ndata=PregnancyData,family=binomial(\"logit\"))\\nThe glm()function builds the linear model that you’ve speciﬁ ed as a logistic regression \\nusing the family=binomial(\"logit\") option. You supply data to the function using the \\ndata=PregnancyData ﬁ eld. Now, you’re probably wondering what PREGNANT ~ . means. \\nThis is a formula in R. It means “train my model to predict the PREGNANT column using all \\nthe other columns.” The ~ means “using” and the period means “all the other columns.” \\nYou can specify a subset of columns as well by typing their column names:\\n> Pregnancy.lm <- glm(PREGNANT ~ \\nImplied.Gender + \\nHome.Apt..PO.Box + \\nPregnancy.Test + \\nBirth.Control,\\ndata=PregnancyData,family=binomial(\"logit\"))\\nBut you’re using the PREGNANT~. notation because you want to use all of the columns \\nto train the model.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 401, 'page_label': '380'}, page_content='380 Data Smart\\nOnce the linear model is built, you can view the coeffi  cients and analyze which vari-\\nables are statistically signiﬁ  cant (similar to the t tests you conducted in Chapter 6) by \\nsummarizing the model:\\n> summary(Pregnancy.lm)\\nCall:\\nglm(formula = PREGNANT ~ ., family = binomial(\"logit\"), \\ndata = PregnancyData)\\nDeviance Residuals: \\n    Min       1Q   Median       3Q      Max  \\n-3.2012  -0.5566  -0.0246   0.5127   2.8658  \\nCoefficients:\\n                    Estimate Std. Error z value Pr(>|z|)    \\n(Intercept)        -0.343597   0.180755  -1.901 0.057315 .  \\nImplied.GenderM    -0.453880   0.197566  -2.297 0.021599 *  \\nImplied.GenderU     0.141939   0.307588   0.461 0.644469    \\nHome.Apt..PO.BoxH  -0.172927   0.194591  -0.889 0.374180    \\nHome.Apt..PO.BoxP  -0.002813   0.336432  -0.008 0.993329    \\nPregnancy.Test      2.370554   0.521781   4.543 5.54e-06 ***\\nBirth.Control      -2.300272   0.365270  -6.297 3.03e-10 ***\\nFeminine.Hygiene   -2.028558   0.342398  -5.925 3.13e-09 ***\\nFolic.Acid          4.077666   0.761888   5.352 8.70e-08 ***\\nPrenatal.Vitamins   2.479469   0.369063   6.718 1.84e-11 ***\\nPrenatal.Yoga       2.922974   1.146990   2.548 0.010822 *  \\nBody.Pillow         1.261037   0.860617   1.465 0.142847    \\nGinger.Ale          1.938502   0.426733   4.543 5.55e-06 ***\\nSea.Bands           1.107530   0.673435   1.645 0.100053    \\nStopped.buying.cig  1.302222   0.342347   3.804 0.000142 ***\\nCigarettes         -1.443022   0.370120  -3.899 9.67e-05 ***\\nSmoking.Cessation   1.790779   0.512610   3.493 0.000477 ***\\nStopped.buying.win  1.383888   0.305883   4.524 6.06e-06 ***\\nWine               -1.565539   0.348910  -4.487 7.23e-06 ***\\nMaternity.Clothes   2.078202   0.329432   6.308 2.82e-10 ***\\n---\\nSignif. codes: 0 \\'***’ 0.001 \\'**’ 0.01 \\'*’ 0.05 \\'.’ 0.1 \\' \\' 1\\nThose coeffi  cients without at least one * next to them are of dubious worth.\\nSimilarly, you can train a random forest model using the randomForest() function:\\n> Pregnancy.rf <- \\nrandomForest(PREGNANT~.,data=PregnancyData,importance=TRUE)\\nThis is the same basic syntax as the glm() call (execute ?randomForest to learn more \\nabout tree count and depth). Note the importance=TRUE  in the call. This allows you to'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 402, 'page_label': '381'}, page_content='381Moving from Spreadsheets into R \\ngraph variable importance using another function, varImpPlot(), which will allow you \\nto understand which variables are important and which are weak. \\nThe randomForest package allows you to look at how much each variable contributes \\nto decreasing node impurity on average. The more a variable contributes, the more useful \\nit is. You can use this to select and pare down the variables you might want to feed into \\nanother model. To look at this data, use the \\nvarImpPlot() function with type=2 to pull \\nrankings based on the node impurity calculation introduced in Chapter 7 (feel free to use \\nthe \\n? command to read up on the diff erence between type=1 and type=2):\\n> varImpPlot(Pregnancy.rf, type=2)\\nThis yields the ranking shown in Figure 10-4. Folic acid ranks ﬁ rst with prenatal vita-\\nmins and birth control trailing. \\nFigure 10-4:  A variable importance plot in R'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 403, 'page_label': '382'}, page_content='382 Data Smart\\nNow that you’ve built the models, you can predict with them using the predict()  \\nfunction in R. Call the function and save the results to two diff  erent variables, so you \\ncan compare models. The way the predict() function generally works is that it accepts \\na model, a dataset to predict on, and any model-speciﬁ c options:\\n> PregnancyData.Test.lm.Preds <-\\npredict(Pregnancy.lm,PregnancyData.Test,type=\"response\")\\n> PregnancyData.Test.rf.Preds <-\\npredict(Pregnancy.rf,PregnancyData.Test,type=\"prob\")\\nYou can see in the two predict calls, that each is provided with a diff erent model, the \\ntest data, and the type parameters that those models need. In the case of a linear model, \\ntype=\"response\" sets the values returned from the prediction to be between 0 and 1 just \\nlike the original PREGNANT values. In the case of the random forest, the type=\"prob\" ensures \\nthat you get back class probabilities—two columns of data, one probability of pregnancy \\nand one probability of no pregnancy.\\nThese outputs are slightly diff erent, but then again, they use diff erent algorithms, diff er-\\nent models, and so on. It’s important to play with these things and read the documentation.\\nHere’s a summary of the prediction output:\\n> summary(PregnancyData.Test.lm.Preds)\\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \\n0.001179 0.066190 0.239500 0.283100 0.414300 0.999200 \\n> summary(PregnancyData.Test.rf.Preds)\\n       0                1         \\n Min.   :0.0000   Min.   :0.0000  \\n 1st Qu.:0.7500   1st Qu.:0.0080  \\n Median :0.9500   Median :0.0500  \\n Mean   :0.8078   Mean   :0.1922  \\n 3rd Qu.:0.9920   3rd Qu.:0.2500  \\n Max.   :1.0000   Max.   :1.0000  \\nThe second column from the random forest predictions then is the probability associ-\\nated with pregnancy (as opposed to a non-pregnancy), so that’s the column that’s akin to \\nthe logistic regression predictions. Using the bracket notation, you can pull out individual \\nrecords or sets of records and look at their input data and predictions (I’ve transposed the \\nrow to make it print prettier):\\n> t(PregnancyData.Test[1,])\\n                       1  \\nImplied.Gender         \"U\"\\nHome.Apt..PO.Box       \"A\"\\nPregnancy.Test         \"0\"\\nBirth.Control          \"0\"\\nFeminine.Hygiene       \"0\"\\nFolic.Acid             \"0\"\\nPrenatal.Vitamins      \"0\"'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 404, 'page_label': '383'}, page_content='383Moving from Spreadsheets into R \\nPrenatal.Yoga          \"0\"\\nBody.Pillow            \"0\"\\nGinger.Ale             \"0\"\\nSea.Bands              \"1\"\\nStopped.buying.ciggies \"0\"\\nCigarettes             \"0\"\\nSmoking.Cessation      \"0\"\\nStopped.buying.wine    \"1\"\\nWine                   \"1\"\\nMaternity.Clothes      \"0\"\\nPREGNANT               \"1\"\\n> t(PregnancyData.Test.lm.Preds[1])\\n             1\\n[1,] 0.6735358\\n> PregnancyData.Test.rf.Preds[1,2]\\n[1] 0.504\\nNote that in printing the input row, I leave the column index blank in the square brack-\\nets [1,] so that all columns’ data is printed. This particular customer has an unknown \\ngender, lives in an apartment, and has bought sea bands and wine, but then stopped buying \\nwine. The logistic regression gives them a score of 0.67 while the random forest is right \\naround 0.5. The truth is that she is pregnant—chalk one up for the logistic regression!\\nNow that you have the two vectors of class probabilities, one for each mode, you \\ncan compare the models in terms of true positive rate and false positive rate just as you \\ndid earlier in the book. Luckily for you, though, in R the \\nROCR  package can compute \\nand plot the ROC curves so you don’t have to. Since you’ve already loaded the ROCR  \\npackage, the ﬁ rst thing you need to do is create two ROCR prediction objects (using the \\nROCR prediction()  function), which simply count up the positive and negative class \\npredictions at various cutoff  levels in the class probabilities:\\n> pred.lm <-\\nprediction(PregnancyData.Test.lm.Preds,\\nPregnancyData.Test$PREGNANT)\\n> pred.rf <-\\nprediction(PregnancyData.Test.rf.Preds[,2],\\nPregnancyData.Test$PREGNANT)\\nNote in the second call that you hit the second column of class probabilities from the \\nrandom forest object just as discussed earlier. You can then turn these prediction objects \\ninto \\nROCR performance objects by running them through the performance() function. A \\nperformance object takes the classiﬁ cations given by the model on the test set for various \\ncutoff  values and uses them to assemble a curve of your choosing (in this case a ROC \\ncurve):\\n> perf.lm <- performance(pred.lm,\"tpr\",\"fpr\")\\n> perf.rf <- performance(pred.rf,\"tpr\",\"fpr\")'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 405, 'page_label': '384'}, page_content='384 Data Smart\\nNOTE\\nIf you’re curious, performance() provides other options besides the tpr and fpr values, \\nsuch as prec for precision and rec for recall. Read the ROCR package documentation \\nfor more detail.\\nYou can then plot these curves using R’s plot() function. First, the linear model curve \\n(the xlim and ylim ﬂ ags are used to set the upper and lower bounds on the x and y axes \\nin the graph):\\n> plot(perf.lm,xlim=c(0,1),ylim=c(0,1))\\nYou can add the random forest curve in using the add=TRUE ﬂ ag to overlay it and the lty=2 \\nﬂ ag (lty stands for “line type”; check out ?plot to learn more) to make this line dashed:\\n> plot(perf.rf,xlim=c(0,1),ylim=c(0,1),lty=2,add=TRUE)\\nThis overlays the two curves with the random forest performance as a dashed line, \\nas shown in Figure 10-5. For the most part, the logistic regression is superior with the \\nrandom forest pulling ahead brieﬂ y on the far right of the graph.\\nFigure 10-5:  Recall and precision graphed in R\\nAll right, so to recap here, you trained two diff erent predictive models, used them on \\na test set, and compared their precision versus recall using the following code:\\n> PregnancyData <- read.csv(\"Pregnancy.csv\")\\n> PregnancyData.Test <- read.csv(\"Pregnancy_Test.csv\")'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 406, 'page_label': '385'}, page_content='385Moving from Spreadsheets into R \\n> PregnancyData$PREGNANT <- factor(PregnancyData$PREGNANT)\\n> PregnancyData.Test$PREGNANT <- factor(PregnancyData.Test$PREGNANT)\\n> install.packages(\"randomForest\",dependencies=TRUE)\\n> install.packages(\"ROCR\",dependencies=TRUE)\\n> library(randomForest)\\n> library(ROCR)\\n> Pregnancy.lm <- glm(PREGNANT ~ .,\\ndata=PregnancyData,family=binomial(\"logit\"))\\n> summary(Pregnancy.lm)\\n> Pregnancy.rf <-\\nrandomForest(PREGNANT~.,data=PregnancyData,importance=TRUE)\\n> PregnancyData.Test.rf.Preds <-\\npredict(Pregnancy.rf,PregnancyData.Test,type=\"prob\")\\n> varImpPlot(Pregnancy.rf, type=2)\\n> PregnancyData.Test.lm.Preds <-\\npredict(Pregnancy.lm,PregnancyData.Test,type=\"response\")\\n> PregnancyData.Test.rf.Preds <-\\npredict(Pregnancy.rf,PregnancyData.Test,type=\"prob\")\\n> pred.lm <-\\nprediction(PregnancyData.Test.lm.Preds,\\nPregnancyData.Test$PREGNANT)\\n> pred.rf <- \\nprediction(PregnancyData.Test.rf.Preds[,2],\\nPregnancyData.Test$PREGNANT)\\n> perf.lm <- performance(pred.lm,\"tpr\",\"fpr\")\\n> perf.rf <- performance(pred.rf,\"tpr\",\"fpr\")\\n> plot(perf.lm,xlim=c(0,1),ylim=c(0,1))\\n> plot(perf.rf,xlim=c(0,1),ylim=c(0,1),lty=2,add=TRUE)\\nPretty straightforward, really. Compared to Excel, look at how easy it was to compare \\ntwo diff erent models. That’s quite nice.\\nForecasting in R\\nNOTE\\nThe CSV ﬁ le used in this section, “SwordDemand.csv,” is available for download at the \\nbook’s website, www.wiley.com/go/datasmart.\\nThis next section is nuts. Why? Because you’re going to regenerate the exponential \\nsmoothing forecast from Chapter 8 so fast it’s going to make your head spin.\\nFirst, load in the sword demand data from SwordDemand.csv and print it to the console:\\n> sword <- read.csv(\"SwordDemand.csv\")\\n> sword\\nSwordDemand\\n1          165\\n2          171'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 407, 'page_label': '386'}, page_content='386 Data Smart\\n3          147\\n4          143\\n5          164\\n6          160\\n7          152\\n8          150\\n9          159\\n10         169\\n11         173\\n12         203\\n13         169\\n14         166\\n15         162\\n16         147\\n17         188\\n18         161\\n19         162\\n20         169\\n21         185\\n22         188\\n23         200\\n24         229\\n25         189\\n26         218\\n27         185\\n28         199\\n29         210\\n30         193\\n31         211\\n32         208\\n33         216\\n34         218\\n35         264\\n36         304\\nAll right, so you have 36 months of demand loaded up, nice and simple. The ﬁ rst thing \\nyou need to do is tell R that this is time series data. There’s a function called ts() that is \\nused for this purpose:\\nsword.ts <- ts(sword,frequency=12,start=c(2010,1))\\nIn this call, you provide the ts() function with the data, a frequency value (the number \\nof observations per unit of time, which in this case is 12 per year), and a starting point \\n(this example uses January 2010).\\nWhen you print \\nsword.ts by typing it in the terminal, R now knows to print it in a \\ntable by month:\\n> sword.ts\\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 408, 'page_label': '387'}, page_content='387Moving from Spreadsheets into R \\n2010 165 171 147 143 164 160 152 150 159 169 173 203\\n2011 169 166 162 147 188 161 162 169 185 188 200 229\\n2012 189 218 185 199 210 193 211 208 216 218 264 304\\nNice!\\nYou can plot the data too:\\n > plot(sword.ts)\\nThis gives the graph shown in Figure 10-6.\\nFigure 10-6:  Graph of sword demand\\nAt this point, you’re ready to forecast, which you can do using the excellent forecast \\npackage. Feel free to look it up on CRAN (http://cran.r-project.org/package=forecast) \\nor watch the author talk about it in this YouTube video: http://www.youtube.com/\\nwatch?v=1Lh1HlBUf8k.\\nTo forecast using the forecast  package, you just feed a time series object into the \\nforecast() function. The forecast() call has been set up to detect the appropriate tech-\\nnique to use. Remember how you ran through a few techniques earlier in the book? The \\nforecast() function is gonna do all that stuff  for you:'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 409, 'page_label': '388'}, page_content='388 Data Smart\\n> install.packages(\"forecast\",dependencies=TRUE)\\n> library(forecast)\\n> sword.forecast <- forecast(sword.ts)\\nAnd that’s it. Your forecast is saved in the sword.forecast object. Now you can print it:\\n> sword.forecast\\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\\nJan 2013       242.9921 230.7142 255.2699 224.2147 261.7695\\nFeb 2013       259.4216 246.0032 272.8400 238.8999 279.9433\\nMar 2013       235.8763 223.0885 248.6640 216.3191 255.4334\\nApr 2013       234.3295 220.6882 247.9709 213.4669 255.1922\\nMay 2013       274.1674 256.6893 291.6456 247.4369 300.8980\\nJun 2013       252.5456 234.6894 270.4019 225.2368 279.8544\\nJul 2013       257.0555 236.7740 277.3370 226.0376 288.0734\\nAug 2013       262.0715 238.9718 285.1711 226.7436 297.3993\\nSep 2013       279.4771 252.0149 306.9392 237.4774 321.4768\\nOct 2013       289.7890 258.1684 321.4097 241.4294 338.1487\\nNov 2013       320.5914 281.9322 359.2506 261.4673 379.7155\\nDec 2013       370.3057 321.2097 419.4018 295.2198 445.3917\\nJan 2014       308.3243 263.6074 353.0413 239.9357 376.7130\\nFeb 2014       327.6427 275.9179 379.3675 248.5364 406.7490\\nMar 2014       296.5754 245.8459 347.3049 218.9913 374.1594\\nApr 2014       293.3646 239.2280 347.5013 210.5698 376.1595\\nMay 2014       341.8187 274.0374 409.5999 238.1562 445.4812\\nJun 2014       313.6061 247.0271 380.1851 211.7823 415.4299\\nJul 2014       317.9789 245.9468 390.0109 207.8153 428.1424\\nAug 2014       322.9807 245.1532 400.8081 203.9538 442.0075\\nSep 2014       343.1975 255.4790 430.9160 209.0436 477.3513\\nOct 2014       354.6286 258.7390 450.5181 207.9782 501.2790\\nNov 2014       391.0099 279.4304 502.5893 220.3638 561.6559\\nDec 2014       450.1820 314.9086 585.4554 243.2992 657.0648\\nYou get a forecast with prediction intervals built-in! And you can print the actual \\nforecasting technique used by printing the method value in the sword.forecast object:\\n> sword.forecast$method\\n[1] \"ETS(M,A,M)\"\\nThe MAM stands for multiplicative error, additive trend, multiplicative seasonality. The \\nforecast()  function has actually chosen to run Holt-Winters exponential smoothing! \\nAnd you didn’t even have to do anything. When you plot it, as shown in Figure 10-7, you \\nautomatically get a fan chart:\\n> plot(sword.forecast)\\nTo recap, here’s the code that replicated Chapter 8:\\n> sword <- read.csv(\"SwordDemand.csv\")\\n> sword.ts <- ts(sword,frequency=12,start=c(2010,1))\\n> install.packages(\"forecast\",dependencies=TRUE)\\n> library(forecast)'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 410, 'page_label': '389'}, page_content='389Moving from Spreadsheets into R \\n> sword.forecast <- forecast(sword.ts)\\n> plot(sword.forecast)\\nFigure 10-7: Fan chart of the demand forecast\\nCrazy. But that’s the beauty of using packages other folks have written specially to do \\nthis stuff .\\nLooking at Outlier Detection\\nNOTE\\nThe CSV ﬁ les used in this section, “PregnancyDuration.csv” and “CallCenter.csv,” are \\navailable for download at the book’s website, www.wiley.com/go/datasmart.\\nIn this section, you’ll do one more of the chapters from this book in R, just to drive home \\nthe ease of this stuff . To start, read in the pregnancy duration data in PregnancyDuration\\n.csv available from the book’s website:\\n> PregnancyDuration <- read.csv(\"PregnancyDuration.csv\")'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 411, 'page_label': '390'}, page_content='390 Data Smart\\nIn Chapter 9, you calculated the median, ﬁ  rst quartile, third quartile, and inner and \\nouter Tukey fences. You can get the quartiles just from summarizing the data:\\n> summary(PregnancyDuration)\\n GestationDays  \\n Min.   :240.0  \\n 1st Qu.:260.0  \\n Median :267.0  \\n Mean   :266.6  \\n 3rd Qu.:272.0  \\n Max.   :349.0     \\nThat makes the interquartile range equal to 272 minus 260 (alternatively, you can call \\nthe built-in IQR() function on the GestationDays column):\\n> PregnancyDuration.IQR <- 272 - 260\\n> PregnancyDuration.IQR <- IQR(PregnancyDuration$GestationDays)\\n> PregnancyDuration.IQR\\n[1] 12\\nYou can then calculate the lower and upper Tukey fences:\\n> LowerInnerFence <- 260 - 1.5*PregnancyDuration.IQR\\n> UpperInnerFence <- 272 + 1.5*PregnancyDuration.IQR\\n> LowerInnerFence\\n[1] 242\\n> UpperInnerFence\\n[1] 290\\nUsing R’s which()  function, it’s easy to determine the points and their indices that \\nviolate the fences. For example:\\n> which(PregnancyDuration$GestationDays > UpperInnerFence)\\n[1]   1 249 252 338 345 378 478 913\\n> PregnancyDuration$GestationDays[\\nwhich(PregnancyDuration$GestationDays > UpperInnerFence)\\n]\\n[1] 349 292 295 291 297 303 293 296\\nOf course, one of the best ways to do this analysis is to use R’s boxplot() function. The \\nboxplot() function will graph the median, ﬁ  rst and third quartiles, Tukey fences, and \\nany outliers. To use it, you simply toss the GestationDays column inside the function:\\n> boxplot(PregnancyDuration$GestationDays)\\nThis yields the visualization shown in Figure 10-8.\\nThe Tukey fences can be modiﬁ ed to be “outer” fences by changing the range ﬂ ag in \\nthe boxplot call (it defaults to 1.5 times the \\nIQR). If you set range=3, then the Tukey fences \\nare drawn at the last point inside three times the IQR instead:\\n> boxplot(PregnancyDuration$GestationDays, range=3)'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 412, 'page_label': '391'}, page_content='391Moving from Spreadsheets into R \\nAs shown in Figure 10-9, note now that you have only one outlier, which is \\nMrs. Hadlum’s pregnancy duration of 349 days. \\nFigure 10-8:  A boxplot of the pregnancy \\nduration data\\nFigure 10-9:  A boxplot with Tukey fences \\nusing three times the IQR'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 413, 'page_label': '392'}, page_content='392 Data Smart\\nYou can also pull this data out of the boxplot in the console rather than plot it. Printing \\nthe stats list, you get the fences and quartiles:\\n> boxplot(PregnancyDuration$GestationDays,range=3)$stats\\n     [,1]\\n[1,]  240\\n[2,]  260\\n[3,]  267\\n[4,]  272\\n[5,]  303\\nPrinting the out list, you get a list of outlier values:\\n> boxplot(PregnancyDuration$GestationDays,range=3)$out\\n[1] 349\\nOkay, so that’s a bit on the pregnancy duration problem. Let’s move on to the harder \\nproblem of ﬁ  nding outliers in the call center employee performance data. It’s in the \\nCallCenter.csv sheet on the book’s website. Loading it up and summarizing, you get:\\n> CallCenter <- read.csv(\"CallCenter.csv\")\\n> summary(CallCenter)\\n       ID             AvgTix          Rating         Tardies     \\n Min.   :130564   Min.   :143.1   Min.   :2.070   Min.   :0.000  \\n 1st Qu.:134402   1st Qu.:153.1   1st Qu.:3.210   1st Qu.:1.000  \\n Median :137906   Median :156.1   Median :3.505   Median :1.000  \\n Mean   :137946   Mean   :156.1   Mean   :3.495   Mean   :1.465  \\n 3rd Qu.:141771   3rd Qu.:159.1   3rd Qu.:3.810   3rd Qu.:2.000  \\n Max.   :145176   Max.   :168.7   Max.   :4.810   Max.   :4.000  \\n   Graveyards       Weekends         SickDays     PercSickOnFri   \\n Min.   :0.000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \\n 1st Qu.:1.000   1st Qu.:1.0000   1st Qu.:0.000   1st Qu.:0.0000  \\n Median :2.000   Median :1.0000   Median :2.000   Median :0.2500  \\n Mean   :1.985   Mean   :0.9525   Mean   :1.875   Mean   :0.3522  \\n 3rd Qu.:2.000   3rd Qu.:1.0000   3rd Qu.:3.000   3rd Qu.:0.6700  \\n Max.   :4.000   Max.   :2.0000   Max.   :7.000   Max.   :1.0000  \\n EmployeeDevHrs  ShiftSwapsReq   ShiftSwapsOffered\\n Min.   : 0.00   Min.   :0.000   Min.   :0.00     \\n 1st Qu.: 6.00   1st Qu.:1.000   1st Qu.:0.00     \\n Median :12.00   Median :1.000   Median :1.00     \\n Mean   :11.97   Mean   :1.448   Mean   :1.76     \\n 3rd Qu.:17.00   3rd Qu.:2.000   3rd Qu.:3.00     \\n Max.   :34.00   Max.   :5.000   Max.   :9.00     \\nJust as in Chapter 9, you need to scale and center the data. To do so, you need only use \\nthe scale() function:\\n> CallCenter.scale <- scale(CallCenter[2:11])\\n> summary(CallCenter.scale)\\n    AvgTix              Rating            Tardies          Graveyards      \\nMin.   :-2.940189   Min.   :-3.08810   Min.   :-1.5061   Min.   :-2.4981'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 414, 'page_label': '393'}, page_content='393Moving from Spreadsheets into R \\n1st Qu.:-0.681684   1st Qu.:-0.61788   1st Qu.:-0.4781   1st Qu.:-1.2396  \\nMedian :-0.008094   Median : 0.02134   Median :-0.4781   Median : 0.0188  \\nMean   : 0.000000   Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.0000  \\n3rd Qu.: 0.682476   3rd Qu.: 0.68224   3rd Qu.: 0.5500   3rd Qu.: 0.0188  \\nMax.   : 2.856075   Max.   : 2.84909   Max.   : 2.6062   Max.   : 2.5359  \\n    Weekends           SickDays        PercSickOnFri     EmployeeDevHrs     \\nMin.   :-1.73614   Min.   :-1.12025   Min.   :-0.8963   Min.   :-1.60222  \\n1st Qu.: 0.08658   1st Qu.:-1.12025   1st Qu.:-0.8963   1st Qu.:-0.79910  \\nMedian : 0.08658   Median : 0.07468   Median :-0.2601   Median : 0.00401  \\nMean   : 0.00000   Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.00000  \\n3rd Qu.: 0.08658   3rd Qu.: 0.67215   3rd Qu.: 0.8088   3rd Qu.: 0.67328  \\nMax.   : 1.90930   Max.   : 3.06202   Max.   : 1.6486   Max.   : 2.94879  \\n ShiftSwapsReq     ShiftSwapsOffered\\nMin.   :-1.4477   Min.   :-0.9710  \\n1st Qu.:-0.4476   1st Qu.:-0.9710  \\nMedian :-0.4476   Median :-0.4193  \\nMean   : 0.0000   Mean   : 0.0000  \\n3rd Qu.: 0.5526   3rd Qu.: 0.6841  \\nMax.   : 3.5530   Max.   : 3.9942   \\nNow that the data is prepped, you can send it through the lofactor() function that’s \\npart of the DMwR package:\\n> install.packages(\"DMwR\",dependencies=TRUE)\\n> library(DMwR)\\nTo call the lofactor() function, you supply it the data and a k value (this example uses \\n5, just like in Chapter 9), and the function spits out LOFs:\\n> CallCenter.lof <- lofactor(CallCenter.scale,5)\\nData with the highest factors (LOFs usually hover around 1) are the oddest points. \\nFor instance, you can highlight the data associated with those employees whose LOF is \\ngreater than 1.5:\\n> which(CallCenter.lof > 1.5)\\n[1] 299 374\\n> CallCenter[which(CallCenter.lof > 1.5),]\\n        ID AvgTix Rating Tardies Graveyards Weekends SickDays\\n299 137155  165.3   4.49       1          3        2        1\\n374 143406  145.0   2.33       3          1        0        6\\n    PercSickOnFri EmployeeDevHrs ShiftSwapsReq ShiftSwapsOffered\\n299          0.00             30             1                 7\\n374          0.83             30             4                 0\\nThese are the same two outlying employees discussed in Chapter 9. But what a huge \\ndiff erence in the number of lines of code it took to get this:\\n> CallCenter <- read.csv(\"CallCenter.csv\")\\n> install.packages(\"DMwR\",dependencies=TRUE)\\n> library(DMwR)'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 415, 'page_label': '394'}, page_content='394 Data Smart\\n> CallCenter.scale <- scale(CallCenter[2:11])\\n> CallCenter.lof <- lofactor(CallCenter.scale,5)\\nThat’s all it took!\\nWrapping Up\\nOkay, this was a fast and furious run-through of some of what you can do in R merely by \\nunderstanding three things:\\n• Loading and working with data in R\\n• Finding and installing relevant packages\\n• Calling functions from those packages on your dataset\\nIs this all you need to know how to do in R? Nope. I didn’t cover writing your own \\nfunctions, a whole lot of plotting, connecting to databases, the slew of apply() functions \\navailable, and so on. But I hope this has given you a taste to learn more. If it has, there are \\nscads of R books out there worth reading as a follow-up to this chapter. Here are a few:\\n• Beginning R: The Statistical Programming Language by Mark Gardener (John Wiley \\n& Sons, 2012)\\n• R in a Nutshell, 2nd Edition by Joseph Adler (O’Reilly, 2012)\\n• Data Mining with R: Learning with Case Studies by Luis Torgo (Chapman and Hall, \\n2010)\\n• Machine Learning for Hackers by Drew Conway and John Myles White (O’Reilly, \\n2012)\\nGo forth and tinker i n R!'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 416, 'page_label': '395'}, page_content='Where Am I? What Just Happened?\\nY\\nou may have started this book with a rather ordinary set of skills in math and spread-\\nsheet modeling, but if you’re here, having made it through alive (and having not just \\nskipped the first 10 chapters), then I imagine you’re now a spreadsheet modeling connois-\\nseur with a good grasp of a variety of data science techniques.\\nThis book has covered topics ranging from classic operations research fodder (optimiza-\\ntion, Monte Carlo, and forecasting) to unsupervised learning (outlier detection, clustering, \\nand graphs) to supervised AI (regression, decision stumps, and naïve Bayes). You should \\nfeel conﬁ dent working with spreadsheet data at this higher level. \\nI also hope that Chapter 10 showed you that now that you understand data science \\ntechniques and algorithms, it’s quite easy to use those techniques from within a program-\\nming language such as R.\\nAnd if there’s a particular topic that really grabbed you in this book, dive deeper! Want \\nmore R, more optimization, more machine learning? Grab one of the sources I recom-\\nmend in each relevant chapter’s conclusion and read on. There’s so much to learn. I’ve \\nonly scraped the surface of analytics practice in this book.\\nBut wait...\\nBefore You Go-Go\\nI want to use this conclusion to off er up some thoughts about what it means to practice \\ndata science in the real world, because merely knowing the math isn’t enough. \\nAnyone who knows me well knows that I’m not the sharpest knife in the drawer. My \\nquantitative skills are middling, but I’ve seen folks much smarter than I fail mightily at \\nworking as analytics professionals. The problem is that while they’re brilliant, they don’t \\nknow the little things that can cause technical endeavors to fail within the business \\nenvironment. So let’s cover these softer items that can mean the success or failure of your \\nanalytics project or career.\\nConclusion'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 417, 'page_label': '396'}, page_content='Data Smart396\\nGet to Know the Problem\\nMy favorite movie of all time is the 1992 ﬁ  lm Sneakers. The movie centers on a band of \\npenetration testers led by Robert Redford that steals a “black box” capable of cracking \\nRSA encryption. Hijinks ensue. (If you haven’t watched it, I envy you, because you have \\nan opportunity to see it for the ﬁ rst time!)\\nThere’s a scene where Robert Redford encounters an electronic keypad on a locked \\noffi  ce door at a think tank, and he needs to break through.\\nHe reaches out to his team via his headset. They’re waiting in a van outside the building.\\n“Anybody ever had to defeat an electronic keypad?” he asks.\\n“Those things are impossible,” Sydney Poitier exclaims. But Dan Aykroyd, also wait-\\ning in the van, comes up with an idea. They explain its complexities to Redford over the \\ncomms.\\nRobert Redford nods his head and says, “Okay, I’ll give it a shot.”\\nHe ignores the keypad and kicks in the door.\\nYou see, the problem wasn’t “defeating an electronic keypad” at all. The problem was \\ngetting inside the room. Dan Aykroyd understood this.\\nThis is the fundamental challenge of analytics: understanding what actually must be \\nsolved. You must learn the situation, the processes, the data, and the circumstances. You \\nneed to characterize everything around the problem as best you can in order to understand \\nexactly what an ideal solution is.\\nIn data science, you’ll often encounter the “poorly posed problem”:\\n 1. Someone else in the business encounters a problem.\\n 2. They use their past experience and (lack of?) analytics knowledge to frame the \\nproblem.\\n 3. They hand their conception of the problem to the analyst as if it were set in stone \\nand well posed.\\n 4. The analytics person accepts and solves the problem as-is.\\nThis can work. But it’s not ideal, because the problem you’re asked to solve is often not \\nthe problem that needs solving. If this problem is really about that problem then analytics \\nprofessionals cannot be passive. \\nYou cannot accept problems as handed to you in the business environment. Never allow \\nyourself to be the analyst to whom problems are “thrown over the fence.” Engage with the \\npeople whose challenges you’re tackling to make sure you’re solving the right problem. \\nLearn the business’s processes and the data that’s generated and saved. Learn how folks \\nare handling the problem now, and what metrics they use (or ignore) to gauge success. \\nSolve the correct, yet often misrepresented, problem. This is something no mathe-\\nmatical model will ever say to you. No mathematical model can ever say, “Hey, good'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 418, 'page_label': '397'}, page_content='397Conclusion\\njob formulating this optimization model, but I think you should take a step back and \\nchange your business a little instead.” And that leads me to my next point: Learn how to \\ncommunicate.\\nWe Need More Translators\\nIf you’ve ﬁ nished this book, it’s safe to say you now know a thing or two about analytics. \\nYou’re familiar with the tools that are available to you. You’ve prototyped in them. And \\nthat allows you to identify analytics opportunities better than most, because you know \\nwhat’s possible. You needn’t wait for someone to bring an opportunity to you. You can \\npotentially go out into the business and ﬁ nd them. \\nBut without the ability to communicate, it becomes diffi  cult to understand others’ chal-\\nlenges, articulate what’s possible, and explain the work you’re doing. \\nIn today’s business environment, it is often unacceptable to be skilled at only one thing. \\nData scientists are expected to be polyglots who understand math, code, and the plain-\\nspeak (or sports analogy-ridden speak ...ugh) of business. And the only way to get good \\nat speaking to other folks, just like the only way to get good at math, is through practice.\\nTake any opportunity you can to speak with others about analytics, formally and \\ninformally. Find ways to discuss with others in your workplace what they do, what you \\ndo, and ways you might collaborate. Speak with others at local meet-ups about what you \\ndo. Find ways to articulate analytics concepts within your particular business context.\\nPush your management to involve you in planning and business development discus-\\nsions. Too often the analytics professional is approached with a project only after  that \\nproject has been scoped, but your knowledge of the techniques and data available makes \\nyou indispensable in early planning.\\nPush to be viewed as a person worth talking to and not as an extension of some number-\\ncrunching machine that problems are thrown at from a distance. The more embedded \\nand communicative an analyst is within an organization, the more eff ective he or she is.\\nFor too long analysts have been treated like Victorian women —separated from the \\nﬁ ner points of business, because they couldn’t possibly understand it all. Oh, please. \\nLet people feel the weight of your well-rounded skill set—just because they can’t crunch \\nnumbers doesn’t mean you can’t discuss a PowerPoint slide. Get in there, get your hands \\ndirty, and talk to folks. \\nBeware the Three-Headed Geek-Monster: Tools, Performance, \\nand Mathematical Perfection\\nThere are many things that can sabotage the use of analytics within the workplace. Politics \\nand inﬁ ghting perhaps; a bad experience from a previous “enterprise, business intelligence,'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 419, 'page_label': '398'}, page_content='Data Smart398\\ncloud dashboard” project; or peers who don’t want their “dark art” optimized or automated \\nfor fear that their jobs will become redundant. \\nNot all hurdles are within your control as an analytics professional. But some are. There \\nare three primary ways I see analytics folks sabotage their own work: overly-complex \\nmodeling, tool obsession, and ﬁ xation on performance.\\nComplexity\\nMany moons ago, I worked on a supply chain optimization model for a Fortune 500 com-\\npany. This model was pretty badass if I do say so myself. We gathered all kinds of busi-\\nness rules from the client and modeled their entire shipping process as a mixed-integer \\nprogram. We even modeled normally distributed future demand into the model in a novel \\nway that ended up getting published.\\nBut the model was a failure. It was dead out of the gate. By dead, I don’t mean that it \\nwas wrong, but rather that it wasn’t used. Frankly, once the academics left, there was no \\none left in that part of the company who could keep the cumulative forecast error means \\nand standard deviations up to date. The boots on the ground just didn’t understand it, \\nregardless of the amount of training we gave.\\nThis is a diff erence between academia and industry. In academia, success is not gauged \\nby usefulness. A novel optimization model is valuable in its own right, even if it is too \\ncomplex for a supply chain analyst to keep running. \\nBut in the industry, analytics is a results-driven pursuit, and models are judged by their \\npractical value as much as by their novelty.\\nIn this case, I spent too much time using complex math to optimize the company’s \\nsupply chain but never realistically addressed the fact that no one would be able to keep \\nthe model up to date. \\nThe mark of a true analytics professional, much like the mark of a true artist, is in knowing \\nwhen to edit. When do you leave some of the complexity of a solution on the cutting room \\nﬂ oor? To get all cliché on you, remember that in analytics great is the enemy of good. The \\nbest model is one that strikes the right balance between functionality and maintainability. \\nIf an analytics model is never used, it’s worthless.\\nTools\\nRight now in the world of analytics (whether you want to call that “data science,” “big \\ndata,” “business intelligence,” “blah blah blah cloud,” and so on), people have become \\nfocused on tools and architecture. \\nTools are important. They enable you to deploy your analytics and data-driven prod-\\nucts. But when people talk about “the best tool for the job,” they’re too often focused on \\nthe tool and not on the job.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 420, 'page_label': '399'}, page_content='399Conclusion\\nSoftware and services companies are in the business of selling you solutions to problems \\nyou may not even have yet. And to make matters worse, many of us have bosses who read \\nstuff  like the Harvard Business Review and then look at us and say, “We need to be doing \\nthis big data thing. Go buy something, and let’s get Hadoop-ing.” \\nThis all leads to a dangerous climate in business today where management looks \\nto tools as proof that analytics are being done, and providers just want to sell us the \\ntools that enable the analytics, but there’s little accountability that actual analytics is \\ngetting done.\\nSo here’s a simple rule: Identify the analytics opportunities you want to tackle in as much \\ndetail as possible before acquiring tools.\\nDo you need Hadoop? Well, does your problem require a divide-and-conquer aggrega-\\ntion of a lot of unstructured data? No? Then the answer may be no. Don’t put the cart \\nbefore the horse and buy the tools (or the consultants who are needed to use the open \\nsource tools) only to then say, “Okay, now what do we do with this?”\\nPerformance\\nIf I had a nickel every time someone raised their eyebrows when I tell them MailChimp \\nuses R in production for our abuse-prevention models, I could buy a Mountain Dew. \\nPeople think the language isn’t appropriate for production settings. If I were doing high-\\nperformance stock trading, it probably wouldn’t be. I’d likely code everything up in C. \\nBut I’m not, and I won’t.\\nFor MailChimp, most of our time isn’t spent in R. It’s spent moving data to send through \\nthe AI model. It’s not spent running the AI model, and it’s certainly not spent training the \\nAI model.\\nI’ve met folks who are very concerned with the speed at which their software can train \\ntheir artiﬁ cial intelligence model. Can the model be trained in parallel, in a low-level \\nlanguage, in a live environment?\\nThey never stop to ask themselves if any of this is necessary and instead end up spend-\\ning a lot of time gold-plating the wrong part of their analytics project. \\nAt MailChimp, we retrain our models offl  ine once a quarter, test them, and then promote \\nthem into production. In R, it takes me a few hours to train the model. And even though \\nwe as a company have terabytes of data, the model’s training set, once prepped, is only 10 \\ngigabytes, so I can even train the model on my laptop. Crazy. \\nGiven that that’s the case, I don’t waste my time on R’s training speed. I focus on more \\nimportant things, like model accuracy.\\nI’m not saying that you shouldn’t care about performance. But keep your head on \\nstraight, and in situations where it doesn’t matter, feel free to let it go.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 421, 'page_label': '400'}, page_content='Data Smart400\\nYou Are Not the Most Important Function of Your \\nOrganization\\nOkay, so there are three things to watch out for. But more generally, keep in mind that \\nmost companies are not in the business of doing analytics. They make their money through \\nother means, and analytics is meant to serve those processes.\\nYou may have heard elsewhere that data scientist  is the “sexiest job of the century!” \\nThat’s because of how data science serves an industry. Serves being the key word.\\nConsider the airline industry. They’ve been doing big data analytics for decades to \\nsqueeze that last nickel out of you for that seat you can barely ﬁ t in. That’s all done through \\nrevenue optimization models. It’s a huge win for mathematics.\\nBut you know what? The most important part of their business is ﬂ ying. The products \\nand services an organization sells matter more than the models that tack on pennies to \\nthose dollars. Your goals should be things like using data to facilitate better targeting, \\nforecasting, pricing, decision-making, reporting, compliance, and so on. In other words, \\nwork with the rest of your organization to do better business , not to do data science for \\nits own sake.\\nGet Creative and Keep in Touch!\\nThat’s enough sage wisdom. If you’ve labored through the preceding chapters then you \\nhave a good base to begin dreaming up, prototyping, and implementing solutions to the \\nanalytics opportunities posed by your business. Talk with your coworkers and get cre-\\native. Maybe there’s an analytical solution for something that’s been patched over with \\ngut feelings and manual processes. Attack it.\\nAnd as you go through the process of implementing these and other techniques in your \\nwork-a-day life, keep in touch. I’m on Twitter at \\n@John4man. Reach out and tell me your tale. \\nOr to give me hell about this book. I’ll take any feedback.\\nHappy data wrangling !'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 422, 'page_label': '401'}, page_content='A\\nabsolute references, Solver, 110\\nabsolute values of errors, median \\nregression, 221\\nadditive smoothing, 86\\nadjacency matrix, 158\\naffi  nity matrix, 159\\nagglomerative clustering, 185\\nAI model\\nBayes rule and, 83–86\\ndummy variables, 210–212\\nfeature set, 207–208\\nversus optimization model, 101–102\\noverview, 206–207\\npregnancy data, 378–385\\npregnant customers (See RetailMart \\n(pregnant customers))\\ntraining data, oversampling, 210\\nAIMMS, 118\\nalgorithms, evolutionary, 115–116\\nalpha value calculation, 276–277\\narrays, formulas, 19–20\\nautocorrelations, 306–313\\nB\\nbag of words model, 79\\nextraneous punctuation, 87–88\\nspaces, 88–91\\nbagged decision stumps, 251\\nbagging, 254. See also decision stumps\\nmodel evaluation, 267–271\\noutliers and, 271\\nrandom forest models, 271\\nBayes rule, 82\\nAI model creation, 83–86\\nBig M, 133–137\\nbinary tree, 193–197\\nBINOMDIST function, 116\\nblending model, 119\\nboosting, 251\\nmodel evaluation, 280–283\\nmodel training, 272–275\\nweighted errors, 272\\nreweighting, 277–278\\nC\\nCDF (cumulative distribution function), \\n146–148, 337\\nmean deviation, 147–148\\nstandard deviation, 147–148\\nscenarios from, 148–150\\ncell formatting, 5–7\\ncentral limit theorem, 146\\nchain rule of probability, 81\\ncharts. See also graphs\\nfan chart, 331–333\\ninserting, in spreadsheets, 8–9\\nclassiﬁ ers, bagging, 254\\ncluster analysis, 29\\ncluster centers, solving for, 46–48\\ncluster labels, 193–197\\nclustering, 29–30\\nagglomerative, 185\\ncluster centroid, 31\\ncommunity detection, 155–156\\ndivisive, 185–192\\nIndex'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 423, 'page_label': '402'}, page_content='Index402\\nhierarchical partitioning, 185\\nimage clustering, 30\\nk-means, 30–35\\ne-mail marketing, 35–66\\nk groups, 30–35\\nk-medians, 66–67\\ncosine distance, 68–69\\nExcel, 69–75\\nManhattan distance, 67–68\\nnetwork graphs, 155, 156–157\\nedges, 156\\nnodes, 156\\nSolver, 34–35\\nresults, 49\\ncoeffi  cient, variables, 214\\ncoeffi  cient standard error, 226–227\\ncoeffi  cient tests, 226–230\\ncommunity detection, clustering and, \\n155–156\\nmodularity maximization, 156\\nConcessions.xlsx ﬁ le, 2\\nconditional formatting, 6–7\\nconditional probabilities, 80\\nBayes rule, 82\\nnaÏve Bayes model, 94–98\\ntoken counting, 92–93\\nconstraints, 110–112\\ncopying\\ndata, 4–5\\nformulas, 4–5\\ncorrelogram, 310–313\\ncosine distance, k-medians clustering, \\n68–69\\ncosine similarity matrix, 172–174\\nCOUNTIF function, 116\\nCOUNTIFS statement, 235\\nCPLEX, 118\\nCRAN (Comprehensive R Archive \\nNetwork), 372–373\\ncritical values, 310–311\\ncutoff  values, 233\\nD\\ndata\\ncopying, 4–5\\nmerging, VLOOKUP and, 12\\nData Laboratory (Gephi), 168–170\\ndata mining, exploratory, 29–30\\ndata sources, k-means clustering, 37–38\\ndata standardization, 40\\ndataframe, 368–370\\ndecision stumps, 254–257, 260–263\\nalpha value calculation, 276–277\\nmacros, 266\\nnumber of, 257–258\\ndependent situations, probability theory, \\n81–82\\ndependent variables, 208\\ndesign matrix (linear regression), 227\\nSSCP, 227–228\\ndistribution\\nCDF (cumulative distribution function), \\n146–148, 337\\nmean deviation, 147–148\\nstandard deviation, 147–148\\ncentral limit theorem, 146\\nMonte Carlo simulation, 149\\nprobability distribution, 145–146\\nstandard normal distribution, 343–344\\nuniform distribution, 146\\ndivisive clustering, 185–192\\nDocGraph, 156\\ndocument classiﬁ cation, 77\\ndouble exponential smoothing, \\n299–313\\ndummy variables, 210–212\\nE\\nedges, network graphs, 156, 158\\nkNN (k nearest neighbors) graph, 176\\nr-neighborhood graphs, 176\\nensemble modeling, 251\\nEnsemble.xlsm, 252\\nerror in calculation column, 217–218\\nEuclidean distance, 41–44, 345–347\\nevolutionary algorithms, 115–116\\nExcel\\nconstraints, 110–112\\nGRG, 218\\nk-medians clustering, 69–75\\nsilhouette, 57–60\\nversion diff erences, 1\\nexploratory data mining, 29–30\\nexponential smoothing, 288–290\\ndouble exponential smoothing, \\n299–313\\nforecast setup, 290–296\\nHolt’s Trend-Corrected Exponential \\nSmoothing, 299–313'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 424, 'page_label': '403'}, page_content='Index 403\\nMultiplicative Holt-Winters Smoothing, \\n313–333\\ntrends, 296–299\\nF\\nF test, 223–225\\nfactoring, R, 364–367\\nfalse positive rate, 236–237\\nfan chart, 331–333\\nfeatures, independent variables, 208\\nﬁ lters, 13–16\\nFind and Replace, 9–10\\nﬂ oating-point underﬂ ow, 86\\nforecasting, 285\\nautocorrelations, 306–313\\ncorrelogram, 310–313\\ncritical values, 310–311\\nfuture periods, 303–304\\ngraphing, 296\\none-step forecast column, 291–292\\nerror optimization, 293–295\\nHolt’s Trend-Correct Exponential \\nSmoothing, 304–306\\nprediction intervals, 285, 327–331\\nR, 385–389\\nsmoothing\\nexponential, 288–299\\nSES (simple exponential smoothing), \\n288–290\\ntime series data, 286–287\\ndeseasonalizing, 318\\nseasonality, 314–315\\nFormat Cells menu, 5–6\\nformatting\\ncells, 5–7\\nconditional, 6–7\\nformulas\\narrays, 19–20\\ncopying, 4–5\\nINDEX, 298\\nLINEST( ), 220\\nSUMPRODUCT, 19–20\\nvalues, locating, 10–11\\nVLOOKUP, 12\\nFreeze Panes, 3\\nFreeze Top Row, 3\\nfunctions\\nBINOMDIST, 116\\nCOUNTIF, 116\\nHLOOKUP, 116\\nIF, 116\\nINDEX, 116\\nLARGE, 116\\nLINEST, 297\\nMATCH, 116\\nMAX, 116\\nMEDIAN, 116\\nMIN, 116\\nMINVERSE, 226\\nMMULT, 226\\nnon-linear, 116\\nNORMDIST, 116, 337\\nOFFSET, 116\\nPERCENTILE, 337–338\\nSUMIF, 116\\nSUMPRODUCT, 109\\nTDIST, 297\\nVLOOKUP, 116\\nG\\nGephi, 158, 159\\nData Laboratory, 168–170\\ngraph layout, 162–164\\ninstallation, 160–162\\nmodularity, 197–198\\nnode degrees, 165–166\\nprinting, 166–168\\nglobal outliers, 353\\ngraphs. See also charts; network graphs\\ndata preparation, 342–345\\nforecasting and, 296\\nkNN (k nearest neighbors), 347–348\\nmodularity\\npenalities, 179–183\\npoints, 179–183\\noutlier detection and, 345–347\\nindegree, 348–351\\nk-distance, 351–353\\nLOFs, 353–358\\nGRG, 218\\nGurobi, 118\\nH\\nHadlum versus Hadlum, 336–337\\nhierarchical partitioning, 185\\nhigh-level class probabilities, 84–85\\nHLOOKUP function, 116\\nHolt’s Trend-Corrected Exponential \\nSmoothing, 299–313'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 425, 'page_label': '404'}, page_content='Index404\\nI\\nidiot’s Bayes. See naïve Bayes\\nIF function, 114, 116\\nimage clustering, 30\\nindegree (graphs), 166\\noutlier detection, 348–351\\nindependent variables, 208\\nINDEX formula, 298\\nINDEX function, 116\\ninteger programming, switches, 133\\nintercept of linear model, 214\\nIQR (Interquartile Range), \\n337–338\\nJ\\nJoey Bag O’Donuts Wholesale Wine \\nEmporium, 36\\njoint probability, 80–81\\nchain rule of probability, 81\\nJuiceLand, 120–121\\nSolver, 124–126\\nK\\nKDD (knowledge discovery in \\ndatabases), 30\\nk-distance, graph outlier detection, \\n351–353\\nk-means clustering, 30–35\\ncluster centers, 46–48\\ndata source, 37–38\\ndistance, 44–46\\nmatrix, 55–56\\nﬁ ve clusters, 60–64\\nfour clusters, 41\\nJoey Bag O’Donuts Wholesale Wine \\nEmporium, 36\\nk groups, 30–35\\nPivotTables, 38–39\\nsilhouette, 53–60\\n5-Means clustering, 64–66\\nspherical k-means, 372–373\\nk-medians clustering, 66–67\\ncosine distance, 68–69\\nExcel, 69–75\\nManhattan distance, 67–68\\nkNN (k nearest neighbor), 336\\noutlier detection and, \\n347–348\\nL\\nLARGE function, 116\\nlaw of total probability, 80\\nlayout, Gephi graph, 162–164\\nlevel sets, 105–106\\nlexical content, stop words and, 91\\nLibreOffi  ce, 1\\nlinear programming, 102, 103–104\\nExcel and, 108–117\\nfractional solutions, 113\\nlevel sets, 105–106\\npolytopes, 103–105\\nsimplex method, 106–108\\nlinear regression\\ncoeffi  cient, 214\\ncutoff  values, 233\\ndesign matrix, 227\\nSSCP, 227–228\\nfalse positive rate, 236–237\\nintercept, 214\\nLINEST( ) formula, 220\\nlogistic regression comparison, 245–248\\nmetric trade-off s, 238–239\\npositive predictive value, 234–235\\nROC (Receiver Operating \\nCharacteristic) curve, 238–239\\nsimple model, 213–215\\nstatistics, 221\\ncoeffi  cient standard error, 226–227\\ncoeffi  cient tests, 226–230\\nF test, 223–225\\nprediction standard error, 226\\nR-squared, 222–223\\nt distribution, 230\\nt test, 226–230\\nsum of squared error, 215\\ntraining the model, 218–220\\ntrue negative rate, 235–236\\ntrue positive rate/recall/sensitivity, 237\\nvalidation set, 231–233\\nLINEST( ) formula, 220\\nLINEST function, 297\\nlink function, 240–241\\nlink spam, 166\\nlocal outliers, 353\\nLOF (local outlier factors), 353–358\\nlogistic regression, 239–240\\nlinear regression comparison, 245–248\\nlink function, 240–241'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 426, 'page_label': '405'}, page_content='Index 405\\nlog-likelihood, 244–245\\nreoptimizing, 241–243\\nstatistical tests, 245\\nlower inner fence (Tukey fences), 338\\nM\\nmachine learning, 30\\nmacros, recording, 266\\nMailChimp.com, 29\\nMandrill.com, 77–79\\nMandrill.com, 77–79\\nMandrill.xlsx, 87\\nManhattan distance, 67–68\\nMATCH function, 116\\nmatrix inversion, 226\\nmatrix multiplication, 226\\nMAX function, 116\\nmean deviation, CDF, 147–148\\nmeasurement, Euclidean distance, 41–44\\nMEDIAN function, 116\\nmedian regression, 221\\nmerging, VLOOKUP and, 12\\nMIN function, 116\\nminimax formulation, 131–132\\nMINVERSE function, 226\\nmissing values, 253–254\\nMMULT function, 226\\nmodularity, Gephi, 197–198\\nmodularity maximization, 156\\npenalities, 179–183\\npoints, 179–183\\nMonte Carlo simulation, 149\\nMultiplicative Holt-Winters Smoothing, \\n313–333\\nN\\nnaïve Bayes, 77\\nbag of words model, 79\\nconditional probability tables, 94–98\\nrare words, 85–86\\nnavigation, Control button, 2–3\\nnetwork graphs, 155\\nadjacency matrix, 158\\naffi  nity matrix, 159\\nbinary tree, 193–197\\ncosine similarity matrix, 172–174\\nDocGraph, 156\\nedges, 156, 158\\nkNN (k nearest neighbors) graph, 176\\nr-neighborhood graphs, 176\\nGephi, 158\\nlayout, 162–164\\nnode degrees, 165–166\\nprinting, 166–168\\nindegree, 166\\nlink spam, 166\\nnodes, 156, 158\\nNodeXL, 158\\noutdegree, 166\\noutlier detection, 166\\nr-Neighborhood graph, 174–185\\nsymmetry, 158\\nundirected, 158\\nvisualizing, 157–158\\nWineNetwork.xlsx, 170–172\\nNLP (natural language processing), 87\\nlexical content, 91\\nstop words, 91\\nnode impurity, 255–256\\nnodes, network graphs, 156, 158\\nNodeXL, 158\\nnon-linear functions, 116\\nNORMDIST function, 116, 337\\nnull hypothesis, 224\\nO\\nOFFSET function, 116\\none-step forecast column, 291–292\\nerror optimization, 293–295\\nHolt’s Trend-Correct Exponential \\nSmoothing, 304–306\\nOpenSolver, 26–27, 118\\nvariables, multiplying, 137–144\\noptimization, need for, 102–103\\nOptimization Model tab, 127\\noptimization models, 20–26, 121–124, \\n127–128\\nversus artiﬁ cial intelligence model, \\n101–102\\nOrangeJuiceBlending.xlsx, 118\\noutdegree (graphs), 166\\noutlier detection, 166, 335–336\\nglobal outliers, 353\\ngraphing, 345–347\\ndata preparation, 342–345\\nindegree, 348–351\\nk-distance, 351–353'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 427, 'page_label': '406'}, page_content='Index406\\nIQR (Interquartile Range), 337–338\\nkNN (k nearest neighbor), 347–348\\nlocal outliers, 353\\nLOF (local outlier factors), 353–358\\nR, 389–394\\nTukey fences, 337–338\\nlimitations, 340–341\\nspreadsheets, 338–340\\nunsupervised machine learning, 336\\noutliers\\nbagging and, 271\\noverview, 335\\noversampling, 210\\nP-Q\\np( ), 79–80\\npartitioning, hierarchical, 185\\nPaste Special, 7–8\\nPERCENTILE function, 337–338\\nPivotTables, 16–19\\nk-means clustering, 38–39\\nPivotTable Builder, 16–17\\npolytopes, 103–105\\nsimplex method, 106–108\\npositive predictive value, 234–235\\nprediction intervals, 327–331\\nprediction standard error, 226\\nPregnancy Duration.xlsx, 336\\npregnancy length, 336–337\\nprinting in Gephi, 166–168\\nprobability distribution, 145–146\\nprobability theory, 79–80\\nBayes rule, 82\\nchain rule of probability, 81\\nconditional probabilities, 80\\nBayes rule, 82\\ntoken counting, 92–93\\ndependent situations, 81–82\\nﬂ oating-point underﬂ ow, 86\\nhigh-level class probabilities, 84–85\\nindependent events, 81\\njoint probability, 80–81\\nlaw of total probability, 80\\nmultiplication rule of probability, 81\\nR\\nR (programming language)\\naggregate( ) function, 374\\nboxplot( ) function, 390–392\\nc( ) function, 364–365\\ncbind( ) function, 368, 375–376\\nCRAN (Comprehensive R Archive \\nNetwork), 372–373\\ndata input, 363\\ndataframe, 368–370\\ndata.frame( ) function, 368–369\\ndownloading, 362\\nfactor( ) function, 369–370\\nfactoring, 364–367\\nforecast( ) function, 387–388\\nforecasting, 385–389\\nfunctions, built-in, 363\\nglm( ) function, 378\\ninstallation, 362\\nIQR( ) function, 390\\nLength( ) function, 365\\nlibrary( ) function, 372–373\\nlofactor( ) function, 393\\nmatrices, 367–368\\nmatrix function, 367\\norder( ) function, 376–377\\noutlier detection, 389–394\\npackages, 363\\nperformance( ) function, 383\\nplot( ) function, 384\\npredict( ) function, 382\\nprint function, 362\\nrandomForest( ) function, 378\\nrbind( ) function, 368\\nread.csv( ) function, 374\\nreading data into, 370–371\\nrow.names( ) function, 374\\nscale( ) function, 392–393\\nsetwd( ) command, 370\\nskmeans( ) function, 373\\nskmeans package, 372\\nspherical k-means, 372–373\\nstr( ) function, 373–374, 378\\nsummary( ) function, 378\\nsummary function, 370\\nt function, 367\\nts( ) function, 386\\nvarImpPlot( ) function, 381\\nvector math, 364–367'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 428, 'page_label': '407'}, page_content='Index 407\\nwhich( ) function, 366, 374, 390\\nworking directory, 370–371\\nwrite.csv( ) function, 374\\nrandom forest model, 251\\nreplacement and, 271\\nrandomForest package, 271\\nrare words, naÏve Bayes and, 85–86\\nreferences, absolute, Solver, 110\\nregression\\nlinear\\ncoeffi  cient, 214\\ncompared to logistic, 245–248\\ncutoff  values, 233\\ndesign matrix, 227–228\\nfalse positive rate, 236–237\\nintercept, 214\\nLINEST( ) formula, 220\\nmetric trade-off s, 238–239\\npositive predictive value, \\n234–235\\nROC (Receiver Operating \\nCharacteristic) curve, 238–239\\nsimple model, 213–215\\nstatistics, 221–230\\nsum of squared error, 215\\ntraining the model, 218–220\\ntrue negative rate, 235–236\\ntrue positive rate/recall/sensitivity, \\n237\\nvalidation set, 231–233\\nlogistic, 239–240\\ncompared to linear, 245–248\\nlink function, 240–241\\nlog-likelihood, 244–245\\nreoptimizing, 241–243\\nstatistical tests, 245\\nmedian, 221\\nresidual sum of squares, 222\\nRetailMart (pregnant customers)\\ndata, 215–217\\ndummy variables, 210–212\\nerror in calculation column, \\n217–218\\nfeature set, 207–208\\nfolic acid stump, 254–257\\nlinear regression, 213–239\\nlogistic regression, 239–248\\ntraining data, 209–210\\nreweighting weighted errors, \\n277–278\\nrisk, 144–145\\ndistribution\\nCDF (cumulative distribution \\nfunction), 146–148\\ncentral limit theorem, 146\\nprobability distribution, 145–146\\nr-neighborhood graph, 174–185\\nROC (Receiver Operating Characteristic) \\ncurve, 238–239, 252\\nrows, freezing, 3\\nR-squared, linear regression, 222–223\\nS\\nscenarios, standard deviation, 148–150\\nconstraints, 151–153\\nschool dance analogy for clustering, \\n31–35\\nseasonality (forecasting), 314–315\\nSES (simple exponential smoothing), \\n288–290\\nsilhouette\\n5-Means clustering, 64–66\\nExcel and, 57–60\\nk-means clustering, 53–60\\nsimplex method, 106–108\\nsmoothing, exponential, 288–290\\ndouble exponential smoothing, 299–313\\nforecast setup, 290–296\\nHolt’s Trend-Corrected Exponential \\nSmoothing, 299–313\\nMultiplicative Holt-Winters Smoothing, \\n313–333\\ntrends, 296–299\\nSolver, 20–26\\nabsolute references, 110\\nclustering, 34–35\\nresults, 49\\nJuiceLand problem, 124–126\\nlinear regression, training the model, \\n218–220\\nOpenSolver, 26–27\\nsorting, 13–16\\nspaces, 88–91\\nspherical k-means, 372–373\\nspreadsheets\\narrays, formulas, 19–20\\ncharts, inserting, 8–9\\ncopying\\ndata, 4–5\\nformulas, 4–5'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 429, 'page_label': '408'}, page_content='Index408\\nﬁ lters, 13–16\\nformatting\\ncells, 5–7\\nconditional, 6–7\\nFreeze Panes, 3\\nFreeze Top Row, 3\\nHolt’s Trend-Corrected Exponential \\nSmoothing, 300–306\\nnavigating, Control button, 2–3\\nPaste Special option, 7–8\\nPivotTables, 16–19\\nsorting, 13–16\\nTukey fences, 338–340\\nlimitations, 340–341\\nSSCP (sum of squares and cross \\nproducts) matrix, 227–228\\nstandard deviation\\nCDF, 147–148\\nscenarios from, 148–150\\nconstraints, 151–153\\nstandard normal distribution, 343–344\\nstandardizing data, 40\\nstatistics, 221\\ncoeffi  cient standard error, 226–227\\ncoeffi  cient tests, 226–230\\nF test, 223–225\\nlogistic regression, 245\\nmatrix inversion, 226\\nmatrix multiplication, 226\\nprediction standard error, 226\\nresidual sum of squares, 222\\nR-squared, 222–223\\nt distribution, 230\\nt test, 226–230\\ntotal sum of squares, 222\\nstop words, 91\\nSUBSTITUTE command, 87–88\\nsum of squared error, 215\\nSUMIF function, 116\\nSUMPRODUCT formula, 19–20\\nSUMPRODUCT function, 109\\nsupervised machine learning, 30\\nswitches, 133\\nSwordForecasting.xlsm, 286\\nsymmetry in network graphs, 158\\nT\\nt distribution, 230\\nt test, 226–230\\nTDIST function, 297\\ntime series data, forecasting and, 286–287\\ntokens, conditional probability, 92–93\\ntotal sum of squares, 222\\ntraining data\\ndecision stumps, 260–263\\noversampling, 210\\nrandom sample, 258–260\\ntrends, forecasting, exponential \\nsmoothing, 296–299\\ntriple exponential smoothing, 313–333\\ntrue negative rate, 235–236\\ntrue positive rate/recall/sensitivity, 237\\nTukey fences, 337–338\\nlimitations, 340–341\\nlower inner fence, 338\\nspreadsheets, 338–340\\nupper inner fence, 338\\nU\\nundirected network graphs, 158\\nuniform distribution, 146\\nunsupervised machine learning, 30, 336\\nupper inner fence (Tukey fences), 338\\nV\\nvalidation set, 231–233\\nvalues\\nlocating, with formulas, 10–11\\nmissing, 253–254\\nvariables\\ncoeffi  cient, 214\\ndependent, 208\\ndummy variables, 210–212\\nindependent, 208\\nmultiplying, 137–144\\nvector math, R, 364–367\\nVLOOKUP formulas, 12\\nVLOOKUP function, 116\\nVoronoi diagram, 32'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 430, 'page_label': '409'}, page_content='Index 409\\nW–Z\\nweak learners, 254–255\\nweighted errors, 272\\nreweighting, 277–278\\nWineKMC.xlsx, 36\\nWineNetwork.xlsx, building graph, \\n170–172\\nworkbooks\\nEnsemble.xlsm, 252\\nMandrill.xlsx, 87\\nOrangeJuiceBlending.xlsx, 118\\nPregnancy Duration.xlsx, 336\\nSwordForecasting.xlsm, 286\\nWineKMC.xlsx, 36'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 431, 'page_label': '410'}, page_content=''),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 432, 'page_label': '411'}, page_content=''),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 433, 'page_label': '412'}, page_content='')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36ffa386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 0, 'page_label': 'C1'}, page_content='')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af4b8ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]', 'creationdate': '2013-10-16T12:18:21+05:30', 'author': 'Foreman, John W.', 'ebx_publisher': '/Wiley', 'moddate': '2013-12-20T12:33:54+10:00', 'title': 'Data Smart', 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf', 'total_pages': 434, 'page': 2, 'page_label': 'i'}, page_content='Data Smart\\nJohn W. Foreman\\nUsing Data Science to \\nTransform Information \\ninto Insight')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "696125c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'Acrobat Distiller 9.0.0 (Windows)',\n",
      " 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]',\n",
      " 'creationdate': '2013-10-16T12:18:21+05:30',\n",
      " 'author': 'Foreman, John W.',\n",
      " 'ebx_publisher': '/Wiley',\n",
      " 'moddate': '2013-12-20T12:33:54+10:00',\n",
      " 'title': 'Data Smart',\n",
      " 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform '\n",
      "           'Information into Insight (John W. Foreman) ().pdf',\n",
      " 'total_pages': 434,\n",
      " 'page': 0,\n",
      " 'page_label': 'C1'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pp(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "186dd55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages = []\n",
    "for doc in loader.lazy_load():\n",
    "    pages.append(doc)\n",
    "    if len(pages) >= 10:\n",
    "        # do some paged operation, e.g.\n",
    "        # index.upsert(page)\n",
    "\n",
    "        pages = []\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4828631d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 409\n",
      "W–Z\n",
      "weak learners, 254–255\n",
      "weighted errors, 272\n",
      "reweighting, 277–278\n",
      "WineKMC.xlsx, 36\n",
      "Wine\n",
      "{'producer': 'Acrobat Distiller 9.0.0 (Windows)',\n",
      " 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]',\n",
      " 'creationdate': '2013-10-16T12:18:21+05:30',\n",
      " 'author': 'Foreman, John W.',\n",
      " 'ebx_publisher': '/Wiley',\n",
      " 'moddate': '2013-12-20T12:33:54+10:00',\n",
      " 'title': 'Data Smart',\n",
      " 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform '\n",
      "           'Information into Insight (John W. Foreman) ().pdf',\n",
      " 'total_pages': 434,\n",
      " 'page': 430,\n",
      " 'page_label': '409'}\n"
     ]
    }
   ],
   "source": [
    "print(pages[0].page_content[:100])\n",
    "pprint.pp(pages[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6381bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20fdd1be",
   "metadata": {},
   "source": [
    "Extract the whole PDF as a single langchain document object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "414cd823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'producer': 'Acrobat Distiller 9.0.0 (Windows)',\n",
      " 'creator': 'calibre (0.9.42) [http://calibre-ebook.com]',\n",
      " 'creationdate': '2013-10-16T12:18:21+05:30',\n",
      " 'author': 'Foreman, John W.',\n",
      " 'ebx_publisher': '/Wiley',\n",
      " 'moddate': '2013-12-20T12:33:54+10:00',\n",
      " 'title': 'Data Smart',\n",
      " 'source': 'D:\\\\Library\\\\Sliit\\\\Data Smart Using Data Science to Transform '\n",
      "           'Information into Insight (John W. Foreman) ().pdf',\n",
      " 'total_pages': 434}\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\n",
    "    \"D:\\Library\\Sliit\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf\",\n",
    "    mode=\"single\",\n",
    ")\n",
    "docs = loader.load()\n",
    "print(len(docs))\n",
    "pprint.pp(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096eb2ce",
   "metadata": {},
   "source": [
    "Add a custom pages_delimiter to identify where are ends of pages in single mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24cb0f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart\n",
      "John W. Foreman\n",
      "Using Data Science to \n",
      "Transform Information \n",
      "into Insight\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart: Using Data Science to Transform Information into Insight\n",
      "Published by\n",
      "John Wiley & Sons, Inc.\n",
      "10475 Crosspoint Boulevard\n",
      "Indianapolis, IN 46256\n",
      "www . wi l ey . com\n",
      "Copyright © 2014 by John Wiley & Sons, Inc., Indianapolis, Indiana\n",
      "Published simultaneously in Canada\n",
      "ISBN: 978-1-118-66146-8\n",
      "ISBN: 978-1-118-66148-2 (ebk)\n",
      "ISBN: 978-1-118-83986-7 (ebk)\n",
      "Manufactured in the United States of America\n",
      "10 9 8 7 6 5 4 3 2 1\n",
      "No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or \n",
      "by any means, electronic, mechanical, photocopying, recording, scanning or otherwise, except as permit-\n",
      "ted under Sections 107 or 108 of the 1976 United States Copyright Act, without either the prior written \n",
      "permission of the Publisher, or authorization through payment of the appropriate per-copy fee to the \n",
      "Copyright Clearance Center, 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax (978) 646-\n",
      "8600. Requests to the Publisher for permission should be addressed to the Permissions Department, John \n",
      "Wiley & Sons, Inc., 111 River Street, Hoboken, NJ  07030, (201) 748-6011, fax (201) 748-6008, or online \n",
      "at \n",
      "http://www.wiley.com/go/permissions.\n",
      "Limit of Liability/Disclaimer of Warranty:  The publisher and the author make no representations or war-\n",
      "ranties with respect to the accuracy or completeness of the contents of this work and speciﬁ  cally disclaim all \n",
      "warranties, including without limitation warranties of ﬁ  tness for a particular purpose. No warranty may be \n",
      "created or extended by sales or promotional materials. The advice and strategies contained herein may not \n",
      "be suitable for every situation. This work is sold with the understanding that the publisher is not engaged in \n",
      "rendering legal, accounting, or other professional services. If professional assistance is required, the services \n",
      "of a competent professional person should be sought. Neither the publisher nor the author shall be liable for \n",
      "damages arising herefrom. The fact that an organization or Web site is referred to in this work as a citation \n",
      "and/or a potential source of further information does not mean that the author or the publisher endorses \n",
      "the information the organization or website may provide or recommendations it may make. Further, readers \n",
      "should be aware that Internet websites listed in this work may have changed or disappeared between when \n",
      "this work was written and when it is read.\n",
      "For general information on our other products and services please contact our Customer Care \n",
      "Department within the United States at (877) 762-2974, outside the United States at (317) 572-3993 or fax \n",
      "(317) 572-4002.\n",
      "Wiley publishes in a variety of print and electronic formats and by print-on-demand. Some material \n",
      "included with standard print versions of this book may not be included in e-books or in print-on-demand. \n",
      "If this book refers to media such as a CD or DVD that is not included in the version you purchased, you \n",
      "may download this material at \n",
      "http://booksupport.wiley.com. For more information about Wiley \n",
      "products, visit www.wiley.com.\n",
      "Library of Congress Control Number:  2013946768\n",
      "Trademarks: Wiley and the Wiley logo are trademarks or registered trademarks of John Wiley & Sons, Inc. \n",
      "and/or its affi  liates, in the United States and other countries, and may not be used without written permission. \n",
      "All other trademarks are the property of their respective owners. John Wiley & Sons, Inc. is not associated \n",
      "with any product or vendor mentioned in this book.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "To my wife, Lydia. What you do each day is impossibly rad. If it weren’t for you, \n",
      "I’d have lost my hair (and my mind) eons ago.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Executive Editor\n",
      "Carol Long\n",
      "Senior Project Editor\n",
      "Kevin Kent\n",
      "Technical Editors\n",
      "Greg Jennings \n",
      "Evan Miller \n",
      "Production Editor\n",
      "Christine Mugnolo\n",
      "Copy Editor\n",
      "Kezia Endsley \n",
      "Editorial Manager\n",
      "Mary Beth Wakeﬁ eld \n",
      "Freelancer Editorial Manager\n",
      "Rosemarie Graham\n",
      "Associate Director of Marketing\n",
      "David Mayhew\n",
      "Marketing Manager\n",
      "Ashley Zurcher\n",
      "Business Manager\n",
      "Amy Knies\n",
      "Vice President and Executive Group \n",
      "Publisher\n",
      "Richard Swadley\n",
      "Associate Publisher\n",
      "Jim Minatel\n",
      "Project Coordinator, Cover\n",
      "Katie Crocker\n",
      "Proofreader\n",
      "Nancy Carrasco\n",
      "Indexer\n",
      "Johnna van Hoose Dinse\n",
      "Cover Image\n",
      "Courtesy of John W. Foreman\n",
      "Cover Designer\n",
      "Ryan Sneed\n",
      "Credits\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "John W. Foreman  is the Chief Data Scientist for MailChimp.com. He’s also a \n",
      "recovering management consultant who’s done a lot of analytics work for large \n",
      "businesses (Coca-Cola, Royal Caribbean, Intercontinental Hotels) and the gov-\n",
      "ernment (DoD, IRS, DHS, FBI). John can often be found speaking about the trials \n",
      "and travails of implementing analytic solutions in business—check \n",
      "John-Foreman\n",
      ".com to see if he’s headed to your town.\n",
      "When he’s not playing with data, John spends his time hiking, watching copious \n",
      "amounts of television, eating all sorts of terrible food, and raising three smelly boys.\n",
      "About the Author\n",
      "Greg Jennings is a data scientist, software engineer, and co-founder of ApexVis. After \n",
      "completing a master’s degree in materials science from the University of Virginia, he \n",
      "began his career with the Analytics group of Booz Allen Hamilton, where he grew \n",
      "a team providing predictive analytics and data visualization solutions for planning \n",
      "and scheduling problems.\n",
      "After leaving Booz Allen Hamilton, Greg cofounded his ﬁ  rst startup, Decision \n",
      "Forge, where he served as CTO and helped develop a web-based data mining plat-\n",
      "form for a government client. He also worked with a major media organization to \n",
      "develop an educational product that assists teachers in accessing targeted content for \n",
      "their students, and with a McLean-based startup to help develop audience modeling \n",
      "applications to optimize web advertising campaigns.\n",
      "After leaving Decision Forge, he cofounded his current business ApexVis, focused \n",
      "on helping enterprises get maximum value from their data through custom data \n",
      "visualization and analytical software solutions. He lives in Alexandria, Virginia, \n",
      "with his wife and two daughters.\n",
      "Evan Miller  received his bachelor’s degree in physics from Williams College in \n",
      "2006 and is currently a PhD student in economics at the University of Chicago. \n",
      "His research interests include speciﬁ cation testing and computational methods in \n",
      "econometrics. Evan is also the author of Wizard, a popular Mac program for per-\n",
      "forming statistical analysis, and blogs about statistics problems and experiment \n",
      "design at \n",
      "http://www.evanmiller.org.\n",
      "About the Technical Editors\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "T\n",
      "his book started after an improbable number of folks checked out my analytics \n",
      "blog, Analytics Made Skeezy. So I’d like to thank those readers as well as my \n",
      "data science Twitter pals who’ve been so supportive. And thanks to Aarron Walter, \n",
      "Chris Mills, and Jon Duckett for passing the idea for this book on to Wiley based \n",
      "on my blog’s silly premise.\n",
      "I’d also like to thank the crew at MailChimp for making this happen. Without \n",
      "the supportive and adventurous culture fostered at MailChimp, I’d not have felt \n",
      "conﬁ dent enough to do something so stupid as to write a technical book while \n",
      "working a job and raising three boys. Speciﬁ  cally, I couldn’t have done it without \n",
      "the daily assistance of Neil Bainton and Michelle Riggin-Ransom. Also, I’m indebted \n",
      "to Ron Lewis, Josh Rosenbaum, and Jason Travis for their work on the cover and \n",
      "marketing video for the book.\n",
      "Thanks to Carol Long at Wiley for taking a chance on me and to all the editors \n",
      "for their expertise and hard work. Big thanks to Greg Jennings for working all the \n",
      "spreadsheets!\n",
      "Many thanks to my parents for reading my sci-ﬁ  novel and not telling me to quit \n",
      "writing.\n",
      "Acknowledgments\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Contents\n",
      "Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii\n",
      " 1 Everything You Ever Needed to Know about Spreadsheets but Were \n",
      "Too Afraid to Ask  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1\n",
      "Some Sample Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .2\n",
      "Moving Quickly with the Control Button . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .2\n",
      "Copying Formulas and Data Quickly . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4\n",
      "Formatting Cells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .5\n",
      "Paste Special Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7\n",
      "Inserting Charts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8\n",
      "Locating the Find and Replace Menus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . .9\n",
      "Formulas for Locating and Pulling Values  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .10\n",
      "Using VLOOKUP to Merge Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n",
      "Filtering and Sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n",
      "Using PivotTables  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n",
      "Using Array Formulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n",
      "Solving Stuff with Solver  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n",
      "OpenSolver: I Wish We Didn’t Need This, but We Do . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n",
      "Wrapping Up  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .27\n",
      " 2 Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  . . . . . . . . 29\n",
      "Girls Dance with Girls, Boys Scratch Their Elbows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n",
      "Getting Real: K-Means Clustering Subscribers in E-mail Marketing . . . . . . . . . . . . . . . . . . . . . . .35\n",
      "Joey Bag O’ Donuts Wholesale Wine Emporium  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .36\n",
      "The Initial Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .36\n",
      "Determining What to Measure  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .38\n",
      "Start with Four Clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n",
      "Euclidean Distance: Measuring Distances as the Crow Flies . . . . . . . . . . . . . . . . . . . . . . . . . 41\n",
      "Distances and Cluster Assignments for Everybody! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n",
      "Solving for the Cluster Centers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n",
      "Making Sense of the Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Contents viii\n",
      "Getting the Top Deals by Cluster . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n",
      "The Silhouette: A Good Way to Let Different K Values \n",
      "Duke It Out . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .53\n",
      "How about Five Clusters?  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n",
      "Solving for Five Clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n",
      "Getting the Top Deals for All Five Clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n",
      "Computing the Silhouette for 5-Means Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n",
      "K-Medians Clustering and Asymmetric Distance Measurements . . . . . . . . . . . . . . . . . . . . . . . . 66\n",
      "Using K-Medians Clustering  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n",
      "Getting a More Appropriate Distance Metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .67\n",
      "Putting It All in Excel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n",
      "The Top Deals for the 5-Medians Clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n",
      "Wrapping Up  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .75\n",
      " 3 Naïve Bayes and the Incredible Lightness of Being an Idiot . . . . . . . . . . . . . . . . . . . . 77\n",
      "When You Name a Product Mandrill, You’re Going to Get Some Signal and \n",
      "Some Noise  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .77\n",
      "The World’s Fastest Intro to Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .79\n",
      "Totaling Conditional Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n",
      "Joint Probability, the Chain Rule, and Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n",
      "What Happens in a Dependent Situation? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n",
      "Bayes Rule  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n",
      "Using Bayes Rule to Create an AI Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .83\n",
      "High-Level Class Probabilities Are Often Assumed to Be Equal . . . . . . . . . . . . . . . . . . . . . 84\n",
      "A Couple More Odds and Ends . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .85\n",
      "Let’s Get This Excel Party Started . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .87\n",
      "Removing Extraneous Punctuation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .87\n",
      "Splitting on Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . 88\n",
      "Counting Tokens and Calculating Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n",
      "And We Have a Model! Let’s Use It. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n",
      "Wrapping Up  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n",
      " 4 Optimization Modeling: Because That “Fresh Squeezed” Orange Juice \n",
      "Ain’t Gonna Blend Itself . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .10 1\n",
      "Why Should Data Scientists Know Optimization?  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .102\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Contents ix\n",
      "Starting with a Simple Trade-Off . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .103\n",
      "Representing the Problem as a Polytope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .103\n",
      "Solving by Sliding the Level Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .105\n",
      "The Simplex Method: Rooting around the Corners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n",
      "Working in Excel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\n",
      "There’s a Monster at the End of This Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n",
      "Fresh from the Grove to Your Glass...with a Pit Stop Through a Blending Model . . . . . . . . . 118\n",
      "You Use a Blending Model  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\n",
      "Let’s Start with Some Specs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\n",
      "Coming Back to Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n",
      "Putting the Data into Excel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n",
      "Setting Up the Problem in Solver . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1 2 4\n",
      "Lowering Your Standards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .126\n",
      "Dead Squirrel Removal: The Minimax Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\n",
      "If-Then and the “Big M” Constraint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1 3 3\n",
      "Multiplying Variables: Cranking Up the Volume to 11  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\n",
      "Modeling Risk  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .144\n",
      "Normally Distributed Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n",
      "Wrapping Up  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .154\n",
      " 5 Cluster Analysis Part II: Network Graphs and Community Detection . . . . . . . . . . .155\n",
      "What Is a Network Graph? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .156\n",
      "Visualizing a Simple Graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n",
      "Brief Introduction to Gephi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\n",
      "Gephi Installation and File Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\n",
      "Laying Out the Graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .162\n",
      "Node Degree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .165\n",
      "Pretty Printing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .166\n",
      "Touching the Graph Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .168\n",
      "Building a Graph from the Wholesale Wine Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .170\n",
      "Creating a Cosine Similarity Matrix  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .172\n",
      "Producing an r-Neighborhood Graph  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\n",
      "How Much Is an Edge Worth? Points and Penalties in Graph Modularity . . . . . . . . . . . . . . . . 178\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Contents x\n",
      "What’s a Point and What’s a Penalty? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .179\n",
      "Setting Up the Score Sheet  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183\n",
      "Let’s Get Clustering! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .185\n",
      "Split Number 1  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .185\n",
      "Split 2: Electric Boogaloo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190\n",
      "And…Split 3: Split with a Vengeance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .192\n",
      "Encoding and Analyzing the Communities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193\n",
      "There and Back Again: A Gephi Tale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .197\n",
      "Wrapping Up  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202\n",
      " 6 The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression . . . . . . . . . . . . 205\n",
      "Wait, What? You’re Pregnant? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\n",
      "Don’t Kid Yourself . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206\n",
      "Predicting Pregnant Customers at RetailMart Using Linear Regression . . . . . . . . . . . . . . . . . 207\n",
      "The Feature Set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\n",
      "Assembling the Training Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209\n",
      "Creating Dummy Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .210\n",
      "Let’s Bake Our Own Linear Regression  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213\n",
      "Linear Regression Statistics: R-Squared, F Tests, t Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . .221\n",
      "Making Predictions on Some New Data and Measuring Performance . . . . . . . . . . . . . . 230\n",
      "Predicting Pregnant Customers at RetailMart Using Logistic Regression . . . . . . . . . . . . . . . . 239\n",
      "First You Need a Link Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240\n",
      "Hooking Up the Logistic Function and Reoptimizing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\n",
      "Baking an Actual Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 4\n",
      "Model Selection—Comparing the Performance of the Linear \n",
      "and Logistic Regressions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .245\n",
      "For More Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248\n",
      "Wrapping Up  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249\n",
      " 7 Ensemble Models: A Whole Lot of Bad Pizza . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .251\n",
      "Using the Data from Chapter 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .252\n",
      "Bagging: Randomize, Train, Repeat  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254\n",
      "Decision Stump Is an Unsexy Term for a Stupid Predictor  . . . . . . . . . . . . . . . . . . . . . . . . 254\n",
      "Doesn’t Seem So Stupid to Me! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .255\n",
      "You Need More Power! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .257\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Contents xi\n",
      "Let’s Train It . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258\n",
      "Evaluating the Bagged Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267\n",
      "Boosting: If You Get It Wrong, Just Boost and \n",
      "Try Again . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272\n",
      "Training the Model—Every Feature Gets a Shot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272\n",
      "Evaluating the Boosted Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280\n",
      "Wrapping Up  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283\n",
      " 8 Forecasting: Breathe Easy; You Can’t Win . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285\n",
      "The Sword Trade Is Hopping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286\n",
      "Getting Acquainted with Time Series Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286\n",
      "Starting Slow with Simple Exponential Smoothing  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288\n",
      "Setting Up the Simple Exponential Smoothing Forecast . . . . . . . . . . . . . . . . . . . . . . . . . . 290\n",
      "You Might Have a Trend . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296\n",
      " Holt’s Trend-Corrected Exponential Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299\n",
      "Setting Up Holt’s Trend-Corrected Smoothing in a Spreadsheet . . . . . . . . . . . . . . . . . . 300\n",
      "So Are You Done? Looking at Autocorrelations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306\n",
      "Multiplicative Holt-Winters Exponential Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313\n",
      "Setting the Initial Values for Level, Trend, and Seasonality . . . . . . . . . . . . . . . . . . . . . . . . . 315\n",
      "Getting Rolling on the Forecast. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319\n",
      "And...Optimize! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .324\n",
      "Please Tell Me We’re Done Now!!! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .326\n",
      "Putting a Prediction Interval around the Forecast . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .327\n",
      "Creating a Fan Chart for Effect  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331\n",
      "Wrapping Up  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .333\n",
      " 9 Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re \n",
      "Unimportant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\n",
      "Outliers Are (Bad?) People, Too . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .335\n",
      "The Fascinating Case of Hadlum v. Hadlum  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .336\n",
      "Tukey Fences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .337\n",
      "Applying Tukey Fences in a Spreadsheet  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .338\n",
      "The Limitations of This Simple Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340\n",
      "Terrible at Nothing, Bad at Everything . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341\n",
      "Preparing Data for Graphing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .342\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Contents xii\n",
      "Creating a Graph  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .345\n",
      "Getting the k Nearest Neighbors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347\n",
      "Graph Outlier Detection Method 1: Just Use the Indegree . . . . . . . . . . . . . . . . . . . . . . . . 348\n",
      "Graph Outlier Detection Method 2: Getting Nuanced with k-Distance . . . . . . . . . . . . . 351\n",
      "Graph Outlier Detection Method 3: Local Outlier Factors Are Where It’s At . . . . . . . .353\n",
      "Wrapping Up  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .358\n",
      " 10 Moving from Spreadsheets into R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .361\n",
      "Getting Up and Running with R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . 362\n",
      "Some Simple Hand-Jamming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .363\n",
      "Reading Data into R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370\n",
      "Doing Some Actual Data Science . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .372\n",
      "Spherical K-Means on Wine Data in Just a Few Lines  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .372\n",
      "Building AI Models on the Pregnancy Data  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .378\n",
      "Forecasting in R  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385\n",
      "Looking at Outlier Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389\n",
      "Wrapping Up  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394\n",
      "  Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395\n",
      "Where Am I? What Just Happened? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . .395\n",
      "Before You Go-Go . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .395\n",
      "Get to Know the Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396\n",
      "We Need More Translators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .397\n",
      "Beware the Three-Headed Geek-Monster: Tools, Performance, and \n",
      "Mathematical Perfection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .397\n",
      "You Are Not the Most Important Function of Your Organization . . . . . . . . . . . . . . . . . 400\n",
      "Get Creative and Keep in Touch! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400\n",
      "  Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . 401\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "What Am I Doing Here?\n",
      "You’ve probably heard the term data science ﬂ oating around recently in the media, in \n",
      "business books and journals, and at conferences. Data science can call presidential races, \n",
      "reveal more about your buying habits than you’d dare tell your mother, and predict just \n",
      "how many years those chili cheese burritos have been shaving off  your life. \n",
      "Data scientists, the elite practitioners of this art, were even labeled “sexy” in a recent \n",
      "Harvard Business Review article, although there’s apparently such a shortage that it’s kind \n",
      "of like calling a unicorn sexy. There’s just no way to verify the claim, but if you could see \n",
      "me as I type this book with my neck beard and the tired eyes of a parent of three boys, \n",
      "you’d know that sexy is a bit of an overstatement.\n",
      "I digress. The point is that there’s a buzz about data science these days, and that buzz \n",
      "is creating pressure on a lot of businesses. If you’re not doing data science, you’re gonna \n",
      "lose out to the competition. Someone’s going to come along with some new product called \n",
      "the “BlahBlahBlahBigDataGraphThing” and destroy your business. \n",
      "Take a deep breath. \n",
      "The truth is most people are going about data science all wrong. They’re starting with \n",
      "buying the tools and hiring the consultants. They’re spending all their money before they \n",
      "even know what they want, because a purchase order seems to pass for actual progress \n",
      "in many companies these days.\n",
      "By reading this book, you’re gonna have a leg up on those jokers, because you’re going \n",
      "to learn exactly what these techniques in data science are and how they’re used. When it \n",
      "comes time to do the planning, and the hiring, and the buying, you’ll already know how \n",
      "to identify the data science opportunities within your own organization.\n",
      "The purpose of this book is to introduce you to the practice of data science in a com-\n",
      "fortable and conversational way. When you’re done, I hope that much of that data science \n",
      "anxiety you’re feeling is replaced with excitement and with ideas about how you can use \n",
      "data to take your business to the next level.\n",
      "Introduction\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "xiv Introduction\n",
      "A Workable Deﬁ nition of Data Science\n",
      "To an extent, data science is synonymous with or related to terms like business analytics, \n",
      "operations research, business intelligence, competitive intelligence, data analysis and modeling, \n",
      "and knowledge extraction (also called knowledge discovery in databases or KDD). It’s just a \n",
      "new spin on something that people have been doing for a long time.\n",
      "There’s been a shift in technology since the heyday of those other terms. Advancements \n",
      "in hardware and software have made it easy and inexpensive to collect, store, and analyze \n",
      "large amounts of data whether that be sales and marketing data, HTTP requests from \n",
      "your website, customer support data, and so on. Small businesses and nonproﬁ  ts can \n",
      "now engage in the kind of analytics that were previously the purview of large enterprises.\n",
      "Of course, while data science is used as a catch-all buzzword for analytics today, data \n",
      "science is most often associated with data mining techniques such as artiﬁ cial intelligence, \n",
      "clustering, and outlier detection. Thanks to the cheap technology-enabled proliferation \n",
      "of transactional business data, these computational techniques have gained a foothold in \n",
      "business in recent years where previously they were too cumbersome to use in produc-\n",
      "tion settings.\n",
      "In this book, I’m going to take a broad view of data science. Here’s the deﬁ nition I’ll \n",
      "work from: \n",
      "Data science is the transformation of data using mathematics and statistics into valuable \n",
      "insights, decisions, and products. \n",
      "This is a business-centric deﬁ nition. It’s about a usable and valuable end product derived \n",
      "from data. Why? Because I’m not in this for research purposes or because I think data \n",
      "has aesthetic merit. I do data science to help my organization function better and create \n",
      "value; if you’re reading this, I suspect you’re after something similar. \n",
      "With that deﬁ nition in mind, this book will cover mainstay analytics techniques such \n",
      "as optimization, forecasting, and simulation, as well as more “hot” topics such as artiﬁ cial \n",
      "intelligence, network graphs, clustering, and outlier detection.\n",
      "Some of these techniques are as old as World War II. Others were introduced in the \n",
      "last 5 years. And you’ll see that age has no bearing on diffi  culty or usefulness. All these \n",
      "techniques—whether or not they’re currently the rage—are equally useful in the right \n",
      "business context. \n",
      "And that’s why you need to understand how they work, how to choose the right tech-\n",
      "nique for the right problem, and how to prototype with them. There are a lot of folks out\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "xvIntroduction\n",
      "there who understand one or two of these techniques, but the rest aren’t on their radar. If \n",
      "all I had in my toolbox was a hammer, I’d probably try to solve every problem by smack-\n",
      "ing it real hard. Not unlike my two-year-old.\n",
      "Better to have a few other tools at your disposal.\n",
      "But Wait, What about Big Data?\n",
      "You’ve heard the term big data even more than data science most likely. Is this a book on \n",
      "big data?\n",
      "That depends on how you deﬁ ne big data. If you deﬁ ne big data as computing simple \n",
      "summary statistics on unstructured garbage stored in massive, horizontally scalable, \n",
      "NoSQL databases, then no, this is not a book on big data.\n",
      "If you deﬁ ne big data as turning transactional business data into decisions and insight \n",
      "using cutting-edge analytics (regardless of where that data is stored), then yes, this is a \n",
      "book about big data. \n",
      "This is not a book that will be covering database technologies, like MongoDB and HBase. \n",
      "This is not a book that will be covering data science coding packages like Mahout, NumPy, \n",
      "various R libraries, and so on. There are other books out there for that stuff .\n",
      "But that’s a good thing. This book ignores the tools, the storage, and the code. Instead, \n",
      "it focuses as much as possible on the techniques. There are many folks out there who \n",
      "think that data storage and retrieval, with a little bit of cleanup and aggregation mixed \n",
      "in, constitutes all there is to know about big data. \n",
      "They’re wrong. This book will take you beyond the spiel you’ve been hearing from the \n",
      "big data software sales reps and bloggers to show you what’s really possible with your data. \n",
      "And the cool thing is that for many of these techniques, your dataset can be any size, small \n",
      "or large. You don’t have to have a petabyte of data and the expenses that come along with \n",
      "it in order to predict the interests of your customer base. If you have a massive dataset, \n",
      "that’s great, but there are some businesses that don’t have it, need it, and will likely never \n",
      "generate it. Like my local butcher. But that doesn’t mean his e-mail marketing couldn’t \n",
      "beneﬁ t from a little bacon versus sausage cluster detection.\n",
      "If data science books were workouts, this book would be all calisthenics—no machine \n",
      "weights, no ergs. Once you understand how to implement the techniques with even the \n",
      "most barebones of tools, you’ll ﬁ nd yourself free to implement them in a variety of tech-\n",
      "nologies, prototype with them with ease, buy the correct data science products from \n",
      "consultants, delegate the correct approach to your developers, and so on.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "xvi Introduction\n",
      "Who Am I?\n",
      "Let me pause a moment to tell you my story. It’ll go a long way to explaining why I teach \n",
      "data science the way I do. Many moons ago, I was a management consultant. I worked \n",
      "on analytics problems for organizations such as the FBI, DoD, the Coca-Cola Company, \n",
      "Intercontinental Hotels Group, and Royal Caribbean International. And through all these \n",
      "experiences I walked away having learned one thing—more people than just the scientists \n",
      "need to understand data science. \n",
      "I worked with managers who bought simulations when they needed an optimization \n",
      "model. I worked with analysts who only understood Gantt charts, so everything needed \n",
      "to be solved with Gantt charts. As a consultant, it wasn’t hard to win over a customer \n",
      "with any old white paper and a slick PowerPoint deck, because they couldn’t tell AI from \n",
      "BI or BI from BS.\n",
      "The point of this book is to broaden the audience of who understands and can imple-\n",
      "ment data science techniques. I’m not trying to turn you into a data scientist against your \n",
      "will. I just want you to be able to integrate data science as best as you can into the role \n",
      "you’re already good at.\n",
      "And that brings me to who you are.\n",
      "Who Are You?\n",
      "No, I haven’t been using data science to spy on you. I have no idea who you are, but thanks \n",
      "for shelling out some money for this book. Or supporting your local library. You can do \n",
      "that, too. \n",
      "Here are some archetypes (or personas for you marketing folks) I had in mind when \n",
      "writing this book. Maybe you are:\n",
      "• The vice president of marketing who wants to use her transactional business data \n",
      "more strategically to price products and segment customers. But she doesn’t under-\n",
      "stand the approaches her software developers and overpriced consultants are rec-\n",
      "ommending she try.\n",
      "• The demand forecasting analyst who knows his organization’s historical purchase \n",
      "data holds more insight about his customers than just the next quarter’s projections. \n",
      "But he doesn’t know how to extract that insight.\n",
      "• The CEO of an online retail start-up who wants to predict when a customer is likely \n",
      "to be interested in buying an item based on their past purchases.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "xviiIntroduction\n",
      "• The business intelligence analyst who sees money going down the tubes from the \n",
      "infrastructure and supply chain costs her organization is accruing, but doesn’t know \n",
      "how to systematically make cost-saving decisions.\n",
      "• The online marketer who wants to do more with his company’s free text customer \n",
      "interactions taking place in e-mail, Facebook, and Twitter, but right now they’re \n",
      "just being read and saved.\n",
      "I have in mind that you are a reader who would beneﬁ  t directly from knowing more \n",
      "about data science but hasn’t found a way to get a foothold into all the techniques. The \n",
      "purpose of this book is to strip away all the distractions around data science (the code, \n",
      "the tools, and the hype) and teach the techniques using practical use cases that someone \n",
      "with a semester of linear algebra or calculus in college can understand. Assuming you \n",
      "didn’t fail that semester. If you did, just read slower and use Wikipedia liberally.\n",
      "No Regrets. Spreadsheets Forever\n",
      "This is not a book about coding. In fact, I’m giving you my “no code” guarantee (until \n",
      "Chapter 10 at least). Why?\n",
      "Because I don’t want to spend a hundred pages at the beginning of this book messing \n",
      "with Git, setting environment variables, and doing the dance of Emacs versus Vi. \n",
      "If you run Windows and Microsoft Offi  ce almost exclusively. If you work for the govern-\n",
      "ment, and they don’t let you download and install random open source stuff  on your box. \n",
      "Even if MATLAB or your TI-83 scared the hell out of you in college, you need not be afraid. \n",
      "Do you need to know how to write code to put most of these techniques in automated, \n",
      "production settings? Absolutely! Or at least someone you work with needs to be able to \n",
      "handle code and storage technologies.\n",
      "Do you need to know how to write code in order to understand, distinguish between, \n",
      "and prototype with these techniques? Absolutely not!\n",
      "This is why I go over every technique in spreadsheet software. \n",
      "Now, this is all a bit of a lie. The ﬁ nal chapter in this book is actually on moving to the \n",
      "data science-focused programming language, R. It’s for those of you that want to use this \n",
      "book as a jumping-off  point to deeper things.\n",
      "But Spreadsheets Are So Démodé!\n",
      "Spreadsheets are not the sexiest tools around. In fact, they’re the Wilford-Brimley-selling-\n",
      "Colonial-Penn of the analytics tool world. Completely unsexy. Sorry, Wilford.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "xviii Introduction\n",
      "But that’s the point. Spreadsheets stay out of the way. They allow you to see the data \n",
      "and to touch (or at least click on) the data. There’s a freedom there. In order to learn these \n",
      "techniques, you need something vanilla, something everyone understands, but nonethe-\n",
      "less, something that will let you move fast and light as you learn. That’s a spreadsheet. \n",
      "Say it with me: “I am a human. I have dignity. I should not have to write a map-reduce \n",
      "job in order to learn data science.”\n",
      "And spreadsheets are great for prototyping! You’re not running a production AI model \n",
      "for your online retail business out of Excel, but that doesn’t mean you can’t look at purchase \n",
      "data, experiment with features that predict product interest, and prototype a targeting \n",
      "model. In fact, it’s the perfect place to do just that.\n",
      "Use Excel or LibreOfﬁ ce\n",
      "All the examples you’re going to work through will be visualized in the book in Excel. \n",
      "On the book’s website (www.wiley.com/go/datasmart) are posted companion spread-\n",
      "sheets for each chapter so that you can follow along. If you’re really adventurous, you can \n",
      "clear out all but the starting data in the spreadsheet and replicate all the work yourself.\n",
      "This book is compatible with Excel versions 2007, 2010, 2011 for Mac, and 2013. Chapter \n",
      "1 will discuss the version diff erences most in depth.\n",
      "Most of you have access to Excel, and you probably already use it for reporting or \n",
      "recordkeeping at work. But if for some reason you don’t have a copy of Excel, you can \n",
      "either buy it or go for LibreOffi  ce (\n",
      "www.libreoffice.org) instead. \n",
      "LibreOffi  ce is open source, free, and has nearly all of the same functionality as Excel. \n",
      "I think its native solver is actual preferable to Excel’s. So if you want to go that route for \n",
      "this book, feel free.\n",
      "WHAT ABOUT GOOGLE DRIVE?\n",
      "Now, some of you might be wondering whether you can use Google Drive. It’s an appeal-\n",
      "ing option since Google Drive is in the cloud and can run on your mobile devices as \n",
      "well as your beige box. But it just won’t work.\n",
      "Google Drive is great for simple spreadsheets, but for where you’re going, Google \n",
      "just can’t hang. Adding rows and columns in Drive is a constant annoyance, the imple-\n",
      "mentation of Solver is dreadful, and the charts don’t even have trendlines. I wish it were \n",
      "otherwise.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "xixIntroduction\n",
      "Conventions\n",
      "To help you get the most from the text and keep track of what’s happening, I’ve used a \n",
      "number of conventions throughout the book.\n",
      "WARNING\n",
      "Warnings hold important, not-to-be-forgotten information that is directly relevant to \n",
      "the surrounding text.\n",
      "NOTE\n",
      "Notes cover tips, hints, tricks, or asides to the current discussion.\n",
      "Frequently in this text I’ll reference little snippets of Excel code like this:\n",
      "=CONCATENATE(“THIS IS A FORMULA”, “ IN EXCEL!”)\n",
      "We highlight new terms and important words when we introduce them. We show ﬁ  le \n",
      "names, URLs, and formulas within the text like so:\n",
      "http://www .john-foreman.com.\n",
      "Let’s Get Going\n",
      "In the ﬁ rst chapter, I’m going to ﬁ ll in a few holes in your Excel knowledge. After that, \n",
      "you’ll move right into use cases. By the end of this book, you’ll not only know about but \n",
      "actually have experience implementing from scratch the following techniques:\n",
      "• Optimization using linear and integer programming\n",
      "• Working with time series data, detecting trends and seasonal patterns, and forecast-\n",
      "ing with exponential smoothing\n",
      "SIDEBARS\n",
      "Sidebars, like the one you just read about Google Drive, touch upon some side issue \n",
      "related to the text in detail.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "xx Introduction\n",
      "• Using Monte Carlo simulation in optimization and forecasting scenarios to quantify \n",
      "and address risk\n",
      "• Artiﬁ cial intelligence using the general linear model, logistic link functions, ensem-\n",
      "ble methods, and naïve Bayes\n",
      "• Measuring distances between customers using cosine similarity, creating kNN \n",
      "graphs, calculating modularity, and clustering customers\n",
      "• Detecting outliers in a single dimension with Tukey fences or in multiple dimen-\n",
      "sions with local outlier factors\n",
      "• Using R packages to “stand on the shoulders” of other analysts in conducting these \n",
      "tasks\n",
      "If any of that sounds exciting, read on! If any of that sounds scary, I promise to keep \n",
      "things as clear and enjoyable as possible.\n",
      "In fact, I prefer clarity well above mathematical correctness, so if you’re an academician \n",
      "reading this, there may be times where you should close your eyes and think of England. \n",
      "Without further ado, then, let’s get number-crunching.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "1\n",
      "T\n",
      "his book relies on you having a working knowledge of spreadsheets, and I’m going to \n",
      "assume that you already understand the basics. If you’ve never used a formula before \n",
      "in your life, then you’ve got a slight uphill battle here. I’d recommend going through a For \n",
      "Dummies book or some other intro-level tutorial for Excel before diving into this.\n",
      "That said, even if you’re a seasoned Excel veteran, there’s some functionality that’ll keep \n",
      "cropping up in this text that you may not have had to use before. It’s not diffi  cult stuff ; \n",
      "just things I’ve noticed not everyone has used in Excel. You’ll be covering a wide variety \n",
      "of little features in this chapter, and the example at this stage might feel a bit disjointed. \n",
      "But you can learn what you can here, and then, when you encounter it organically later \n",
      "in the book, you can slip back to this chapter as a reference.\n",
      "As Samuel L. Jackson says in Jurassic Park, “Hold on to your butts!”\n",
      "EXCEL VERSION DIFFERENCES\n",
      "As mentioned in the book’s introduction, these chapters work with Excel 2007, 2010, \n",
      "2013, 2011 for Mac, and LibreOffi  ce. Sadly, in each version of Excel, Microsoft has \n",
      "moved stuff  around for the heck of it.\n",
      "For example, things on the Layout tab on 2011 are on the View tab in the other ver-\n",
      "sions. Solver is the same in 2010 and 2013, but the performance is actually better in \n",
      "2007 and 2011 even though 2007’s Solver interface is grotesque.\n",
      "The screen captures in this text will be from Excel 2011. If you have an older or newer \n",
      "version, sometimes your interactions will look a little diff erent—mostly when it comes \n",
      "to where things are on the menu bar. I will do my best to call out these diff erences. If \n",
      "you can’t ﬁ nd something, Excel’s help feature and Google are your friends.\n",
      "The good news is that whenever we’re in the “spreadsheet part of the spreadsheet,” \n",
      "everything works exactly the same.\n",
      "As for LibreOffi  ce, if you’ve chosen to use open source software for this book, then \n",
      "I’m assuming you’re a do-it-yourself kind of person, and I won’t be referencing the \n",
      "LibreOffi  ce interface directly. Never you mind, though. It’s a dead ringer for Excel.\n",
      "Everything You Ever \n",
      "Needed to Know about \n",
      "Spreadsheets but Were \n",
      "Too Afraid to Ask\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "2 Data Smart\n",
      "Some Sample Data\n",
      "NOTE\n",
      "The Excel workbook used in this chapter, “Concessions.xlsx,” is available for download \n",
      "at the book’s website at www.wiley.com/go/datasmart.\n",
      "Imagine you’ve been terribly unsuccessful in life, and now you’re an adult, still living \n",
      "at home, running the concession stand during the basketball games played at your old \n",
      "high school. (I swear this is only semi-autobiographical.) \n",
      "You have a spreadsheet full of last night’s sales, and it looks like Figure 1-1.\n",
      "Figure 1-1: Concession stand sales\n",
      "Figure 1-1 shows each sale, what the item was, what type of food or drink it was, the \n",
      "price, and the percentage of the sale going toward proﬁ t.\n",
      "Moving Quickly with the Control Button\n",
      "If you want to peruse the records, you can scroll down the sheet with your scroll wheel, \n",
      "track pad, or down arrow. As you scroll, it’s helpful to keep the header row locked at \n",
      "the top of the sheet, so you can remember what each column means. To do that, choose\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "3Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \n",
      "Freeze Panes or Freeze Top Row from the “View” tab on Windows (“Layout” tab on Mac \n",
      "2011 as shown in Figure 1-2).\n",
      "Figure 1-2: Freezing the top row\n",
      "To move quickly to the bottom of the sheet to look at how many transactions you have, \n",
      "you can select a value in one of the populated columns and press Ctrl+ ↓ (Command+↓ \n",
      "on a Mac). You’ll zip right to the last populated cell in that column. In this sheet, the ﬁ nal \n",
      "row is 200. Also, note that using Ctrl/Command to jump around the sheet from left to \n",
      "right works much the same.\n",
      "If you want to take an average of the sales prices for the night, below the price column, \n",
      "column C, you can jot the following formula:\n",
      "=AVERAGE(C2:C200)\n",
      "The average is $2.83, so you won’t be retiring wealthy anytime soon. Alternatively, you \n",
      "can select the last cell in the column, C200, hold Shift+Ctrl+↑ to highlight the whole col-\n",
      "umn, and then select the Average calculation from the status bar in the bottom right of the \n",
      "spreadsheet to see the simple summary statistic (see Figure 1-3). On Windows, you’ll need \n",
      "to right-click the status bar to select the average if it’s not there. On Mac, if your status bar \n",
      "is turned off , click the View menu and select “Status Bar” to turn it on.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "4 Data Smart\n",
      "Figure 1-3: Average of the price column in the status bar\n",
      "Copying Formulas and Data Quickly\n",
      "Perhaps you’d like to view your proﬁ ts in actual dollars rather than as percentages. You \n",
      "can add a header to column E called “Actual Proﬁ t.” In E2, you need only to multiply the \n",
      "price and proﬁ t columns together to obtain this:\n",
      "=C2*D2 \n",
      "For beer, it’s $2. You don’t have to rewrite this formula in every cell in the column. \n",
      "Instead, Excel lets you grab the right-bottom corner of the cell and drag the formula \n",
      "where you like. The referenced cells in columns C and D will update relative to where you \n",
      "copy the formula. If, as in the case of the concession data, the column to the left is fully \n",
      "populated, you can double-click the bottom-right corner of the formula to have Excel ﬁ ll \n",
      "the whole column (see Figure 1-4). Try this double-click action for yourself, because I’ll \n",
      "be using it all over the place in this book, and if you get the hang of it now, you’ll save \n",
      "yourself a whole lot of heartache. \n",
      "Now, what if you don’t want the cells in the formula to change relative to the target when \n",
      "they’re dragged or copied? Whatever you don’t want changed, just add a \n",
      "$ in front of it.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "5Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \n",
      "For example, if you changed the formula in E2 to: \n",
      "=C$2*D$2\n",
      "Figure 1-4:  Filling in a formula by dragging the corner\n",
      "Then when you copy the formula down, nothing changes. The formula continues to \n",
      "reference row 2. \n",
      "If you copy the formula to the right, however, C would become D, D would become E, \n",
      "and so on. If you don’t want that behavior, you need to put a $ in front of the column refer-\n",
      "ences as well. This is called an absolute reference as opposed to a relative reference.\n",
      "Formatting Cells\n",
      "Excel off ers static and dynamic options for formatting values. Take a look at column E, the \n",
      "Actual Proﬁ t column you just created. Select column E by clicking on the gray E column \n",
      "label. Then right-click the selection and choose Format Cells.\n",
      "From within the Format Cells menu, you can tell Excel the type of number to be found \n",
      "in column E. In this case you want it to be Currency. And you can set the number of \n",
      "decimal places. Leave it at two decimals, as shown in Figure 1-5. Also available in Format \n",
      "Cells are options for changing font colors, text alignment, ﬁ ll colors, borders, and so on.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "6 Data Smart\n",
      "Figure 1-5: The Format Cells menu\n",
      "But here’s a conundrum. What if you want to format only the cells that have a certain \n",
      "value or range of values in them? And what if you want that formatting to change with \n",
      "the values?\n",
      "That’s called conditional formatting, and this book makes liberal use of it. \n",
      "Cancel out of the Format Cells menu and navigate to the Home tab. In the Styles \n",
      "section (Mac calls it Format), you’ll find the Conditional Formatting button (see \n",
      "Figure 1-6). Click the button to drop down a menu of options. The conditional formatting \n",
      "most used in this text is Color Scales. Pick a scale for column E and note how each cell \n",
      "in the column is colored based on its high or low value.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "7Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \n",
      "Figure 1-6:  Applying conditional formatting to the proﬁ  t\n",
      "To remove conditional formatting, use the Clear Rules options under the Conditional \n",
      "Formatting menu.\n",
      "Paste Special Values\n",
      "It’s often in your best interest not to have a formula lying around like you see in Column E \n",
      "in Figure 1-4. If you were using the \n",
      "RAND() formula to generate a random value, for example, \n",
      "it changes each time the spreadsheet auto-recalculates, which while awesome, can also be \n",
      "extremely annoying. The solution is to copy and paste these cells back to the sheet as ﬂ at \n",
      "values. \n",
      "To convert formulas to values only, simply copy a column ﬁ  lled with formulas (grab \n",
      "column E) and paste it back using the Paste Special option (found on the Home tab under \n",
      "the Paste option on Windows and under the Edit menu on Mac). In the Paste Special win-\n",
      "dow, choose to paste as values (see Figure 1-7). Note also that Paste Special allows you to \n",
      "transpose the data from vertical to horizontal and vice versa when pasting. You’ll be using \n",
      "that a fair bit in the chapters to come.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "8 Data Smart\n",
      "Figure 1-7: The Paste Special window in Excel 2011\n",
      "Inserting Charts\n",
      "In the concession stand sales workbook, there’s also a tab called Calories with a tiny table \n",
      "that shows the calorie count of each item the concession stand sells. You can chart data \n",
      "like this in Excel easily. On the Insert tab (Charts on a Mac), there is a charts section that \n",
      "provides diff erent visualization options such as bar charts, line graphs, and pie charts. \n",
      "NOTE\n",
      "In this book, we’re going to use mostly column charts, line graphs, and scatter plots. \n",
      "Never be caught using a pie chart. And especially never use the 3D pie charts Excel \n",
      "off ers, or my ghost will personally haunt you when I die. They’re ugly, they don’t com-\n",
      "municate data well, and the 3D eff ect has less aesthetic value than the seashell paintings \n",
      "hanging on the wall of my dentist’s offi  ce.\n",
      "Highlighting columns A:B on the Calories workbook, you can select a Clustered Column \n",
      "chart to visualize the data. Play around with the graph. Sections can be right-clicked to \n",
      "bring up formatting menus. For example, right-clicking the bars, you can select “Format\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "9Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \n",
      "Data Series…” under which you can change the ﬁ  ll color on the bars from the default \n",
      "Excel blue to any number of pleasing shades—black, for instance. \n",
      "There’s no reason for the default legend, so you should select it and press delete to \n",
      "remove it. You might also want to select various text sections on the graph and increase \n",
      "the size of their font (font size is under the Home tab in Excel). This gives the graph \n",
      "shown in Figure 1-8.\n",
      "Figure 1-8:  Inserting a calories column chart\n",
      "Locating the Find and Replace Menus\n",
      "You’re going to use ﬁ nd and replace a fair bit in this book. On Windows you can either \n",
      "press Ctrl+F to open up the Find window (Ctrl+H for replace) or navigate to the Home \n",
      "tab and use the Find button in the Editing section. On Mac, there’s a search ﬁ eld on the \n",
      "top right of the sheet (press the down arrow for the Replace menu), or you can just press \n",
      "Cmd+F to bring up the Find and Replace menu.\n",
      "Just to test it out, open up the replace menu on the Calories sheet. You can replace every \n",
      "instance of the word “Calories” with the word “Energy” (see Figure 1-9) by popping the \n",
      "words in the Find and Replace window and pressing Replace All.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "10 Data Smart\n",
      "Figure 1-9: Running a Find and Replace\n",
      "Formulas for Locating and Pulling Values\n",
      "If I didn’t assume you at least knew some formulas in Excel (SUM, MAX, MIN, PERCENTILE, and \n",
      "so on), we’d be here all day. And I want to get started. But there are some formulas used a \n",
      "lot in this book that you’ve probably not used unless you’ve dug deep into the wonderful \n",
      "world of spreadsheets. These formulas deal with ﬁ nding a value in a range and returning its \n",
      "location or on the ﬂ ip side ﬁ nding a location in a range and returning its value. \n",
      "I want to cover a few of those on the Calories tab.\n",
      "Sometimes you want to know the place in line of some element in a column or row. Is it \n",
      "ﬁ rst, second, third? The \n",
      "MATCH formula handles that quite nicely. Below your calorie data, \n",
      "label A18 as Match. You can implement the formula one cell over in B18 to ﬁ nd where in \n",
      "the item list above the word “Hamburger” appears. To use the formula, you supply it a \n",
      "value to look for, a range to search in, and a 0 to force it to give you back the position of \n",
      "the keyword itself:\n",
      "=MATCH(\"Hamburger\",A2:A15,0)\n",
      "This yields a 6, because “Hamburger” is the sixth item in the list (see Figure 1-10).\n",
      "Next up is the \n",
      "INDEX formula. Label A19 as Index. \n",
      "This formula takes in a range of values and a row and column number and returns \n",
      "the value in the range at that location. For example, you can feed the INDEX formula our \n",
      "calorie table A1:B15, and to pull back the calorie count for bottled water, feed in 3 rows \n",
      "down and 2 columns over:\n",
      "=INDEX(A1:B15,3,2)\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "11Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \n",
      "This yields a calorie count of 0 as expected (see Figure 1-10).\n",
      "Another formula you’ll see a lot in this text is OFFSET. Go ahead and label A20 as Off set, \n",
      "and you can play with the formula in B20. \n",
      "With this formula, you provide a range that acts like a cursor which is moved around \n",
      "with row and column off sets (similar to INDEX for the single valued case except it’s 0-based). \n",
      "For example, you can provide OFFSET with a reference to the top left of the sheet, A1, and \n",
      "then pull back the value 3 cells below by providing a row off set of 3 and a column off set \n",
      "of 0:\n",
      "=OFFSET(A1,3,0)\n",
      "This returns the name of the third item on the list, “Chocolate Bar.” See Figure 1-10.\n",
      "The last formula I want to look at in this section is \n",
      "SMALL (it has a counterpart called \n",
      "LARGE that works the same way). If you have a list of values and you want to return, say, \n",
      "the third smallest, SMALL does that for you. To see this, label A21 as Small and in B21 feed \n",
      "in the list of calorie counts and an index of 3:\n",
      "=SMALL(B2:B15,3)\n",
      "This hands back a value of 150 which is the third smallest after 0 (bottled water) and \n",
      "120 (soda). See Figure 1-10. \n",
      "Now, there’s one more formula used for looking up values that’s kind of like MATCH on \n",
      "steroids and that’s VLOOKUP (and its horizontal counterpart HLOOKUP). That’s got its own \n",
      "section next because it’s a beast.\n",
      "Figure 1-10:  Formulas you should learn\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "12 Data Smart\n",
      "Using VLOOKUP to Merge Data\n",
      "Go ahead and ﬂ ip back to the Basketball Game Sales tab. You can still reference a cell \n",
      "here from the previous tab, Calories, by simply placing the tab name and “!” in front of a \n",
      "referenced cell. For example, \n",
      "Calories!B2 is a reference to the calories in beer regardless \n",
      "of what sheet you’re working in.\n",
      "Now, what if you wanted to toss the calorie data into a column back on the sales sheet \n",
      "so that next to each item sold the appropriate calorie count was listed? You’d somehow \n",
      "have to look up the calorie count of each item sold and place it into a column next to the \n",
      "transaction. Well, it turns out there’s a formula for that called \n",
      "VLOOKUP.\n",
      "Go ahead and label Column F in the spreadsheet Calories for this purpose. Cell F2 \n",
      "will include the calorie count for the ﬁ rst beer transaction from the Calories table. Using \n",
      "the VLOOKUP  formula, you supply the item name from cell A2, a reference to the table \n",
      "Calories!$A$1:$B$15 , and the relative column off set you want your return value to be \n",
      "read out of, which is to say the second column:\n",
      "=VLOOKUP(A2,Calories!$A$1:$B$15,2,FALSE)\n",
      "The FALSE at the end of the VLOOKUP formula means that you will not accept approximate \n",
      "matches for “Beer.” If the formula can’t ﬁ nd “Beer” on the calories table, it returns an error.  \n",
      "When you enter the formula, you can see that 200 calories is read in from the table \n",
      "on the Calories tab. Since you’ve put the $ in front of the table references in the formula, \n",
      "you can copy this formula down the column by double-clicking the bottom-right corner \n",
      "of the cell. Voila! As shown in Figure 1-11, you have calorie counts for every transaction.\n",
      "Figure 1-11: Using VLOOKUP to grab calorie counts\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "13Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \n",
      "Filtering and Sorting\n",
      "Now that you have calories in there, say you now want to view only those transactions \n",
      "from the Frozen Treats category. What you want to do then is ﬁ lter the sheet. To do so, ﬁ rst \n",
      "you select the data in range A1:F200. You can put the cursor in A1 and press Shift+Ctrl+↓ \n",
      "then →. An even easier method is to click the top of column A and hold the click as you \n",
      "mouse over to column F to highlight all six columns. \n",
      "Then to place auto-ﬁ ltering on these six columns, you press the Filter button in the \n",
      "Data section of the ribbon. It looks like a gray funnel as shown in Figure 1-12.\n",
      "Figure 1-12: Place auto-ﬁ lter on a selected range\n",
      "Once auto-ﬁ lter is activated, you can click the drop-down menu that appears in cell B1 \n",
      "and choose to show only certain categories (in this case, only the Frozen Treats transac-\n",
      "tions will be displayed). See Figure 1-13. \n",
      "Once you’ve ﬁ ltered, highlighting columns of data allows the summary bar in Excel to \n",
      "give you rolled-up information just on the cells that remain. For example, having ﬁ ltered \n",
      "just the Frozen Treats, we can highlight the values in column E and use the summary bar \n",
      "to get a quick total of proﬁ t just from that category. See Figure 1-14.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "14 Data Smart\n",
      "Figure 1-13: Filtering on category\n",
      "Figure 1-14: Summarizing a ﬁ  ltered column\n",
      "Auto-ﬁ lter allows you to sort as well. For example, if you want to sort by proﬁ  t, just \n",
      "click the auto-ﬁ lter menu on the Proﬁ t cell (D1) and select Sort Ascending (or “Smallest \n",
      "to Largest” in some versions). See Figure 1-15.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "15Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \n",
      "Figure 1-15: Sorting in ascending order by proﬁ  t\n",
      "To remove all the ﬁ ltering you’ve applied, either you can go back into the Category ﬁ lter \n",
      "menu and check the other boxes, or you can un-toggle the ﬁ lter button on the ribbon that \n",
      "you pressed in the ﬁ rst place. You’ll see that although you have all of your data back, the \n",
      "Frozen Treats are still in the order you sorted them in.\n",
      "Excel also off ers the Sort interface for doing more complex sorts than might be possible \n",
      "with auto-ﬁ lter. To use the feature, you highlight the data to be sorted (grab A:F again) \n",
      "and select Sort from the Sort & Filter section of the Data tab in Excel. This will bring up \n",
      "the sort menu. On Mac, to get this window, you must press the down arrow in the sort \n",
      "button and select Custom Sort….\n",
      "In the sort menu, shown in Figure 1-16, you can note whether your data has column \n",
      "headers or not, and if it does have headers like this example does, then you can select, by \n",
      "name, the columns to be sorted.\n",
      "Now, the most awesome part of this sorting interface is that under the “Options…” \n",
      "button, you can select to sort left to right instead of column data. That’s something you \n",
      "cannot do with auto-ﬁ lter. In top to bottom of this book you’ll need to randomly sort data \n",
      "by both columns and rows in two quick steps, and this interface is going to be your friend. \n",
      "For now, just cancel out of it as the data is already ordered the way you want it.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "16 Data Smart\n",
      "Figure 1-16: Using the Sort menu\n",
      "Using PivotTables\n",
      "What if you wanted to know the total counts of each item type you sold? Or you wanted \n",
      "to know revenue totals by item?\n",
      "These questions are akin to “aggregate” or “group by” queries that you’d run in a tra-\n",
      "ditional SQL database. But this data isn’t in a database. It’s in a spreadsheet. That’s where \n",
      "PivotTables come to the rescue.\n",
      "Just as when you ﬁ ltered your data, you start by selecting the data you want to manipu-\n",
      "late—in this case, the purchase data in the range A1:F:200. From the Insert tab (Data tab \n",
      "on Mac), you can press the PivotTable button and select for Excel to create a new sheet \n",
      "with a PivotTable. While some versions of Excel allow you to insert a PivotTable into an \n",
      "existing sheet, it’s standard practice to select the new sheet option unless you have a really \n",
      "good reason not to. \n",
      "In this new sheet, the PivotTable Builder will be aligned to the right of the table (it ﬂ oats \n",
      "on a Mac). The builder allows you to take the columns from the original selected data and \n",
      "use them as report ﬁ lters, column and row labels for grouping, or values. A report ﬁ  lter \n",
      "is similar in function to a ﬁ lter from the previous section—it allows you to select only a \n",
      "subset of the data, such as Frozen Treats. The Column Labels and Row Labels ﬁ  ll in the \n",
      "meat of the PivotTable report with distinct values from the selected columns.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "17Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \n",
      "On Windows, the initial PivotTable built will be completely empty, while on Mac it is \n",
      "often prepopulated with distinct values from the ﬁ rst selected column down the rows of \n",
      "the table and distinct values from the second column across the columns. If you’re on a \n",
      "Mac, go ahead and uncheck all the boxes in the builder, so that you can work along from \n",
      "an empty table.\n",
      "Now, say you wanted to know total revenue by item. To get at that, you’d drag the Item \n",
      "tile in the PivotTable Builder into the Rows section and the Price tile into the Values sec-\n",
      "tion. This means that you’ll be operating on revenue grouped by item name.\n",
      "Initially, however, the PivotTable is set up to merely count the number of price records \n",
      "that are within a group. For example, there are 20 Beer rows. See Figure 1-17. \n",
      "Figure 1-17: The PivotTable builder and a count of sales by item\n",
      "You need to change the count to a sum in order to examine revenue. To do so, on \n",
      "Windows, drop the menu down on the Price tile in the Values section of the builder and \n",
      "select “Value Field Settings….” On Mac, press the little “i” button. From there, “sum” can \n",
      "be selected from the various summary options.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "18 Data Smart\n",
      "What if you wanted to break out these sums by category? To do so, you drag the Category \n",
      "tile into the Columns section of the builder. This gives the table shown in Figure 1-18. \n",
      "Note that the PivotTable in the ﬁ gure automatically totals up rows and columns for you.\n",
      "Figure 1-18: Revenue by item and category\n",
      "And if you want to ever get rid of something from the table, just uncheck it or grab the \n",
      "tile from the section it’s in and drag it out of the sheet as if you were tossing it away. Go \n",
      "ahead and drop the Category tile.\n",
      "Once you get a report you want in a PivotTable, you can always select the values and \n",
      "paste them to another sheet to work on further. In this example, you can copy the table \n",
      "(A5:B18 on Mac) and Paste Special its values into a new tab called Revenue By Item (see \n",
      "Figure 1-19).\n",
      "Feel free to swap in various row and column labels until you get the hang of what’s \n",
      "going on. For instance, try to get a total calorie count sold by category using a PivotTable.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "19Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \n",
      "Figure 1-19: Revenue by Item tab created by pasting values from a PivotTable\n",
      "Using Array Formulas\n",
      "In the concession transaction workbook, there is a tab called Fee Schedule. As it turns out, \n",
      "Coach O’Shaughnessy would let you run the snack stand only if you kicked some of the \n",
      "proﬁ t back to him (perhaps to subsidize his tube sock-buying habit). The Fee Schedule \n",
      "tab shows the percent cut he takes on each item sold. \n",
      "So how much money do you owe him for last night’s game? To answer that question, \n",
      "you need to multiply the total revenue of each item from the PivotTable by the cut for the \n",
      "coach and sum them all up.\n",
      "There’s a great formula for this operation that will do all the multiplication and sum-\n",
      "mation in a single step. Rather creatively named, it’s called \n",
      "SUMPRODUCT . In cell E1 on \n",
      "the Revenue By Item sheet, add a label called Total Cut for Coach. In C2, determine the \n",
      "SUMPRODUCT of the revenue and the fees by adding this formula:\n",
      "=SUMPRODUCT(B2:B15,'Fee Schedule'!B2:O2)\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "20 Data Smart\n",
      "Uh oh. There’s an error; the cell just reads #Value. What’s going wrong?\n",
      "Even though you’ve selected two ranges of equal size and put them in SUMPRODUCT , \n",
      "the formula can’t see that the ranges are equal because one range is vertical and one’s \n",
      "horizontal.\n",
      "Fortunately, Excel has a function for ﬂ ipping arrays in the right direction. It’s called \n",
      "TRANSPOSE. You need to write the formula like this:\n",
      "=SUMPRODUCT(B2:B15,TRANSPOSE('Fee Schedule'!B2:O2))\n",
      "Nope! Still getting an error.\n",
      "The reason you’re still getting an error is that every formula in Excel, by default, returns \n",
      "a single value. Even TRANSPOSE returns the ﬁ rst value in the transposed array. If you want \n",
      "the whole array returned, you have to turn TRANSPOSE into an “array formula,” which means \n",
      "exactly what you might think. Array formulas hand you back arrays, not single values.\n",
      "You don’t have to change the way you type your SUMPRODUCT to make this happen. All \n",
      "you need to do is when you’re done typing the formula, instead of pressing Enter, press \n",
      "Ctrl+Shift+Enter. On the Mac, you use Command+Return.\n",
      "Victory! As shown in Figure 1-20, the calculation now reads $57.60. But I suggest round-\n",
      "ing that down to $50, because how many socks does Coach really need?\n",
      "Figure 1-20: Taking a SUMPRODUCT with an array formula \n",
      "Solving Stuff with Solver\n",
      "Many of the techniques you’ll study in this book can be boiled down to optimization mod-\n",
      "els. An optimization problem is one where you have to make the best decision (choose \n",
      "the best investments, minimize your company’s costs, ﬁ  nd the class schedule with the\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "21Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \n",
      "fewest morning classes, or so on). In optimization models then, the words “minimize” \n",
      "and “maximize” come up a lot when articulating an objective.\n",
      "In data science, many of the practices, whether that’s artiﬁ cial intelligence, data mining, \n",
      "or forecasting, are actually just some data prep plus a model-ﬁ tting step that’s actually an \n",
      "optimization model. So it’d make sense to teach optimization ﬁ rst. But learning all there \n",
      "is to know about optimization is tough to do straight off  the bat. So you’ll do an in-depth \n",
      "optimization study in Chapter 4 after you do some more fun machine learning problems \n",
      "in Chapters 2 and 3. To ﬁ ll in the gaps though, it’s best if you get a little practice with \n",
      "optimization now. Just a taste.\n",
      "In Excel, optimization problems are solved using an Add-In that ships with Excel \n",
      "called Solver. \n",
      "• On Windows, Solver may be added in by going to File (in Excel 2007 it’s the top \n",
      "left Windows button) ➪ Options ➪ Add-ins, and under the Manage drop-down \n",
      "choosing Excel Add-ins and pressing the Go button. Check the Solver Add-In box \n",
      "and press OK. \n",
      "• On Mac, Solver is added by going to Tools then Add-ins and selecting Solver.xlam \n",
      "from the menu.\n",
      "A Solver button will appear in the Analysis section of the Data tab in every version.\n",
      "All right! Now that Solver is installed, here’s an optimization problem: You are told you \n",
      "need 2,400 calories a day. What’s the fewest number of items you can buy from the snack \n",
      "stand to achieve that? Obviously, you could buy 10 ice cream sandwiches at 240 calories \n",
      "a piece, but is there a way to do it for fewer items than that?\n",
      "Solver can tell you!\n",
      "To start, make a copy of the Calories sheet, name the sheet Calories-Solver, and clear \n",
      "out everything but the calories table on the copy. If you don’t know how to make a copy \n",
      "of a sheet in Excel, you simply right-click the tab you’d like to copy and select the Move \n",
      "or Copy menu. This gives you the new sheet shown in Figure 1-21.\n",
      "To get Solver to work, you need to provide it with a range of cells it can set with deci-\n",
      "sions. In this case, Solver needs to decide how many of each item to buy. So in Column C \n",
      "next to the calorie counts, label the column How many? (or whatever you feel like), and \n",
      "you can allow Solver to store its decisions in this column.\n",
      "Excel considers blank cells to be 0s so you needn’t ﬁ ll in these cells with anything to \n",
      "start. Solver will do that for you.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "22 Data Smart\n",
      "Figure 1-21: The copied Calories-Solver sheet\n",
      "In cell C16, sum up the number of items to be bought above as:\n",
      "=SUM(C2:C15)\n",
      "And below that you can sum up the total calorie count of these items (which you’ll \n",
      "want eventually to equal 2,400) using the SUMPRODUCT formula:\n",
      "=SUMPRODUCT(B2:B15,C2:C15)\n",
      "This gives the initial sheet shown in Figure 1-22.\n",
      "Now you’re ready to build the model, so bring up the Solver window by pressing the \n",
      "Solver button on the Data tab.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "23Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \n",
      "Figure 1-22: Getting calorie and item counts set up\n",
      "NOTE\n",
      "The Solver window, shown in Figure 1-23 in Excel 2011, looks pretty similar in Excel \n",
      "2010, 2011, and 2013. In Excel 2007, the layout is slightly diff  erent, but the only \n",
      "substantive diff erence is that there is no algorithm selection box. Rather, there’s an \n",
      "“Assume Linear Model” checkbox under the Options menu. We’ll learn all about these \n",
      "elements later.\n",
      "The main elements you plug into Solver to solve a problem, as shown in Figure 1-23, \n",
      "are an objective cell, an optimization direction (minimization or maximization), some \n",
      "decision variables that can be changed by Solver, and some constraints.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "24 Data Smart\n",
      "Figure 1-23: The uninitialized Solver window\n",
      "In your case, the objective is to minimize the total items in cell C16. The cells that can \n",
      "be altered are the item selections in C2:C15. And the constraints are that C17, the total \n",
      "calories, needs to be equal to 2,400. Also, we’ll need to add a constraint that our decisions \n",
      "be counting numbers, so we’ll need to check the non-negative box (under the options menu \n",
      "in Excel 2007) and add an integer constraint to the decisions. After all, you can’t buy 1.7 \n",
      "sodas. These integer constraints will be covered in depth in Chapter 4.\n",
      "To add in the total calorie constraint, press the Add button and set C17 equal to 2,400 \n",
      "as shown in Figure 1-24. \n",
      "Figure 1-24: Adding the calorie constraint\n",
      "Similarly, add a constraint setting C2:C15 to be integers as shown in Figure 1-25.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "25Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \n",
      "Figure 1-25: Adding an integer constraint\n",
      "Press OK. \n",
      "In Excel 2010, 2011, and 2013, make sure the solving method is set to Simplex LP. \n",
      "Simplex LP is appropriate for this problem, because this problem is linear (the “L ” in LP \n",
      "stands for linear as you’ll see in Chapter 4). By linear, I mean that the problem involves \n",
      "nothing but linear combinations of the decisions in C2 through C15 (sums, products with \n",
      "constants such as calorie counts, etc.). \n",
      "If we had non-linear calculations in the model (perhaps a square root of a decision, a \n",
      "logarithm, or an exponential function), then we could use one of the other algorithms \n",
      "Excel provides in Solver. Chapter 4 covers this in great detail. \n",
      "In Excel 2007, you would denote the problem as linear by clicking the Assume Linear \n",
      "Model under the Options screen. Your ﬁ nal setup should appear as in Figure 1-26. \n",
      "Figure 1-26: Final Solver setup for minimizing items needed for 2,400 calories\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "26 Data Smart\n",
      "All right! Go ahead and press the Solve button. Excel should ﬁ  nd a solution almost \n",
      "immediately. And that solution, as shown in Figure 1-27, is 5. Now, your Excel might \n",
      "pick a diff erent 5 items than mine in the screenshot, but the minimum is 5 nonetheless.\n",
      "Figure 1-27: The optimized item selection\n",
      "OpenSolver: I Wish We Didn’t Need This, but We Do\n",
      "This book was originally designed to work completely with Excel’s built-in Solver. However, \n",
      "as it turns out, functionality was removed from Solver in later versions for mysterious and \n",
      "unadvertised reasons.\n",
      "What that means is that while this whole book works using vanilla Solver in Excel 2007 \n",
      "and Excel 2011 for Mac, in Excel 2010 and Excel 2013, the built-in Solver will occasion-\n",
      "ally complain that a linear optimization model is too large (I’ll give you a heads-up in this \n",
      "book whenever a model gets that complex).\n",
      "Luckily, there’s an excellent free tool called OpenSolver that’s available for the Windows \n",
      "versions of Excel that addresses this deﬁ ciency. With OpenSolver, you can still build your \n",
      "model in the regular Solver interface, but OpenSolver provides a button that you press to \n",
      "use its Simplex LP algorithm implementation, which is blazingly fast.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "27Everything You Ever Needed to Know about Spreadsheets but Were Too Afraid to Ask \n",
      "To set up OpenSolver, navigate to http://OpenSolver.org and download the zip ﬁ le. \n",
      "Uncompress the ﬁ le into a folder, and whenever you want to solve a beefy model, just set \n",
      "it up in a spreadsheet like normal and double-click the OpenSolver.xlam ﬁ le, which will \n",
      "give you an OpenSolver section on the Data tab in Excel. Press the Solve button to solve \n",
      "an existing model. As shown in Figure 1-28, I’ve applied OpenSolver in Excel 2013 to the \n",
      "model from the previous section, and it buys ﬁ ve slices of pizza.\n",
      "Figure 1-28: OpenSolver buys pizza like a madman\n",
      "Wrapping Up\n",
      "All right, you’ve learned how to navigate and select ranges quickly, how to leverage absolute \n",
      "references, how to paste special values, how to use VLOOKUP and other matching formulas, \n",
      "how to sort and ﬁ lter data, how to create PivotTables and charts, how to execute array \n",
      "formulas, and how and when to bust out Solver.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "28 Data Smart\n",
      "Here’s either a depressing or fun fact depending on your perspective. I’ve known man-\n",
      "agement consultants at prominent ﬁ rms who earn excellent salaries by doing what I call \n",
      "the “consulting two-step”:\n",
      " 1. Talk about nonsense with clients (sports, vacation, barbeque ... not that there’s \n",
      "anything nonsensical about smoked meats). \n",
      " 2. Summarize data in Excel.\n",
      "You may not know all there is to know about college football (I certainly don’t), but if \n",
      "you internalize this chapter, you’ll have point number two knocked out.\n",
      "But you’re not here to become a management consultant. You’re here to drive deep into \n",
      "data science, and that starts in the next chapter where we’ll get started with a little bit of \n",
      "unsupervised machine lear ning.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "2\n",
      "I \n",
      "work in the e-mail marketing industry for a website called MailChimp.com. We help \n",
      "customers send e-mail newsletters to their audience, and every time someone uses the \n",
      "term “e-mail blast,” a little part of me dies. \n",
      "Why? Because e-mail addresses are no longer black boxes that you lob “blasts” at like \n",
      "ﬂ ash grenades. No, in e-mail marketing (as with many other forms of online engagement, \n",
      "including tweets, Facebook posts, and Pinterest campaigns), a business receives feedback \n",
      "on how their audience is engaging at the individual level through click tracking, online \n",
      "purchases, social sharing, and so on. This data is not noise. It characterizes your audience. \n",
      "But to the uninitiated, it might as well be Greek. Or Esperanto.\n",
      "How do you take a bunch of transactional data from your customers (or audience, users, \n",
      "subscribers, citizens, and so on) and use it to understand them? When you’re dealing with \n",
      "lots of people, it’s hard to understand each customer personally, especially if they all have \n",
      "their own diff erent ways in which they’ve engaged with you. Even if you could understand \n",
      "everyone at a personal level, that can be tough to act on.\n",
      "You need to take this customer base and ﬁ  nd a happy medium between “blasting” \n",
      "everyone as if they were the same faceless entity and understanding everything about \n",
      "everyone to create personalized marketing for each individual recipient. One way to strike \n",
      "this balance is to use clustering to create a market segmentation of your customers so that \n",
      "you can market to segments of your base with targeted content, deals, etc.\n",
      "Cluster analysis is the practice of gathering up a bunch of objects and separating them \n",
      "into groups of similar objects. By exploring these diff  erent groups—determining how \n",
      "they’re similar and how they’re diff erent—you can learn a lot about the previously amor-\n",
      "phous pile of data you had. And that insight can help you make better decisions at a level \n",
      "that’s more detailed than before.\n",
      "In this way, clustering is called exploratory data mining, because these clustering tech-\n",
      "niques help tease out relationships in large datasets that are too hard to identify with an \n",
      "eyeball. And revealing relationships in your population is useful across industries whether \n",
      "it’s for recommending ﬁ lms based on the habits of folks in a taste cluster, identifying crime \n",
      "Cluster Analysis \n",
      "Part I: Using K-Means \n",
      "to Segment Your \n",
      "Customer Base\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "30 Data Smart\n",
      "hot spots within urban areas, or grouping return-related ﬁ nancial investments to ensure \n",
      "a diversiﬁ ed portfolio spans clusters.\n",
      "One of my favorite uses for clustering is image clustering—lumping together image \n",
      "ﬁ les that “look the same” to the computer. For example, in photo sharing services like \n",
      "Flickr, a user will generate a lot of content, and there may end up being too many photos \n",
      "to navigate simply. But using clustering techniques, you can cluster similar images together \n",
      "and allow users to navigate between these clusters before drilling down.\n",
      "This chapter looks at the most common type of clustering, called k-means clustering, \n",
      "which originated in the 1950s and has since become a go-to clustering technique for \n",
      "knowledge discovery in databases (KDD) across industries and the government. \n",
      "K-means isn’t the most mathematically rigorous of techniques. It’s born of the kind \n",
      "of practicality and common sense you might see in soul food. Soul food doesn’t have the \n",
      "snooty pedigree of French cuisine, but it hits the spot sometimes. Cluster analysis with \n",
      "k-means, as you’ll soon see, is part math, part story-telling. But its intuitive simplicity is \n",
      "part of the attraction. \n",
      "To see how it works, you’ll start with a simple example.\n",
      "Girls Dance with Girls, Boys Scratch Their Elbows\n",
      "The goal in k-means clustering is to take some points in space and put them into k groups \n",
      "(where k is any number you want to pick). Those k groups are each deﬁ ned by a point in \n",
      "the center, kind of like a ﬂ ag stuck in the moon that says, “Hey, this is the center of my \n",
      "SUPERVISED VERSUS UNSUPERVISED MACHINE LEARNING\n",
      "By deﬁ nition, in exploratory data mining, you don’t know ahead of time what you’re \n",
      "looking for. You’re an explorer. Like Dora. You may be able to articulate when two \n",
      "customers look the same and when they look diff erent, but you don’t know the best \n",
      "way to segment your customer base. So when you ask a computer to segment your \n",
      "customers for you, that’s called unsupervised machine learning, because you’re not \n",
      "“supervising”—telling the computer how to do its job.\n",
      "This is in contrast to supervised machine learning, which usually crops up when artiﬁ -\n",
      "cial intelligence makes the front page of the paper. If I know I want to divide customers \n",
      "into two groups—say “likely to purchase” and “not likely to purchase”—and I provide \n",
      "the computer with historical examples of such customers and tell it to assign all new \n",
      "leads to one of these two groups, that’s supervised.\n",
      "If instead I say, “here’s what I know about my customers and here’s how to measure \n",
      "whether they’re diff erent or similar. Tell me what’s interesting,” that’s unsupervised.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "31Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "group. Join me if you’re closer to this ﬂ ag than any others.” This group center (formally \n",
      "called the cluster centroid) is the mean from which k-means gets its name.\n",
      "Take as an example a middle school dance. If you’ve blocked the horror of middle school \n",
      "dances from your mind, I apologize for resurfacing such painful memories.\n",
      "Those in attendance at the McAcne Middle School dance, romantically called the “Under \n",
      "the Sea Gala,” are scattered about the ﬂ oor as shown in Figure 2-1. I’ve even Photoshopped \n",
      "some parquet ﬂ oor into the ﬁ gure to help with the illusion.\n",
      "And here’s a sampling of the songs these young leaders of the free world will be dancing \n",
      "awkwardly to if you’d like to listen along in Spotify:\n",
      "• Styx: Come Sail Away\n",
      "• Everything But the Girl: Missing\n",
      "• Ace of Bass: All that She Wants\n",
      "• Soft Cell: Tainted Love\n",
      "• Montell Jordan: This is How We Do It\n",
      "• Eiff el 65: Blue\n",
      "Figure 2-1: McAcne Middle School students tearing up the dance ﬂ  oor\n",
      "Now, k-means clustering demands that you specify how many clusters you want to put \n",
      "the attendees in. Let’s pick three clusters to start (later in this chapter we’ll look at how\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "32 Data Smart\n",
      "to choose k). The algorithm is going to plant three ﬂ ags on the dance ﬂ oor, starting with \n",
      "some initial feasible solution, such as that pictured in Figure 2-2, where you have three \n",
      "initial means spread on the ﬂ oor, denoted by black circles. \n",
      "Figure 2-2: Initial cluster centers placed\n",
      "In k-means clustering, dancers are assigned to the cluster that’s nearest them, so \n",
      "between any two cluster centers on the ﬂ oor, you can draw a line of demarcation, whereby \n",
      "if a dancer is on one side of the line they’re in one group, but if they’re on the other side, \n",
      "their group changes (see Figure 2-3).\n",
      "Using these lines of demarcation, you can assign dancers to their groups and shade \n",
      "them appropriately, as in Figure 2-4. This diagram, one that divides the space into poly-\n",
      "topes based on which regions are assigned to which cluster centers by distance, is called \n",
      "a Voronoi diagram.\n",
      "Now, this initial assignment doesn’t feel right, does it? You’ve sliced the space up in a \n",
      "rather odd way, leaving the bottom-left group empty and a lot of folks on the border of \n",
      "the top-right group.\n",
      "The k-means clustering algorithm slides these three cluster centers around the dance \n",
      "ﬂ oor until it gets the best ﬁ t. \n",
      "How is “best ﬁ t” measured? Well, each attendee is some distance away from their clus-\n",
      "ter center. Whichever arrangement of cluster centers minimizes the average distance of \n",
      "attendees from their center is best.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "33Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "Figure 2-3: Lines denote the borders of the clusters.\n",
      "Figure 2-4:  Cluster assignments given by shaded regions in the Voronoi diagram\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "34 Data Smart\n",
      "Now, as I mentioned in Chapter 1, the word “minimize” is a tip-off   that you’ll need \n",
      "optimization modeling to best place the cluster centers. So in this chapter, you’ll be busting \n",
      "out Solver to move the cluster centers around. The way Solver is going to get the centers \n",
      "placed just right is by intelligently and iteratively moving them around, keeping track of \n",
      "many of the good placements it has found and combining them (literally mating them \n",
      "like race horses) to get the best placement.\n",
      "So while the diagram in Figure 2-4 looks pretty bad, Solver might eventually bump the \n",
      "centers to something like Figure 2-5. This gets the average distance between each dancer \n",
      "and their center down a bit.\n",
      "Figure 2-5: Moving the centers just a tad\n",
      "Eventually though, Solver would ﬁ  gure out that the centers should be placed in the \n",
      "middle of our three groups of dancers as shown in Figure 2-6.\n",
      "Nice! This is what an ideal clustering looks like. The cluster centroids are at the centers \n",
      "of each group of dancers, minimizing the average distance between dancer and nearest \n",
      "center. And now that you have a clustering, you can move on to the fun part: trying to \n",
      "understand what the clusters mean.\n",
      "If you investigated the dancers’ hair colors, political persuasions, or mile run speeds, \n",
      "the clusters may not make much sense. But the moment you were to evaluate the genders \n",
      "and ages of the attendees in each cluster, you’d start to see some common themes. The \n",
      "small group at the bottom is all old people—they must be the dance chaperones. The left \n",
      "group is all young males, and the right group is all young females. Everyone is too afraid \n",
      "to dance with each other.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "35Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "Figure 2-6: Optimal 3-means clustering of the McAcne dance\n",
      "All right! So k-means has allowed you to segment this dance attendee population and \n",
      "correlate attendee descriptors with cluster membership to understand the why behind \n",
      "the assignments.\n",
      "Now, you’re probably saying to yourself, “Yeah, but that’s stupid. I already knew the \n",
      "answer to start.” You’re right. In this example, you did. The reason this is a toy problem, is \n",
      "that you can already solve it by just looking at the points. Everything is in two-dimensional \n",
      "space, which is super easy for your eyeballs to cluster. \n",
      "But what if you ran a store that sold thousands of products? Some customers have bought \n",
      "one or two in the past year. Other customers have bought tens. And the items purchased \n",
      "vary from customer to customer.\n",
      "How do you cluster them on their “dance ﬂ oor?” Well, your dance ﬂ oor isn’t in a two-\n",
      "dimensional space or three-dimensional space. It’s in a thousand-dimensional product \n",
      "purchase space in which a customer has either purchased or not purchased the product in \n",
      "each single dimension. Very quickly, you see, a clustering problem can exceed the limits \n",
      "of the “Mark I Eyeball,” as my military friends like to say.\n",
      "Getting Real: K-Means Clustering Subscribers in \n",
      "E-mail Marketing\n",
      "Let’s move on to a more substantive use case. I’m an e-mail-marketing guy, so I’m going \n",
      "to use an example from MailChimp.com where I work. But this same example would work \n",
      "on retail purchase data, ad conversion data, social media data, and so on. It works with\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "36 Data Smart\n",
      "basically any type of data where you’re reaching out to customers with marketing mate-\n",
      "rial, and they’re choosing to engage with you.\n",
      "Joey Bag O’ Donuts Wholesale Wine Emporium\n",
      "Let’s imagine that you live in New Jersey where you run Joey Bag O’ Donuts Wholesale \n",
      "Wine Emporium. It’s an import-export business focused on bringing bulk wine to the \n",
      "states and selling it to select wine and liquor stores across the country. The way the busi-\n",
      "ness works is that Joey Bags travels the globe ﬁ nding incredible deals on large quantities of \n",
      "wine. Joey ships it back to Jersey, and it’s your job to sell this stuff  on to stores at a proﬁ t.\n",
      "You reach out to customers in a number of ways—a Facebook page, Twitter, even the \n",
      "occasional direct mailing—but the e-mail newsletter drums up the most business. For the \n",
      "past year, you’ve sent one newsletter per month. Usually there are two or three wine deals \n",
      "in each e-mail, perhaps one would be on Champagne, another on Malbec. Some deals are \n",
      "amazing, 80 percent or more off  of retail. In total, you’ve off ered 32 deals this year, all of \n",
      "which have gone quite well. \n",
      "But just because things are going well, doesn’t mean you can’t do better. It’d be nice \n",
      "if you could understand the customers a little more. Sure, you can look at a particular \n",
      "purchase—like how some person with the last name Adams bought some Espumante \n",
      "in July at a 50 percent discount—but you can’t tell whether that’s because he liked that \n",
      "the minimum purchase requirement was one six-bottle box or the price or that it hadn’t \n",
      "passed its peak yet. \n",
      "It would be nice if you could segment the list into groups based on interest. Then, you \n",
      "could customize the newsletter to each segment and maybe drum up some more busi-\n",
      "ness. Whichever deal you thought matched up better with the segment could go in the \n",
      "subject line and would come ﬁ rst in the newsletter. That type of targeting can result in a \n",
      "bump in sales. \n",
      "But how do you segment the list? Where do you start? \n",
      "This is an opportunity to let the computer segment the list for you. Using k-means \n",
      "clustering, you can ﬁ nd the best segments and then try to understand why they’re the \n",
      "best segments.\n",
      "The Initial Dataset\n",
      "NOTE\n",
      "The Excel workbook used in this chapter, “WineKMC.xlsx,” is available for down-\n",
      "load at the book’s website at www.wiley.com/go/datasmart. This workbook includes \n",
      "all the initial data if you want to work from it. Or you can just read along using the \n",
      "sheets I’ve put together in the workbook.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "37Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "Starting out, you have two interesting sources of data:\n",
      "• The metadata on each off er is saved in a spreadsheet, including varietal, minimum \n",
      "bottle quantity for purchase, discount off  retail, whether the wine is past its peak, \n",
      "and country or state of origin. This data is housed in a tab called Off erInformation, \n",
      "as shown in Figure 2-7\n",
      "• You also know which customers bought which off ers, so you can dump that infor-\n",
      "mation out of MailChimp and into the spreadsheet with the off er metadata in a tab \n",
      "called Transactions. This transactional data, as shown in Figure 2-8, is simply rep-\n",
      "resented as the customer who made the purchase and which off er they purchased.\n",
      "Figure 2-7: The details of the last 32 offers\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "38 Data Smart\n",
      "Figure 2-8: A list of offers taken by customer \n",
      "Determining What to Measure\n",
      "So here’s a conundrum. In the middle school dance problem, measuring distances between \n",
      "dancers and cluster centers was easy, right? Just break out the measuring tape!\n",
      "But what do you do here?\n",
      "You know there were 32 deals offered in the last year, and you have a list in the \n",
      "Transactions tab of the 324 purchases, broken out by customer. But in order to measure \n",
      "the distance between each customer and a cluster center, you need to position them in \n",
      "this 32-deal space. In other words, you need to understand the deals they did not take, \n",
      "and create a matrix of deals-by-customers, where each customer gets their own 32-deal \n",
      "column full of 1s for the deals they took and 0s for the ones they didn’t. \n",
      "In other words, you need to take this row-oriented Transactions tab and turn it into a \n",
      "matrix with customers in columns and off ers in rows. And the best way to create such a \n",
      "matrix is to use a PivotTable.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "39Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "NOTE\n",
      "For a primer on PivotTables, see Chapter 1.\n",
      "So here’s what you’re going to do. In the Transactions tab, highlight columns A and \n",
      "B and then insert a PivotTable. Using the PivotTable Builder, simply select deals as row \n",
      "labels, customers as column labels, and take a count of deals for the values. This count \n",
      "will be 1 if a customer/deal pair was present in the original data and 0 otherwise (0 ends \n",
      "up as a blank cell in this case). The resulting PivotTable is pictured in Figure 2-9.  \n",
      "Figure 2-9: PivotTable of deals versus customers \n",
      "Now that you have your purchases in matrix form, copy the Off erInformation tab and \n",
      "name it Matrix. In this new sheet, paste the values from the PivotTable (you don’t need \n",
      "to copy and paste the deal number, because it’s already in the off er information) into the \n",
      "new tab starting at column H. You end up with a ﬂ  eshed out version of the matrix that \n",
      "has consolidated the deal descriptions with the purchase data, as pictured in Figure 2-10.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "40 Data Smart\n",
      "Figure 2-10: Deal description and purchase data merged into a single matrix \n",
      "STANDARDIZING YOUR DATA\n",
      "In this chapter, each dimension of your data is the same type of binary purchase \n",
      "data. But in many clustering problems, this is not the case. Envision a scenario \n",
      "where people are clustered based on height, weight, and salary. These three types of \n",
      "data are all on diff erent scales. Height may range from 60 inches to 80 inches while \n",
      "weight may range from 100 to 300 pounds. \n",
      "In this context, measuring the distance between customers (like dancers on the dance \n",
      "ﬂ oor) gets tricky. So it’s common to standardize each column of data by subtracting out \n",
      "the average and dividing through by a measure of spread we’ll encounter in Chapter \n",
      "4 called the standard deviation. This puts each column on the same scale, centered \n",
      "around 0.\n",
      "While our data in Chapter 2 does not require standardization, you can see it in action \n",
      "in the outlier detection chapter, Chapt er 9.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "41Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "Start with Four Clusters\n",
      "All right, so now you have all of your data consolidated into a single, useable format. \n",
      "In order to begin clustering, you need to pick k, which is the number of clusters in the \n",
      "k-means clustering algorithm. Often the approach in k-means is to try a bunch of diff er-\n",
      "ent values for k (I’ll get to how to choose between them later), but for the sake of starting, \n",
      "you need to choose just one. \n",
      "You’ll want to choose a number of clusters to start with that’s in the ball park of what \n",
      "you’re willing to act on. You’re not going to create 50 clusters and send 50 targeted ad \n",
      "campaigns to a couple of folks in each group. That defeats the purpose of the exercise in \n",
      "the ﬁ rst place. You want something small in this case. For this example, then, start with \n",
      "four—in an ideal world, maybe you’d get your list divided into four perfectly understand-\n",
      "able groups of 25 customers each (this isn’t likely).\n",
      "All right then, if you were to split the customers into four groups, what are the best \n",
      "four groups for that?\n",
      "Rather than dirty up the pretty Matrix tab, copy the data into a new tab and call it 4MC. \n",
      "You can then insert four columns after Past Peak in columns H through K that will be the \n",
      "cluster centers. (To insert a column, right-click Column H and select Insert. A column \n",
      "will be added to the left.) Label these clusters Cluster 1 through Cluster 4. You can also \n",
      "place some conditional formatting on them so that whenever each cluster center is set \n",
      "you can see how they diff er.\n",
      "The 4MC tab will appear as shown in Figure 2-11. \n",
      "These cluster centers are all 0s at this point. But technically, they can be anything \n",
      "you want, and what you’d like to see is that they, like in the middle school dance case, \n",
      "distribute themselves to minimize the distances between each customer and their closest \n",
      "cluster center.\n",
      "Obviously then, these centers will have values between 0 and 1 for each deal since all \n",
      "the customer vectors are binary.\n",
      "But what does it mean to measure the distance between a cluster center and a customer?\n",
      "Euclidean Distance: Measuring Distances as the Crow Flies\n",
      "You now have a single column per customer, so how do you measure the dance-ﬂ  oor \n",
      "distance between them? Well, the offi  cial term for that is “as-the-crow-ﬂ ies,” measuring \n",
      "tape distance is the Euclidean distance.\n",
      "Let’s return to the dance ﬂ oor problem to understand how to compute it.\n",
      "I’m going to lay down a horizontal and a vertical axis on the dance ﬂ oor, and in Figure \n",
      "2-12, you can see that you have a dancer at (8, 2) and a cluster center at (4, 4). To compute \n",
      "the Euclidean distance between them, you have to remember the Pythagorean theorem \n",
      "you learned back in middle school.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "42 Data Smart\n",
      "Figure 2-11: Blank cluster centers placed on the 4MC tab\n",
      "(8,2)\n",
      "(4,4)\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "23456 789 1 0\n",
      "Figure 2-12: A dancer at (8,2) and a cluster center at (4,4)\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "43Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "These two points are 8 – 4 = 4 feet apart in the vertical direction. They’re 4 – 2 = 2 feet \n",
      "apart in the horizontal direction. By the Pythagorean theorem then, the squared distance \n",
      "between these two points is 4^2 + 2^2 = 16 + 4 = 20 feet. So the distance between them is \n",
      "the square root of 20, which is approximately 4.47 feet (see Figure 2-13). \n",
      "(8,2)\n",
      "(4,4)\n",
      "2\n",
      "4\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "234567891 0\n",
      "42 +  22 =  4.47\n",
      "Figure 2-13: Euclidean distance is the square root of the sum of squared distances in each \n",
      "single direction\n",
      "In the context of the newsletter subscribers, you have more than two dimensions, but \n",
      "the same concept applies. Distance between a customer and a cluster center is calculated \n",
      "by taking the diff erence between the two points for each deal, squaring them, summing \n",
      "them up, and taking the square root.\n",
      "So for instance, let’s say in the 4MC tab, you wanted to take the Euclidean distance \n",
      "between the Cluster 1 center in column H and the purchases of customer Adams in col-\n",
      "umn L.\n",
      "In cell L34, below Adams’ purchases, you can take the diff erence of Adams’ vector and \n",
      "the cluster center, square it, sum it, and square root the sum, using the following array \n",
      "formula (note the absolute references that allow you to drag this formula to the right or \n",
      "down without the cluster center reference changing):\n",
      "{=SQRT(SUM((L$2:L$33-$H$2:$H$33)^2))}\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "44 Data Smart\n",
      "You have to use an array formula (enter the formula and press Ctrl + Shift + Enter or \n",
      "Cmd + Return on Mac as covered in Chapter 1) because the (L2:L33 – H2:H33)^2 por-\n",
      "tion of the formula needs to know to go item by item taking diff  erences and squaring \n",
      "them. The end result, however, is a single number: 1.732 in this case (see Figure 2-14). \n",
      "This makes sense because Adams took three deals, but the initial cluster center is all 0s, \n",
      "and the square root of 3 is 1.732. \n",
      "Figure 2-14: The distance between Adams and Cluster 1 \n",
      "In the spreadsheet shown in Figure 2-14, I’ve frozen panes (see Chapter 1) between \n",
      "columns G and H and labeled row 34 in G34 as Distance to Cluster 1 just to keep track \n",
      "of things when you scroll to the right.\n",
      "Distances and Cluster Assignments for Everybody!\n",
      "So now you know how to calculate the distance between a purchase vector and a cluster \n",
      "center.\n",
      "It’s time to add the distance calculations for Adams to the other centers by dragging \n",
      "cell L34 down through L37 and then changing the cluster center reference manually from \n",
      "column H to I, J, and K in the descending cells. You end up with the following 4 formulas \n",
      "in L34:L37:\n",
      "{=SQRT(SUM((L$2:L$33-$H$2:$H$33)^2))}\n",
      "{=SQRT(SUM((L$2:L$33-$I$2:$I$33)^2))}\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "45Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "{=SQRT(SUM((L$2:L$33-$J$2:$J$33)^2))}\n",
      "{=SQRT(SUM((L$2:L$33-$K$2:$K$33)^2))}\n",
      "Since you’ve used absolute references (the $ sign in the formulas; see Chapter 1 for \n",
      "more details) for the cluster centers, you can drag L34:L37 over through DG34:DG37 to \n",
      "calculate distances between each customer and all four cluster centers. Also, in column \n",
      "G, label rows 35 through 37 Distance to Cluster 2, and so on. These new distances are \n",
      "pictured in Figure 2-15.\n",
      "Figure 2-15: Distance calculations from each customer to each cluster \n",
      "For each customer then, you know their distance to all four cluster centers. Their cluster \n",
      "assignment is to the nearest one, which you can calculate in two steps.\n",
      "First, going back to customer Adams in column L, let’s calculate the minimum distance \n",
      "to a cluster center in cell L38. That’s just:\n",
      "=MIN(L34:L37) \n",
      "And then to determine which cluster center matches that minimum distance, you can \n",
      "use the MATCH formula (see Chapter 1 for more details). Placing the following MATCH formula \n",
      "in L39, you can determine which cell index in the range L34 to L37 counting up from 1 \n",
      "matches the minimum distance:\n",
      "=MATCH(L38,L34:L37,0)\n",
      "In this case the minimum distance is a tie between all four clusters, so MATCH picks the \n",
      "ﬁ rst (L34) by returning index 1 (see Figure 2-16).\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "46 Data Smart\n",
      "You can drag these two formulas across the sheet through DG38:DG39 as well. Add \n",
      "Minimum Cluster Distance and Assigned Cluster in Column G as labels for rows 38 and \n",
      "39 just to keep things organized. \n",
      "Figure 2-16: Cluster matches added into the sheet \n",
      "Solving for the Cluster Centers\n",
      "You now have distance calculations and cluster assignments in the spreadsheet. To set the \n",
      "cluster centers to their best locations, you need to ﬁ nd the values in columns H through \n",
      "K that minimize the total distance between the customers and their assigned clusters \n",
      "denoted on row 39 beneath each customer. \n",
      "And if you read Chapter 1, you know exactly what to think when you hear the word \n",
      "minimize: This is an optimization step, and an optimization step means using Solver.\n",
      "In order to use Solver, you need an objective cell, so in cell A36, let’s sum up all the \n",
      "distances between customers and their cluster assignments:\n",
      "=SUM(L38:DG38)\n",
      "This sum of customers’ distances from their closest cluster center is exactly the objec-\n",
      "tive function encountered earlier when clustering on the McAcne Middle School dance\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "47Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "ﬂ oor. But Euclidean distance with its squares and square roots is crazy non-linear (read \n",
      "“wicked non-linear” if you live in Massachusetts), so you need to use the evolutionary \n",
      "solving method instead of the simplex method to set the cluster centers. \n",
      "In Chapter 1, you used the simplex algorithm. Simplex is faster than other methods \n",
      "when it’s allowable, but it’s not possible when you’re squaring, square rooting, or other-\n",
      "wise, taking non-linear functions of your decisions. Likewise, OpenSolver (introduced \n",
      "in Chapter 1), which uses an implementation of simplex on steroids is of no use here.\n",
      "In this case, the evolutionary algorithm built into Solver uses a combination of random \n",
      "search and good solution “breeding” to ﬁ nd good solutions similarly to how evolution \n",
      "works in biological contexts.\n",
      "NOTE\n",
      "For a full treatment of optimization, see Chapter 4.\n",
      "Notice that you have everything you need to set up a problem in Solver:\n",
      "• Objective: Minimize the total distances of customers from their cluster cen-\n",
      "ters (A36).\n",
      "• Decision variables : The deal values of each row within the cluster center \n",
      "(H2:K33).\n",
      "• Constraints: Cluster centers should have values somewhere between 0 and 1.\n",
      "Open Solver and hammer in the requirements. You’ll set Solver to minimize A36 by \n",
      "changing H2:K33 with the constraint that H2:K33 be <= 1 just like all the deal vectors. \n",
      "Make sure that the variables are checked as non-negative and that the evolutionary solver \n",
      "is chosen. See Figure 2-17.\n",
      "Also, setting these clusters isn’t a cakewalk for Solver, so you should beef up some of \n",
      "the evolutionary solver’s options by pressing the options button within the Solver win-\n",
      "dow and toggling over to the evolutionary tab. It’s useful to bump up the Maximum Time \n",
      "Without Improvement parameter somewhere north of 30 seconds, depending on how long \n",
      "you want to wait for the Solver to ﬁ nish. In Figure 2-18, I’ve set mine to 600 seconds (10 \n",
      "minutes). That way, I can set the Solver to run and go to dinner. And if you ever want to \n",
      "kill Solver early, just press Escape and then exit with the best solution it’s found so far.\n",
      "If you’re curious, the inner workings of the evolutionary solver are covered in greater \n",
      "detail in Chapter 4 and at \n",
      "http://www.solver.com.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "48 Data Smart\n",
      "Figure 2-17: The Solver setup for 4-means clustering \n",
      "Figure 2-18: The evolutionary solver options tab\n",
      "Press Solve and watch Excel do its thing until the evolutionary algorithm converges.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "49Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "Making Sense of the Results\n",
      "Once Solver gives you the optimal cluster centers, the fun starts. You get to mine the \n",
      "groups for insight! So in Figure 2-19, you can see that Solver calculated an optimal total \n",
      "distance of 140.7, and the four cluster centers, thanks to the conditional formatting, all \n",
      "look very diff erent.\n",
      "Note that your cluster centers may look diff erent from the spreadsheet provided with \n",
      "the book, because the evolutionary algorithm employs random numbers and does not \n",
      "give the same answer each time. The clusters may be fundamentally diff  erent or, more \n",
      "likely, they may be in a diff erent order (for example, my Cluster 1 is very close to your \n",
      "Cluster 4, and so on).\n",
      "Because you pasted the deal descriptions in columns B through G when you set up the \n",
      "tab, you can read off  the details of the deals in Figure 2-19 that seem important to the \n",
      "cluster centers.\n",
      "Figure 2-19: The four optimal cluster centers \n",
      "For Cluster 1 in column H, the conditional formatting calls out deals 24, 26, 17, and to \n",
      "a lesser degree, 2. Reading through the details of those deals, the main thing they have \n",
      "in common: They’re all Pinot Noir. \n",
      "If you look at column I, the green cells all have a low minimum quantity in common. \n",
      "These are the buyers who don’t want to have to buy in bulk to get a deal.\n",
      "But I’ll be honest; the last two cluster centers are kind of hard to interpret. Well, how \n",
      "about instead of interpreting the cluster center, you investigate the members of the cluster \n",
      "and determine which deals they like? That might be more elucidating.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "50 Data Smart\n",
      "Getting the Top Deals by Cluster\n",
      "So instead of looking at which dimensions are closer to 1 for a cluster center, let’s check \n",
      "who is assigned to each cluster and which deals they prefer.\n",
      "To do this, let’s start by making a copy of the Off erInformation tab and calling it 4MC – \n",
      "TopDealsByCluster. On this new tab, label columns H through K as 1, 2, 3, and 4 (see \n",
      "Figure 2-20).\n",
      "Figure 2-20: Setting up a tab to count popular deals by cluster\n",
      "Back on tab 4MC, you have cluster assignments listed (1-4) on row 39. All you need to \n",
      "do to get deal counts by cluster is check the column title on tab 4MC – TopDealsByCluster \n",
      "in columns H through K, see who on 4MC was assigned to that cluster using row 39, and \n",
      "then sum up their values for each deal row. That’ll give you the total customers from a \n",
      "given cluster that took a deal.\n",
      "Start with cell H2, that is, the count of customers in Cluster 1 who took off  er #1, the \n",
      "January Malbec off er. You want to sum across L2:DG2 on the 4MC tab but only for those \n",
      "customers who are in Cluster 1, and that is a classic use case for the \n",
      "SUMIF formula. The \n",
      "formula looks like this:\n",
      "=SUMIF('4MC'!$L$39:$DG$39,'4MC - TopDealsByCluster'!H$1,'4MC'!$L2:$DG2)\n",
      "The way the SUMIF statement works is that you provide it with some values to check \n",
      "in the ﬁ rst section '4MC'!$L$39:$DG$39, which are checked against the 1 in the column \n",
      "header ('4MC - TopDealsByCluster'!H$1), and then for any match, you sum up row 2 by \n",
      "specifying '4MC'!$L2:$DG2 in the third section of the formula.\n",
      "Note that you’ve used absolute references (the $ in the formula) in front of everything \n",
      "in the cluster assignment row, in front of the row number for our column headers, and \n",
      "in front of the column letter for our deals taken. By making these references absolute, \n",
      "you can then drag this formula through range H2:K33 to get deal counts for every cluster \n",
      "center and deal combination, as pictured in Figure 2-21. You can place some conditional \n",
      "formatting on these columns to make them more readable.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "51Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "By selecting columns A through K and auto-ﬁ  ltering (see Chapter 1), you can make \n",
      "this data sortable. Sorting from high to low on column H, you can then see which deals \n",
      "are most popular within Cluster 1 (see Figure 2-22).\n",
      "Just as noted earlier, the four top deals for this cluster are all Pinot. These folks have \n",
      "watched Sideways one too many times. When you sort on Cluster 2, it becomes abundantly \n",
      "clear that these are the low volume buyers (see Figure 2-23).\n",
      "But when you sort on Cluster 3, things aren’t quite as clear. There are more than a hand-\n",
      "ful of top deals; the drop-off  between in deals and out deals is not as stark. But the most \n",
      "popular ones seem to have a few things in common—the discounts are quite good, ﬁ  ve \n",
      "out of the top six deals are bubbly in nature, and France is in three of the top four deals. \n",
      "But nothing is conclusive (see Figure 2-24).\n",
      "As for Cluster 4, these folks really loved the August Champaign deal for whatever rea-\n",
      "son. Also, ﬁ ve out of the top six deals are from France, and nine of the top 10 deals are \n",
      "high volume (see Figure 2-25). Perhaps this is the French-leaning high volume Cluster? \n",
      "The overlap between clusters 3 and 4 is somewhat troubling.\n",
      "This leads to a question: Is 4 the right number for k in k-means clustering? Perhaps \n",
      "not. But how do you tell?\n",
      "Figure 2-21: Totals of each deal taken broken out by cluster\n",
      "Figure 2-22: Sorting on Cluster 1—Pinot, Pinot, Pinot!\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "52 Data Smart\n",
      "Figure 2-23: Sorting on Cluster 2—small-timers\n",
      "Figure 2-24: Sorting on Cluster 3 is a bit of a mess\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "53Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "Figure 2-25: Sorting on Cluster 4—these folks just like Champagne in August? \n",
      "The Silhouette: A Good Way to Let Different K Values \n",
      "Duke It Out\n",
      "There’s nothing wrong with just doing k-means clustering for a few values of k until you \n",
      "ﬁ nd something that makes intuitive sense to you. Of course, maybe the reason that a \n",
      "given k doesn’t “read well” is not because k is wrong but because the off er information is \n",
      "leaving something out that would help describe the clusters better.\n",
      "So is there another way (other than just eyeballing the clusters) to give a thumbs-up \n",
      "or -down to a particular value of k? \n",
      "There is—by computing a score for your clusters called the silhouette. The cool thing \n",
      "about the silhouette is that it’s relatively agnostic to the value of k, so you can compare \n",
      "diff erent values of k using this single score.\n",
      "The Silhouette at a High Level: How Far Are Your Neighbors from You?\n",
      "You can compare the average distance between each customer and their friends in the \n",
      "cluster they’ve been assigned to with the average distance to the customers in the cluster \n",
      "with the next nearest center.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "54 Data Smart\n",
      "If I’m a lot closer to the people in my cluster than to the people in the neighboring \n",
      "cluster, these folks are a good group for me, right? But what if the folks from the next \n",
      "nearest cluster are nearly as close to me as my own clustered brethren? Well, then my \n",
      "cluster assignment is a bit shaky, isn’t it? \n",
      "A formal way to write this value is:\n",
      "(Average distance to those in the nearest neighboring cluster – Average distance to those \n",
      "in my cluster)/The maximum of those two averages\n",
      "The denominator in the calculation keeps the value between -1 and 1. \n",
      "Think about that formula. As the residents of the next closet cluster get farther and \n",
      "farther away (more ill-suited to me), the value approaches 1. And if the two average dis-\n",
      "tances are nearly the same? Then the value approaches 0.\n",
      "Taking the average of this calculation for each customer gives you the silhouette. If the \n",
      "silhouette is 1, it’s perfect. If it’s 0, the clusters are rather ill suited. If it’s less than 0, lots \n",
      "of customers are better off  hanging out in another cluster, which is the pits. \n",
      "And for diff erent values of k, you can compare silhouettes to see if you’re improving.\n",
      "To see this concept more clearly, go back to the middle school dance example. Figure 2-26 \n",
      "shows an illustration of the distance calculations used in forming the silhouette. Note \n",
      "that one of the chaperone’s distance from the other two chaperones is being compared \n",
      "to the distances from the next nearest cluster, which is the ﬂ ock of middle school boys. \n",
      "Now, the other two chaperones are by far closer than the herd of awkward teenagers, \n",
      "so that would make the distance ratio calculation far greater than 0 for this chaperone.\n",
      "Figure 2-26: The distances considered for a chaperone’s contribution to the silhouette calculation\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "55Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "Creating a Distance Matrix\n",
      "In order to implement the silhouette, there’s one major piece of data you need: the distance \n",
      "between customers. And while cluster centers may move around, the distance between \n",
      "two customers never changes. So you can just create a single Distances tab and use it \n",
      "in all of your silhouette calculations no matter what value of k you use or where those \n",
      "centers end up.\n",
      "Let’s start by creating a blank sheet called Distances and pasting in customers across \n",
      "the top and down the rows. A cell in the matrix will hold the distance between the cus-\n",
      "tomer on the row and the customer on the column. To paste customers down the rows, \n",
      "copy H1:DC1 from the Matrix tab and use Paste Special to paste the values, making sure \n",
      "to choose the Transpose option in the Paste Special window.  \n",
      "You need to keep track of where customers are on the Matrix tab, so number the cus-\n",
      "tomers from 0 to 99 in both directions. Let’s put these numbers in column A and row 1, \n",
      "so insert blank rows and columns to the left and above the names you’ve already pasted \n",
      "by right-clicking column A and row 1 and inserting a new row 1 and a new column A. \n",
      "NOTE\n",
      "FYI, there are a lot of ways to put those 0–99 counts into Excel. For instance, you \n",
      "can type the ﬁ rst few in, 0, 1, 2, 3, and then highlight them and drag the bottom \n",
      "corner of the selection through the rest of the customers. Excel will understand and \n",
      "extend the count. The resulting empty matrix is pictured in Figure 2-27.\n",
      "Consider cell C3, which is the distance between Adams and Adams, in other words \n",
      "between Adams and himself. This should be 0, right? You can’t get any closer to you \n",
      "than you! \n",
      "So how do you calculate that? Well, column H on the Matrix tab shows Adams’ deal \n",
      "vector. To calculate the Euclidean distance between Adams and himself, it’s just column \n",
      "H minus column H, square the diff erences, sum them up, and take the square root.\n",
      "But how do you drag that calculation around to every cell in the matrix? I’d hate to \n",
      "type them in manually. That’d take forever. What you need to use is the \n",
      "OFFSET formula \n",
      "in cell C3 (see Chapter 1 for an explanation of OFFSET).\n",
      "The OFFSET formula takes in an anchoring range of cells; in this case make it Adams’ \n",
      "deal vector Matrix!$H$2:$H$33, and moves the entire range a given number of rows and \n",
      "columns in the direction you specify.\n",
      "So for instance, OFFSET(Matrix!$H$2:$H$33,0,0) is just Adams’ deal vector because \n",
      "you’re moving the original range 0 rows down and 0 columns to the right.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "56 Data Smart\n",
      "Figure 2-27: The bare bones Distances tab \n",
      "But OFFSET(Matrix!$H$2:$H$33,0,1) is Allen’s deal column. \n",
      "OFFSET(Matrix!$H$2:$H$33,0,2) is Anderson, and so on.\n",
      "And this is where those indices 0 – 99 in row 1 and column A are going to come in \n",
      "handy. For example:\n",
      "{=SQRT(SUM((OFFSET(Matrix!$H$2:$H$33,0,Distances!C$1)-OFFSET(Matrix!$H$2:$\n",
      "H$33,0,Distances!$A3))^2))}\n",
      "That’s the distance between Adams and himself. Note that you’re pulling Distances!C$1 \n",
      "for the column off set in the ﬁ rst deal vector and Distances!$A3 for the column off set in \n",
      "the second deal vector. \n",
      "That way, when you drag this calculation across and down in the sheet, everything \n",
      "is anchored to Adams’ deal vector, but the OFFSET  formula shifts the vectors over the \n",
      "appropriate amount using the indices in column A and row 1. This way, it will grab the \n",
      "appropriate two deal vectors for the customers you care about. Figure 2-28 shows the \n",
      "ﬁ lled out distance matrix.\n",
      "Also, keep in mind that just like on tab 4MC, these distances are array formulas.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "57Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "Figure 2-28: The completed distance matrix \n",
      "Implementing the Silhouette in Excel\n",
      "All right, now that you have a Distances tab, you can create another tab called 4MC \n",
      "Silhouette for the ﬁ nal silhouette calculation.\n",
      "To start, let’s copy the customers and their community assignments from the 4MC tab \n",
      "and Paste Special the customer names down column A and the assignments down B (don’t \n",
      "forget to check that Transpose box in the Paste Special window).\n",
      "Next, you can use the Distances tab to calculate the average distance between each \n",
      "customer and those in a particular cluster. So label columns C through F Distance from \n",
      "People in 1 through Distance from People in 4.\n",
      "In my workbook, Adams has been assigned to Cluster 2, so calculate in cell C2 the \n",
      "distance between him and all the customers in Cluster 1. You need to look up customers \n",
      "and see which ones are in Cluster 1 and then average their distances from Adams on row \n",
      "3 of the Distances tab.\n",
      "Sounds like a case for the \n",
      "AVERAGEIF formula:\n",
      "=AVERAGEIF('4MC'!$L$39:$DG$39,1,Distances!$C3:$CX3)\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "58 Data Smart\n",
      "AVERAGEIF checks the cluster assignments and matches them to Cluster 1 before aver-\n",
      "aging the appropriate distances from C3:CX3.\n",
      "For columns D through F, the formulas are the same except Cluster 1 is replaced with \n",
      "2, 3, and 4 in the formula. You can then double-click these formulas to copy them to all \n",
      "customers, yielding the table shown in Figure 2-29. \n",
      "Figure 2-29: Average distance between each customer and the customers in every cluster \n",
      "In column G, you can calculate the closest group of customers using the MIN formula. \n",
      "For instance, for Adams, it’s simply:\n",
      "=MIN(C2:F2) \n",
      "And in column H, you can calculate the second closest group of customers using the \n",
      "SMALL formula (the 2 in the formula is for second place):\n",
      "=SMALL(C2:F2,2) \n",
      "Likewise, you can calculate the distance to your own community members (which is \n",
      "probably the same as column G but not always) in column I as:\n",
      "=INDEX(C2:F2,B2) \n",
      "The INDEX  formula is used to count over to the appropriate distance column in C \n",
      "through F using the assignment value in B as an index.\n",
      "And for the silhouette calculation, you also need the distance to the closest group of \n",
      "customers who are not in your cluster, which is most likely column H but not always. To \n",
      "get this in column J, you check your own cluster distance in I against the closest cluster \n",
      "in G, and if they match, the value is H. Otherwise, it’s G.\n",
      "=IF(I2=G2,H2,G2) \n",
      "Copying all these values down, you’ll get the spreadsheet shown in Figure 2-30.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "59Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "Figure 2-30: Average distances to the folks in my own cluster and to the closest group whose cluster \n",
      "I’m not in \n",
      "Once you’ve placed those values together, adding the silhouette values for a particular \n",
      "customer in column K is simple:\n",
      "=(J2-I2)/MAX(J2,I2) \n",
      "You can just copy that formula down the sheet to get these ratios for each customer. \n",
      "You’ll notice that for some customers, these values are closer to 1. For example, the \n",
      "silhouette value for Anderson in my clustering solution is 0.544 (see Figure 2-31). Not \n",
      "bad! But for other customers, such as Collins, the value is actually less than 0, implying \n",
      "that all things being equal Collins would be better off  in his neighboring cluster than in \n",
      "his current one. Poor guy.\n",
      "Now, you can average these values to get the ﬁ nal silhouette ﬁ gure. In my case, as shown \n",
      "in Figure 2-31, it’s 0.1492, which seems a lot closer to 0 than 1. That’s disheartening, but \n",
      "not entirely surprising. After all, two out of four of the clusters were very shaky when you \n",
      "tried to interpret them with the deal descriptions.\n",
      "Figure 2-31: The ﬁ nal silhouette for 4-means clustering\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "60 Data Smart\n",
      "Okay. Now what?\n",
      "Sure, the silhouette is 0.1492. But what does that mean? How do you use it? You try \n",
      "other values of k! Then you can use the silhouette to see if you’re doing better.\n",
      "How about Five Clusters?\n",
      "Try bumping k up to 5 and see what happens.\n",
      "Here’s the good news: Because you’ve already done four clusters, you don’t have to start \n",
      "the spreadsheets from scratch. You don’t have to do anything with the Distances sheet at \n",
      "all. That one’s good to go.\n",
      "You start by creating a copy of the 4MC tab and calling it 5MC. All you need to do is \n",
      "add a ﬁ fth cluster to the sheet and work it into your calculations.\n",
      "First, let’s right-click column L and insert a new column called Cluster 5. You also need \n",
      "to insert a Distance to Cluster 5 row by right-clicking row 38 and selecting Insert. You can \n",
      "copy down the Distance to Cluster 4 row into row 38 and change column K to L, to create \n",
      "the Distance to Cluster 5 row. As for the Minimum Cluster Distance and Assigned Cluster \n",
      "rows, references to row 37 should be revised to 38 to include the new cluster distance.\n",
      "You’ll end up with the sheet pictured in Figure 2-32. \n",
      "Figure 2-32: The 5-means clustering tab\n",
      "Solving for Five Clusters\n",
      "Opening up Solver, you need only change $H$2:$K$33 to $H$2:$L$33 in both the decision vari-\n",
      "ables and constraints sections to include the new ﬁ fth cluster. Everything else stays the same.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "61Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "Press Solve and let this new problem run. \n",
      "In my run, the Solver terminated with a total distance of 135.1, as shown in Figure 2-33.\n",
      "Figure 2-33: The optimal 5-means clusters\n",
      "Getting the Top Deals for All Five Clusters\n",
      "All right. Let’s see how you did. \n",
      "You can create a copy of the 4MC – TopDealsByCluster tab and rename it 5MC – \n",
      "TopDealsByCluster, but you’ll need to revise a few of the formulas to get it to work.\n",
      "First of all, you need to make sure that this worksheet is ordered by Off er # in column \n",
      "A. Then label column L with a 5 and drag the formulas from K over to L. You should also \n",
      "highlight columns A through L and reapply the auto-ﬁ  ltering to make Cluster 5’s deal \n",
      "purchases sortable.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "62 Data Smart\n",
      "Everything on this sheet is currently pointing to tab 4MC, so it’s time to break out the \n",
      "ol’ Find and Replace. The cluster assignments on tab 5MC are shifted one row down and \n",
      "one column to the right, so the reference to \n",
      "'4MC'!$L$39:$DG$39 in the SUMIF formulas \n",
      "should become '5MC'!$M$40:$DH$40 . As shown in Figure 2-34, you can use Find and \n",
      "Replace to change this.\n",
      "Figure 2-34  Replacing 4-means cluster assignments with 5-means cluster assignments \n",
      "NOTE\n",
      "Keep in mind that your results will diff er from mine due to the evolutionary solver. \n",
      "Sorting on Cluster 1, you clearly have your Pinot Noir cluster again (see Figure 2-35). \n",
      "Figure 2-35: Sorting on Cluster 1—Pinot Noir out the ears\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "63Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "Cluster 2 is the low-volume buyer cluster (see Figure 2-36).\n",
      "Figure 2-36: Sorting on Cluster 2—small quantities only, please \n",
      "As for Cluster 3, this one hurts my head. It seems only to be a South African Espumante \n",
      "that’s important for some reason (Figure 2-37).\n",
      "Figure 2-37: Sorting on Cluster 3—is Espumante that important? \n",
      "The Cluster 4 customers are interested in high volume, primarily French deals with \n",
      "good discounts. There may even be a propensity toward sparkling wines. This cluster is \n",
      "tough to read; there’s a lot going on (see Figure 2-38).\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "64 Data Smart\n",
      "Figure 2-38: Sorting on Cluster 4—all sorts of interests  \n",
      "Sorting on Cluster 5 gives you results similar to Cluster 4, although high volume and \n",
      "high discounts seem to be the primary drivers (see Figure 2-39).\n",
      "Computing the Silhouette for 5-Means Clustering\n",
      "You may be wondering whether ﬁ ve clusters did any better than four. From an eyeball \n",
      "perspective, there doesn’t seem to be a whole lot of diff erence. Let’s compute the silhouette \n",
      "for ﬁ ve clusters and see what the computer thinks.\n",
      "Start by making a copy of 4MC Silhouette and renaming it 5MC Silhouette. Next, right-\n",
      "click column G, insert a new column, and name it Distance From People in 5. Drag the \n",
      "formula from F2 over into G2, change the cluster check from 4 to 5, and then double-click \n",
      "the cell to shoot it down the sheet.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "65Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "Figure 2-39: Sorting on Cluster 5—high volume \n",
      "Identical to the previous section, you’ll need to Find and Replace '4MC'!$L$39:$DG$39 \n",
      "with '5MC'!$M$40:$DH$40. \n",
      "In cells H2, I2, and J2, you should include distances to folks in Cluster 5 in your cal-\n",
      "culations, so any ranges that stop at F2 should be expanded to include G2. You can then \n",
      "highlight H2:J2 and double-click the bottom right to send these updated calculations \n",
      "down the sheet.\n",
      "Lastly, you need to copy and Paste Special values from the cluster assignments on row \n",
      "40 of the 5MC tab into column B on the 5MC Silhouette tab. This means you have to check \n",
      "the Transpose button when using Paste Special.\n",
      "Once you’ve revised the sheet, you should get something like what’s pictured in \n",
      "Figure 2-40.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "66 Data Smart\n",
      "Figure 2-40:  The silhouette for 5-means clustering \n",
      "Well, this is depressing, isn’t it? The silhouette isn’t all that diff  erent. At 0.134, it’s \n",
      "actually a little worse! But that’s not much of a surprise after mining the clusters. In both \n",
      "cases, you had three clusters that really made sense. The others were noisy. Maybe you \n",
      "should go the other direction and try k=3? If you want to give this a shot, I leave it as an \n",
      "exercise for you to try on your own.\n",
      "Instead, let’s give a little thought to what may be going wrong here to cause these noisy, \n",
      "perplexing clusters. \n",
      "K-Medians Clustering and Asymmetric Distance \n",
      "Measurements\n",
      "Usually doing vanilla k-means clustering with Euclidean distance is just ﬁ  ne, but you’ve \n",
      "run into some problems here that many who do clustering on sparse data (whether that’s \n",
      "in retail or text classiﬁ cation or bioinformatics) often encounter.\n",
      "Using K-Medians Clustering\n",
      "The ﬁ rst obvious problem is that your cluster centers are decimals even though each \n",
      "customer’s deal vector is made of solid 0s and 1s. What does 0.113 of a deal really mean? \n",
      "I want cluster centers that commit to a deal or don’t!\n",
      "If you modify the clustering algorithm to use only values present in the customers’ deal \n",
      "vectors, this is called K-medians clustering, rather than K-means clustering.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "67Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "And if you wanted to stick with Euclidean distance, all you’d need to do is add a binary \n",
      "constraint, (bin) in Solver to all of your cluster centers. \n",
      "But if you make your cluster centers binary, is Euclidean distance what you want?\n",
      "Getting a More Appropriate Distance Metric\n",
      "When folks switch from k-means to k-medians, they typically stop using Euclidean dis-\n",
      "tance and start using something called Manhattan distance. \n",
      "Although a crow can ﬂ y from point A to B in a straight line, a cab in Manhattan has to \n",
      "stay on the grid of straight streets; it can only go north, south, east, and west. So while in \n",
      "Figure 2-13, you saw that the distance between a middle school dancer and their cluster \n",
      "center was approximately 4.47, their Manhattan distance was 6 feet (that’s 4 feet down \n",
      "plus 2 feet across).\n",
      "In terms of binary data, like the purchase data, the Manhattan distance between a \n",
      "cluster center and a customer’s purchase vector is just the count of the mismatches. If the \n",
      "cluster center has a 0 and I have a 0, in that direction there’s a distance of 0, and if you \n",
      "have mismatched 0 and 1, you have a distance of 1 in that direction. Summing them up, \n",
      "you get the total distance, which is just the number of mismatches. When working with \n",
      "binary data like this, Manhattan distance is also commonly called Hamming distance.\n",
      "Does Manhattan Distance Solve the Issues?\n",
      "Before you dive headﬁ rst into doing k-medians clustering using Manhattan distance, stop \n",
      "and think about the purchase data. \n",
      "What does it mean when customers take a deal? It means they really wanted that \n",
      "product!\n",
      "What does it mean when customers don’t take a deal? Does it mean that they didn’t \n",
      "want the product as much as they did want the one they bought? Is a negative signal as \n",
      "strong as a positive one? Perhaps they like Champagne but already have a lot in stock. \n",
      "Maybe they just didn’t see your e-mail newsletter that month. There are a lot of reasons \n",
      "why someone doesn’t take an action, but there are few reasons why someone does.\n",
      "In other words, you should care about purchases, not non-purchases.\n",
      "The fancy way to say this is that there’s an “asymmetry” in the data. The 1s are worth \n",
      "more than the 0s. If a customer matches another customer on three 1s, that’s more impor-\n",
      "tant than matching some other customer on three 0s. What stinks though is that while \n",
      "the 1s are so important, there are very few of them in the data—hence, the term “sparse.”\n",
      "But think about what it means for a customer to be close to a cluster center from a \n",
      "Euclidean perspective. If I have a customer with a 1 for one deal and a 0 for another, both \n",
      "of those are just as important in calculating whether a customer is near a cluster center.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "68 Data Smart\n",
      "What you need is an asymmetric distance calculation. And for binary encoded transac-\n",
      "tional data, like these wine purchases, there are a bunch of good ones.\n",
      "Perhaps the most widely used asymmetric distance calculation for 0-1 data is something \n",
      "called cosine distance.\n",
      "Cosine Distance Isn’t Scary Despite the Trigonometry\n",
      "The easiest way to explain cosine distance is to explain its opposite: cosine similarity.\n",
      "Say you had a couple of two-dimensional binary purchase vectors (1,1) and (1,0). In the \n",
      "ﬁ rst vector, both products were purchased, whereas in the second, only the ﬁ rst product \n",
      "was purchased. You can visualize these two purchase vectors in space and see that they \n",
      "have a 45-degree angle between them (see Figure 2-41). Go on, break out the protractor \n",
      "and check it.\n",
      "You can say that they have a cosine similarity then of cos(45 degrees) = 0.707. But why?\n",
      "It turns out the cosine of an angle between two binary purchase vectors is equal to: \n",
      "The count of matched purchases in the two vectors divided by the product of the square \n",
      "root of the number of purchases in the first vector times the square root of the number of \n",
      "purchases in the second vector.\n",
      "In the case of the two vectors (1,1) and (1,0), they have one matched purchase, so the \n",
      "calculation is 1 divided by the square root of 2 (two deals taken), times the square root \n",
      "of one deal taken. And that’s 0.707 (see Figure 2-41).\n",
      "Why is this calculation so cool?\n",
      "Three reasons:\n",
      "• The numerator in the calculation counts numbers of matched purchases only, so \n",
      "this is an asymmetric measure, which is what you’re looking for.\n",
      "• By dividing through by the square root of the number of purchases in each vec-\n",
      "tor, you’re accounting for the fact that a vector where everything is purchased, call \n",
      "it a promiscuous purchase vector, is farther away from another vector than one \n",
      "who matches on the same deals and has not taken as many other deals. You want \n",
      "to match up vectors whose taste matches, not where one vector encompasses the \n",
      "taste of another.\n",
      "• For binary data, this similarity value ranges between 0 and 1, where two vectors \n",
      "don’t get a 1 unless their purchases are identical. This means that 1 – cosine similarity\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "69Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "can be used as a distance metric called cosine distance, which also ranges between \n",
      "0 and 1.\n",
      "(1,1)\n",
      "(1,0)45 degree angle\n",
      "cos(45°) ==  .7071 matched purchase\n",
      "2 purchases 1 purchase\n",
      "Figure 2-41:  An illustration of cosine similarity on two binary purchase vectors \n",
      "Putting It All in Excel\n",
      "It’s time to give k-medians clustering with cosine distance in Excel a shot. \n",
      "NOTE\n",
      "Clustering with cosine distance is also sometimes called spherical k-means. In Chapter 10, \n",
      "you’ll look at spherical k-means in R.\n",
      "For consistency’s sake, continue using k = 5.\n",
      "Start by making a copy of the 5MC tab and naming it 5MedC. Since the cluster centers \n",
      "need to be binary, you might as well delete what Solver left in there.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "70 Data Smart\n",
      "The only items you need to change (other than adding the binary constraint in Solver \n",
      "for k-medians) are the distance calculations on rows 34 through 38. Start in cell M34, \n",
      "which is the distance between Adams and the center of Cluster 1.\n",
      "To count the deal matches between Adams and Cluster 1, you need to take a \n",
      "SUMPRODUCT \n",
      "of the two columns. If either or both have 0s, they get nothing for that row, but if both \n",
      "have a 1, that match will get totaled by the \n",
      "SUMPRODUCT (since 1 times 1 is 1 after all).\n",
      "As for taking the square root of the number of deals taken in a vector, that’s just a SQRT \n",
      "laid on a SUM of the vector. Thus, the overall distance equation can be written as:\n",
      "=1-SUMPRODUCT(M$2:M$33,$H$2:$H$33)/\n",
      "    (SQRT(SUM(M$2:M$33))*SQRT(SUM($H$2:$H$33)))\n",
      "Note the 1– at the beginning of the formula, which changes from cosine similarity to \n",
      "distance. Also, unlike with Euclidean distance, the cosine distance calculation does not \n",
      "require the use of array formulas.\n",
      "However, when you stick this into cell M34, you should add an error check in case the \n",
      "cluster center is all 0s:\n",
      "=IFERROR(1-SUMPRODUCT(M$2:M$33,$H$2:$H$33)/\n",
      "    (SQRT(SUM(M$2:M$33))*SQRT(SUM($H$2:$H$33))),1)\n",
      "Adding the IFERROR formula prevents you from having a division by 0 situation. If for \n",
      "some reason Solver picks an all-0s cluster center, then you can consider that center to \n",
      "have a distance of 1 from everything instead (1 being the largest possible distance in this \n",
      "binary setup).\n",
      "You can then copy M34 down through M38 and change the references from column \n",
      "H to I, J, K, and L respectively. Just like in the Euclidean distance case, you use absolute \n",
      "references ($) in the formula so that you can drag it across without the cluster center \n",
      "columns changing.\n",
      "This gives you a 5MedC sheet (see Figure 2-42) that’s remarkably similar to the earlier \n",
      "5MC tab.\n",
      "Now, to ﬁ nd the clusters, you need to open Solver and change the \n",
      "<= 1 constraint for \n",
      "H2:L33 to instead read as a binary or bin constraint.\n",
      "Press Solve. You can take a load off  for a half hour while the computer ﬁ nds the optimal \n",
      "clusters. Now, you’ll notice visually that the cluster centers are all binary, so likewise the \n",
      "conditional formatting goes to two shades, which is much more stark.\n",
      "The Top Deals for the 5-Medians Clusters\n",
      "When Solver completes, you end up with ﬁ ve cluster centers, each which have a smattering \n",
      "of 1s, indicating which deals are preferred by that cluster. In my Solver run, I ended up with \n",
      "an optimal objective value of 42.8, although yours may certainly vary (see Figure 2-43).\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "71Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "Figure 2-42:  The 5MedC tab not yet optimized \n",
      "Figure 2-43:  The ﬁ ve-cluster medians\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "72 Data Smart\n",
      "Let’s make sense of these clusters using the same deal counting techniques you’ve \n",
      "used in k-means. To do so, the ﬁ  rst thing you need to do is make a copy of the 5MC – \n",
      "TopDealsByCluster tab and rename it 5MedC – TopDealsByCluster.\n",
      "On this tab, all you need to do to make it work is to ﬁ nd and replace 5MC with 5MedC. \n",
      "Because the layout of rows and columns between these two sheets is identical, all the \n",
      "calculations carry over once the sheet reference is changed.\n",
      "Now, your clusters may be slightly diff erent than mine in both order and composition \n",
      "due to the evolutionary algorithm, but hopefully not substantively so. Let’s walk through \n",
      "my clusters one at a time to see how the algorithm has partitioned the customers.\n",
      "Sorting on Cluster 1, it’s apparent that this is the low-volume cluster (see Figure 2-44). \n",
      "Figure 2-44:  Sorting on Cluster 1—low-volume customers \n",
      "Cluster 2 has carved out customers who only buy sparkling wine. Champagne, Prosecco, \n",
      "and Espumante dominate the top 11 spots in the cluster (see Figure 2-45). It’s interesting \n",
      "to note that the k-means approach did not so clearly demonstrate the bubbly cluster with \n",
      "k equal to 4 or 5. \n",
      "Cluster 3 is our Francophile cluster. The top ﬁ ve deals are all French (see Figure 2-46). \n",
      "Don’t they know California wines are better?\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "73Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "Figure 2-45:  Sorting on Cluster 2—not all who sparkle are vampires \n",
      "Figure 2-46:  Sorting on cluster—Francophiles\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "74 Data Smart\n",
      "As for Cluster 4, all the deals are high volume. And the top rated deals are all well \n",
      "discounted and not past their peak (Figure 2-47).\n",
      "Figure 2-47: Sorting on Cluster 4—high volume for 19 deals in a row\n",
      "Cluster 5 is the Pinot Noir cluster once again (see Figure 2-48).\n",
      "That feels a lot cleaner doesn’t it? That’s because in the k-medians case, using an asym-\n",
      "metric distance measure like cosine distance, you can cluster customers based on their \n",
      "interests more than their disinterests. And that’s really what you care about.\n",
      "What a diff erence a distance measure makes!\n",
      "So now you can take these ﬁ ve cluster assignments, import them back into MailChimp\n",
      ".com as a merge ﬁ eld on the list of e-mails, and use the values to customize your e-mail \n",
      "marketing per cluster. This should help you better target customers and drive sales.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "75Cluster Analysis Part I: Using K-Means to Segment Your Customer Base  \n",
      "Figure 2-48:  Sorting on cluster 5—mainlining Pinot Noir \n",
      "Wrapping Up\n",
      "This chapter covered all sorts of good stuff . To summarize, you looked at:\n",
      "• Euclidean distance\n",
      "• k-means clustering using Solver to optimize the centers\n",
      "• How to understand the clusters once you have them\n",
      "• How to calculate the silhouette of a given k-means run\n",
      "• K-medians clustering\n",
      "• Manhattan/Hamming distance\n",
      "• Cosine similarity and distance\n",
      "If you made it through the chapter, you should feel conﬁ  dent not only about how to \n",
      "cluster data, but also which questions can be answered in business through clustering, \n",
      "and how to prepare your data to make it ready to cluster.\n",
      "K-means clustering has been around for decades and is deﬁ nitely the place to start for \n",
      "anyone looking to segment and pull insights from their customer data. But it’s not the \n",
      "most “current” clustering technique. In Chapter 5, you’ll explore using network graphs\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "76 Data Smart\n",
      "to ﬁ nd communities of customers within this same dataset. You’ll even take a ﬁ  eld trip \n",
      "outside of Excel, very brieﬂ y, to visualize the data.\n",
      "If you want to go further with k-means clustering, keep in mind that vanilla Excel \n",
      "tops out at 200 decision variables in Solver, so you need to upgrade to a better non-linear \n",
      "Solver (for example Premium Solver available at Solver.com or just migrate over to using \n",
      "the non-linear Solver in LibreOffi  ce) to cluster on data with many deal dimensions and \n",
      "a high value of k.\n",
      "Most statistical software off ers clustering capabilities. For example, R comes with the \n",
      "kmeans() function; however, the capabilities of the fastcluster package, which includes \n",
      "k-medians and a variety of distance functions, is preferable. In Chapter 10, you’ll look at \n",
      "the \n",
      "skmeans package for performing sphe rical k-means.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "3\n",
      "I\n",
      "n the previous chapter, you hit the ground running with a bit of unsupervised learning. \n",
      "You looked at k-means clustering, which is like the chicken nugget of the data mining \n",
      "world: simple, intuitive, and useful. Delicious too. \n",
      "In this chapter you’re going to move from unsupervised into supervised artiﬁ cial intel-\n",
      "ligence models by training up a naïve Bayes model, which is, for lack of a better metaphor, \n",
      "also a chicken nugget, albeit a supervised one.\n",
      "As mentioned in Chapter 2, in supervised artiﬁ  cial intelligence, you “train” a model \n",
      "to make predictions using data that’s already been classiﬁ  ed. The most common use of \n",
      "naïve Bayes is for document classiﬁ cation. Is this e-mail spam or ham? Is this tweet happy \n",
      "or angry? Should this intercepted satellite phone call be classiﬁ ed for further investigation \n",
      "by the spooks? You provide “training data,” i.e. classiﬁ ed examples, of these documents \n",
      "to the training algorithm, and then going forward, the model can classify new documents \n",
      "into these categories using its knowledge.\n",
      "The example you’ll work through in this chapter is one that’s close to my own heart. \n",
      "Let me explain. \n",
      "When You Name a Product Mandrill, You’re Going to \n",
      "Get Some Signal and Some Noise\n",
      "Recently the company I work for, MailChimp, started a new product called Mandrill.com. \n",
      "It has the most frightening logo I’ve seen in a while (see Figure 3-1).\n",
      "Mandrill is a transactional e-mail product for software developers who want their apps \n",
      "to send one-off  e-mails, receipts, password resets, and anything else that’s one-to-one. \n",
      "Because it allows you to track opens and clicks of individual transactional e-mails, you \n",
      "can even wire it into your personal e-mail account and track whether your relatives are \n",
      "actually viewing those pictures of your cat you keep sending them. (Take it from a data \n",
      "scientist—they’re not.) \n",
      "Naïve Bayes and the \n",
      "Incredible Lightness of \n",
      "Being an Idiot\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart78\n",
      "Figure 3-1: The trance-inducing Mandrill logo\n",
      "But ever since Mandrill was released, one thing has perpetually annoyed me. Whereas \n",
      "a “MailChimp” is a something we invented, a mandrill, also a primate, has been kicking \n",
      "it here on earth for a while. And they’re quite popular. Heck, Darwin called the mandrill’s \n",
      "colorful butt “extraordinary.” \n",
      "That means that if you go onto Twitter and want to look at any tweets mentioning the prod-\n",
      "uct Mandrill, you get something like what you see in Figure 3-2. The bottom tweet is about \n",
      "a new module hooking up the Perl programming language to Mandrill. That one is relevant. \n",
      "But the two above it are about Spark Mandrill from the Super Nintendo game Megaman X \n",
      "and a band called Mandrill.\n",
      "Figure 3-2: Three tweets, only one of which matters\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "79Naïve Bayes and the Incredible Lightness of Being an Idiot\n",
      "Yuck. \n",
      "Even if you enjoyed Megaman X when you were a teen, many of these tweets aren’t \n",
      "relevant to your search. Indeed, there are more tweets about the band plus the game plus \n",
      "the animal plus other Twitter users with “mandrill” in their handle combined than there \n",
      "are about Mandrill.com. That’s a lot of noise.\n",
      " So is it possible to create a model that can distinguish the signal from the noise? Can \n",
      "an AI model alert you only to the tweets about the e-mail product Mandrill?\n",
      "This then is a classic document classiﬁ cation problem. If a document, such as a Mandrill \n",
      "tweet, can belong to multiple classes (about Mandrill.com, about other things), which \n",
      "class should it go in?\n",
      "And the most typical way of attacking this problem is using a bag of words model in \n",
      "combination with a naïve Bayes classiﬁ  er. A bag of words model treats documents as a \n",
      "collection of unordered words. “John ate Little Debbie” is the same as “Debbie ate Little \n",
      "John”; they both are treated as a collection of words {“ate,” “Debbie,” “John,” “Little”}.\n",
      "A naïve Bayes classiﬁ er takes in a training set of these bags of words that are already \n",
      "classiﬁ ed. For instance, you might feed it some bags of about-Mandrill-the-app words and \n",
      "some bags of about-other-mandrills words and train it to distinguish between the two. \n",
      "Then in the future, you can feed it an unknown bag of words, and it’ll classify it for you.\n",
      "So that’s what you’re going to build in this chapter—a naïve Bayes document classiﬁ er \n",
      "that treats the Mandrill tweets as bags of words and gives you back a classiﬁ  cation. And \n",
      "it’s going to be really fun. Why?\n",
      "Because naïve Bayes is often called “idiot’s Bayes.” As you’ll see, you get to make lots of \n",
      "sloppy, idiotic assumptions about your data, and it still works! It’s like the splatter-paint \n",
      "of AI models, and because it’s so simple and easy to implement (it can be done in 50 lines \n",
      "of code), companies use it all the time for simple classiﬁ cation jobs. You can use it to clas-\n",
      "sify company e-mails, customer support transcripts, AP wire articles, the police blotter, \n",
      "medical documents, movie reviews, whatever!\n",
      "Now, before you get started implementing this thing in Excel (which is really quite \n",
      "easy), you’re going to have to learn some probability theory. My apologies. If you get lost \n",
      "in the math, press on to the implementation, and you’ll see how simply it all shakes out.\n",
      "The World’s Fastest Intro to Probability Theory\n",
      "In the next couple sections I’m going to use the notation p() to talk about probability. \n",
      "For instance: \n",
      "  p (Michael Bay’s next ﬁ lm will be terrible) = 1\n",
      "  p (John Foreman will ever go vegan) = 0.0000001\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart80\n",
      "Sorry, it’s extremely unlikely that I’ll ever give up Conecuh smoked sausage—the one \n",
      "thing I like that comes out of Alabama.\n",
      "Totaling Conditional Probabilities\n",
      "Now, the previous two examples are simple probabilities, but what you’re going to be work-\n",
      "ing with a lot in this chapter are conditional probabilities. Here’s a conditional probability:\n",
      "  p (John Foreman will go vegan | you pay him $1B) = 1\n",
      "Although the odds of me ever going vegan are extremely low, the probability of me \n",
      "going vegan given you pay me a billion dollars is 100 percent. That vertical bar | in the \n",
      "statement is used to separate the event from what it’s being conditioned on.\n",
      "How do you reconcile the 0.0000001 overall vegan probability with the virtually assured \n",
      "conditional probability? Well, you can use the law of total probability. The way it works is \n",
      "the probability of my going vegan equals the sum of the probabilities of my going vegan \n",
      "conditioned on all possible cases times their probability of happening:\n",
      "  p (vegan) = p($1B) * p(vegan | $1B) + p(not $1B)* p(vegan | not $1B) = .0000001\n",
      "The overall probability is the weighted sum of all conditional probabilities multiplied \n",
      "by the probability of that condition. And the probability of the condition that you will \n",
      "pay me one billion dollars is 0 (pretty sure that’s a safe assumption). Which means that \n",
      "p(not $1B) is 1, so you get:\n",
      "  p (vegan) = 0*p(vegan | $1B) + 1* p(vegan | not \n",
      "$1B) = .0000001 \n",
      "  p (vegan) = 0*1 + 1*.0000001 = .0000001 \n",
      "Joint Probability, the Chain Rule, and Independence\n",
      "Another concept in probability theory is that of the joint probability, which is just a fancy \n",
      "way of saying “and.” Think back to your SAT days.\n",
      "Here’s the probability that I’ll eat Taco Bell for lunch today:\n",
      "  p (John eats Taco Bell) = .2\n",
      "It’s a once-a-week thing for me. And here’s the probability that I’ll listen to some cheesy \n",
      "electronic music today:\n",
      "  p (John listens to cheese) = .8\n",
      "It’s highly likely.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "81Naïve Bayes and the Incredible Lightness of Being an Idiot\n",
      "So what are the odds that I do both today? That’s called the joint probability, and it’s \n",
      "written as follows:\n",
      "  p (John eats Taco Bell, John listens to cheese)\n",
      "You just separate the two events with a comma. \n",
      "Now, in this case these events are independent . That means that my listening doesn’t \n",
      "aff ect my eating and vice versa. Given this independence, you can then multiply these \n",
      "two probabilities together to get their joint likelihood:\n",
      "  p (John eats Taco Bell, John listens to cheese) = .2 * .8 = .16\n",
      "This is sometimes called the multiplication rule of probability. Note that the joint prob-\n",
      "ability is less than the probability of either occurring, which makes perfect sense. Winning \n",
      "the lottery on the day you get struck by lightning is far less likely to happen than either \n",
      "event alone. \n",
      "One way to see this is through the chain rule of probability, which goes like this:\n",
      "  p (John eats Taco Bell, John listens to cheese) = p(John eats Taco Bell) * \n",
      "p(John listens \n",
      "to cheese | John eats Taco Bell)\n",
      "The joint probability is the probability of one event happening times the probability of \n",
      "the other event happening given that the ﬁ rst event happens. But since these two events \n",
      "are independent, the condition doesn’t matter. I’m going to listen to cheesy techno the \n",
      "same amount regardless of lunch, so:\n",
      "  p (John listens to cheese | John eats Taco Bell) = p(John listens to cheese)\n",
      "That reduces the chain rule setup to simply:\n",
      "  p (John eats Taco Bell, John listens to cheese) = p(John eats Taco Bell) * p(John listens \n",
      "to cheese) = .16\n",
      "What Happens in a Dependent Situation?\n",
      "I’ll introduce another probability, the probability that I listen to Depe che Mode today:\n",
      "  p (John listens to Depeche Mode) = .3\n",
      "There’s a 30 percent chance I’ll rock some DM today. Don’t judge. I now have two events \n",
      "that have dependencies on each other: listening to Depeche Mode and listening to cheesy \n",
      "electronic music. Why? Because Depeche Mode is cheesy techno. That means that:\n",
      "  p (John listens to cheese | John listens to Depeche Mode) = 1\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart82\n",
      "If I listen to Depeche Mode today then there’s a 100 percent chance I’m listening to \n",
      "cheesy techno. It’s a tautology. Since Depeche Mode is cheesy, the probably that I’m listen-\n",
      "ing to cheesy techno given that I’m listening to Depeche Mode must be 1.\n",
      "And that means that when I want to calculate their joint probability, I’m not just going \n",
      "to get the product of the two probabilities. Using the chain rule:\n",
      "  p (John listens to cheese, John listens to DM) = p(John listens to Depeche Mode) * p(John \n",
      "listens to cheese | John listens to Depeche Mode) = .3 * 1 = .3\n",
      "Bayes Rule\n",
      "Since I’ve deﬁ ned Depeche Mode as cheesy techno, the probability of my listening to cheesy \n",
      "techno given I listen to Depeche Model is 1. But what about the other way around? You \n",
      "don’t yet have a probability for this statement:\n",
      "  p (John listens to Depeche Mode | John listens to cheese)\n",
      "After all, there are other techno groups out there. Kraftwerk anyone? The new Daft \n",
      "Punk album, maybe?\n",
      "Well, a kindly gentleman named Bayes came up with this rule: \n",
      "  p (cheese) * p(DM | cheese) = p(DM) * p(cheese | DM)\n",
      "This rule allows you to relate the probability of a conditional event to the probability \n",
      "when the event and condition are swapped.\n",
      "Rearranging the terms then, we can isolate the probability we do not know (the prob-\n",
      "ability that I’m listening to Depeche Mode given that I’m listening to cheesy music):\n",
      "p(DM | cheese) = p(DM) * p(cheese | DM) / p(cheese)\n",
      "The preceding formula is the way you’ll encounter Bayes Rule most often. It’s merely a \n",
      "way of ﬂ ipping around conditional probabilities. When you know a conditional probability \n",
      "going only one way, yet you know the total probabilities of the event and the condition, \n",
      "you can ﬂ ip everything around.\n",
      "Plugging in values, you’ll get:\n",
      "p(DM | cheese) = .3 * 1 / .8 = .375\n",
      "I typically have a 30 percent chance of listening to Depeche Mode on any day. However, \n",
      "if I know I’m going to listen to some kind of cheesy techno today, the odds of listening to \n",
      "Depeche Mode jump up to 37.5 percent given that knowledge. Cool!\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "83Naïve Bayes and the Incredible Lightness of Being an Idiot\n",
      "Using Bayes Rule to Create an AI Model\n",
      "All right, it’s time to leave my music taste behind and think on this Mandrill tweet prob-\n",
      "lem. You’re going to treat each tweet as a bag of words, meaning you’ll break each tweet \n",
      "up into words (often called tokens) at spaces and punctuation. There are two classes of \n",
      "tweets—called app for the Mandrill.com tweets and other for everything else. \n",
      "You care about these two probabilities:\n",
      "  p (app | word\n",
      "1, word2, word3, …)\n",
      "  p (other | word1, word2, word3, …)\n",
      "These are the probabilities of a tweet being either about the app or about something \n",
      "else given that we see the words “word1,” “word2,” “word3,” etc.\n",
      "The standard implementation of a naïve Bayes model classiﬁ es a new document based \n",
      "on which of these two classes is most likely given the words. In other words, if:\n",
      "  p (app | word1, word2, word3, …) > p(other | word1, word2, word3, …)\n",
      "then you have a tweet about the Mandrill app. \n",
      "This decision rule—which picks the class that’s most likely given the words—is called \n",
      "the maximum a posteriori rule (MAP rule).\n",
      "But how do you calculate these two probabilities? The ﬁ rst step is to use the Bayes Rule \n",
      "on them. Using the Bayes Rule, you can rewrite the conditional app probability as follows:\n",
      "  p (app | word1, word2, …) = p(app) p(word1, word2, …| app) / p(word1, word2, …)\n",
      "Similarly, you get:\n",
      "  p (other | word1, word2, …) = p(other) p(word1, word2, …| other) / p(word1, word2, …)\n",
      "But note that both of these calculations have the same denominator:\n",
      "p(word1, word2, …)\n",
      "This is just the probability of getting these words in a document in general. Because this \n",
      "quantity doesn’t change based on the class, you can drop it out of the MAP comparison, \n",
      "meaning you care only about which of these two values is larger:\n",
      "p(app) p(word\n",
      "1, word2, …| app)\n",
      "p(other) p(word1, word2, …| other)\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart84\n",
      "But how do you calculate the probability of getting a bag of words given that it’s an app \n",
      "tweet or an other tweet?\n",
      "This is where things get idiotic!\n",
      "Assume that the probabilities of these words being in the document are independent \n",
      "from one another. Then you get:\n",
      "  p (app) p(word\n",
      "1, word2, …| app) = p(app) p(word1| app) p(word2| app) p(word3| app)…\n",
      "  p (other ) p(word1, word2, …| other ) = p(other ) p(word1| other ) p(word2| other ) \n",
      "p(word3| other)…\n",
      "The independence assumption allows you to break that joint conditional probability of \n",
      "the bag of words given the class into probabilities of single words given the class.\n",
      "And why is this idiotic? Because words are not independent of one another in \n",
      "a document!\n",
      "If you were classifying spam e-mails and you had two words in the document,—\n",
      "“erectile” and “dysfunction”—this would assume:\n",
      "  p (erectile, dysfunction | spam) = p(erectile | spam) p(dysfunction | spam)\n",
      "But this is idiotic, isn’t it? It’s naïve, because if I told you that I got a spam e-mail \n",
      "with the word “dysfunction” in it and I asked you to guess what the previous word was, \n",
      "you’d almost certainly guess “erectile.” There’s a dependency there that’s being blatantly \n",
      "ignored.\n",
      "The funny thing is though that for many practical applications, somehow this idiocy \n",
      "doesn’t matter. That’s because the MAP rule doesn’t really care that you calculated your \n",
      "class probabilities correctly; it just cares about which incorrectly calculated probability \n",
      "is larger. And by assuming independence of words, you’re injecting all sorts of error into \n",
      "that calculation, but at least this sloppiness is across the board. The comparisons used in \n",
      "the MAP rule tend to come out in the same direction they would have had you applied all \n",
      "sorts of fancier linguistic understanding to the model.\n",
      "High-Level Class Probabilities Are Often Assumed to Be Equal\n",
      "So then to recap, in the case of the Mandrill app, you want to classify tweets based on \n",
      "which of these two values is higher:\n",
      "  p (app) p(word\n",
      "1| app) p(word2| app) p(word3| app)…\n",
      "  p (other) p(word1| other) p(word2| other) p(word3| other)…\n",
      "So what are p(app) and p(other)? You can log on to Twitter and see that p(app) is really \n",
      "about 20 percent. Eighty percent of tweets using the word mandrill are about other stuff .\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "85Naïve Bayes and the Incredible Lightness of Being an Idiot\n",
      "Although this is true now, it may shift over time, and I’d prefer to get too many tweets \n",
      "classiﬁ ed as app tweets (false positives) rather than ﬁ  lter some relevant ones out (false \n",
      "negatives), so I’m going to assume my odds are 50/50. You’ll see this assumption con-\n",
      "stantly in naïve Bayes classiﬁ cation in the real world, especially in spam ﬁ  ltering where \n",
      "the percentage of e-mail that’s spam shifts over time and may be hard to measure globally.\n",
      "But if you assume both p(app) and p(other) are 50 percent, then when comparing the \n",
      "two values using the MAP decision rule, you might as well just drop them out. Thus, you \n",
      "can classify a tweet as app-related if:\n",
      "  p (word\n",
      "1| app) p(word2| app) … >= p(word1| other) p(word2| other) …\n",
      "But how do you calculate the probability of a word given the class it’s in? For example, \n",
      "contemplate the following probability:\n",
      "  p (“spark” | app)\n",
      "To ﬁ gure this out, you can pull a set of training tweets in for the app, tokenize them \n",
      "into words, count up the words, and ﬁ gure out what percentage of those words are “spark.” \n",
      "It’ll probably be 0 percent since most “spark” mandrill tweets are about video games.\n",
      "Pause a moment and contemplate this point. To build a naïve Bayes classiﬁ cation model, \n",
      "you need only track frequencies of historic app-related and non-app-related words. Well \n",
      "that’s not hard!\n",
      "A Couple More Odds and Ends\n",
      "Now, before you get started in Excel, you have to address two practical hurdles in imple-\n",
      "menting naïve Bayes in Excel or in any programming language: \n",
      "• Rare words\n",
      "• Floating-point underﬂ ow\n",
      "Dealing with Rare Words\n",
      "The ﬁ rst is the problem of rare words. What if you get a tweet that you’re supposed to \n",
      "classify, but there’s the word “Tubal-cain” in it? Based on past data in the training set, \n",
      "perhaps one or both classes have never seen this word. A place where this happens a lot \n",
      "on Twitter is with shortened URLs, since each new tweet of a URL might have a diff erent, \n",
      "never-seen-before encoding. \n",
      "You can assume:\n",
      "  p (“Tubal-cain” | app) = 0\n",
      "But then you’d get:\n",
      "  p (“Tubal-cain” | app) p(word\n",
      "2| other) p(word3| other)… = 0\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart86\n",
      "Tubal-cain eff ectively “zeros out” the entire probability calculation. \n",
      "Instead, assume that you’ve seen “Tubal-cain” once before. You can do this for all rare \n",
      "words.\n",
      "But wait—that’s unfair to the words you actually  have seen once. Okay, so add 1 to \n",
      "them, too. \n",
      "But that’s unfair to the words you’ve actually seen twice. Okay, so add one to every count. \n",
      "This is called additive smoothing, and it’s often used to accommodate heretofore-unseen \n",
      "words in bag of words models.\n",
      "Dealing with Floating-Point Underﬂ ow\n",
      "Now that you’ve addressed rare words, the second problem you have to face is called \n",
      "ﬂ oating-point underﬂ ow. \n",
      "A lot of these words are rare, so you end up with very small probabilities. In this data, \n",
      "most of the word probabilities will be less than 0.001. And because of the independence \n",
      "assumption, you’ll be multiplying these individual word probabilities together.\n",
      "What if you have a 15-word tweet with probabilities all under 0.001? You’ll end up \n",
      "with a value in the MAP comparison that’s tiny, such as 1x10\n",
      "-45. Now, in truth, Excel can \n",
      "handle a number as small as 1x10-45. It craps out somewhere in the hundreds of 0s after \n",
      "the decimal place. So for classifying tweets, you’d probably be all right. But for longer \n",
      "documents (e.g. e-mails, news articles), tiny numbers can wreak havoc on calculations.\n",
      "Just to be on the safe side, you need to ﬁ  nd a way to not make the MAP evaluation \n",
      "directly:\n",
      "  p (word\n",
      "1| app) p(word2| app) … >= p(word1| other) p(word2| other) …\n",
      "You can solve this problem using the log function (natural log in Excel is available \n",
      "through the LN formula). \n",
      "Here’s a math fun fact for you. Say you have a product:\n",
      "  . 2 * .8\n",
      "If you take the log of it, the following is true:\n",
      "  ln (.2 * .8) = ln(.2) + ln(.8)\n",
      "And when you take the natural log of any value between 0 and 1, instead of getting a \n",
      "tiny decimal, you get a solid negative number. So you can take the natural log of each of \n",
      "the probabilities and sum them to conduct the maximum a posteriori comparison. This \n",
      "gives a value that the computer won’t barf on.\n",
      "If you’re a bit confused, don’t worry. This will become very clear in Excel.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "87Naïve Bayes and the Incredible Lightness of Being an Idiot\n",
      "Let’s Get This Excel Party Started\n",
      "NOTE\n",
      "The Excel workbook used in this chapter, “Mandrill.xlsx,” is available for download \n",
      "at the book’s website at www.wiley.com/go/datasmart.This workbook includes all the \n",
      "initial data if you want to work from that. Or you can just read along using the sheets \n",
      "I’ve already put together in the workbook.\n",
      "In this chapter’s workbook, called Mandrill.xlsx, you have two tabs of input data to start \n",
      "with. One tab, AboutMandrillApp, contains 150 tweets, one per row, pertaining to Mandrill.\n",
      "com. The other tab, AboutOther, contains 150 tweets about other mandrill-related things.\n",
      "I just want to say before you get started—welcome to the world of natural language \n",
      "processing (NLP). Natural language processing concerns itself with chewing on human-\n",
      "written text and spitting out knowledge. And that almost always means prepping that \n",
      "human-written content (like tweets) for computer consumption. It’s time to get prepping.\n",
      "Removing Extraneous Punctuation\n",
      "The primary step in creating a bag of words from a tweet is tokenizing the words wherever \n",
      "there’s a space between them. But before you divide the words wherever there’s whitespace, \n",
      "you must lowercase everything and replace most of the punctuation with spaces since \n",
      "punctuation in tweets isn’t always meaningful. The reason why you lowercase everything \n",
      "is because the words “e-mail” and “E-mail” aren’t meaningfully diff erent.\n",
      "So in cell B2 on the two tweet tabs, add this formula:\n",
      "=LOWER(A2)\n",
      "This will lowercase the ﬁ rst tweet. In C2, strip out any periods. You don’t want to mangle \n",
      "the URLs, so strip out any periods with a space after them using the SUBSTITUTE command:\n",
      "=SUBSTITUTE(B2,\". \",\" \")\n",
      "This formula substitutes the string \". \" for a single space \" \".\n",
      "You can also point cell D2 at cell C2 and replace any colons with a space after them \n",
      "with a single space:\n",
      "=SUBSTITUTE(C2,\": \",\" \")\n",
      "In cells E2 through H2, you should make similar substitutions with the strings \"?\", \n",
      "\"!\", \";\", and \",\":\n",
      "=SUBSTITUTE(D2,\"?\",\" \")\n",
      "=SUBSTITUTE(E2,\"!\",\" \")\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart88\n",
      "=SUBSTITUTE(F2,\";\",\" \")\n",
      "=SUBSTITUTE(G2,\",\",\" \")\n",
      "You don’t need to add a space after the punctuation in the previous four formulas \n",
      "because they don’t appear in URLs (especially in shortened links) that often.\n",
      "Highlight cells B2:H2 on both tabs and double-click the formulas to send them down \n",
      "through row 151. This gives you two tabs like the ones shown in Figure 3-3.\n",
      "Figure 3-3: Prepped tweet data\n",
      "Splitting on Spaces\n",
      "Next, create two new tabs and call them AppTokens and OtherTokens.\n",
      "You need to count how many times each word is used across all tweets in a category. \n",
      "That means you need all the tweets’ words in a single column. It’s safe to assume that \n",
      "each tweet contains no more than 30 words (feel free to expand this to 40 or 50 if you \n",
      "like), so if you’re going to extract one token from a tweet per row, that means you need \n",
      "150 x 30 = 4,500 rows.\n",
      "To start, in these two tabs label A1 as Tweet. \n",
      "Highlight A2:A4501 and Paste Special the tweet values from column H of the initial two \n",
      "tabs. This will give you a list of the processed tweets, as shown in Figure 3-4. Note that \n",
      "because you’re pasting 150 tweets into 4,500 rows, Excel automatically repeats everything \n",
      "for you. Ginchy.\n",
      "That means that if you extract the ﬁ rst word from the ﬁ rst tweet on row 2, that same \n",
      "tweet is repeated to extract the second word from it on row 152, then the third word on \n",
      "row 302, and so on.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "89Naïve Bayes and the Incredible Lightness of Being an Idiot\n",
      "Figure 3-4:  The initial AppTokens sheet\n",
      "In column B, you need to indicate the position of each successive space between words \n",
      "in a tweet. You can label this column something like Space Position. Because there is no \n",
      "space at the beginning of each tweet, begin by placing a 0 in A2:A151 to indicate that \n",
      "words begin at the ﬁ rst character of each tweet. \n",
      "Beginning at B152 when the tweets repeat for the ﬁ rst time, you can calculate the next \n",
      "space as follows:\n",
      "=FIND(\" \",A152,B2+1)\n",
      "The FIND formula will search the tweet for the next empty space beginning with the \n",
      "character after the previous space referenced in cell B2, which is 150 cells above. See \n",
      "Figure 3-5.\n",
      "Figure 3-5:  The space position of the second word in the tweet on row 152\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart90\n",
      "However, note that this formula will give an error once you run out of spaces if there \n",
      "are fewer words than the 30 you’ve planned for, so to accommodate this, you need \n",
      "to wrap the formula in an \n",
      "IFERROR statement and just return one plus the tweet length to \n",
      "indicate the position after the last word:\n",
      "=IFERROR(FIND(\" \",A152,B2+1),LEN(A152)+1)\n",
      "You can then double-click this formula to send it down the sheet through A4501. This \n",
      "will produce the sheet shown in Figure 3-6.\n",
      "Figure 3-6:  Positions of each space in the tweet\n",
      "Next in column C, you can begin to extract single tokens from the tweets. Label column \n",
      "C as Token, and beginning in cell C2, you can pull the appropriate word from the tweet \n",
      "using the MID function. MID takes in a string of text, a start position, and the number of \n",
      "characters to yank. So in C2, your text is in A2, the starting position is one past the last \n",
      "space (B2 + 1), and the length is the diff erence between the subsequent space position in \n",
      "cell B152 and the current space position in B2 minus 1 (keeping in mind that identical \n",
      "tweets are off set by 150 rows). \n",
      "This yields the following formula:\n",
      "MID(A2,B2+1,B152-B2-1) \n",
      "Now, once again, you can get into some tight spots at the end of the string when you run \n",
      "out of words. So, if there’s an error, turn the token into \".\" so it will be easy to ignore later:\n",
      "=IFERROR(MID(A2,B2+1,B152-B2-1),\".\")\n",
      "You can then double-click this formula and send it down the sheet to tokenize every \n",
      "tweet, as shown in Figure 3-7.\n",
      "Add a Length column to column D, and in cell D2 take the length of the token in C2 as:\n",
      "=LEN(C2)\n",
      "You can double-click this to send it down the sheet. This value allows you to ﬁ nd and \n",
      "delete any token three characters or less, which tend overall to be meaningless.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "91Naïve Bayes and the Incredible Lightness of Being an Idiot\n",
      "Figure 3-7: Every tweet token\n",
      "NOTE\n",
      "Typically in these kind of natural language processing tasks, rather than drop all the \n",
      "short words, a list of stop words for the particular language (English in this case) would \n",
      "be removed. Stop words are words which have very little lexical content, which is like \n",
      "nutritional content, for bag of words models. \n",
      "For instance, “because” or “instead” might be stop words, because they’re common \n",
      "and they don’t really do much to distinguish one type of document from another. The \n",
      "most common stop words in English do happen to be short, such as “a,” “and,” “the,” \n",
      "etc., which is why in this chapter you’ll take the easier, yet more Draconian, route of \n",
      "removing short words from tweets only.\n",
      "If you follow these steps, you’ll have the AppTokens sheet shown in Figure 3-8 (the \n",
      "OtherTokens sheet is identical except for the tweets pasted in column A).\n",
      "Figure 3-8:  App tokens with their respective lengths\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart92\n",
      "Counting Tokens and Calculating Probabilities\n",
      "Now that you’ve tokenized your tweets, you’re ready to calculate the conditional prob-\n",
      "ability of a token, p(token | class).\n",
      "To do so, you need to determine how many times each token is used. Start with the \n",
      "AppTokens tab by selecting the token and length range C1:D4501 and then inserting the \n",
      "data into a PivotTable. Rename the created pivot table tab AppTokensProbability.\n",
      "In the PivotTable Builder, ﬁ lter on token length, make the tokens the row labels, and in \n",
      "the values box set the value to be a count of each token. This gives you the Builder setup \n",
      "shown in Figure 3-9.\n",
      "In the actual pivot, drop down the length ﬁ  lter and uncheck tokens of length 0, 1, 2, \n",
      "or 3 from being used. (On Windows you have to instruct Excel to Select Multiple Items \n",
      "in the drop-down.) This is also pictured in Figure 3-9.\n",
      "Figure 3-9: PivotTable Builder setup for token counting\n",
      "You now have only the longer tokens from each tweet, all counted up.\n",
      "You can now tack on the probabilities to each token, but before you run the numbers, apply \n",
      "the additive smoothing concept discussed earlier in the chapter by adding one to each token.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "93Naïve Bayes and the Incredible Lightness of Being an Idiot\n",
      "Label column C Add One To Everything, and set C5 = B5+1 (C4 = B4+1 on Windows, \n",
      "where Excel builds pivot tables one row higher just to annoy this book). You can double-\n",
      "click the formula to send it down the page.\n",
      "Since you’ve added one to everything, you’ll also need a new grand total token count. \n",
      "So at the bottom of the table (row 828 in the AppTokensProbability tab), set the cell to \n",
      "sum the counts above it. Once again, note that if you’re on Windows everything is one \n",
      "row higher (C4:C826 for the summation range):\n",
      "=SUM(C5:C827)\n",
      "In column D, you can calculate the probability of each token as its count in column C \n",
      "divided by the total token count. Label column D as P(Token|App). The probability of the \n",
      "ﬁ rst token in D5 (D4 on Windows) is calculated as:\n",
      "=C5/C$828\n",
      "Note the absolute reference to the token total count. This allows you to double-click \n",
      "the formula and send it down column D. Then in column E (call it LN(P)), you can take \n",
      "the natural log of the probability in D5 as follows:\n",
      "=LN(D5)\n",
      "Sending this down the sheet, you now have the values you need for the MAP rule. See \n",
      "Figure 3-10.\n",
      "Figure 3-10:  The logged probabilities for the app tokens\n",
      "Also, create an identical tab using the non-app tokens called OtherTokensProbabilies.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart94\n",
      "And We Have a Model! Let’s Use It\n",
      "Unlike with a regression model (which you’ll encounter in Chapter 6), there’s no optimi-\n",
      "zation step here. No Solver, no model ﬁ tting. A naïve Bayes model is nothing more than \n",
      "these two conditional probability tables.\n",
      "This is one of the reasons why programmers love this model. There’s no complicated \n",
      "model-ﬁ tting step—they just chunk up some tokens and count them. And you can dump \n",
      "that dictionary of tokens out to disk for later use. It’s terribly easy.\n",
      "Okay, so now that the naïve Bayes model is trained, you can use it. In the TestTweets \n",
      "tab of the workbook, you’ll ﬁ nd 20 tweets, 10 about the app and 10 about other mandrills. \n",
      "You’re going to prep these tweets, tokenize them (you’ll do the tokenizing a bit diff  er-\n",
      "ently this time for kicks), calculate their logged token probabilities for both classes, and \n",
      "determine which class is most likely.\n",
      "To begin then, copy cells B2:H21 from AboutMandrillApp and paste them into D2:J21 of \n",
      "the TestTweets tab in order to prep the tweets. This gives you the sheet shown in Figure 3-11.\n",
      "Figure 3-11: Prepped test tweets\n",
      "Next, create a tab called TestPredictions. In the tab, paste the Number and Class col-\n",
      "umns from TestTweets. Name column C Prediction, which you’ll ﬁ ll in with the predicted \n",
      "class values. Then label column D as Tokens, and in D2:D21, paste the values from column \n",
      "J on the TestTweets tab. This gives you the sheet shown in Figure 3-12.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "95Naïve Bayes and the Incredible Lightness of Being an Idiot\n",
      "Figure 3-12: The TestPredictions tab \n",
      "Unlike when you built the probability tables, you don’t want to combine these tokens \n",
      "across tweets. You want to evaluate each tweet separately, and this makes tokenizing \n",
      "rather simple.\n",
      "To start, highlight the tweets in D2:D21 and choose Text to Columns on the Data tab of \n",
      "the Excel ribbon. In the Convert Text to Columns wizard that pops up, select Delimited \n",
      "and press Next.\n",
      "On the second screen of the wizard, specify Tab and Space as delimiters. You can also \n",
      "choose Treat Consecutive Delimiters As One and make sure that the Text Qualiﬁ er is set \n",
      "to {none}. This gives the setup shown in Figure 3-13.\n",
      "Figure 3-13: The Text to Columns Wizard setup\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart96\n",
      "Press Finish. This chunks up the tweets into columns going all the way out to column \n",
      "AI (see Figure 3-14).\n",
      "Figure 3-14: The tokens from the test tweets \n",
      "Below the tokens starting in column D on row 25, you should look up the app prob-\n",
      "abilities for each token. To do so, you can use the VLOOKUP function (see Chapter 1 for \n",
      "more on VLOOKUP), starting with cell D25:\n",
      "=VLOOKUP(D2,AppTokensProbability!$A$5:$E$827,5,FALSE)\n",
      "The VLOOKUP function takes the corresponding token from D2 and tries to ﬁ  nd it in \n",
      "column A on the AppTokensProbability tab. When it ﬁ  nds the token, the lookup grabs \n",
      "the value from column E.\n",
      "But this isn’t suffi  cient, because you need to deal with the rare words not on the lookup \n",
      "table—these tokens will get an N/A value from the VLOOKUP  as it stands. As discussed \n",
      "earlier, these rare words should get a probability of 1 divided by the total token count in \n",
      "cell B828 on the AppTokensProbability tab.\n",
      "To handle these rare words, you just wrap the \n",
      "VLOOKUP in an ISNA check and slide in \n",
      "the rare word logged probability if needed:\n",
      "IF(ISNA(VLOOKUP(D2,AppTokensProbability!$A$5:$E$827,5,FALSE)),\n",
      "LN(1/AppTokensProbability!$C$828),VLOOKUP(D2,AppTokensProbability!\n",
      "$A$5:$E$827,5,FALSE))\n",
      "The one thing this solution hasn’t addressed yet are the small tokens you want to throw \n",
      "away. Since you’re going to sum these logged probabilities, you can set any small token’s \n",
      "logged probability to zero (this is akin to setting the probability to 1 on both sides, that \n",
      "is, throwing it away).\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "97Naïve Bayes and the Incredible Lightness of Being an Idiot\n",
      "To do this, you just wrap the whole formula in one more IF statement that checks length:\n",
      "=IF(LEN(D2)<=3,0,IF(ISNA(VLOOKUP(D2,AppTokensProbability!\n",
      "$A$5:$E$827,5,FALSE)),LN(1/AppTokensProbability!$C$828),\n",
      "VLOOKUP(D2,AppTokensProbability!$A$5:$E$827,5,FALSE)))\n",
      "Note that absolute references are used on the AppTokensProbability tab so that you \n",
      "can drag this formula around.\n",
      "Since the tweet tokens reach all the way to column AI, you can drag this formula from \n",
      "D25 through AI44 to score each token. This gives the worksheet shown in Figure 3-15.\n",
      "Figure 3-15: App logged probabilities assigned to tokens\n",
      "Starting at cell D48, you can use the same formula as in D25 except that it should ref-\n",
      "erence the OtherTokensProbability tab, and the range on the probability tab changes to \n",
      "$A$5:$E$810 in the \n",
      "VLOOKUP with the total token count being on $C$811.\n",
      "This then yields the sheet shown in Figure 3-16.\n",
      "Figure 3-16: Both sets of logged probabilities assigned to the test tweets\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart98\n",
      "In column C, you can sum each row of probabilities, yielding the sheet shown in Figure \n",
      "3-17. For example, C25 is simply:\n",
      "=SUM(D25:AI25)\n",
      "Figure 3-17: Sums of logged conditional token probabilities\n",
      "In cell C2, you can classify this ﬁ rst tweet by simply comparing its scores below in cells \n",
      "C25 and C48 using the following IF statement:\n",
      "=IF(C25>C48,\"APP\",\"OTHER\")\n",
      "Copying this formula down through C21, you get all of the classiﬁ  cations, as shown \n",
      "in Figure 3-18.\n",
      "It gets 19 out of 20 correct! Not bad. If you look at the one tweet that was misclassiﬁ ed, \n",
      "the language is quite vague—the scores are close to tied. \n",
      "And that’s it. Model built, predictions done.\n",
      "Wrapping Up\n",
      "This chapter is super short compared to others in this book. Why? Because naïve Bayes \n",
      "is easy! And that’s why folks love it. Naïve Bayes appears to be working some kind of \n",
      "complex magic when in reality it just relies on the computer to have a good memory of \n",
      "how often each token in the training data showed up in each class.\n",
      "There’s a proverb that goes, “Experience is the father of wisdom and memory the \n",
      "mother.” Nowhere is this truer than with naïve Bayes. Its entire faux-wisdom stems from \n",
      "a combination of past data and storage with a little bit of mathematical duct tape.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "99Naïve Bayes and the Incredible Lightness of Being an Idiot\n",
      "Figure 3-18: Test tweets classiﬁ ed\n",
      "Naïve Bayes lends itself particularly well to simple implementations in code. For exam-\n",
      "ple, here’s a C# implementation:\n",
      "http://msdn.microsoft.com/en-us/magazine/jj891056.aspx\n",
      "Here’s a tiny version someone posted online in Python:\n",
      "http://www.mustapps.com/spamfilter.py\n",
      "Here’s one in Ruby:\n",
      "http://blog.saush.com/2009/02/11/naive-bayesian-classifiers-and-ruby/\n",
      "One of the great things about this type of model is that it works well even when there \n",
      "are a boatload of features (AI model inputs) you’re predicting with (in the case of this data, \n",
      "each word was a feature). But that said, keep in mind that a simple bag of words model \n",
      "does have some drawbacks. Chieﬂ  y, the naïve bit of the model can cause problems. I’ll \n",
      "give you an example.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart100\n",
      "Suppose I build a naïve Bayes classiﬁ er that tries to classify tweets about movies into \n",
      "“thumbs up” and “thumbs down.” When someone says something like:\n",
      "  Michael  Bay’s new movie is a steaming pile of misogynistic garbage, full of explosions \n",
      "and poor acting, signifying nothing. And I, for one, loved the ride!\n",
      "Is the model going to get that correct? You have a bunch of thumbs-down tokens fol-\n",
      "lowed by a thumbs-up token at the end.\n",
      "Since a bag of words model throws away the structure of the text and tokens are \n",
      "assumed to be unordered, this could be a problem. Many naïve Bayes models actually \n",
      "take in phrases rather than individual words as tokens. That helps contextualize words \n",
      "a little bit (and makes the naïve assumption even more ludicrous...but who cares!). You \n",
      "need more training data to make that work because the space of possible n-word phrases \n",
      "is larger than the space of possible words.\n",
      "For something like this movie review you might need a model that actually cares about \n",
      "the position of a word in the review. Which phrase “had the last word?” Incorporating \n",
      "that kind of information immediately does away with this simple bag of words concept.\n",
      "But, hey, this is nitpicking. Naïve Bayes is a straightforward and versatile AI tool. It’s \n",
      "easy to prototype and test with. So you can try out a modeling idea with naïve Bayes, and \n",
      "if it works well enough, you’re good. If it shows promise but is poor, you can move on to \n",
      "something beeﬁ er, like an ensemble model (which is covered in Chapt er 7).\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "4\n",
      "B\n",
      "usiness Week recently published an article about how The Coca-Cola Company uses a \n",
      "large analytics model to determine how to blend raw orange juices to create the perfect \n",
      "not-from-concentrate product.\n",
      "I was discussing this article with some folks, and one of them blurted something like, \n",
      "“But you could never do that with an artiﬁ cial intelligence model!”\n",
      "They were right. You can’t. Because Coca-Cola doesn’t use an artiﬁ cial intelligence model. \n",
      "It uses an optimization model. Huh? What’s the diff erence?\n",
      "An artiﬁ cial intelligence model predicts the result of a process by analyzing its inputs. \n",
      "That’s not what Coca-Cola is doing. Coca-Cola doesn’t need to predict the outcome when \n",
      "they combine juice A with juice B. It needs to decide which combination of juice A, B, C, \n",
      "D, and so on to buy and blend together. Coca-Cola is taking some data and some business \n",
      "rules (their inventory, their demand, their specs, and so on) and deciding how to blend a \n",
      "product. These decisions enable Coca-Cola to blend juices with complementary strengths \n",
      "and weaknesses (maybe one is too sweet and another not sweet enough) to get exactly the \n",
      "right taste for the minimum cost and the maximum proﬁ t.\n",
      "There’s no one outcome that needs predicting. The model gets to change the future. \n",
      "Optimization modeling is analytics’ Arminianism to AI’s Calvinism. Free will, baby! \n",
      "(Sorry, that’s the last historical theological joke in this book.)\n",
      "Companies across industries use optimization models every day to answer questions \n",
      "such as these:\n",
      "• How do I schedule my call center employees to accommodate their vacation requests, \n",
      "balance overtime, and eliminate back-to-back graveyard shifts for any one employee? \n",
      "• Which oil drilling opportunities do I explore to maximize return while keeping \n",
      "risk under control? \n",
      "• When do I place new orders to China, and how do I get them shipped to minimize \n",
      "cost and meet anticipated demand?\n",
      "Optimization \n",
      "Modeling: Because \n",
      "That “Fresh Squeezed” \n",
      "Orange Juice Ain’t \n",
      "Gonna Blend Itself\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart102\n",
      "Optimization, you see, is the practice of mathematically formulating a business problem \n",
      "and then solving that mathematical representation for the best solution. And as noted in \n",
      "Chapter 1, this objective is always a minimization or a maximization where the “best solution” \n",
      "gets to mean whatever you like—lowest cost, highest proﬁ t, or least likely to land you in jail.\n",
      "The most widely used and understood form of mathematical optimization, called linear \n",
      "programming, was developed in secret by the Soviet Union in the late 1930s and gained \n",
      "traction through its extensive use in World War II for transportation planning and resource \n",
      "allocation to minimize cost and risk and maximize damage to the enemy.\n",
      "In this chapter, I’ll go into detail on the linear  part of linear programming. The \n",
      "programming part is a holdover from wartime terminology and has nothing to do with \n",
      "computer programming. Just ignore it. \n",
      "This chapter covers linear, integer, and a bit of non-linear optimization. It focuses on \n",
      "how to formulate business problems in a language in which the computer can solve them. \n",
      "The chapter also discusses at a high level how the industry-standard optimization meth-\n",
      "ods built into Excel’s Solver tool attack these problems and close in on the best solutions.\n",
      "Why Should Data Scientists Know Optimization?\n",
      "If you watch a bunch of James Bond or Mission Impossible movies, you’ll notice that they \n",
      "often have a big action sequence before the opening credits. Nothing draws viewers in \n",
      "like an explosion. \n",
      "The previous chapters on data mining and artiﬁ  cial intelligence were just that—our \n",
      "explosions. But now, like in any good action movie, the plot must advance. In Chapter 2 \n",
      "you used a bit of optimization modeling in ﬁ nding the optimal placement of cluster cen-\n",
      "troids, but you had only been given enough optimization knowledge in Chapter 1 to make \n",
      "that happen. In this chapter, you’re going to dive deep into optimization and get lots of \n",
      "experience with how to formulate models that solve business problems.\n",
      "Artiﬁ cial intelligence is making waves these days for its use at tech companies and \n",
      "start-ups. Optimization, on the other hand, seems to be more of a Fortune 500 business \n",
      "practice. Reengineering your supply chain to reduce the fuel costs of your ﬂ eet is anything \n",
      "but sexy. But optimization, whether it’s trimming the fat or making the most of economies \n",
      "of scale, is fundamental  to eff ectively running a business. \n",
      "And when we talk data science, the truth is that optimization is fundamental there \n",
      "too. As you’ll see in this book, not only is optimization a worthwhile analytic practice to \n",
      "understand on its own, but any data science practitioner worth their salt is going to need \n",
      "to use optimization on the way to implementing other data science techniques. In this \n",
      "book alone, optimization makes a cameo in four other chapters:\n",
      "• Determining optimal cluster centers in k-means clustering as seen in Chapter 2\n",
      "• Maximizing modularity for community detection (Chapter 5)\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "103Optimization Modeling\n",
      "• Training coeffi  cients for an AI model (ﬁ tting a regression in Chapter 6)\n",
      "• Optimally setting smoothing parameters in a forecasting model (Chapter 8)\n",
      "Optimization problems are embedded everywhere in data science, so you need to master \n",
      "solving them before you move on.\n",
      "Starting with a Simple Trade-Off\n",
      "This section begins by discussing economists’ two favorite resources—guns and butter. \n",
      "The year is 1941, and you’ve been airdropped behind enemy lines where you’ve assumed \n",
      "the identity of one Jérémie (or Ameline) Galiendo, a French dairy farmer. \n",
      "Your day job: milking cows and selling sweet, creamy butter to the local populace.\n",
      "Your night job: building and selling machine guns to the French resistance.\n",
      "Your job is complex and fraught with peril. You’ve been cut off  from HQ and are left \n",
      "on your own to run the farm while not getting caught by the Nazis. You only have so \n",
      "much money in the budget to make ends meet while producing guns and butter; you must \n",
      "stay solvent throughout the war. You cannot lose the farm and your cover along with it.\n",
      "After sitting and thinking about your plight, you’ve found a way to characterize your \n",
      "situation in terms of three elements:\n",
      "• The objective: You get $195 dollars (or, uh, francs, although honestly my Excel is \n",
      "set to dollars, and I’m not going to change it for the ﬁ gures here) in revenue from \n",
      "every machine gun you sell to your contact, Pierre. You get $150 for every ton of \n",
      "butter you sell in the market. You need to bring in as much revenue as you can each \n",
      "month to keep the farm going.\n",
      "• The decisions: You need to ﬁ gure out what mix of guns and tons of butter to pro-\n",
      "duce each month to maximize total proﬁ t. \n",
      "• The constraints:  It costs $100 to produce a ton of butter and $150 to produce a \n",
      "machine gun. You have a budget of $1,800 a month to devote to producing new \n",
      "product for sale. You also have to store this stuff  in your 21 cubic meter cellar. Guns \n",
      "take up 0.5 cubic meters once packaged, and a ton of butter takes up 1.5 cubic \n",
      "meters. You can’t store the butter elsewhere or it’ll spoil. You can’t store the guns \n",
      "elsewhere or you’ll get caught by the Nazis.\n",
      "Representing the Problem as a Polytope\n",
      "This problem as it’s been laid out is called a linear program. A linear program is char-\n",
      "acterized as a set of decisions that need to be made to optimize an objective in light of \n",
      "some constraints, where both the constraints and the objective are linear. Linear in this \n",
      "case means that any equation in the problem can only add decisions, subtract decisions, \n",
      "multiply decisions by constants, or some combination of those things.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "104 Data Smart\n",
      "In linear programming, you can’t shove your decisions through any non-linear func-\n",
      "tions, which might include: \n",
      "• Multiplying decisions together (guns times butter cannot be used anywhere)\n",
      "• Sending a decision variable through a kind of logic check, such as an if statement \n",
      "(“If you only store butter in the cellar, then you can give it a little squish and make \n",
      "the capacity 22 cubic meters.”)\n",
      "As you’ll see later in this chapter, restrictions breed creativity. \n",
      "Now, back to the problem. Start by graphing the “feasible region” for this problem. The \n",
      "feasible region is the set of possible solutions. Can you produce no guns and no butter? \n",
      "Sure, that’s feasible. It won’t maximize revenue, but it’s feasible. Can you produce 100 guns \n",
      "and 1,000 tons of butter? Nope, not in the budget, and not in the cellar. Not feasible. \n",
      "Okay, so where do you start graphing? Well, you can’t produce negative quantities of guns or \n",
      "butter. This isn’t theoretical physics. So you’re dealing with the ﬁ rst quadrant of the x-y plane. \n",
      "In terms of the budget, at $150 a pop you can make 12 guns from the $1,800 budget. \n",
      "At $100 a ton, you can make 18 tons of butter.\n",
      "So if you graph the budget constraint as a line on the x-y plane, it’d pass right through \n",
      "12 guns and 18 tons of butter. As shown in Figure 4-1, the feasible region is then a triangle \n",
      "of positive values in which you can produce, at most, 12 guns and 18 tons of butter, or \n",
      "some middling linear combination of the two extremes.\n",
      "30 40\n",
      "Budget Constraint\n",
      "Butter\n",
      "Guns\n",
      "5010\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "20\n",
      "Figure 4-1: The budget constraint makes the feasible region a triangle.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "105Optimization Modeling\n",
      "Now, this triangle is more generally called a polytope. A polytope is nothing more than \n",
      "a geometric shape with ﬂ at sides. You’ve probably heard the term polygon. Well, a polygon \n",
      "is just a polytope in a two-dimensional space. If you’ve got a big fat rock of an engagement \n",
      "ring on your hand…Bam! The diamond is a polytope.\n",
      "All linear programs can have their feasible regions expressed as polytopes. Some algo-\n",
      "rithms, as you’ll see momentarily, exploit this fact to arrive quickly at solutions to linear \n",
      "programming problems.\n",
      "Concerning the problem at hand, it’s time to consider the second constraint—the cellar. \n",
      "If you produced only guns, you’d be able to pack 42 of them in the cellar. On the other \n",
      "hand, you could shove 14 tons of butter in the cellar, maximum. So adding this constraint \n",
      "to the polytope, you shave off  part of the feasible region, as shown in Figure 4-2.\n",
      "30 40\n",
      "Cellar Constraint\n",
      "Butter\n",
      "Guns\n",
      "5010\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "20\n",
      "Budget Constraint\n",
      "Figure 4-2: The cellar constraint cuts a chunk out of the feasible region.\n",
      "Solving by Sliding the Level Set\n",
      "Now that you’ve determined the feasible region, you can begin to ask the question, “Where \n",
      "in that region is the best guns/butter mix?”\n",
      "To answer that question, begin by deﬁ ning something called the level set. A level set for your \n",
      "optimization model is a region in the polytope where all the points give the same revenue.\n",
      "Because your revenue function is $150*Butter + $195*Guns, each level set can be deﬁ ned \n",
      "by the line $150*Butter + $195*Guns = C, where C is a ﬁ xed amount of revenue.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "106 Data Smart\n",
      "Consider the case where C is $1950.  For the level set $150*Butter + $195*Guns = $1950, both \n",
      "the points (0,10) and (13,0) exist in the level set as does any combination of guns and butter \n",
      "where $150*Butter + $195*Guns comes out to $1950. This level set is pictured in Figure 4-3.\n",
      "Using this idea of the level set, you could then think of solving the revenue maximization \n",
      "problem by sliding the level set in the direction of increasing revenue (this is perpendicular \n",
      "to the level set itself) until the last possible moment before you left the feasible region.\n",
      "In Figure 4-3, a level set is pictured with a dashed line, while the arrow and dashed \n",
      "line together represent your objective function. \n",
      "30 40\n",
      "Butter\n",
      "Guns\n",
      "50\n",
      "Level Set\n",
      "10\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "20\n",
      "Figure 4-3: The level set and objective function for the revenue optimization\n",
      "The Simplex Method: Rooting around the Corners\n",
      "To reiterate, if you want to know which feasible points are optimal, you can just slide that \n",
      "level set along the direction of increasing revenue. Right at the border before the level set \n",
      "leaves the polytope, that’s where the best points would be. And here’s what’s cool about that: \n",
      "One of these optimal points at the border will always be a corner of the polytope. \n",
      "Go ahead and conﬁ rm this in Figure 4-3. Lay a pencil on the level set and move it up \n",
      "and right in the direction of increasing revenue. See how it leaves the polytope at a corner?\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "107Optimization Modeling\n",
      "Why is that cool? Well, the polytope in Figure 4-3 has an inﬁ  nite number of feasible \n",
      "solutions. Searching the entire space would be hell. Even the edges have an inﬁ nite num-\n",
      "ber of points! But there are only four corners, and there’s an optimal solution in one of \n",
      "them. Much better odds.\n",
      "It turns out there’s an algorithm that’s been designed to check corners. And even in \n",
      "problems with hundreds of millions of decisions, it’s very eff ective. The algorithm is called \n",
      "the simplex method.\n",
      "Basically, the simplex method starts at a corner of the polytope and slides along edges \n",
      "of the polytope that beneﬁ t the objective. When it hits a corner whose departing edges all \n",
      "are detrimental to the objective, well, then that corner is the best one.\n",
      "In the case of selling guns and butter, assume that you start out at point (0,0). It’s a \n",
      "corner, but it’s got $0 in revenue. Surely you can do better. \n",
      "Well, as seen in Figure 4-3, the bottom edge of the polytope increases revenue as you \n",
      "move right. So sliding along the bottom edge of the polytope in this direction, you hit the \n",
      "corner (14,0)—14 tons of butter and no guns will produce $2,100 dollars (see Figure 4-4).\n",
      "30 40\n",
      "Butter\n",
      "Guns\n",
      "5010\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "20\n",
      "Figure 4-4:  Testing out the all-butter corner\n",
      "From the all-butter corner, you can then slide along the cellar storage edge in the direc-\n",
      "tion of increasing revenue. The next corner you hit is (12.9, 3.4), which gives you revenue \n",
      "just shy of $2,600. All the edges departing the corner lead to worse nodes, so you’re done. \n",
      "As pictured in Figure 4-5, this is the optimum!\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "108 Data Smart\n",
      "30 40\n",
      "Butter\n",
      "Guns\n",
      "5010\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "20\n",
      "Figure 4-5:  Located the optimal corner\n",
      "Working in Excel\n",
      "Before you leave this simple problem behind for something a little tougher, I want to build \n",
      "and solve it in Excel. The ﬁ rst thing you’re going to do in a blank Excel workbook is create \n",
      "spaces for the objective and decision variables, so you’ll label cell B2 as the spot where the \n",
      "total revenue will go and cells B4:C4 as the range where the production decisions will go.\n",
      "Below the objective and decision sections, add the size and price information for guns \n",
      "and butter, the limits on storage space and budget, and each item’s contribution to revenue. \n",
      "The barebones spreadsheet should look like Figure 4-6.\n",
      "Figure 4-6:  Guns and butter data placed, lovingly, in Excel\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "109Optimization Modeling\n",
      "To this data, you need to add several calculations, namely, the constraint calculations \n",
      "and the revenue calculation. In Column E, next to the Limit cells, you can multiply the \n",
      "amounts of guns and butter produced times their respective sizes and prices, and sum \n",
      "them up in a Used column. For example, in E7 you can place how much space is used in \n",
      "the cellar using the formula:\n",
      "=SUMPRODUCT(B4:C4,B7:C7)\n",
      "Note that this formula is linear because only one range, B4:C4, is a decision range. The \n",
      "other range just houses the storage coeffi  cients. You can do the same calculation to gather \n",
      "the total amount spent on guns and butter.\n",
      "For the objective function, you need only take a SUMPRODUCT  of the purchased \n",
      "quantities on row 4 with their revenue on row 9. Placing a feasible solution, such as \n",
      "1 gun, 1 ton of butter, into the decision cells now yields a sheet like that pictured \n",
      "in Figure 4-7.\n",
      "Figure 4-7: Revenue and constraint calculations within the guns and butter problem\n",
      "All right, so how do you now get Excel to set the decision variables to their optimal \n",
      "values? To do this, you use Solver! Start by popping open an empty Solver window (pic-\n",
      "tured in Figure 4-8). For more on adding Solver to Excel see Chapter 1.\n",
      "Just as was mocked up earlier in the chapter, you need to provide Solver with an objec-\n",
      "tive, decisions, and constraints. The objective is the revenue cell created in B1. Also, make \n",
      "sure that you choose the Max radio button since you’re maximizing, not minimizing, \n",
      "revenue. If you were working a problem with cost or risk in the objective function, you \n",
      "would use the Min option instead.\n",
      "The decisions are in B4:C4. After you add them to the “By Changing Variable Cells” \n",
      "section, the Solver window will look like Figure 4-9.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "110 Data Smart\n",
      "Figure 4-8:  The Solver window\n",
      "As for the constraints, there are two you have to add. Start with the cellar storage \n",
      "constraint. Click on the Add button next to the constraints section. Filling out the small \n",
      "dialog box, you need to indicate that cell E7 must be less than or equal to (≤) cell D7 (see \n",
      "Figure 4-10). The amount of space you’re using must be less than the limit.\n",
      "NOTE\n",
      "Note that Solver will add absolute references ($) to everything in your formulation. It \n",
      "doesn’t matter that Solver does this. Honestly, I don’t know why it does because you \n",
      "can’t drag formulas in the context of a Solver model. See Chapter 1 for more on absolute \n",
      "references.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "111Optimization Modeling\n",
      "Figure 4-9: Objective and decisions populated in Solver\n",
      "NOTE\n",
      "Before pressing OK, look at the other constraint types Solver off ers you. Beyond ≤, ≥, \n",
      "and =, there are some funky ones, namely int, bin, and dif. These odd constraints can \n",
      "be placed on cells to make them integers, binary (0 or 1), or “all diff  erent.” Keep the \n",
      "int constraint in mind. You’re going to return to it in a second.\n",
      "Press OK to add the constraint, and then add the budget constraint the same way (E8 ≤ \n",
      "D8). Conﬁ rm also that the Make Unconstrained Variables Non-Negative box is checked to \n",
      "make sure the guns and butter production doesn’t become negative for some odd reason. \n",
      "(Alternatively, you can just add a B4:C4 ≥ 0 constraint, but the check box makes it easy.) \n",
      "Now, from Select a Solving Method, make sure the Simplex LP algorithm is selected. \n",
      "You’re ready to go (see Figure 4-11).\n",
      "Figure 4-10:  The Add Constraint dialog box\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "112 Data Smart\n",
      "USING EXCEL 2007\n",
      "In Excel 2007, there is no Make Unconstrained Variables Non-Negative checkbox. \n",
      "Instead, go to the Options screen and check off  the Assume Non-negative box. Also, \n",
      "there’s no Solving Method selection. Instead, check the Assume Linear Model box in \n",
      "order to activate the simplex algorithm.\n",
      "When you press Solve, Excel quickly ﬁ nds the solution to the problem and pops up a \n",
      "box letting you know. You can either accept the solution found or restore the values in the \n",
      "decision cells (see Figure 4-12). If you press OK to accept the solution, you would see that \n",
      "it’s 3.43 guns and 12.86 tons of butter just like you’d graphed (see Figure 4-13).\n",
      "Figure 4-11: Completed Guns and Butter formulation in Solver\n",
      "Figure 4-12: Solver lets you know when it’s solved the problem.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "113Optimization Modeling\n",
      "Figure 4-13: Optimized guns and butter workbook\n",
      "But You Can’t Make 3.43 Guns\n",
      "Now, your French alter ego is most likely shouting, “Zut alors!” Why? Because you can’t \n",
      "make 43 percent of a gun. And I concede this point. \n",
      "When working with linear programs, the fractional solutions can sometimes be an \n",
      "annoyance. If you were producing guns and butter in the millions, the decimal could be \n",
      "ignored without too much danger of infeasibility or revenue changes. But for this problem, \n",
      "the numbers are small enough to where you really need Solver to make them integers.\n",
      "So, hopping back into the Solver window, add a constraint to force the decision cells \n",
      "B4:C4 to be integers (see Figure 4-14). Click OK to return to the Solver Parameters window.\n",
      "Figure 4-14: Making the guns and butter decisions integers\n",
      "Under the Options section next to Simplex LP, make sure that the Ignore Integer \n",
      "Constraints box is not checked. Press OK.\n",
      "Press Solve and a new solution pops up. At $2,580, you’ve only lost about $17. Not bad! \n",
      "Note that by forcing the decisions to be integers, you can never do better, only worse, \n",
      "because you’re tightening up the possible solutions.\n",
      "Guns have moved up to an even 4 while butter has dropped to 12. And while the budget \n",
      "is completely used up, note that you’ve got a spare 1 cubic meter of storage left in the cellar.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "114 Data Smart\n",
      "So why not just make your decisions integers all the time? Well, sometimes you just \n",
      "don’t need them. For instance, if you’re blending liquids, fractions can be just ﬁ ne. \n",
      "Also, behind the scenes the algorithm Solver uses actually changes when integers are \n",
      "introduced, and performance degrades as a result. The algorithm Solver uses when it \n",
      "encounters the integer or binary constraints is called “Branch and Bound,” and at a high \n",
      "level, it has to run the simplex algorithm over and over again on pieces of your original \n",
      "problem, rooting around for integer-feasible solutions at each step.\n",
      "Let’s Make the Problem Non-Linear for Kicks\n",
      "Even though you’ve added an integer constraint to the decisions, the basic problem at \n",
      "hand is still a linear one.\n",
      "What if you got a $500 bonus from your contact Pierre if you were able to bring him 5 \n",
      "or more guns each month? Well, you can place an \n",
      "IF statement in the revenue function \n",
      "that checks gun production in cell B4:\n",
      "=SUMPRODUCT(B9:C9,B4:C4) + IF(B4>=5,500,0)\n",
      "Once you tack on that IF statement, the objective function becomes non-linear. By \n",
      "graphing the IF statement in Figure 4-15, you can easily see the large non-linear discon-\n",
      "tinuity at 5 guns.\n",
      "$0\n",
      "$500\n",
      "Guns\n",
      "Bonus from Pierre\n",
      "51 01 52 0\n",
      "Figure 4-15: A graph of Pierre’s $500 bonus\n",
      "If you were to open Solver and use Simplex LP again to solve this problem, Excel would \n",
      "politely complain that “the linearity conditions required by this LP Solver are not satis-\n",
      "ﬁ ed” (see Figure 4-16).\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "115Optimization Modeling\n",
      "Figure 4-16:  Excel won’t let you put the decision variables through an IF statement when using \n",
      "Simplex LP.\n",
      "Luckily, Solver provides two other algorithms for resolving this problem, called the \n",
      "“Evolutionary” and “GRG Nonlinear” algorithms. You’ll give the evolutionary approach \n",
      "a shot here, with which you’re already familiar if you’ve worked through Chapter 2. (In \n",
      "Excel 2007, since there is no algorithm selection box, leaving the Assume Linear Model \n",
      "box unchecked will activate a non-linear optimization algorithm.)\n",
      "The way an evolutionary algorithm works is loosely modeled on the way evolution \n",
      "works in biology:\n",
      "• Generate a pool of initial solutions (kind of like a “gene pool”), some feasible and \n",
      "some infeasible.\n",
      "• Each solution has some level of ﬁ tness for survival.\n",
      "• Solutions breed through crossover, meaning components are selected and combined \n",
      "from two or three existing solutions.\n",
      "• Solutions mutate to create new solutions.\n",
      "• Some amount of local search takes place, wherein new solutions are generated within \n",
      "the close vicinity of the current best solution in the population.\n",
      "• Selection occurs when randomly selected poor performing candidate solutions are \n",
      "dropped from the gene pool.\n",
      "Note that this approach does not inherently require that the problem structure be \n",
      "linear, quadratic, or otherwise. To an extent, the problem can be treated like a black box.\n",
      "What that means is that when modeling a linear program in Excel, you’re limited to \n",
      "things like the +/- signs, the SUM and AVERAGE  formulas, and the SUMPRODUCT  formula, \n",
      "where only one range contains decisions. But with the evolutionary solver, your formula\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "116 Data Smart\n",
      "choices expand to just about anything your little heart desires, including these useful \n",
      "non-linear functions:\n",
      "• Logical checks: \n",
      "• IF\n",
      "• COUNTIF\n",
      "• SUMIF\n",
      "• Statistical functions: \n",
      "• MIN\n",
      "• MAX\n",
      "• MEDIAN\n",
      "• LARGE\n",
      "• NORMDIST, BINOMDIST, and so on\n",
      "• Lookup functions: \n",
      "• VLOOKUP\n",
      "• HLOOKUP\n",
      "• OFFSET\n",
      "• MATCH\n",
      "• INDEX\n",
      "Now, I know you’re getting pumped, so let me deﬂ ate the excitement just a little bit. \n",
      "There are a number of problems with the evolutionary solver:\n",
      "• It gives no guarantees that it can ﬁ nd an optimal solution. All it does is keep track \n",
      "of the best solution in a population until time runs out, until the population hasn’t \n",
      "changed enough in a while to merit continuing, or until you kill Solver with the \n",
      "Esc key. You can modify these “stopping criteria” in the evolutionary algorithm \n",
      "options section of Excel Solver.\n",
      "• The evolutionary solver can be quite slow. With complex feasible regions, it often \n",
      "barfs, unable to ﬁ nd even a good starting place.\n",
      "• In order to get the evolutionary algorithm to work well in Excel, you should specify \n",
      "hard bounds for each decision variable. If you have a decision that’s more or less \n",
      "unbounded, you have to pick a really large number to bound it. \n",
      "Concerning this last bullet point, for the guns and butter problem, you should add \n",
      "a constraint that both decisions must stay below 25, giving the new setup pictured in \n",
      "Figure 4-17.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "117Optimization Modeling\n",
      "Figure 4-17: Formulation for the evolutionary solver\n",
      "Press OK then Solve. The algorithm kicks off  and should eventually ﬁ nd a solution of \n",
      "6 guns and 9 tons of butter. So the evolutionary algorithm decided to take Pierre up on his \n",
      "$500 bonus. Nice! But notice that even on such a small problem, this took a while. About \n",
      "30 seconds on my laptop. Think about what that might mean for a production model.\n",
      "There’s a Monster at the End of This Chapter\n",
      "Okay, so that’s an imaginary problem. In the next section, I’m going to demonstrate the \n",
      "powers of Solver on something a bit meatier. You’ll also spend time learning how to model \n",
      "non-linear functions (such as Pierre’s $500 gun bonus) in linear ways, so that you can still \n",
      "use the fast Simplex LP algorithm.\n",
      "If you’re chomping at the bit to move on to another topic, you now know most of what \n",
      "you need to know to succeed in the following chapters. Stick around at least through the \n",
      "If-Then and the “Big M” Constraint section of this chapter in order to learn what you \n",
      "need for Chapter 5 on clustering in graphs. Or, better yet, strap in and work through all \n",
      "the remaining problems here! But be warned, the last two business rules modeled in this \n",
      "chapter are monsters.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "118 Data Smart\n",
      "OTHER TOOLS\n",
      "Huge models don’t ﬁ t very well in Excel. The version of Solver that comes packaged \n",
      "with Excel allows only 100 – 200 decision variables and constraints, depending on \n",
      "the version you’re running. That’s going to limit the size of the problems you can \n",
      "attack in this book. \n",
      "If you want to go larger in Excel, you can buy a bigger version of Solver from Frontline \n",
      "Systems. Even better, if you’re on a Windows box, use OpenSolver just as you’ll do in \n",
      "the later sections of this chapter. OpenSolver, introduced in Chapter 1, calls an open \n",
      "source solver called COIN Branch and Cut (\n",
      "http://www.coin-or.org/) that is excellent \n",
      "for midsized optimization problems. I’ve used OpenSolver on hundreds of thousands \n",
      "of variables eff ectively.\n",
      "Other beeﬁ er linear programming engines include Gurobi and CPLEX. I generally \n",
      "recommend that developers and other people who like their software “in the cloud” \n",
      "check out Gurobi, whereas CPLEX, owned by IBM, is the go-to enterprise solution.\n",
      "Interfacing with these industrial strength tools happens in all sorts of ways. For \n",
      "instance, CPLEX comes packaged with an environment called OPL where you can write \n",
      "models in a specialized language that’s got excellent hooks into spreadsheets. There \n",
      "are plenty of hooks into programming languages for embedding these algorithms and \n",
      "models within production systems.\n",
      "My favorite tool for plugging into the heavy-duty solvers like CPLEX and Gurobi is \n",
      "called AIMMS (\n",
      "www.AIMMS.com). The software lets you build out optimization models \n",
      "and then slap a user interface on them without having to write code. Also, the software \n",
      "can talk to spreadsheets and databases. \n",
      "For the rest of this book, you’re going to stick with Excel and Solver, but just know \n",
      "that there are cutting-edge modeling environments out there for solving bigger problems, \n",
      "should your needs grow beyond what Excel can handle.\n",
      "Fresh from the Grove to Your Glass...with a Pit Stop \n",
      "through a Blending Model\n",
      "NOTE\n",
      "The Excel workbook used in this chapter, “OrangeJuiceBlending.xlsx,” is available \n",
      "for download at the book’s website at www.wiley.com/go/datasmart .This workbook \n",
      "includes all the initial data if you want to work from that. Or you can just read along \n",
      "using the sheets I’ve already put together in the workbook.”\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "119Optimization Modeling\n",
      "When you were a child, perhaps there came that day when someone explained to you that \n",
      "Santa Claus didn’t exist, outside of men with bad rosacea dressed up at the mall.\n",
      "Well, today I’m going to shatter another belief: your not-from-concentrate premium \n",
      "orange juice was not hand squeezed. In fact, the pulp in it is probably from diff erent oranges \n",
      "than the juice, and the juice has been pulled from diff  erent vats and blended according \n",
      "to mathematical models to ensure that each carafe you drink tastes the same as the last.\n",
      "Consistent taste in OJ year round isn’t something that just anyone can pull off . Oranges \n",
      "aren’t in season in Florida year round. And at diff erent times of the year, diff erent orange \n",
      "varietals are ripe. Pull fruit too early and it tastes “green.” Get fruit from another country \n",
      "that’s in season instead, and the juice might be another color. Or sweeter. Consumers \n",
      "demand consistency. That might be easy with Sunny D, but how do you get that out of a \n",
      "bunch of vats of freshly squeezed, very chilled orange juice?\n",
      "You Use a Blending Model\n",
      "On the hit TV show Downton Abbey, the wealthy Lord Grantham invests all his family’s \n",
      "money in a single railroad venture. It’s risky. And he loses big. Apparently in the early \n",
      "1900s, diversiﬁ cation was not a popular concept. \n",
      "By averaging the risk and return of an investment portfolio across multiple investments, \n",
      "the odds of you striking it rich probably decrease, but so do the odds of your going broke. \n",
      "This same approach applies to orange juice production today.\n",
      "Juice can be procured from all around the world, from diff  erent oranges in diff erent \n",
      "seasons. Each product has diff  erent specs—some might be a bit more tart, some a bit \n",
      "more astringent, and others might be sickly sweet. By blending this “portfolio” of juices, \n",
      "a single consistent taste can be maintained.\n",
      "That’s the problem you’ll work through in this section. How do you build a blending \n",
      "model that reduces cost while maintaining quality, and what type of wrenches might get \n",
      "thrown into the works that would need to get mathematically formulated along the way?\n",
      "Let’s Start with Some Specs\n",
      "Let’s say you’re an analyst working at JuiceLand and your boss, Mr. Juice R. Landingsly \n",
      "III (your company is full of nepotism), has asked you to plan the procurement of juice \n",
      "from your suppliers for January, February, and March of this coming year. Along with this \n",
      "assignment, Mr. Landingsly hands you a sheet of specs from your suppliers containing \n",
      "the country of origin and varietal, the quantity available for purchase over the next three \n",
      "months, and the price and shipping cost per 1,000 gallons.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "120 Data Smart\n",
      "The specs sheet rates the color of the juice on a scale from one to ten and three ﬂ avor \n",
      "components:\n",
      "• Brix/Acid ratio: Brix is a measure of sweetness in the juice, so Brix/Acid ratio is a \n",
      "measure of sweetness to tartness, which in the end, is really what orange juice is \n",
      "all about.\n",
      "• Acid (%): Acid as a percentage of the juice is broken out individually, because at \n",
      "a certain point, it doesn’t really matter how sweet the juice is, it’s still too acidic.\n",
      "• Astringency (1–10 scale):  A measure of the “green” quality of the juice. It’s that \n",
      "bitter, unripe, planty ﬂ avor that can creep in. This scale is assessed by a panel of \n",
      "tasters at each juicing facility on a scale of 1–10.\n",
      "All of these speciﬁ cations are represented in the speciﬁ  cations spreadsheet pictured \n",
      "in Figure 4-18.\n",
      "Figure 4-18: The specs sheet for raw orange juice procurement\n",
      "Whatever juice you choose to buy will be shipped to your blending facility in large, \n",
      "aseptic chilled tanks, either by cargo ship or rail. That’s why there isn’t a shipping cost \n",
      "for the Florida Valencia oranges—the blending facility is located in your Florida grove \n",
      "(where, back in the good old days, you grew all the oranges you needed).\n",
      "Look over the specs pictured in Figure 4-18. What can you say about them? The juice \n",
      "is coming from an international selection of varietals and localities.\n",
      "Some juice, such as that from Mexico, is cheap but a bit off . In Mexico’s case, the astrin-\n",
      "gency is very high. In other cases, such as the Sunstar oranges from Texas, the juice is \n",
      "sweeter and less astringent, but the cost is higher.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "121Optimization Modeling\n",
      "Which juice you buy for the next three months depends on some considerations:\n",
      "• If you’re minimizing cost, can you buy whatever you want?\n",
      "• How much juice do you need?\n",
      "• What are the ﬂ avor and color bounds for each batch?\n",
      "Coming Back to Consistency\n",
      "Through taste tests and numerous customer interviews, JuiceLand has determined what \n",
      "their orange juice should taste and look like. Any deviation outside the allowable range \n",
      "of these specs and customers are more likely to label the juice as generic, cheap, or even \n",
      "worse, from concentrate. Eek.\n",
      "Mr. Landingsly III lays out the requirements for you:\n",
      "• He wants the lowest cost purchase plan for January, February, and March that \n",
      "meets a projected demand of 600,000 gallons of juice in January and February and \n",
      "700,000 gallons in March.\n",
      "• JuiceLand has entered an agreement with the state of Florida which provides the \n",
      "company tax incentives so long as the company buys at least 40 percent of its juice \n",
      "each month from Florida Valencia growers. Under no circumstances are you to \n",
      "violate this agreement.\n",
      "• The Brix/Acid ratio (BAR) must stay between 11.5 and 12.5 in each month’s blend.\n",
      "• The acid level must remain between 0.75 and 1 percent.\n",
      "• The astringency level must stay at 4 or lower.\n",
      "• Color must remain between 4.5 and 5.5. Not too watery, not too dark.\n",
      "Real quickly shove those requirements into an outline of an LP formulation:\n",
      "• Objective: Minimize procurement costs.\n",
      "• Decisions: Amount of each juice to buy each month\n",
      "• Constraints:\n",
      "• Demand\n",
      "• Supply\n",
      "• Florida Valencia requirement\n",
      "• Flavor\n",
      "• Color\n",
      "Putting the Data into Excel\n",
      "To model the problem in Excel, the ﬁ rst thing you need to do is create a new tab to house \n",
      "the formulation. Call it Optimization Model.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "122 Data Smart\n",
      "In cell A2, under the label Total Cost, put a placeholder for the objective. \n",
      "Below that, in cell A5, paste everything from the Specs tab, but insert four columns \n",
      "between the Region and Qty Available columns to make way for the decision variables as \n",
      "well as their totals by row. \n",
      "The ﬁ rst three columns will be labeled January, February, and March, while the fourth \n",
      "will be their sum, labeled Total Ordered. In the Total Ordered column, you need to sum \n",
      "the three cells to the left, so for example in the case of Brazilian Hamlin oranges, cell F6 \n",
      "contains:\n",
      "=SUM(C6:E6)\n",
      "You can drag cell F6 down through F16. Placing some conditional formatting on the \n",
      "range C6:E16, the resulting spreadsheet looks like the one in Figure 4-19.\n",
      "Figure 4-19: Setting up the blending spreadsheet\n",
      "Below the monthly purchase ﬁ  elds, add some ﬁ  elds for monthly procurement and \n",
      "shipping costs. For January, place the monthly procurement cost in cell C17 as follows:\n",
      "=SUMPRODUCT(C6:C16,$L6:$L16)\n",
      "Once again, since only the C column is a decision variable, this calculation is linear. \n",
      "Similarly, you need to add the following calculation to C18 to calculate shipping costs \n",
      "for the month:\n",
      "=SUMPRODUCT(C6:C16,$M6:$M16)\n",
      "Dragging these formulas across columns D and E, you’ll have all of your procurement \n",
      "and shipping costs calculated. You can then set the objective function in cell A2 as the \n",
      "sum of C17:E18. The resulting spreadsheet is pictured in Figure 4-20.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "123Optimization Modeling\n",
      "Figure 4-20: Cost calculations added to the juice blending worksheet\n",
      "Now add the calculations you need to satisfy the demand and Florida Valencia con-\n",
      "straints. On row 20, sum the total quantity of juice procured on that month, and on row 21, \n",
      "place the required levels of 600, 600, and 700, respectively into columns C through E.\n",
      "As for total Valencia ordered from Florida, map C8:E8 to cells C23:E23 and place the \n",
      "required 40 percent of total demand (240, 240, 280) below the values.\n",
      "This yields the spreadsheet shown in Figure 4-21.\n",
      "Now that you’ve covered the objective function, the decision variables, and the supply, \n",
      "demand, and Valencia calculations, all you have left are the taste and color calculations \n",
      "based on what you order.\n",
      "Let’s tackle Brix/Acid ratio ﬁ rst. In cell B27, put the minimum BAR of the blend, which \n",
      "is 11.5. Then in cell C27, you can use the \n",
      "SUMPRODUCT of the January orders (column C) \n",
      "with their Brix/Acid specs in column H, divided by total demand, to get the average Brix/\n",
      "Acid ratio. \n",
      "WARNING\n",
      "Do not divide through by total ordered, as that’s a function of your decision variables! \n",
      "Decisions divided by decisions are highly non-linear.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "124 Data Smart\n",
      "Figure 4-21: Demand and Valencia calculations added\n",
      "Just remember, you’ll be setting the total ordered amount equal to projected demand \n",
      "as a constraint, so there’s no reason not to just divide through by demand when getting \n",
      "the average BAR of the blend. Thus, cell C27 looks as follows:\n",
      "=SUMPRODUCT(C$6:C$16,$H$6:$H$16)/C$21\n",
      "You can drag that formula to the right through column E. In column F, you’ll ﬁ  nish \n",
      "off  the row by typing in the maximum BAR of 12.5. You can then repeat these steps to \n",
      "set up calculations for acid, astringency, and color in rows 28 through 30. The resulting \n",
      "spreadsheet is pictured in Figure 4-22.\n",
      "Setting Up the Problem in Solver\n",
      "All right, so you have all the data and calculations you need to set up the blending prob-\n",
      "lem in Solver. The ﬁ rst thing you need to specify in Solver is the total cost function in A2 \n",
      "that you’re minimizing.\n",
      "The decision variables are the monthly purchase amounts of each varietal housed in \n",
      "the cell range C6:E16. Once again, these decisions can’t be negative, so make sure the\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "125Optimization Modeling\n",
      "Make Unconstrained Variables Non-Negative box is checked (Assume Linear Model is \n",
      "checked in Excel 2007).\n",
      "Figure 4-22: Adding taste and color constraints to the worksheet\n",
      "When it comes to adding constraints, this problem really deviates from the guns and \n",
      "butter example. There are a lot of them.\n",
      "The ﬁ rst constraint is that the orders on row 20 must equal demand on row 21 for each \n",
      "month. Similarly, the Florida Valencia orders on row 23 should be greater than or equal \n",
      "to the required amount on row 24. Also, the total quantity ordered from each geography, \n",
      "calculated in F6:F16, should be less than or equal to what’s available in G6:G16.\n",
      "With supply and demand constraints added, you need to add the taste and color \n",
      "constraints. \n",
      "Now, Excel won’t let you put a constraint on two diff erently sized ranges, so if you enter \n",
      "C27:E30 ≥ B27:B30, it’s not going to understand how to handle that. (I ﬁ nd this terribly \n",
      "irritating.) Instead, you have to add constraints for columns C, D, and E individually. For \n",
      "example, for January orders you have C27:C30 ≥ B27:B30 and C27:C30 ≤ F27:F30. And \n",
      "the same goes for February and March.\n",
      "After you add all those constraints, make sure that Simplex LP is the chosen solving \n",
      "method. The ﬁ nal formulation should look like Figure 4-23.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "126 Data Smart\n",
      "Figure 4-23: The populated Solver dialog for the blending problem\n",
      "Solving, you get an optimal cost of $1.23 million dollars in procurement costs (see \n",
      "Figure 4-24). Note how Florida Valencia purchases hug their lower bound. Obviously, \n",
      "these oranges aren’t the best deal, but the model is being forced to make do for tax pur-\n",
      "poses. The second most popular orange is the Verna out of Mexico, which is dirt cheap \n",
      "but otherwise pretty awful. The model balances this bitter, acidic juice with mixtures of \n",
      "Belladonna, Biondo Commune, and Gardner, which are all milder, sweeter, and superior \n",
      "in color. Pretty neat!\n",
      "Lowering Your Standards\n",
      "Excited, you bring your optimal blend plan to your manager, Mr. Landingsly III. You \n",
      "explain how you arrived at your answer, and he eyes it with suspicion. Even though you \n",
      "claim it’s optimal, he wants you to shave an additional 5 percent off  the cost. He explains \n",
      "his seemingly nonsensical position using mostly sports analogies about “playing all four \n",
      "quarters” and “giving 110 percent.”\n",
      "There’s no use arguing against sports analogies. If $1,170,000 is the sweet spot, then so \n",
      "be it. You explain that there’s no way to achieve that within the current quality bounds, \n",
      "and he merely grunts and tells you to “bend reality a bit.”\n",
      "Hmmm…\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "127Optimization Modeling\n",
      "You return to your spreadsheet ﬂ ustered.\n",
      "How do you get the best blend for a cost of $1,170,000?\n",
      "After the heart to heart with Mr. Landingsly, cost is no longer an objective. It’s a con-\n",
      "straint! So what’s the objective? \n",
      "Figure 4-24: Solution to the orange juice-blending problem\n",
      "Your new objective based on the bossman’s grunts appears to be ﬁ nding the solution \n",
      "that degrades quality the least for 1.17 million dollars. And the way to implement that is to \n",
      "stick a decision variable in the model that loosens up the quality constraints.\n",
      "Go ahead and copy the Optimization Model tab into a new sheet, called Relaxed Quality. \n",
      "You don’t have to change a whole lot to make this work.\n",
      "Take a moment and think about how you might change things around to accommo-\n",
      "date the new relaxed quality objective and cost constraint. Don’t peak ahead until your \n",
      "head hurts!\n",
      "All right.\n",
      "The ﬁ rst thing you do is pop $1,117,000 as the cost limit in cell B2 right next to the old \n",
      "objective. Also, copy and paste values of the old minima and maxima for taste and color \n",
      "into columns H and I, respectively. And in column G on rows 27 through 30, add a new \n",
      "decision variable called % Relaxed.\n",
      "Now consider how you might use the Brix/Acid relaxation decision in cell G27 to relax \n",
      "the lower bound of 11.5. Currently, the allowable band of Brix/Acid is 11.5 to 12.5, which\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "128 Data Smart\n",
      "is a width of 1. So a 10 percent broadening at the bottom of the constraint would make \n",
      "the minimum 11.4.\n",
      "Following this approach, replace the minimum in B27 with this formula: \n",
      "=H27-G27*(I27-H27)\n",
      "This takes the old minimum, now in H27, and subtracts from it the percent relaxation \n",
      "times the distance of the old maximum from the old minimum (I27 minus H27). You can \n",
      "copy this formula down through row 30. Similarly, implement the relaxed maximum in \n",
      "column F.\n",
      "For the objective, take the average of the relaxation decisions in G27:G30. Placing this \n",
      "calculation in cell D2, the new sheet now looks like Figure 4-25.\n",
      "Figure 4-25: Relaxed quality model\n",
      "Open Solver and change the objective to minimize the average relaxation of the quality \n",
      "bounds calculated in cell D2. You also need to add G27:G30 to the list of decision variables \n",
      "and set the cost in A2 as less than or equal to the limit in B2. This new formulation is \n",
      "pictured in Figure 4-26.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "129Optimization Modeling\n",
      "To recap then, you’ve transformed your previous cost objective into a constraint with \n",
      "an upper bound. You’ve also transformed your hard constraints on quality into soft con-\n",
      "straints that can be relaxed by altering G27:G30. Your objective in D2 is to minimize the \n",
      "average amount you must degrade quality across your specs. Press Solve.\n",
      "Figure 4-26: Solver implementation of the relaxed quality model\n",
      "Excel ﬁ nds that with an average relaxation of 35 percent on each end of the bounds, a \n",
      "solution can be achieved that meets the cost constraint, as shown in Figure 4-27.\n",
      "Now that you have the model set up, one thing you can do is provide more information \n",
      "to Mr. Landingsly than he asked for. You know that for $1.23 million you get a quality \n",
      "degradation of 0 percent, so why not step down the cost in increments of 20 grand or so \n",
      "and see what quality degradation results? At $1.21 million it’s 5 percent, at $1.19 million \n",
      "it’s 17 percent, and so forth, including 35 percent, 54 percent, 84 percent, and 170 percent. \n",
      "If you try to dip below $1.1 million the model becomes infeasible.\n",
      "Creating a new tab called Frontier, you can paste all these solutions and graph them \n",
      "to illustrate the trade-off  between cost and quality (see Figure 4-28). To insert a graph \n",
      "like the one pictured in Figure 4-28, simply highlight the two columns of data on the \n",
      "Frontier sheet and insert a Smoothed Line Scatter plot from the Scatter selection in Excel \n",
      "(see Chapter 1 for more on inserting charts).\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "130 Data Smart\n",
      "Figure 4-27: Solution to the relaxed quality model\n",
      "Figure 4-28: Graphing the trade-off between cost and quality\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "131Optimization Modeling\n",
      "Dead Squirrel Removal: The Minimax Formulation\n",
      "If you look at the relaxed quality solution for a cost bound of $1.17 million, there’s a poten-\n",
      "tial problem. Sure, the average relaxation across the taste and color bounds is 35 percent, \n",
      "but for color it’s 80 percent and for Brix/Acid ratio it’s 51 percent. The average hides this \n",
      "variability.\n",
      "What you’d rather do in this situation is minimize the maximum relaxation across the four \n",
      "quality bounds. This problem is commonly called a “minimax” problem because you’re \n",
      "minimizing a maximum, and it’s fun to say really fast. Minimax, minimax, minimax.\n",
      "But how can you do that? If you make your objective function \n",
      "MAX(G27:G30), you’ll be \n",
      "non-linear. You could try that with the evolutionary solver, but it’ll take forever to solve. \n",
      "It turns out there’s a way to model this non-linear problem in a linear way.\n",
      "First, copy the relaxed model to a new tab called Minimax Relaxed Quality. \n",
      "Now, how many of you have had to pick up and get rid of a dead animal? Last summer \n",
      "I had a squirrel die in my blisteringly hot attic here in Atlanta, and the smell knocked \n",
      "many brave men and women to their knees.\n",
      "How did I get rid of that squirrel? \n",
      "I refused to touch it or deal with it directly. \n",
      "Instead, I scooped it from below with a shovel and pressed down on it from above with \n",
      "a broom handle. It was like picking it up with giant salad tongs or chopsticks. Ultimately, \n",
      "this pincer move had the same eff ect as grabbing the squirrel with my bare hands, but it \n",
      "was less gross.\n",
      "You can handle the calculation \n",
      "MAX(G27:G30)  in the same way I handled that dead \n",
      "squirrel. Since you’re no longer computing the average of G27:G30, you can clear out the \n",
      "objective in D2. That’s where you would compute the \n",
      "MAX() function, but you can leave \n",
      "the cell blank. It needs to be lifted up to the max somehow without being touched directly.\n",
      "Here’s how you can do it:\n",
      " 1. Set the objective, D2, to be a decision variable, so that the algorithm can move it as \n",
      "needed. Keep in mind that since you’ve set the model to be a minimization, Simplex \n",
      "is going to try to send this cell down as far as it can go.\n",
      " 2. Set G27:G30 to be less than or equal to D2 using the Add Constraint window. \n",
      "D2 must go in the right side of the Add Constraint dialogue for Excel to allow an \n",
      "unequal number of cells (4 cells in a range on the left side and 1 upper bound on \n",
      "the right side). Unlike elsewhere in this chapter where you couldn’t use two dif-\n",
      "ferent sized ranges in a constraint, this works because Excel has been designed to \n",
      "understand the case where the right side of the constraint is a single cell.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "132 Data Smart\n",
      "Okay, so what did you just do?\n",
      "Well, as the objective function of the model, simplex will try to force D2 down to 0, \n",
      "while the taste and color constraints will force it up to maintain a workable blend. Where \n",
      "will cell D2 land? The lowest it can go will be the maximum of the four relaxation per-\n",
      "centages in G27 through G30.\n",
      "Once the objective strikes that maximum, the only way the Solver can make progress \n",
      "is by forcing that maximum down. Just like with the squirrel, the constraints are the \n",
      "shovel under the squirrel and the minimization objective is the mop handle pressing \n",
      "down. Hence, you get the term “minimax.” Pretty cool, ain’t it? Or gross...depending on \n",
      "how you feel about dead squirrels.\n",
      "Now that you’ve cleared out the formula in D2, the implementation in Solver (making \n",
      "D2 a variable and adding G27:G30 ≤ D2) looks like Figure 4-29.\n",
      "Figure 4-29: Solver setup for minimax quality reduction\n",
      "Solving this setup yields a quality reduction of 58.7 percent, which, while greater than \n",
      "the average 34.8 percent from the previous model, is a vast improvement over the worst-\n",
      "case color relaxation of 84 percent.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "133Optimization Modeling\n",
      "If-Then and the “Big M” Constraint\n",
      "Now that you have a feel for vanilla linear modeling, you can add some integers. Mr. \n",
      "Landingsly III eventually signs off   on your original procurement plan, but when you \n",
      "deliver it to the supply chain team, their eyes start twitching uncontrollably.\n",
      "They refuse to procure juice in any given month from more than four suppliers. Too \n",
      "much paperwork, apparently.\n",
      "Okay, so how do you handle this within the model? \n",
      "Take a minute and think about what model modiﬁ  cations might be required before \n",
      "moving on.\n",
      "Start by copying the original Optimization Model sheet to a new tab called Optimization \n",
      "Model (Limit 4).\n",
      "Now, regardless of how much juice you buy from a supplier, whether it’s 1,000 gallons \n",
      "or 1,000,000 gallons, that counts as an order from one supplier. In other words, you need \n",
      "to ﬁ nd a way to ﬂ ick a switch the moment you order a drop of juice from a supplier.\n",
      "In integer programming, a “switch” is a binary decision variable, which is merely a cell \n",
      "that Solver can set to 0 or 1 only.\n",
      "So what you want to do is deﬁ ne a range the same size as your order variables only it’ll \n",
      "hold 0s and 1s, where a 1 is set when an order gets placed.\n",
      "You can place these variables in range C34:E44. Now, assuming they’re going to be \n",
      "set to 1 when you place an order from the supplier, you can sum up each column in row \n",
      "45 and make sure the sum is less than the limit of 4, which you can toss in row 46. The \n",
      "resulting spreadsheet is pictured in Figure 4-30.\n",
      "Here’s the tricky part though. You can’t use an \n",
      "IF formula that sets the indicator to 1 \n",
      "when the order quantity above is nonzero. That would be non-linear, which would force \n",
      "you to use the much slower evolutionary algorithm. For truly large problems with if-then \n",
      "constraints, the slower non-linear algorithms become useless. So you’ll need to “turn on” \n",
      "the indicator using linear constraints instead.\n",
      "But say you add a constraint to have the Brazilian Hamlin indicator variable turn on \n",
      "when you place an order by using the constraint C34 ≥ C6. \n",
      "If C34 is supposed to be binary, then that’s going to limit C6 to a max of 1 (that is, \n",
      "1,000 gallons ordered).\n",
      "Thus, you have to model this if-then statement, “if we order, then turn on the binary \n",
      "variable,” using something colloquially called a “Big M” constraint. “Big M” is just a num-\n",
      "ber, a big number, called M. In the case of C34, M should be big enough that you’d never\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "134 Data Smart\n",
      "order more Brazilian Hamlin than M. Well, you’ll never order more juice than is available, \n",
      "right? For Hamlin, the available quantity is 672 thousand gallons. So make that M.\n",
      "Figure 4-30:  Adding indicator variables to the spreadsheet\n",
      "Then you can set a constraint where 672*C34 ≥ C6. When C6 is 0, C34 is allowed to \n",
      "be zero. And when C6 is greater than zero, C34 is forced to ﬂ ip to 1 in order to raise the \n",
      "upper bound from 0 to 672.\n",
      "To implement this in the spreadsheet, you set up a new range of cells in F34:H44 where \n",
      "you’ll multiply the indicators to the left times their respective available quantities in range \n",
      "G6:G16. The result is pictured in Figure 4-31.\n",
      "In Solver, you need to add C34:E44 to the range of decision variables. You also need \n",
      "to make them binary, which you accomplish by putting a \n",
      "bin constraint on the range.\n",
      "To put the “Big M” constraint in eff ect, you set C6:E16 ≤ F34:H44. You can then check \n",
      "the supplier counts and make sure they’re under four by setting C45:E45 ≤ C46:E46. The \n",
      "resulting spreadsheet is pictured in Figure 4-32.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "135Optimization Modeling\n",
      "Figure 4-31: Setting up our “Big M” constraint values\n",
      "Figure 4-32: Initializing Solver\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "136 Data Smart\n",
      "Press Solve. You’ll notice that the problem takes longer to solve with the addition of the \n",
      "binary variables. When using integer and binary variables in your formulation, Solver will \n",
      "display the best “incumbent” solution it ﬁ nds in the status bar. If for some reason Solver \n",
      "is taking too long, you can always press the Escape key and keep the best incumbent it’s \n",
      "found so far.\n",
      "As shown in Figure 4-33, the optimal solution of the model restricted to four suppliers \n",
      "per month is $1.24 million, about $16,000 more than the original optimum. Armed with \n",
      "this plan, you can return to the supply chain team and ask them if their reduced paper-\n",
      "work is worth an extra $16,000. \n",
      "Quantifying the introduction of new business rules and constraints in this way is \n",
      "one of the hallmarks of employing optimization modeling in a business. You can place \n",
      "a dollar ﬁ gure to a business practice and make an informed decision to the question, “Is \n",
      "it worth it?”\n",
      "Figure 4-33: Optimal solution limited to four suppliers per period\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "137Optimization Modeling\n",
      "That’s how “Big M” constraints are set up; you’ll encounter them again in the graph \n",
      "clustering problem in Chapter 5.\n",
      "Multiplying Variables: Cranking Up the Volume to 11\n",
      "OPENSOLVER NEEDED FOR EXCEL 2010 AND EXCEL 2013\n",
      "That last bit was tough, but it was child’s play compared to this next business rule \n",
      "you’re going to model.\n",
      "For this next problem, please keep the worked spreadsheet available for download \n",
      "with you for reference. This is a tough one but worth learning if your business is con-\n",
      "fronted with complex optimization problems. Also, nothing in the book is dependent \n",
      "on you learning this section, so if it gets too hard, just skip ahead. That said, I urge you \n",
      "to dig deep and give it a shot.\n",
      "If you’re working in Excel 2010 or Excel 2013, you’ll want to have OpenSolver \n",
      "installed and loaded (see Chapter 1 for an explanation). If you don’t use OpenSolver to \n",
      "solve the problem in those versions of Excel, you’ll get an error saying the optimization \n",
      "model is too large. To use OpenSolver in this chapter, set up the problem normally as \n",
      "shown in this section, but when it comes time to solve, use OpenSolver’s Solve button \n",
      "on the ribbon.\n",
      "Before you implement the limited supplier plan, you’re informed that the new “acid-\n",
      "reducers” have been hooked up in the blending facility. Using ion exchange with a bed \n",
      "of calcium citrate, the technology is able to neutralize 20 percent of the acid in the juice \n",
      "that’s run through it. This not only reduces acid percent by 20 but also increases the Brix/\n",
      "Acid ratio by 25 percent.\n",
      "But the power and raw materials needed to run the reducer cost $20 per 1,000 gal-\n",
      "lons of juice put through it. Not all orders from suppliers need to be put through the de-\n",
      "acidiﬁ cation process; however, if an order is processed through the ion exchanger, the \n",
      "entire order must be pumped through.\n",
      "Can you create a new optimal plan that tries to use ion exchange to reduce the optimal \n",
      "cost? Think about how you might set this one up. You now have to make a new set of \n",
      "decisions regarding when and when not to reduce the acid. How might those decisions \n",
      "interact with order quantities?\n",
      "Start by copying the Optimization Model (Limit 4) tab to a new tab. Call it Optimization \n",
      "Model Integer Acid.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "138 Data Smart\n",
      "The problem with this business rule is that the natural way to model it is non-linear, \n",
      "and that would force you to use a slow optimization algorithm. You could have a binary \n",
      "variable that you “turn on” when you want to de-acidify an order, but that means that the \n",
      "cost of that de-acidifcation is: \n",
      "De-acid indicator * Amount purchased * $20\n",
      "You can’t multiply two variables together unless you want to switch to using the non-\n",
      "linear solver, but that thing is never gonna ﬁ gure out the complexities of this model. There \n",
      "has to be a better way to do this. Keep this in mind when doing linear programming: There \n",
      "are very few things that cannot be linearized through the judicious use of new variables \n",
      "manipulated by additional constraints and the objective function like a pair of salad tongs.\n",
      "The ﬁ rst thing you’re going to need is a set of new binary variables that get “turned \n",
      "on” when you choose to de-acidify a batch of juice. You can insert a new chunk of them \n",
      "in a rectangle between the Valencia orders and the quality constraints (cells C26:E36).\n",
      "Furthermore, you can’t use the product of \n",
      "De-acid indicator * Amount purchased, \n",
      "so instead you’ll create a new grid of variables below the indicators that you’re going to \n",
      "force to equal this amount without expressly touching them (a la dead squirrel). Insert \n",
      "these empty cells in C38:E48.\n",
      "The spreadsheet now has two empty grids of variables—the indicators and the total \n",
      "amount of juice being fed through acid reduction—as shown in Figure 4-34.\n",
      "Now, if you want to multiply a de-acidiﬁ  cation binary variable times the amount of \n",
      "juice you’ve ordered, what are the values that product can take on? There are a number \n",
      "of distinct possibilities:\n",
      "• If both the indicator and the product purchase amount are 0, their product is 0. \n",
      "• If you order some juice but decide to not reduce the acid, the product is still 0. \n",
      "• If you choose to reduce, the product is merely the amount of juice ordered.\n",
      "In every case, the total possible juice that can be de-acidiﬁ  ed is limited by the de-\n",
      "acidiﬁ cation indicator variable times the total juice available to purchase. If you don’t \n",
      "reduce the acid, this upper bound goes to zero. If you choose to reduce, the upper bound \n",
      "pops up to the max available for purchase. This is a “Big M” constraint just like in the \n",
      "last section.\n",
      "For Brazilian Hamlin then, this “Big M” constraint could be calculated as the indicator \n",
      "in cell C26 times the amount available for purchase, 672,000 gallons, in cell G6. Adding \n",
      "this calculation next to the indicator variables in cell G26, you can copy it to the remain-\n",
      "ing months and varietals.\n",
      "This yields the worksheet shown in Figure 4-35.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "139Optimization Modeling\n",
      "Figure 4-34:  Indicator and amount variables added for the de-acidiﬁ  cation decision\n",
      "On the ﬂ  ip side, the total possible juice that can be de-acidiﬁ  ed is limited by the \n",
      "amount you decide to purchase, given in C6:E16. So now you have two upper bounds on \n",
      "this product:\n",
      "• De-acid indicator * Amount available for purchase\n",
      "• Amount purchased\n",
      "That’s one upper bound per variable in the original non-linear product.\n",
      "But you can’t stop there. If you decide to de-acidify a batch, you need to send the whole \n",
      "batch through. That means you have to add a lower bound to the two upper bounds to \n",
      "help “scoop up” the de-acidiﬁ ed amount in C38:E48.\n",
      "So how about just using the purchase amount as the lower bound? In the case where \n",
      "you decide to de-acidify, that works perfectly. You’ll have a lower bound of the purchase\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "140 Data Smart\n",
      "amount, an upper bound of the purchase amount, and an upper bound of the total amount \n",
      "available for purchase times a de-acidiﬁ cation indicator set to 1. These upper and lower \n",
      "bounds force the amount going through de-acidiﬁ cation to be the whole shipment, which \n",
      "is what you want. \n",
      "Figure 4-35: Calculation added for upper bound on how much juice can be de-acidiﬁ  ed\n",
      "But what if you choose not to de-acidify a batch? Then one of the upper bounds becomes \n",
      "an indicator of 0 times the amount available to purchase, whereas the lower bound is still \n",
      "the amount purchased. In that case, a non-zero purchase amount that’s not de-acidiﬁ ed \n",
      "becomes impossible.\n",
      "Hmmm. \n",
      "So you need a way to “turn off ” this lower bound in the situation where you choose \n",
      "not to de-acidify the juice.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "141Optimization Modeling\n",
      "Instead of making the lower bound the amount you ordered, why not make it the \n",
      "following: \n",
      "Amount purchased - Amount available for purchase * (1 – de-acid \n",
      "indicator)\n",
      "In the case where you choose to de-acidify, this lower bound bounces up to the amount \n",
      "you purchased. In the case where you don’t de-acidify, this value becomes less than or \n",
      "equal to 0. The constraint still exists, but it’s for all intents worthless.\n",
      "It’s a bit janky, I know.\n",
      "Try working it through an example. You buy 40,000 gallons of the Brazilian Hamlin \n",
      "juice. Furthermore, you decide to de-acidify. \n",
      "The upper bounds on the amount you’re de-acidifying are the amount purchased of 40 \n",
      "and the de-acid indicator times the amount available of 672.\n",
      "The lower bound on the amount you’re de-acidifying is 40 – 672 * (1-1) = 40. In other \n",
      "words, you have upper and lower bounds of 40, so you’ve sandwiched the amount you’re \n",
      "de-acidifying right into \n",
      "De-acid indicator * Amount purchased without ever calculat-\n",
      "ing this quantity.\n",
      "If I choose not to de-acidify the Hamlin, the indicator is set to 0. In that case you have \n",
      "upper bounds of 40 and 672*0 = 0. You have a lower bound of 40 – 672 * (1-0) = -632. \n",
      "And since you’ve checked the box making all the variables be non-negative, that means \n",
      "that the amount of Hamlin you’re de-acidifying is sandwiched between 0 and 0. \n",
      "Perfect! \n",
      "All right, so let’s add this lower bound in a grid to the right of the upper bound calcula-\n",
      "tion. In cell K26 you’d type:\n",
      "=C6-$G6*(1-C26)\n",
      "And you can copy that formula to each varietal and month, giving you the spreadsheet \n",
      "in Figure 4-36.\n",
      "Next to the Total Reduced section, subtract that value from the total purchases in \n",
      "C6:E16 to get the remaining Not Reduced quantities of juice. For example, in cell G38, \n",
      "you place:\n",
      "=C6 – C38\n",
      "You can drag this across and down to the remaining cells in the grid (see Figure 4-37).\n",
      "Wrapping up the formulation, you need to alter the cost, Brix/Acid, and Acid % calcula-\n",
      "tions. For cost, you can just add $20 times the sum of the month’s Total Reduced values \n",
      "into the Price cell. For example, January’s Price calculation would become:\n",
      "=SUMPRODUCT(C6:C16,$L6:$L16)+20*SUM(C38:C48)\n",
      "which you can then drag across to February and March.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "142 Data Smart\n",
      "Figure 4-36: Adding in a lower bound on de-acidiﬁ  cation\n",
      "The Brix/Acid and Acid % calculations will now be calculated off  of the split quantities \n",
      "in the Total Reduced and Not Reduced sections of the spreadsheet. Not Reduced values will \n",
      "be put through a \n",
      "SUMPRODUCT with their original specs, whereas the same SUMPRODUCT using \n",
      "the reduced acid juice will be scaled by 1.25 and 0.8, respectively, for BAR and Acid and \n",
      "added to the total in the monthly averages.\n",
      "For example, Brix/Acid for January in C51 can be calculated as:\n",
      "=(SUMPRODUCT(G38:G48,$H6:$H16)+SUMPRODUCT(C38:C48,$H6:$H16)*1.25)/C21\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "143Optimization Modeling\n",
      "Now you need to modify the model in Solver. The objective function remains the same \n",
      "(sum of price and shipping), but the decision variables now include the de-acid indicators \n",
      "and amounts to be reduced located in C26:E36 and C38:E48.\n",
      "As for the constraints, you need to indicate that C26:E36 is \n",
      "bin. Also, C38:C48 is less \n",
      "than or equal to the two upper bounds in C6:E16 and G26:I36. Also, you need a lower \n",
      "bound constraint where C38:E48 is greater than or equal to K26:M36.\n",
      "This all yields the new model pictured in Figure 4-38.\n",
      "Figure 4-37: Adding a “Not Reduced” calculation\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "144 Data Smart\n",
      "Figure 4-38:  Solver formulation for the de-acidiﬁ  cation problem\n",
      "Press Solve and let the Branch and Bound do its thing. You’ll end up with an optimal \n",
      "solution that’s about $4,000 lower than in the previous formulation. Examining the new \n",
      "decision variables, you ﬁ nd that two batches—one from Arizona and one from Texas—are \n",
      "going through the de-acidiﬁ cation process. The lower and upper bounds for those two \n",
      "batches match precisely to force the product of the variables into place (see Figure 4-39).\n",
      "Modeling Risk\n",
      "That last business rule was a toughie, but it illustrates how a modeler can linearize most \n",
      "business problems by adding more constraints and variables. However, no matter how \n",
      "easy or hard the previous problems were, they all had one thing in common—they treat \n",
      "the input data as gospel.\n",
      "This doesn’t always conform to the reality many businesses ﬁ nd themselves in. Parts are \n",
      "not all to spec, shipments don’t always arrive on time, demand doesn’t match the forecast, \n",
      "and so on. In other words, there’s variability and risk in the data.\n",
      "So how do you take that risk and model it within an optimization model?\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "145Optimization Modeling\n",
      "Figure 4-39: Solved de-acidiﬁ  cation model\n",
      "Normally Distributed Data\n",
      "In the orange juice problem, you’re trying to blend juices to take out variability, so is it \n",
      "reasonable to expect that the product you’re getting from your suppliers won’t have vari-\n",
      "able specs?\n",
      "Chances are that shipment of Biondo Commune orange juice you’re getting from Egypt \n",
      "won’t have an exact 13 Brix/Acid ratio. That may be the expected number, but there’s \n",
      "probably some give around it. And oftentimes, that wiggle room can be characterized \n",
      "using a probability distribution.\n",
      "A probability distribution, loosely speaking, gives a likelihood to each possible outcome \n",
      "of some situation, and all the probabilities add up to 1. Perhaps the most famous and \n",
      "widely used distribution is the normal distribution, otherwise known as the “bell curve.” \n",
      "The reason why the bell curve crops up a lot is because when you have a bunch of inde-\n",
      "pendent, complex, real-world factors added together that produce randomly distributed\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "146 Data Smart\n",
      "data, that data will often be distributed in a normal or bell-like way. This is called the \n",
      "central limit theorem.\n",
      "To see this, let’s do a little experiment. Pull out your cell phone and grab the last four \n",
      "digits of each of your saved contacts’ phone numbers. Digit one will probably be uniformly \n",
      "distributed between 0 and 9, meaning each of those digits will show up roughly the same \n",
      "amount. Same goes for digits 2, 3, and 4. \n",
      "Now, let’s take these four “random variables” and sum them. The lowest number you \n",
      "could get is 0 (0 + 0 + 0 + 0). The highest is 36 (9 + 9 + 9 + 9). There’s only one way to get \n",
      "0 and 36. There are four ways to get 1 and four ways to get 35, but there’s a ton of ways to \n",
      "get 20. So if you did this to enough phone numbers and graphed a bar chart of the various \n",
      "sums, you’d have a bell curve that looks like Figure 4-40 (I used 1,000 phone numbers to \n",
      "get the ﬁ gure, because I’m just that popular).  \n",
      "1\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "3 5 7 9 11 13 15 17 19 21 23\n",
      "Sum of 4 digits\n",
      "Summing the last 4 digits of the numbers\n",
      "in your cell phone’s contact list\n",
      "Count of instances\n",
      "25 27 29 31 33\n",
      "Figure 4-40:  Combining independent random variables to illustrate how they gather into a bell curve\n",
      "The Cumulative Distribution Function\n",
      "There’s another way of drawing this distribution that’s going to be super helpful, and it’s \n",
      "called the cumulative distribution  function (CDF). The cumulative distribution function \n",
      "gives the probability of an outcome that’s less than or equal to a particular value.\n",
      "In the case of the cell phone data, only 12 percent of the cases are less than or equal to \n",
      "10, whereas 100 percent of the cases are less than or equal to 36 (since that’s the largest \n",
      "possible value). This cumulative distribution is pictured in Figure 4-41.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "147Optimization Modeling\n",
      "1\n",
      "0%\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "90%\n",
      "100%\n",
      "3579 1 1 1 3 1 5 1 7 1 9 2 1 2 3\n",
      "Digit sum\n",
      "Cumulative distribution function for summing last 4\n",
      "phone number digits\n",
      "% that’s LESS THAN OR EQUAL TO digit sum\n",
      "25 27 29 31 33\n",
      "Figure 4-41:  The cumulative distribution function for the cell phone contact sums \n",
      "And here’s the cool thing about the cumulative distribution function— you can read it \n",
      "backward to generate samples from the distribution. \n",
      "For example, if you wanted to generate a random value from this contact list four-digit \n",
      "sum distribution, you could generate a random number between 0 and 100 percent. Say \n",
      "you come up with 61 percent as your random value. Looking that up on the vertical axis \n",
      "of the CDF, 61 percent lines up with 19 on the horizontal axis. And you could do this \n",
      "over and over to generate a lot of samples from the distribution.\n",
      "Now, a normal CDF can be described completely by two numbers: a mean and a standard \n",
      "deviation . The mean is nothing more than the center of the distribution. The standard \n",
      "deviation measures the variability or spread of the bell curve around the mean. \n",
      "Say in the case of the juice you order from Egypt, it has a Brix/Acid mean of 13 and a \n",
      "standard deviation of 0.9. That means that 13 is the center of the probability distribution \n",
      "and 68 percent of orders are going to be within +/-0.9 of 13, 95 percent will be within two \n",
      "standard deviations (+/-1.8), and 99.7 percent will be within three standard deviations \n",
      "(+/-2.7). This is sometimes called the “68-95-99.7” rule.\n",
      "In other words, it’s pretty likely you’ll receive a 13.5 Brix/Acid batch from Egypt, but \n",
      "it’s very unlikely you’ll receive a 10 Brix/Acid batch.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "148 Data Smart\n",
      "CALCULATING THE SAMPLE MEAN AND STANDARD DEVIATION\n",
      "For those of you who haven’t calculated standard deviation before and are interested \n",
      "to know how it’s done, it’s super easy.\n",
      "Figure 4-42 shows the past 11 orders of the Biondo Commune orange juice from \n",
      "Egypt and their respective Brix/Acid measurements in column B. The sample mean of \n",
      "those measures is 13, as given in the original specs spreadsheet. \n",
      "The sample estimate of the standard deviation is just the square root of the mean \n",
      "squared error. By “error,” I just mean the deviation of each order from the expected \n",
      "value of 13.\n",
      "In column C of Figure 4-42, you can see the error calculation, and the squared error \n",
      "calculation is in column D. The mean squared error is \n",
      "AVERAGE(D2:D12), which comes \n",
      "out to 0.77. The square root of the mean squared error is then 0.88. Easy enough!\n",
      "In practice however, when calculating the sample standard deviation for a small \n",
      "number of orders, you get a better estimate if you sum the squared error and divide \n",
      "through by 1 less than your total orders (in this case 10 instead of 11).\n",
      "If you make this adjustment, the standard deviation becomes 0.92, as shown in \n",
      "Figure 4-42.\n",
      "Figure 4-42:  An example of the sample standard deviation calculation\n",
      "Generating Scenarios from Standard Deviations in the Blending Problem\n",
      "NOTE\n",
      "Just as in the previous section, those using Excel 2010 and Excel 2013 will need to \n",
      "employ OpenSolver. Just set the problem up normally and use the OpenSolver Solve \n",
      "button on the ribbon when the time comes. See Chapter 1 for more detail on OpenSolver.\n",
      "Imagine instead of receiving the Specs tab, you received standard deviations along with \n",
      "your speciﬁ cations in a tab titled Specs Variability, as shown in Figure 4-43. The goal is\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "149Optimization Modeling\n",
      "to ﬁ nd a blending plan that’s less than $1.25 million dollars that best meets the quality \n",
      "expectations in light of supplier variability.\n",
      "You can create a copy of the original Minimax Relaxed Quality tab called the Robust \n",
      "Optimization Model, where the new standard deviations will go in N6:Q16 adjacent to \n",
      "the old speciﬁ cations.\n",
      "Once they’re in there, what do you do with them? \n",
      "You’re going to use the mean and standard deviation for the specs to take a Monte \n",
      "Carlo simulation approach to solving this problem. The Monte Carlo method means that \n",
      "instead of somehow incorporating the distribution directly into the model, you sample \n",
      "the distribution, creating scenarios or instantiations from each set of samples, and then \n",
      "include those samples in the model.\n",
      "A scenario is one possible answer to the question, “If these are the distributions for \n",
      "my stats, what would an actual order look like?” To draw a scenario, you read the nor-\n",
      "mal CDF—characterized by the mean and standard deviation—backward, as discussed \n",
      "previously with Figure 4-41.\n",
      "Figure 4-43:  Speciﬁ  cations with standard deviation added\n",
      "The formula in Excel for reading the normal CDF backward (or “inverted” if you like) \n",
      "is NORMINV.\n",
      "So generate a scenario in column B, starting at row 33 below everything that’s in the \n",
      "worksheet already. You can call this Scenario 1.\n",
      "In B34:B44 you’ll generate an actual scenario of Brix/Acid values for all the suppliers. \n",
      "In B34 generate a random value for Brazilian Hamlin where its mean Brix/Acid is 10.5 \n",
      "(H6) and its standard deviation is 2 (N6) using the \n",
      "NORMINV formula:\n",
      "=NORMINV(RAND(),$H6,$N6)\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "150 Data Smart\n",
      "You’re feeding a random number between 0 and 100 percent into NORMINV along with \n",
      "the mean and standard deviation, and out pops a random Brix/Acid value. Let’s drag that \n",
      "formula down to B44. \n",
      "Starting at B45, you can do the same thing for Acid, then Astringency, then Color. The \n",
      "range B34:B77 now contains a single scenario, randomly drawn from the distributions. \n",
      "Dragging this scenario across the columns all the way to CW (note the absolute refer-\n",
      "ences that allow for this), you can generate 100 such random spec scenarios. Solver can’t \n",
      "understand them if they remain non-linear formulas, so go ahead and copy and paste the \n",
      "scenarios on top of themselves as values only. Now the scenarios are ﬁ xed data.\n",
      "This mound of scenario data in B34:CW77 is pictured in Figure 4-44.\n",
      "Figure 4-44:  100 generated juice spec scenarios\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "151Optimization Modeling\n",
      "Setting Up the Scenario Constraints\n",
      "Okay, so what you want to do is ﬁ nd a solution that relaxes the quality bounds the least \n",
      "in order to meet them in each and every scenario you’ve generated. Just ﬁ  nd a solution \n",
      "that protects the product.\n",
      "So under the ﬁ rst scenario in cell B79 calculate the BAR for January as:\n",
      "=SUMPRODUCT($C$6:$C$16,B34:B44)/$C$21\n",
      "You can do the same for February and March on rows 80 and 81 and then drag the \n",
      "entire calculation right through column CW to get a Brix/Acid for each scenario.\n",
      "Doing the same for the other specs, you end up with calculations on each scenario, as \n",
      "shown in Figure 4-45.\n",
      "Figure 4-45:  Spec calculations for each scenario\n",
      "Setting up the model isn’t all that diffi  cult. You put a cost upper bound of $1.25 mil-\n",
      "lion in B2. You’re still minimizing D2, the quality relaxation, in a minimax setup. All you \n",
      "need to do is place the quality bounds around all of the scenarios rather than just the \n",
      "expected quality values.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "152 Data Smart\n",
      "Thus, for BAR, you add that B79:CW81 ≥ B27 and ≤ F27 and similarly for Acid, Astringency, \n",
      "and Color, yielding the formulation shown in Figure 4-46.\n",
      "Figure 4-46:  Solver setup for robust optimization\n",
      "Press Solve. You’ll get a solution rather quickly. Now, if you generated the random \n",
      "scenarios yourself rather than keeping the ones provided in the spreadsheet available for \n",
      "download, the solution you get will be diff erent. For my 100 scenarios, the best quality I \n",
      "could get is a 133 percent relaxation while keeping cost under $1.25 million.\n",
      "For giggles, you can up the cost upper bound to $1.5 million and solve again. You get a \n",
      "114 percent relaxation without the cost even going to the upper bound but rather staying \n",
      "at about $1.3 million. It seems that upping the cost higher than that doesn’t give you any \n",
      "more leeway to improve quality (see the solution in Figure 4-47). \n",
      "And that’s it! You now have a balance of cost and quality that meets constraints even \n",
      "in random, real-world situations.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "153Optimization Modeling\n",
      "Figure 4-47: Solution to the robust optimization model\n",
      "AN EXERCISE FOR THE READER\n",
      "If you’re a glutton for pain, I’d like to off er one more formulation to work through. \n",
      "In the previous problem, you minimized the percent you had to lower and raise the \n",
      "quality bounds such that every constraint was satisﬁ  ed. But what if you cared only \n",
      "that 95 percent of the scenarios were satisﬁ ed? \n",
      "You would still minimize the quality relaxation percentage, but you’d need to stick \n",
      "an indicator variable on each scenario and use constraints to set it to 1 when the sce-\n",
      "nario’s quality constraints were violated. The sum of these indicators could then be set \n",
      "≤5 as a constraint.\n",
      "Give it a shot. See if you can work it.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "154 Data Smart\n",
      "Wrapping Up\n",
      "If you stuck with me on those last couple of models, then bravo. Those suckers weren’t toy \n",
      "problems. In fact, this may be the hardest chapter in this book. It’s all downhill from here! \n",
      "Here’s a little recap of what you just learned:\n",
      "• Simple linear programming\n",
      "• The minimax formulation\n",
      "• Adding integer variables and constraints\n",
      "• Modeling if-then logic using a “Big M” constraint\n",
      "• Modeling the product of decision variables in a linear way\n",
      "• The normal distribution, central limit theorem, cumulative distribution functions, \n",
      "and the Monte Carlo method\n",
      "• Using the Monte Carlo method to model risk within a linear program\n",
      "Your head is probably spinning with all sorts of applications of this stuff  to your busi-\n",
      "ness right now. Or you’ve just downed a stiff  drink and never want to deal with linear \n",
      "programming again. I hope it’s the former, because the truth is, you can get arbitrarily \n",
      "creative and complex with linear programming. In many business contexts you’ll often \n",
      "ﬁ nd models with tens of millions of decision variables.\n",
      "PRACTICE, PRACTICE, PRACTICE! AND READ SOME MORE\n",
      "Modeling linear programs, especially when you have to execute funky “squirrel \n",
      "removal” tricks, can be rather non-intuitive. The best way to get good at it is to ﬁ nd \n",
      "some opportunities in your own line of work that could use modeling and have at it. \n",
      "You can’t memorize this stuff ; you have to get a feel for how to address certain mod-\n",
      "eling peculiarities. And that comes with practice.\n",
      "If you want some additional linear programming literature to supplement your prac-\n",
      "tice, here are some free online resources that I highly recommend:\n",
      "• The AIMMS optimization modeling book available at http://www.aimms.com/\n",
      "downloads/manuals/optimization-modeling  is an incredible resource. Don’t \n",
      "skip their two Tips and Tricks chapters; those things are awesome.\n",
      "• “Formulating Integer Linear Programs: A Rogue’s Gallery” from Brown and Dell \n",
      "of the Naval Postgraduate School: \n",
      "http://faculty.nps.edu/gbrown/docs/\n",
      "Brown_Dell_INFORMS_Transactions_on_Education_January2 0 07.pdf.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "5\n",
      "T\n",
      "his chapter continues the discussion on cluster identification and analysis using the \n",
      "wholesale wine dataset from Chapter 2. Although it’s perfectly fine to jump around \n",
      "in this book, in this case I recommend at least skimming Chapter 2 before reading this \n",
      "chapter, because I don’t repeat the data preparation steps, and you’re going to be using \n",
      "cosine similarity, which was discussed at the end of Chapter 2.\n",
      "Also, the techniques used here rely on the “Big M” constraint optimization techniques \n",
      "introduced in Chapter 4, so some familiarity with that will be helpful.\n",
      "This chapter continues addressing the problem of detecting interesting groups of cus-\n",
      "tomers based on their purchases, but it approaches the problem from a fundamentally \n",
      "diff erent direction. \n",
      "Rather than thinking about customers huddling around ﬂ ags planted on the dance ﬂ oor \n",
      "to assign them to groups, as you did with k-means clustering (Chapter 2), you’re going to \n",
      "look at your customers in a more relational way. Customers buy similar things, and in that \n",
      "way, they’re related to each other. Some are more “friendly” than others, in that they’re \n",
      "interested in the same stuff . So by thinking about how related or not related each customer \n",
      "is to the others, you can identify communities of customers without needing to plant a set \n",
      "number of ﬂ ags in the data that get moved around until people feel at home.\n",
      "The key concept that allows you to approach customer clustering in this relational way \n",
      "is called a network graph. A network graph, as you’ll see in the next section, is a simple \n",
      "way to store and visualize entities (such as customers) that are connected (by purchase \n",
      "data for instance).\n",
      "These days, network visualization and analysis are all the rage, and the techniques used \n",
      "to mine insights from network graphs often work better than traditional techniques (such \n",
      "as k-means clustering in Chapter 2), so it’s important that a modern analyst understand \n",
      "and be able to leverage network graphs in their work.\n",
      "When doing cluster analysis on a network, people often use the term community detection \n",
      "instead, which makes sense because many network graphs are social in nature and their \n",
      "Cluster Analysis \n",
      "Part II: Network Graphs \n",
      "and Community \n",
      "Detection\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "156 Data Smart\n",
      "clusters do indeed make up communities. This chapter focuses on a particular community \n",
      "detection algorithm called modularity maximization.\n",
      "At a high level, modularity maximization rewards you every time you place two good \n",
      "friends in a cluster together and penalizes you every time you shove some strangers together. \n",
      "By grabbing all the rewards you can and avoiding as many penalties as possible, the tech-\n",
      "nique leads to a natural clustering of customers. And here’s the cool part, which you’ll see \n",
      "later—unlike the k-means clustering approach, you don’t need to choose k. The algorithm \n",
      "does it for you! In this way, the clustering technique used here takes unsupervised machine \n",
      "learning to a whole new level of knowledge discovery.\n",
      "Also, from a mathematical-sex-appeal perspective, k-means clustering, while rad, has \n",
      "been around for over half a century. The techniques you’ll use in this chapter were devel-\n",
      "oped in just the past several years. This is cutting edge stuff .\n",
      "What Is a Network Graph?\n",
      "A network graph is a collection of things called nodes that are connected by relationships \n",
      "called edges. Social networks like Facebook provide a lot of network-graphable data, such \n",
      "as friends who are connected to you and possibly to each other. Hence, the term “the social \n",
      "graph” has come up a lot in recent years.\n",
      "The nodes in a network graph don’t have to be people of course, and the edges that \n",
      "represent relationships don’t have to be interpersonal relationships. For instance, you \n",
      "could have nodes that are Facebook users and other nodes that are product pages they \n",
      "like. Those “likes” comprise the edges of the graph. Similarly, you could create a network \n",
      "graph of all the stops on your city’s transportation system. Or all the destinations and \n",
      "routes on Delta’s ﬂ ight map (in fact, if you look at the route map on any airline’s website, \n",
      "you’ll see it’s a canonical network graph).\n",
      "Or you could get all spy-like and graph anyone who has called anyone on a GPS sat \n",
      "phone within al-Qaeda in the Islamic Magreb. With the release of material on the NSA’s \n",
      "spying eff orts by Edward Snowden, this last type of network graph has been getting a \n",
      "lot of attention in the media. One example is the congressional discussion around NSA’s \n",
      "ability to perform a “three-hop” query—that is go into their network graph of phone call \n",
      "data and ﬁ nd people three hops from a known terrorist (nodes connected to a terrorist \n",
      "by a three edge path in the graph).\n",
      "Whatever your business is, I guarantee you have a graph hiding in your data. One \n",
      "of my favorite network graphing projects is called DocGraph (\n",
      "http://notonlydev.com/\n",
      "docgraph/). Some intrepid folks have used a Freedom of Information Act request to cre-\n",
      "ate a graph of all kinds of Medicare referral data. Doctors get connected to other doctors \n",
      "via referrals, and the graph can be used to identify communities, inﬂ  uential providers \n",
      "(the doctor everyone goes to for the ﬁ nal opinion on a tricky diagnosis), and even cases \n",
      "of fraud and abuse.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "157Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "Network graphs are a rare contradiction in the analytics world. They are aesthetically \n",
      "beautiful and yet extremely utilitarian in the way they store and enable certain analyses. \n",
      "These graphs allow analysts to discover all sorts of insights both visually and algorithmi-\n",
      "cally, such as clusters, outliers, local inﬂ uencers, and bridges between diff erent groups.\n",
      "In the next section, you’ll visualize some network data to get a feel for how these things \n",
      "work. \n",
      "Visualizing a Simple Graph\n",
      "The TV show Friends was one of the most popular sitcoms of the 1990s and early 2000s. \n",
      "The show centered around six friends: Ross, Rachel, Joey, Chandler, Monica, and Phoebe. \n",
      "If you’ve never heard of the show or these characters, you’re either super young or trapped \n",
      "in a cave.\n",
      "These six characters become involved in a lot of romances with each other of various \n",
      "types: real romances, fantasy romances that never amount to anything, play romances \n",
      "based on some dare or competition, and so on.\n",
      "Think of these characters as six nodes or vertices on the graph. The relationships \n",
      "between them are edges. Off  the top of my head, I can think of these edges:\n",
      "• Ross and Rachel, obviously\n",
      "• Monica and Chandler end up married.\n",
      "• Joey and Rachel have a little romance going but ultimately decide it’s too weird.\n",
      "• Chandler and Rachel meet each other in a ﬂ ashback episode over a pool table mis-\n",
      "hap, and Rachel imagines what it’d be like to be with Chandler.\n",
      "• Chandler and Phoebe play at a relationship and end up having to kiss, because \n",
      "Chandler refuses to admit he’s with Monica.\n",
      "These six characters and their ﬁ ve edges can be visualized as shown in Figure 5-1.\n",
      "Ross Rachel\n",
      "MonicaJoey\n",
      "Chandler Phoebe\n",
      "Figure 5-1: Diagram of ro(faux)mances on Friends\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "158 Data Smart\n",
      "Pretty simple, right? Nodes and edges. That’s all a network graph is. And note how net-\n",
      "work graphs have nothing whatsoever to do with the graphs you may be familiar with, such \n",
      "as dot plots, line charts, and bar charts. No, these graphs are a diff erent animal entirely.\n",
      "Figure 5-1 is what’s called an undirected network graph, because the relationships are \n",
      "mutual by deﬁ nition. Something like Twitter data on the other hand is directed, that is, \n",
      "I can follow you, but you don’t have to follow me. When visualizing a directed graph, the \n",
      "edges are usually directional arrows.\n",
      "Now, one of the drawbacks about using Excel to work on network graphs is that, unlike \n",
      "other graphing and charting capabilities, Excel does not provide tools for visualizing \n",
      "network graphs.\n",
      "So for this chapter, I’m going to break my own ground rules for this book and use an \n",
      "external tool called Gephi for some visualization and computation, which is discussed \n",
      "more in the next section. That said, you can ignore all the Gephi aspects of this chapter if \n",
      "you want to. All the actual data mining on network data can be done without visualizing \n",
      "the network in Gephi; you’re just doing that part for fun.\n",
      "But visualization aside, if you want to work on this type of graph, you need a numerical \n",
      "representation of the data. One intuitive representation is called an adjacency matrix. An \n",
      "adjacency matrix is just a node-by-node grid of 0s and 1s, where a 1 in a particular cell \n",
      "means “put an edge here” and a 0 means “these nodes are unconnected.”\n",
      "You can create an adjacency matrix out of the Friends data, as shown in Figure 5-2 (the \n",
      "matrix looks a bit like a Galaga-style lobster to me). The friends’ names line the columns \n",
      "and rows, and relationships between them are shown with 1s. Notice how the graph is \n",
      "symmetric along the diagonal, because the graph is undirected. If Joey has an edge with \n",
      "Rachel, then the converse is true, and the adjacency matrix shows this. If relationships \n",
      "were one-sided, you could have a matrix without this symmetry.\n",
      "Although the edges here are represented with 1s, they don’t have to be. You can \n",
      "add weights to the edges, such as capacities—think of diff  erent planes with diff  erent \n",
      "NODEXL\n",
      "If you’re in Excel 2007 or 2010, the Social Media Research Foundation has released a \n",
      "template that allows network visualization in Excel called NodeXL. It’s not covered \n",
      "in this book because it’s still early days for the software, and LibreOffi  ce and Excel \n",
      "2011 for Mac users wouldn’t be able to follow along. If you’re interested, you can \n",
      "check out NodeXL for yourself at \n",
      "http://www.smrfoundation.org/nodexl/.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "159Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "capacities ﬂ ying routes or varying bandwidths available on diff erent links of an IT network. \n",
      "A weighted adjacency matrix is also called an affi  nity matrix.\n",
      "Figure 5-2: An adjacency matrix for the Friends data\n",
      "Brief Introduction to Gephi\n",
      "Let’s go ahead and get Gephi running so you can import and visualize the Friends dataset. \n",
      "Then you’ll know your way around later when things get real all up in here. \n",
      "Gephi is an open source network visualization tool written in Java, and it’s the main \n",
      "culprit behind many of the network visualization graphics you see in the media today. \n",
      "It’s easy to produce striking pictures, and people seem to have taken to it for graphing \n",
      "tweets like bunnies to carrots.\n",
      "The reason why I’ve waived my usual hesitancy to stay in Excel is that Gephi ﬁ  lls in \n",
      "the network visualization gap in Excel, it’s free, and it works on Windows, Mac OS, and \n",
      "Linux, so no matter what computer you’re using, you can follow along.\n",
      "You don’t have to do these visualization steps. If you just want to follow along in the \n",
      "ﬁ gures feel free, but I recommend getting your hands dirty. It’s fun. Keep in mind, though, \n",
      "that this book is not about Gephi. If you want to get really crazy with this tool, check out \n",
      "the resources at \n",
      "wiki.gephi.org for deeper instruction.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "160 Data Smart\n",
      "Gephi Installation and File Preparation\n",
      "To download Gephi, navigate to gephi.org in your browser, and then download and install \n",
      "the package following the instructions for your OS at http://gephi.org/users/install/. \n",
      "If you want a general tutorial on Gephi, check out the quick start guide at https://\n",
      "gephi.org/users/quick-start/. Also, inside the application, Gephi has a Help selection \n",
      "in the menu bar if you need it.\n",
      "Once Gephi is installed, you need to prep the adjacency matrix for importing into the \n",
      "visualization tool.\n",
      "Now, I ﬁ nd that importing an adjacency matrix into Gephi takes one step more than it \n",
      "should. Why? Because Gephi doesn’t accept comma-separated adjacency matrices. Each \n",
      "value has to be separated by a semicolon.\n",
      "Although Kurt Vonnegut said in A Man Without A Country, “Do not use semicolons. \n",
      "They are transvestite hermaphrodites representing absolutely nothing. All they do is show \n",
      "you’ve been to college,” Gephi has ignored his sound advice. My apologies. So follow along, \n",
      "and I’ll take you through the import process.\n",
      "I’ve made the FriendsGraph.xlsx spreadsheet available with the book (download at the \n",
      "book’s website at \n",
      "www.wiley.com/go/datasmart), or if you like, you can just hand-jam in \n",
      "the small dataset from the adjacency matrix pictured in Figure 5-2.\n",
      "The ﬁ rst thing you’re going to do to import this graph into Gephi is save it as a CSV, \n",
      "which is a plain-text, comma-separated ﬁ le format. To do so, go to Save As in Excel and \n",
      "choose CSV from the format list. The ﬁ lename will end up as FriendsGraph.csv, and when \n",
      "you save it, Excel may bark some warnings at you, which I give you permission to ignore.\n",
      "Once you’ve exported the ﬁ le, you need to replace all the commas in it with semico-\n",
      "lons. To do this, open the ﬁ le in a text editor (such as Notepad on Windows or TextEdit \n",
      "on Mac OS) and ﬁ nd and replace the commas with semicolons. Save the ﬁ  le. Figure 5-3 \n",
      "shows this process in Mac OS TextEdit.\n",
      "Figure 5-3: Replacing commas with semicolons in the Friends graph CSV\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "161Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "Once that’s completed, open your freshly installed copy of Gephi, and using the Open \n",
      "Graph File option on the Welcome screen (see Figure 5-4), select the FriendsGraph.csv \n",
      "ﬁ le you just edited.\n",
      "Figure 5-4:  Open the FriendsGraph.csv ﬁ  le in Gephi.\n",
      "When you attempt to open the ﬁ le, an Import Report window will pop up. Note that six \n",
      "nodes and ten edges have been detected. The reason why ten edges are listed is because \n",
      "the adjacency matrix is symmetric, so each relationship is duplicated. To resolve this \n",
      "duplication, change the Graph Type from directed to undirected in the import window \n",
      "(see Figure 5-5). Press OK.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "162 Data Smart\n",
      "Figure 5-5:  Importing the Friends graph\n",
      "Laying Out the Graph\n",
      "Make sure the Overview tab is selected in the top left of the Gephi window. If it is selected, \n",
      "your Gephi window should look something like Figure 5-6. The nodes and edges are laid \n",
      "out haphazardly in space. The zoom is all out of whack so the graph is barely visible. Your \n",
      "initial layout will likely appear diff erent.\n",
      "Let’s make this graph a little prettier. A couple of navigational items you should be \n",
      "aware of—you can zoom in with the scroll wheel on your mouse, and you can move the \n",
      "canvas around by right-clicking in the space and dragging the graph until it’s centered.\n",
      "By clicking the T button at the foot of the overview window, you can add labels to the \n",
      "graph nodes so you know which character is which node. After zooming in, adjusting, \n",
      "and adding labels, the graph now looks as shown in Figure 5-7.\n",
      "You need to lay this graph out in a nicer fashion. And luckily, Gephi has a bunch of \n",
      "algorithms for automating this process. Many of them use forces such as gravity between \n",
      "connected nodes and repulsion between unconnected nodes to settle things into place. \n",
      "The layout section of Gephi is in the bottom-left window of the overview panel. Feel free \n",
      "to select things haphazardly from the menu to try them out.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "163Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "Figure 5-6:  Initial layout of the Friends graph\n",
      "NOTE\n",
      "Be warned that some of the layout algorithms are going to shrink or expand the \n",
      "graph such that you’ll have to zoom in or out to see the graph again. Also, the sizes \n",
      "of your labels are going to get out of whack, but there’s a Label Adjust selection \n",
      "under the Layout drop-down menu to ﬁ x that.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "164 Data Smart\n",
      "Figure 5-7: The Friends graph is decipherable but messy.\n",
      "To get my preferred layout, the ﬁ rst thing I’m going to do is select ForceAtlas 2 from the \n",
      "layout menu and press the Run button. This is going to move my nodes around to better \n",
      "positions. But the labels are now huge (see Figure 5-8).\n",
      "Select Label Adjust from the menu and press Run. You’ll get something that looks much \n",
      "better. I can see that Rachel and Chandler are really the most well-connected in the graph. \n",
      "Obviously, Monica and Ross are distant because they’re brother and sister, and so on.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "165Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "Node Degree\n",
      "One concept in network graphing that’s going to be important in this chapter is that of \n",
      "degree. The degree of a node is simply the count of edges connected to it. So Chandler \n",
      "has a degree of 3, whereas Phoebe has a degree of 1. You can use these degrees in Gephi \n",
      "to resize nodes.\n",
      "Figure 5-8:  After running ForceAtlas 2 on the Friends graph\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "166 Data Smart\n",
      "To get a sense of the average degree of the graph and who has what degree, press the \n",
      "Average Degree button on the right side of Gephi in the Statistics section. This will pop \n",
      "up a window like the one shown in Figure 5-9, where the average degree of the graph is \n",
      "1.6667 with four nodes of degree 1 and two nodes of degree 3 (Rachel and Chandler).\n",
      "Close this window and navigate to the Ranking section of the Overview window in \n",
      "the top left box. Select the Nodes section and the red gemstone label that indicates node \n",
      "resizing. Select Degree from the drop-down and toggle the minimum and maximum sizes \n",
      "for nodes. When you press Apply, Gephi will resize the nodes using degree as a proxy for \n",
      "importance. I’ve called out this section of the Overview window in Figure 5-10.\n",
      "Pretty Printing\n",
      "Although these pictures look okay, you’re not going to hang them on your wall. To prepare \n",
      "the graph for printing an image, click the Preview pane at the top of Gephi.\n",
      "INDEGREE, OUTDEGREE, IMPORTANCE, AND BAD BEHAVIOR\n",
      "In a directed graph, the count of edges going into a node is called the indegree. The \n",
      "count of outbound edges is the outdegree. Indegree in a social network is a simple \n",
      "way to gauge the prestige of a node. This is often the ﬁ rst value people look at on \n",
      "Facebook or Twitter to gauge importance. “Oh, they have a lot of followers…they \n",
      "must be a big deal.”\n",
      "Now, this metric can certainly be gamed. Who exactly are these followers whose \n",
      "edges ﬂ ow into your node? Maybe they’re all fake users you signed up for to heighten \n",
      "your own prestige.\n",
      "Google uses indegree (in search engine speak this is a backlink  count) in their \n",
      "PageRank algorithm. When someone fakes inbound links to their website to heighten \n",
      "its prestige and move up the search results, that’s called link spam. In contexts such as an \n",
      "Internet search where rankings mean big business, more complex measures of prestige, \n",
      "inﬂ uence, and centrality have evolved to account for such bad behavior.\n",
      "As you’ll see in Chapter 9, these network graph concepts are useful in outlier  \n",
      "detection. Rather than ﬁ nding who is central in a graph, you can use indegree to ﬁ  nd \n",
      "who’s on the periphery.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "167Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "Under the Preview Settings tab, select the Black Background preset from the Presets \n",
      "drop-down (because you have hacker delusions), and click the Refresh button at the bot-\n",
      "tom left of the window.\n",
      "Gephi will paint the graph with stunning, curvy beauty (see Figure 5-11). Note how the \n",
      "labels are resized with the nodes, which is awesome. I ﬁ nd the edges of this graph a little \n",
      "on the thin side, so I bumped the edge thickness up from 1 to 3 on the left settings pane.\n",
      "Figure 5-9: Calculating the average degree of a graph\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "168 Data Smart\n",
      "Figure 5-10:  Resizing the graph according to node degree\n",
      "If you want to export this image to a graphics ﬁ  le (for example, a .png ﬁ le), press the \n",
      "Export button in the bottom left of the preview settings section. You can then distribute \n",
      "the graph on a website, in a PowerPoint presentation, or even in a book on data science.\n",
      "Touching the Graph Data\n",
      "Before you move back to Excel to confront the wholesale wine problem from Chapter 2, I \n",
      "want to take you through the Data Laboratory section of Gephi. Click Data Laboratory at \n",
      "the top of Gephi to see the underlying data that you’ve imported into the graph.\n",
      "Note that there are two sections of data: Nodes and Edges. In the Nodes section, you \n",
      "see the six characters. And because you went through the Average Degree calculation \n",
      "earlier, a column for Degree has been added to the node dataset. If you want to, you can \n",
      "export this column back to Excel by pressing the Export Table button on the menu bar. \n",
      "See Figure 5-12.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "169Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "Figure 5-11: A prettier Friends graph\n",
      "Figure 5-12: Node information with degree count in the Data Laboratory\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "170 Data Smart\n",
      "Clicking the edges section, the ﬁ ve edges with their endpoints are laid out. Each edge \n",
      "was a weight of 1, because you imported an adjacency matrix with all 1s. If you had \n",
      "changed some of those values to be higher in the case of, say, an actual marriage then \n",
      "those higher weights would be reﬂ  ected in this column (they also would have aff  ected \n",
      "the ForceAtlas 2 layout).\n",
      "All right! So there’s your 30,000-foot tour of Gephi. Let’s get back to clustering the \n",
      "wholesale wine data, and you’ll return to Gephi later to do some more visualizations and \n",
      "computations.\n",
      "Building a Graph from the Wholesale Wine Data\n",
      "NOTE\n",
      "The Excel workbook used in this chapter, “WineNetwork.xlsx,” is available for \n",
      "download at the book’s website at www.wiley.com/go/datasmart. This workbook \n",
      "includes all the initial data if you want to work from that. Or you can just read along \n",
      "using the sheets I’ve already put together in the workbook.\n",
      "In this chapter, I want to demonstrate how to detect clusters within your customer pur-\n",
      "chase data by representing that data as a graph. Some businesses have data that’s already \n",
      "graphable, such as the Medicare referral data discussed earlier. \n",
      "But in this case, the wine purchase matrix from Chapter 2 does not represent customer-\n",
      "to-customer relationships out of the box.\n",
      "To start, you should ﬁ gure out how to graph the wholesale wine dataset as a network. \n",
      "And that means constructing an adjacency matrix similar to the Friends adjacency matrix \n",
      "shown in Figure 5-2. From there you’ll be able to visualize and compute whatever you \n",
      "want on the graph.\n",
      "I’ll pick up the analysis using the Matrix tab in the WineNetwork.xlsx workbook \n",
      "(available for download with this book). If you remember, this is the same Matrix tab \n",
      "you created at the beginning of Chapter 2 from the wine sale transactional data and the \n",
      "wholesale deal metadata. \n",
      "Pictured in Figure 5-13, the rows of the Matrix tab give details of the 32 wine deals \n",
      "off ered by Joey Bag O’ Donuts Wine Emporium last year. In the columns of the sheet are\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "171Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "customer names, and each (deal, customer) cell has a value of 1 if that customer purchased \n",
      "that deal.\n",
      "Figure 5-13: The Matrix tab showing who bought what \n",
      "So you need to turn this data from Chapter 2 into something similar to the Friends \n",
      "adjacency matrix, but how do you go about doing that? \n",
      "If you created the Distances matrix for the k-means silhouette in Chapter 2, you’ve \n",
      "already seen something similar. For that calculation, you created a matrix of distances \n",
      "between each customer based on the deals they took (shown in Figure 5-14).\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "172 Data Smart\n",
      "Figure 5-14: The customer distances tab from Chapter 2\n",
      "This dataset was oriented in a customer-to-customer fashion just like the Friends data-\n",
      "set. Connections between customers were characterized by how their purchases aligned.\n",
      "But there are a couple of problems with this customer-to-customer distance matrix \n",
      "created in Chapter 2:\n",
      "• At the end of Chapter 2 you discovered that asymmetric similarity and distance \n",
      "measures between customers work much better than Euclidean distance in the case \n",
      "of purchase data. You care about purchases, not “non-purchases.” \n",
      "• If you want to draw edges between two customers, you want to do so because the \n",
      "two customers are similar not because they are distant, so this calculation needs to \n",
      "be reversed. This closeness of purchases is captured via cosine similarity, so you \n",
      "need to create a similarity matrix in contrast to Chapter 2’s distance matrix.\n",
      "Creating a Cosine Similarity Matrix\n",
      "In this section, you’ll take the Matrix tab in your notebook and construct from it a cus-\n",
      "tomer-to-customer graph using cosine similarity. The process for doing this in Excel, \n",
      "using numbered rows and columns together with the \n",
      "OFFSET formula, is identical to that \n",
      "used in Chapter 2 for the Euclidean distances sheet. For more on OFFSET, see Chapter 1.\n",
      "You’ll start by creating a tab called Similarity in which you will paste a customer-by-\n",
      "customer grid, whereby each customer is numbered in each direction. Remember that\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "173Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "copying and pasting customers from the Matrix tab down the rows requires using the \n",
      "Paste Special feature in Excel with the Transpose box checked. \n",
      "This empty grid is shown in Figure 5-15.\n",
      "Figure 5-15: The empty grid for the cosine similarity matrix\n",
      "Start by computing the cosine similarity between Adams and himself (which should \n",
      "be 1). As a refresher, recall the deﬁ  nition of cosine similarity between two customers’ \n",
      "binary purchase vectors that you read in Chapter 2:\n",
      "The count of matched purchases in the two vectors divided by the product of the square root \n",
      "of the number of purchases in the ﬁ rst vector times the square root of the number of purchases \n",
      "in the second vector. \n",
      "Adams’ purchase vector is Matrix!$H$2:$H$33;  so in order to compute the cosine \n",
      "similarity of Adams to himself, you use the following formula in cell C3:\n",
      "=SUMPRODUCT(Matrix!$H$2:$H$33,Matrix!$H$2:$H$33)/\n",
      "  (SQRT(SUM(Matrix!$H$2:$H$33))*SQRT(SUM(Matrix!$H$2:$H$33)))\n",
      "In the top of the formula you take the SUMPRODUCT  of the purchase vectors you care \n",
      "about to count matched purchases. In the denominator, you take the square roots of the \n",
      "number of purchases for each customer and multiply them.\n",
      "Now, this computation works for Adams, but you want to drag it around the \n",
      "sheet so you don’t have to type each formula individually. And to make that \n",
      "happen, you use the \n",
      "OFFSET  formula. By replacing Matrix!$H$2:$H$33  with \n",
      "OFFSET(Matrix!$H$2:$H$33,0,Similarity!C$1)  for the columns and, similarly using\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "174 Data Smart\n",
      "OFFSET(Matrix!$H$2:$H$33,0,Similarity!$A3) for the rows, you get a formula that uses \n",
      "the customer numbers in column A and row 1 to shift the purchase vectors being used \n",
      "in the similarity calculation.\n",
      "This leads to a slightly more ugly (sorry!) formula for cell C3:\n",
      "=SUMPRODUCT(OFFSET(Matrix!$H$2:$H$33,0,Similarity!C$1),\n",
      "   OFFSET(Matrix!$H$2:$H$33,0,Similarity!$A3))/\n",
      "   (SQRT(SUM(OFFSET(Matrix!$H$2:$H$33,0,Similarity!C$1)))\n",
      "   *SQRT(SUM(OFFSET(Matrix!$H$2:$H$33,0,Similarity!$A3))))\n",
      "This formula locks down Matrix!$H$2:$H$33 by the absolute references, so as you drag \n",
      "the formula around the sheet, it stays the same. Similarity!C$1 will change columns but \n",
      "will stay on row 1 where you want it, and Similarity!$A3 will stay in column A.\n",
      "But you’re not quite done. You’re interested in creating a graph of customers who are \n",
      "similar to each other, but honestly, you don’t care about the diagonal of the matrix. Yes, \n",
      "Adams is identical to himself and has a cosine similarity of 1, but you’re not interested \n",
      "in drawing a graph with edges that loop back to point where they start, so you need to \n",
      "make all those entries 0 instead.\n",
      "This just means wrapping the cosine similarity calculation in an \n",
      "IF statement to check \n",
      "whether the customer on the row equals the one in the column. Thus, you get the ﬁ  nal \n",
      "formula of:\n",
      "IF(C$1=$A3,0,SUMPRODUCT(OFFSET(Matrix!$H$2:$H$33,0,Similarity!C$1),\n",
      "   OFFSET(Matrix!$H$2:$H$33,0,Similarity!$A3))/\n",
      "   (SQRT(SUM(OFFSET(Matrix!$H$2:$H$33,0,Similarity!C$1)))\n",
      "   *SQRT(SUM(OFFSET(Matrix!$H$2:$H$33,0,Similarity!$A3)))))\n",
      "Now that you have a formula that you can drag around, grab the bottom-right corner \n",
      "of C3, drag it across the sheet to CX3, and drag it down to CX102.\n",
      "You now have a cosine similarity matrix that shows which customers match each other. \n",
      "Placing some conditional formatting on the grid, you get what’s pictured in Figure 5-16.\n",
      "Producing an r-Neighborhood Graph\n",
      "The Similarity tab is a weighted graph. Each pair of customers either has a 0 between \n",
      "them or some non-zero cosine similarity value that shows how strong their edge should \n",
      "be. As it is, this similarity matrix is an affi  nity matrix.\n",
      "So why not just dump this affi  nity matrix out and peek at it in Gephi? Maybe you’re \n",
      "all set to do the analysis on the graph as is.\n",
      "Sure, exporting the CSV and importing it into Gephi is possible at this step. But let \n",
      "me save you the heartache and just throw up an image (Figure 5-17) of the graph after \n",
      "it’s been laid out in Gephi. It’s a huge mess of edges going every which way. Too many \n",
      "connections prevent the layout algorithms from properly moving nodes away from each \n",
      "other, so in the end you have an oblong chunk of noise.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "175Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "Figure 5-16: The completed customer cosine similarity matrix\n",
      "Figure 5-17: The mess of a cosine similarity customer-to-customer graph\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "176 Data Smart\n",
      "You’ve taken about 300 purchases and turned them into thousands of edges in the \n",
      "graph. Some of these edges you can probably chalk up to randomness. Yeah, maybe you \n",
      "and I lined up on 1 of our 10 wine purchases, and you have a teeny tiny cosine similarity, \n",
      "but is that edge worth drawing on the graph?\n",
      "In order to make sense of the data, it’s best if you prune edges from the graph that \n",
      "really don’t matter all that much, and keep only the strongest relationships on there—the \n",
      "relationships that don’t just come from one lucky shared purchase.\n",
      "Okay, so which edges should you drop? \n",
      "There are two popular techniques for pruning edges from network graphs. You can \n",
      "take the affi  nity matrix and build one of the following:\n",
      "• An r-neighborhood graph:  In an r-neighborhood graph, you keep only the edges \n",
      "that are of a certain strength. For instance, in the affi  nity matrix, edge weights range \n",
      "from 0 to 1. Maybe you should drop all edges below 0.5. That’d be an example of \n",
      "an r-neighborhood graph where r is 0.5.\n",
      "• A k nearest neighbors  (kNN) graph: In a kNN graph, you keep a set number of \n",
      "edges (k) going out of each node. For instance, if you set k to 5, you’d keep the ﬁ ve \n",
      "edges coming out of each node that have the highest affi  nities.\n",
      "Neither graph is superior to the other. It depends on the situation.\n",
      "This chapter focuses on the ﬁ rst option, an r-neighborhood graph. I leave it as an exer-\n",
      "cise for you to go back and work the problem with a kNN graph. It’s pretty easy to imple-\n",
      "ment in Excel using the \n",
      "LARGE formula (see Chapter 1 for more on LARGE). In Chapter 9, \n",
      "we’ll use a kNN graph for outlier detection.\n",
      "All right. So how do you take the Similarity tab and turn it into an r-neighborhood \n",
      "adjacency matrix? Well, ﬁ rst you need to settle on what r should be.\n",
      "In the white space below the similarity matrix, count how many edges (non-zero simi-\n",
      "larity values) you have in the affi  nity matrix using the formula in cell C104:\n",
      "=COUNTIF(C3:CX102,\">0\")\n",
      "This returns 2,950 edges made from the original 324 sales. What if you kept only the \n",
      "top 20 percent of them? What would the value of r have to be to make that happen? Well, \n",
      "because you have 2,950 edges, the 80th percentile similarity value would be whatever the \n",
      "590th edge has. So below the edge count in C105, you can use the \n",
      "LARGE formula to get \n",
      "the 590th largest edge weight (see Figure 5-18):\n",
      "=LARGE(C3:CX102,590)\n",
      "This returns a value of 0.5. So you can keep the top 20 percent of edges by throwing \n",
      "away everything with a cosine similarity of less than 0.5.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "177Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "Figure 5-18:  Calculating the 80th percentile of edge weights \n",
      "Now that you have the cutoff for the r-neighborhood graph, construction of \n",
      "the adjacency matrix is super easy. First create a new tab in the workbook called \n",
      "r-NeighborhoodAdj, and paste the customer names in column A and row 1 to create a grid.\n",
      "In any cell in the grid, you put a 1 if the similarity value on the previous Similarity tab \n",
      "is greater than 0.5. So, for example, in cell B2, you can use the following formula:\n",
      "=IF(Similarity!C3>=Similarity!$C$105,1,0)\n",
      "The IF formula simply checks the appropriate similarity value against the cutoff   in \n",
      "Similarity$C$105 (0.5) and assigns a 1 if it’s large enough. Because Similarity$C$105 \n",
      "is locked down with absolute references, you can drag this formula across the columns \n",
      "and down the rows to ﬁ ll in the whole adjacency matrix, as shown in Figure 5-19 (I’ve \n",
      "used some conditional formatting for the beneﬁ t of the ﬁ gure).\n",
      "You now have the r-neighborhood graph of the customer purchase data. You’ve trans-\n",
      "formed the purchase data into customer relationships and then whittled those down to \n",
      "a set of meaningful ones.\n",
      "If you were to now export the r-neighborhood adjacency matrix to Gephi and lay it out, \n",
      "you would get something much improved over Figure 5-17. Export the graph yourself, do \n",
      "the semicolon two-step, and take a peek along with me.\n",
      "As shown in Figure 5-20, there are at least two tightly knit communities in the graph \n",
      "that kinda look like tumors. One of them is well-separated from the rest of the herd, \n",
      "which is awesome, because it means their interests separate them from other customers.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "178 Data Smart\n",
      "Figure 5-19: The 0.5-neighborhood adjacency matrix\n",
      "Figure 5-20: Gephi visualization of the r-neighborhood graph\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "179Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "And then there’s poor old Parker, the one customer who didn’t end up with any edges \n",
      "greater than or equal to 0.5 cosine similarity. So he’s by himself, crying in his tea. I hon-\n",
      "estly feel bad for the guy, because the layout algorithms are going to try to toss him as far \n",
      "as possible from the connected part of the graph.\n",
      "All right! So now you have a graph that you can eyeball. And in fact, just laying a graph \n",
      "out and eyeballing it—separating it into communities by inspection—isn’t half bad. You’ve \n",
      "taken high-dimensional data and distilled it into something ﬂ  at like the middle school \n",
      "dance ﬂ oor from Chapter 2. But if you had thousands of customers instead of a hundred, \n",
      "your eyeballs wouldn’t be terribly helpful. Indeed, even now, there’s a mesh of custom-\n",
      "ers in the graph who are hard to group together. Are they in one community or several?\n",
      "This is where modularity maximization comes into play. The algorithm uses these \n",
      "relationships in the graph to make community assignment decisions even when your \n",
      "eyeballs might have trouble.\n",
      "How Much Is an Edge Worth? Points and Penalties in \n",
      "Graph Modularity\n",
      "Pretend that I’m a customer hanging out in my graph, and I want to know who belongs \n",
      "in a community with me.\n",
      "How about that lady who’s connected to me by an edge? Maybe. Probably. We are con-\n",
      "nected after all.\n",
      "How about the guy on the other side of the graph who shares no edge with me? Hmmm, \n",
      "it’s much less likely.\n",
      "Graph modularity quantiﬁ es this gut feeling that communities are deﬁ ned by connections. \n",
      "The technique assigns scores to each pair of nodes. If two nodes aren’t connected, I need \n",
      "to be penalized for putting them in a community. If two nodes are connected, I need to be \n",
      "rewarded. Whatever community assignment I make, the modularity of the graph is driven \n",
      "by the sum of those scores for each pair of nodes that ends up in a community together.\n",
      "Using an optimization algorithm (you knew Solver was coming!), you can “try out” dif-\n",
      "ferent community assignments on the graph and see which one rakes in the most points \n",
      "with the fewest penalties. This will get you a winning modularity score.\n",
      "What’s a Point and What’s a Penalty?\n",
      "In modularity maximization you give yourself one point every time you cluster two nodes \n",
      "that share an edge in the adjacency matrix. You get zero points every time you cluster \n",
      "those who don’t.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "180 Data Smart\n",
      "Easy.\n",
      "What about penalties?\n",
      "This is where the modularity maximization algorithm really gets creative. Consider \n",
      "again the Friends graph, originally pictured in Figure 5-1.\n",
      "Modularity maximization bases its penalties for putting two nodes together on one \n",
      "question:\n",
      "If you had this graph and you erased the middle of each edge and “rewired” it a bunch of \n",
      "times at random, what is the expected number of edges you’d get between two nodes? \n",
      "That expected number of edges is the penalty.\n",
      "Why is the expected number of edges between two nodes the penalty? Well, you don’t \n",
      "want to reward the model as much for clustering people based on a relationship that was \n",
      "likely to happen anyway because both parties are extremely social. \n",
      "I want to know how much of that graph is intentional  relationship and connection, \n",
      "and how much of it is just because, “Yeah, well, Chandler’s connected to a lot of people, \n",
      "so odds are Phoebe would be one of them.” This means that edges between two highly \n",
      "selective individuals are “less random” and worth more than edges between two socialites.\n",
      "To understand this more clearly, look at a version of the Friends graph in which I’ve \n",
      "erased the middle of each edge. These half-edges are called stubs. See Figure 5-21.\n",
      "Ross Rachel\n",
      "MonicaJoey\n",
      "Chandler Phoebe\n",
      "Figure 5-21: Stubby Friends graph\n",
      "Now, think about wiring the graph up randomly. In Figure 5-22, I’ve drawn an ugly \n",
      "random rewiring. And yes, in a random rewiring it’s totally possible to connect someone \n",
      "to him or herself if they have multiple stubs coming out of them. Trippy.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "181Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "Ross Rachel\n",
      "MonicaJoey\n",
      "Chandler Phoebe\n",
      "Figure 5-22: A rewiring of the Friends graph\n",
      "Figure 5-22 is just one way to wire it up, right? There are tons of possibilities even \n",
      "with a graph with just ﬁ ve edges. Notice that Ross and Rachel were chosen. What were \n",
      "the odds of that happening? Based on that probability, what is the expected number of \n",
      "edges between the two if you rewired the graph randomly over and over and over again?\n",
      "Well, when drawing a random edge, you need to select two stubs at random. So what’s \n",
      "the probability that a node’s stubs will be selected?\n",
      "In the case of Rachel, she has three stubs out of a total of ten (two times the number \n",
      "of edges) on the graph. Ross has one stub. So the probability that you’d select Rachel for \n",
      "any edge is 30 percent, and the probability that you’d select Ross’s stub for any edge is 10 \n",
      "percent. The node selection probabilities are shown in Figure 5-23.\n",
      "Ross Rachel\n",
      "MonicaJoey\n",
      "Chandler Phoebe\n",
      "1\n",
      "10\n",
      "1\n",
      "10\n",
      "3\n",
      "10\n",
      "3\n",
      "10\n",
      "1\n",
      "10\n",
      "1\n",
      "10\n",
      "Figure 5-23: Node selection probabilities on the Friends graph\n",
      "So if you were randomly selecting nodes to link up, you could select Ross and then \n",
      "Rachel or Rachel and then Ross. That’s roughly 10 percent times 30 percent or 30 percent \n",
      "times 10 percent, which is 2 times 0.3 times 0.1. That comes out to 6 percent.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "182 Data Smart\n",
      "But you’re not drawing just one edge, are you? You need to draw a random graph with \n",
      "ﬁ ve edges, so you get ﬁ ve tries to pick that combo. The expected number of edges between \n",
      "Ross and Rachel then is roughly 6 percent times 5, or 0.3 edges. Yes, that’s right, expected \n",
      "edges can be fractional. \n",
      "Did I just blow your mind Inception-style? Think of it like this. If I ﬂ ip a Sacagawea dol-\n",
      "lar coin, which you get to keep if it lands on heads but not tails, then ﬁ fty percent of the \n",
      "time you’re going to get a dollar and ﬁ fty percent of the time you get nothing. Your expected \n",
      "payoff  is 0.5 * $1 = $0.50, even though you’ll never actually win ﬁ fty cents in a game.\n",
      "Similarly here, you’ll only ever encounter graphs where Ross and Rachel are or are not \n",
      "connected, but their expected edge value is nevertheless 0.3.\n",
      "Figure 5-24 shows these calculations in detail.\n",
      "Ross Rachel\n",
      "Probability of getting Ross-Rachel:\n",
      "Expected number of Ross-Rachel connections:MonicaJoey\n",
      "Chandler Phoebe\n",
      "1\n",
      "10\n",
      "1\n",
      "10\n",
      "3\n",
      "10\n",
      "3\n",
      "10\n",
      "1\n",
      "10\n",
      "1\n",
      "10\n",
      "2 # Ross Stubs\n",
      "2 ∗ # Edges\n",
      "# Rachel Stubs\n",
      "2 ∗ # Edges\n",
      "= 2 1\n",
      "10\n",
      "3\n",
      "10\n",
      "# Ross Stubs ∗ # Rachel Stubs\n",
      "2 ∗ # Edges\n",
      "= 3\n",
      "10\n",
      "2 ∗ Edges # Ross Stubs\n",
      "2 ∗ # Edges\n",
      "# Rachel Stubs\n",
      "2 ∗ # Edges\n",
      "= \n",
      "Figure 5-24: The expected number of edges between Ross and Rachel\n",
      "Bringing the points and penalties together, things should now become clear.\n",
      "If you put Ross and Rachel in a community together, you don’t get a full 1 point. This is \n",
      "because you get penalized 0.3 points since that’s the expected number of edges a random \n",
      "graph would have anyway. That leaves you with a score of 0.7.\n",
      "If you didn’t cluster Ross and Rachel, then you would receive 0 rather than 0.7 points.\n",
      "On the other hand, Rachel and Phoebe aren’t connected. They have the same expected \n",
      "edge value of 0.3 though. That means that if you put them in a community together, you’d \n",
      "still get the penalty but you’d receive no points, so the score would be adjusted by −0.3.\n",
      "Why? Because the fact that there’s no edge between Rachel and Phoebe means some-\n",
      "thing! The expected number of edges was 0.3 and yet this graph doesn’t have one, so the \n",
      "score should account for that possibly intentional separation.\n",
      "If you didn’t put Rachel and Phoebe in a community together, then they’d receive no \n",
      "score at all, so all things being equal, you’re best separating them into diff erent clusters.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "183Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "To sum it all up then, the points and penalties capture the amount that the graph’s \n",
      "structure deviates from the expected  graph structure. You need to assign communities \n",
      "that account for these deviations.\n",
      "The modularity of a community assignment is just the sum of these points and penalties \n",
      "for pairs of nodes placed in community together, divided by the total number of stubs in \n",
      "the graph. You divide by the number of stubs so that whatever the size of the graph, the \n",
      "maximum modularity score is 1, which facilitates comparisons across graphs.\n",
      "Setting Up the Score Sheet\n",
      "Enough talk! Let’s actually calculate these scores for each pair of customers in the graph.\n",
      "To start, let’s count how many stubs are coming out of each customer and how many \n",
      "total stubs there are in the graph. Note that the stub count of a customer is just the degree \n",
      "of the node.\n",
      "So on the r- NeighborhoodAdj tab you can count the degree of a node simply by sum-\n",
      "ming down a column or across a row. If there’s a 1, that’s an edge, hence a stub, hence it’s \n",
      "counted. So, for example, how many stubs does Adams have? In cell B102, you can just \n",
      "place the following formula to count them:\n",
      "=SUM(B2:B101)\n",
      "You get 14. Similarly, you could sum across row 2 by placing in CX2 the formula:\n",
      "=SUM(B2:CW2)\n",
      "You get 14 in that case as well, which is what you’d expect since the graph is undirected.\n",
      "Copying these formulas across and down respectively, you can count the stubs for each \n",
      "node. And by simply summing column CX in row 102, you get the total number of stubs \n",
      "for the graph. As shown in Figure 5-25, the graph has a total of 858 stubs.\n",
      "Now that you have the stub counts, you can create a Scores tab in your workbook \n",
      "where you place the customers’ names across row 1 and down column A, just as in the \n",
      "r-NeighborhoodAdj tab.\n",
      "Consider cell B2, which is the score for Adams connecting with himself. Does this \n",
      "get one point or none? Well, you can read in the value from the adjacency matrix, \n",
      "'r-NeighborhoodAdj'!B2, and you’re done. If the adjacency matrix is a 1, it’s copied in. \n",
      "Simple.\n",
      "As for the expected edge calculation that you need to tack on as a penalty, you can \n",
      "calculate it the same way that was shown in Figure 5-24:\n",
      "# stubs customer A * # stubs customer B / Total stubs \n",
      "By bringing these points and penalties together in cell B2, you end up with this formula:\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "184 Data Smart\n",
      "='r-NeighborhoodAdj'!B2 – \n",
      "(('r-NeighborhoodAdj'!$CX2*'r-NeighborhoodAdj'!B$102)/\n",
      "'r-NeighborhoodAdj'!$CX$102)\n",
      "Figure 5-25: Counting edge stubs on the r-Neighborhood graph\n",
      "You have the 0/1 adjacency score minus the expected count.\n",
      "Note that the formula uses absolute cell references on the stub values so that when you \n",
      "drag the formula, everything changes appropriately. Thus, dragging the formula across \n",
      "and down the Scores tab, you end up with the values shown in Figure 5-26.\n",
      "Figure 5-26: The Scores tab\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "185Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "To drive this score home, check out cell K2. This is the score for an Adams/Brown \n",
      "clustering. It’s 0.755.\n",
      "Adams and Brown share an edge on the adjacency matrix so you get 1 point for cluster-\n",
      "ing them ('r-NeighborhoodAdj'!K2 in the formula), but Adams has a stub count of 14 \n",
      "and Brown is a 15, so their expected edge count is 14 * 15 / 858. That second part of the \n",
      "formula looks like this:\n",
      "(('r-NeighborhoodAdj'!$CX2*'r-NeighborhoodAdj'!K$102)/\n",
      "'r-NeighborhoodAdj'!$CX$102) \n",
      "which comes out to 0.245. Bringing it all together, you get 1 - 0.245 = 0.755 for the score.\n",
      "Let’s Get Clustering!\n",
      "You now have the scores you need. All you need to do now is set up an optimization model \n",
      "to ﬁ nd optimal community assignments.\n",
      "Now, I’m going to be honest with you up front. Finding optimal communities using \n",
      "graph modularity is a more intense optimization setup than what you encountered in \n",
      "Chapter 2. This problem is often solved with complex heuristics such as the popular \n",
      "“Louvain” method (see \n",
      "http://perso.uclouvain.be/vincent.blondel/research/\n",
      "louvain.html for more info), but this is a code-free zone, so you’re going to make do with \n",
      "Solver.\n",
      "To make this possible, you’re going to attack the problem using an approach called \n",
      "divisive clustering or hierarchical partitioning. All that means is that you’re going to set up \n",
      "the problem to ﬁ nd the best way to split the graph into two communities. Then you’re \n",
      "going to split those two into four, and on and on until Solver decides that the best way to \n",
      "maximize modularity is to stop dividing the communities.\n",
      "NOTE\n",
      "Divisive clustering is the opposite of another often-used approach called \n",
      "agglomerative clustering. In agglomerative clustering, each customer starts in their \n",
      "own cluster, and you recursively glom together the two closest clusters until you \n",
      "reach a stopping point.\n",
      "Split Number 1\n",
      "All right. So you start this divisive clustering process by dividing the graph into two com-\n",
      "munities so the modularity score is maximized.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "186 Data Smart\n",
      "First create a new sheet called Split1 and paste customers down column A. Each cus-\n",
      "tomer’s community assignment will go in column B, which you should label Community. \n",
      "Since you’re splitting the graph in half, have the Community column be a binary decision \n",
      "variable in Solver, where the 0/1 value will denote whether you’re in community 0 or \n",
      "community 1. Neither community is better than the other. There’s no shame in being a 0.\n",
      "Scoring Each Customer’s Community Assignment\n",
      "In column C, you’re going to calculate the scores you get by placing each customer in \n",
      "their respective community. By that, I mean if you place Adams in community 1, you’ll \n",
      "calculate his piece of the total modularity score by summing all the values from his row \n",
      "in the Scores tab whose customer columns also landed in community 1.\n",
      "Consider how you’d add these scores in a formula. If Adams is in community 1, you need \n",
      "to sum all values from the Scores tab on row 2 where the corresponding customer in the \n",
      "optimization model is also assigned a 1. Because assignment values are 0/1, you can use \n",
      "SUMPRODUCT to multiply the community vector by the score vector and then sum the result.\n",
      "Although the score values go across the Scores tab, in the optimization model, the \n",
      "assignments go top to bottom, so you need to TRANSPOSE the score values in order to make \n",
      "this work (and using TRANSPOSE means making this an array formula): \n",
      "{=SUMPRODUCT(B$2:B$101,TRANSPOSE(Scores!B2:CW2))}  \n",
      "The formula simply multiplies the Scores values for Adams times the community assign-\n",
      "ments. Only scores matching community assignment 1 stay, whereas the others get set \n",
      "to 0. The \n",
      "SUMPRODUCT just sums everything.\n",
      "But what if Adams were assigned to community 0? You need only ﬂ ip the community \n",
      "assignments by subtracting them from 1 in order to make the sum of scores work.\n",
      "{=SUMPRODUCT(1-(B$2:B$101),TRANSPOSE(Scores!B2:CW2))}\n",
      "In an ideal world, you could put these two together with an IF formula that checks \n",
      "Adams’ community assignment and then uses one of these two formulas to sum up the \n",
      "correct neighbors’ scores. But in order to use an \n",
      "IF formula, you need to use the non-linear \n",
      "solver (see Chapter 4 for details), and in this particular case, maximizing modularity is too \n",
      "hard for the non-linear solver to handle effi  ciently. You need to make the problem linear.\n",
      "Making the Score Calculation into a Linear Model\n",
      "If you read Chapter 4, you’ll recall a method for modeling the IF formula using linear \n",
      "constraints, called a “Big M” constraint. You’re going to use this tool here.\n",
      "Both of the previous two formulas are linear; so what if you just set a score variable \n",
      "for Adams to be less than both of them? You’re trying to maximize the total modularity \n",
      "scores, so Adams’ score will want to rise until it bumps up against the lowest of these two \n",
      "constraining formulas.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "187Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "But how do you know which score calculation corresponding to Adams’ actual com-\n",
      "munity assignment is the lowest? You don’t.\n",
      "To ﬁ  x that, you need to deactivate  whichever of those two formulas isn’t in play. If \n",
      "Adams is assigned a 1, the ﬁ rst formula becomes an upper bound and the second formula \n",
      "is turned off . If Adams is a zero, you have the opposite.\n",
      "How do you turn off  one of the two upper bounds? Add a “Big M” to it— just big enough \n",
      "that its bound is meaningless, because the legit bound is lower.\n",
      "Consider this modiﬁ cation to the ﬁ rst formula:\n",
      "{=SUMPRODUCT(B$2:B$101,TRANSPOSE(Scores!B2:CW2))+ \n",
      "(1-B2)*SUM(ABS(Scores!B2:CW2))}\n",
      "If Adams is assigned to community 1, the addition you made at the end of the formula \n",
      "turns to 0 (because you’re multiplying by 1-B2). In this way, the formula becomes identical \n",
      "to the ﬁ rst one you examined. But if Adams gets assigned to community 0, this formula \n",
      "no longer applies and needs to be turned off  . So the \n",
      "(1-B2)*SUM(ABS(Scores!B2:CW2)  \n",
      "piece of the formula adds one times the sum of all the absolute values of the scores Adams \n",
      "could possibly get, which guarantees the formula is higher than its ﬂ ipped version that’s \n",
      "now in play:\n",
      "{=SUMPRODUCT(1-(B$2:B$101),TRANSPOSE(Scores!B2:CW2))+\n",
      "B2*SUM(ABS(Scores!B2:CW2))} \n",
      "All you’re doing is setting Adams’ score to be less than or equal to the correct calcula-\n",
      "tion and removing the other formula from consideration by making it larger. It’s a ghetto-\n",
      "hacked \n",
      "IF statement.\n",
      "Thus, in column C you can create a score column that will be a decision variable, \n",
      "whereas in columns D and E in the spreadsheet you can place these two formulas as upper \n",
      "bounds on the score (see Figure 5-27).\n",
      "Figure 5-27: Adding two upper bounds to each customer’s score variable\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "188 Data Smart\n",
      "Note that in the formula absolute references are used on the community assignment \n",
      "range, so that as you drag the formulas down, nothing shifts.\n",
      "Summing the scores in cell G2 for each eventual community assignment in col-\n",
      "umn C, you get the total score, which you can normalize by the total stub count in \n",
      "'r-NeighborhoodAdj'!CX102 in order to get the modularity calculation:\n",
      "=SUM(C2:C101)/'r-NeighborhoodAdj'!CX102\n",
      "This gives the sheet shown in Figure 5-28.\n",
      "Figure 5-28: Filled out Split1 tab, ready for optimization \n",
      "Setting Up the Linear Program\n",
      "Now everything is set up for optimizing. Open the Solver window and specify that you’re \n",
      "maximizing the graph modularity score in cell G2. The decision variables are the com-\n",
      "munity assignments in B2:B101 and their modularity scores are in C2:C101.\n",
      "You need to add a constraint forcing the community assignments in B2:B101 to be \n",
      "binary. Also, you need to make the customer score variables in column C less than both \n",
      "the upper bounds in columns D and E.\n",
      "As shown in Figure 5-29, you can then set all the variables to be non-negative with the \n",
      "checkbox and select Simplex LP as the optimization algorithm.\n",
      "But wait. There’s more!\n",
      "One of the problems with using a “Big M” constraint is that Solver often has trouble \n",
      "conﬁ rming it’s actually found the optimal solution. So it’ll just sit there and spin its wheels \n",
      "even though it’s got a great solution in its back pocket. To prevent that from happening, \n",
      "press the Options button in Solver and set the Max Subproblems value to 15,000. That \n",
      "ensures that Solver quits after about 20 minutes on my laptop. \n",
      "Go ahead and press Solve—regardless of whether you’re using Solver or OpenSolver \n",
      "(see the nearby sidebar) when the algorithm terminates due to a user-deﬁ  ned limit, it\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "189Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "may tell you that while it found a feasible solution, it didn’t solve to optimality. This just \n",
      "means that the algorithm didn’t prove optimality (similar to how non-linear solvers are \n",
      "unable to prove optimality), but in this case, your solution should be strong nonetheless.\n",
      "Figure 5-29: The LP formulation for the ﬁ  rst split\n",
      "Once you have a solution, the Split1 tab should appear as in Figure 5-30.\n",
      "EXCEL 2010 AND 2013 MUST USE OPENSOLVER\n",
      "If you’re in Excel 2010 or 2013 on Windows, this problem is too hard for the Solver \n",
      "provided you, and you’ll need to use OpenSolver, as discussed in Chapters 1 and 4.\n",
      "If you use OpenSolver, set up the problem with regular Solver, but before solving, \n",
      "open the OpenSolver plugin to beef up your system. OpenSolver has the same diffi  culty \n",
      "with “Big M” constraints, so before running the model, click the OpenSolver options \n",
      "button and set the time limit to 300 seconds. If you don’t do this, the default run time \n",
      "on OpenSolver is really high, and it may just spin its wheels, forcing you to kill Excel.\n",
      "If you’re in Excel 2007 or Excel 2011 for Mac, you’re good to go with vanilla \n",
      "Solver, although if you’d like to use OpenSolver with Excel 2007, you can. If you’re in \n",
      "LibreOffi  ce, you should be just ﬁ ne.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "190 Data Smart\n",
      "Figure 5-30:  Optimal solution for the ﬁ  rst split\n",
      "My Solver run came up with 0.464 for the modularity; your solution may be better if \n",
      "you use OpenSolver. Running down column B, you can see who ended up in community \n",
      "0 and who’s in community 1. The question then is, are you done? Are there only two com-\n",
      "munities or are there more?\n",
      "In order to answer that question, you need to try to split these two communities up. \n",
      "If you’re done, Solver won’t have any of it. But if making three or four communities from \n",
      "these two improves modularity, well, then Solver is going to do it.\n",
      "Split 2: Electric Boogaloo\n",
      "All right. Split these communities up like you’re doing cell division. You start by making \n",
      "a copy of the Split1 tab and calling it Split2.\n",
      "The ﬁ rst thing you need to do is insert a new column after the community values in \n",
      "column B. Label this new column C Last Run and copy the values over from B into C. \n",
      "This gives the sheet pictured in Figure 5-31.\n",
      "In this model, the decisions are the same—customers are given a 1 or a 0. But you need \n",
      "to keep in mind that if two customers are given 1s this time around they’re not necessarily \n",
      "in the same community. If one of them was in community 0 on the ﬁ rst run and the other \n",
      "was in community 1, they’re in two diff erent communities.\n",
      "In other words, the only scores Adams might get for being in, say, community 1-0 \n",
      "are from those customers who were also placed in community 0 on the ﬁ  rst split and in \n",
      "community 1 on the second. Thus, you need to change the upper bounds on the score\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "191Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "calculation. The score calculation for column E (here you show E2) then requires a check \n",
      "against the previous run in column C:\n",
      "{=SUMPRODUCT(B$2:B$101,IF(C$2:C$101=C2,1,0),TRANSPOSE(Scores!B2:CW2))}\n",
      "Figure 5-31: The Split2 tab with previous run values \n",
      "The IF statement IF(C$2:C$101=C2,1,0) prevents Adams from getting points unless \n",
      "his neighbors are with him on the ﬁ rst split.\n",
      "You can use an IF statement here, because column C isn’t a decision variable this time \n",
      "around. That split was ﬁ xed on the last run, so there’s nothing non-linear about this. You \n",
      "can add the same IF statement into the “Big M” part of the formula to make the ﬁ  nal \n",
      "calculation in column E:\n",
      "=SUMPRODUCT(B$2:B$101,IF(C$2:C$101=C2,1,0),TRANSPOSE(Scores!B2:CW2))+\n",
      "(1-B2)*SUMPRODUCT(IF(C$2:C$101=C2,1,0),TRANSPOSE(ABS(Scores!B2:CW2)))\n",
      "Similarly, you can add the same IF statements into the second upper bound in column F:\n",
      "=SUMPRODUCT(1-(B$2:B$101),IF(C$2:C$101=C2,1,0),TRANSPOSE(Scores!B2:CW2))\n",
      "+B2*SUMPRODUCT(IF(C$2:C$101=C2,1,0),TRANSPOSE(ABS(Scores!B2:CW2)))\n",
      "All you’ve done is silo-ed the problem—those who were split into community 0 the \n",
      "ﬁ rst time around have their own little world of scores to play with and the same goes for \n",
      "those who ended up in 1 the ﬁ rst time.\n",
      "And here’s the cool part—you don’t have to change the Solver formulation at all! Same \n",
      "formulation, same options! If you’re using OpenSolver, it may not have saved your maxi-\n",
      "mum time limit options from the previous tab. Reset the option to three hundred seconds. \n",
      "Solve again.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "192 Data Smart\n",
      "In my run on Split2, I ended up with a ﬁ nal modularity of 0.546 (see Figure 5-32), which \n",
      "is a substantial improvement over 0.464. That means that splitting was a good idea. (Your \n",
      "solution may end up diff erent and possibly better.)\n",
      "Figure 5-32: The optimal solution for Split2\n",
      "And…Split 3: Split with a Vengeance\n",
      "Okay, so should you stop here or should you keep going? The way to tell is to split again, \n",
      "and if Solver can’t do better than 0.546, you’re through.\n",
      "Start by creating a Split3 tab, renaming Last Run to Last Run 2, and then inserting a \n",
      "new Last Run in column C. Then copy the values from column B into C.\n",
      "Add more IF statements to the upper bounds to check for community assignments in \n",
      "the previous run. For example, F2 becomes:\n",
      "=SUMPRODUCT(B$2:B$101,\n",
      "IF(D$2:D$101=D2,1,0),IF(C$2:C$101=C2,1,0),\n",
      "TRANSPOSE(Scores!B2:CW2))+\n",
      "(1-B2)*SUMPRODUCT(\n",
      "IF(C$2:C$101=C2,1,0),IF(D$2:D$101=D2,1,0),\n",
      "TRANSPOSE(ABS(Scores!B2:CW2)))\n",
      "Once again, the Solver formulation doesn’t change. Reset your maximum solving time \n",
      "if need be, press Solve, and let the model run its course. In the case of my model, I saw \n",
      "no improvement in modularity (see Figure 5-33).\n",
      "Splitting again added nothing, so this means that modularity was eff ectively maximized \n",
      "on Split2. Let’s take the cluster assignments from that tab and investigate.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "193Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "Encoding and Analyzing the Communities\n",
      "In order to investigate these community assignments, the ﬁ rst thing you should do is take \n",
      "this binary tree that’s been created by the successive splits and turn those columns into \n",
      "single cluster labels.\n",
      "Create a tab called Communities and paste the customer name, community, and last \n",
      "run values from the Split2 tab. You can rename the two binary columns Split2 and Split1. \n",
      "To turn their binary values into single numbers, Excel provides a nifty binary-to-decimal \n",
      "formula called \n",
      "BIN2DEC. So in column D, starting at D2, you can add:\n",
      "=BIN2DEC(CONCATENATE(B2,C2))\n",
      "Figure 5-33: No modularity improvement in Split 3\n",
      "Copying that formula down, you get the community assignments shown in Figure 5-34 \n",
      "(your assignments may vary depending on Solver).\n",
      "Figure 5-34:  Final community labels for modularity maximization\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "194 Data Smart\n",
      "You get four clusters with labels 0 to 3 out of the decimal encoding. So what are these \n",
      "four optimal clusters? Well, you can ﬁ nd out in the same way you delved into clusters in \n",
      "Chapter 2—by investigating the most popular purchases of their members.\n",
      "To begin, just as in Chapter 2, create a tab called TopDealsByCluster and paste the deal \n",
      "information from columns A through G on the Matrix tab. Next to the matrix, place the \n",
      "cluster labels 0, 1, 2, and 3 in columns H through K. This gives you the sheet pictured in \n",
      "Figure 5-35.\n",
      "Figure 5-35: The initial TopDealsByCluster tab\n",
      "For label 0 in column H, you now want to look up all customers on the Communities \n",
      "tab who have been assigned to community 0 and sum how many of them took each deal. \n",
      "Just as in Chapter 2 and in the previous Split tabs, you use \n",
      "SUMPRODUCT with an IF state-\n",
      "ment to achieve this:\n",
      "{=SUMPRODUCT(IF(Communities!$D$2:$D$101=TopDealsByCluster!H$1,1,0),\n",
      "    TRANSPOSE(Matrix!$H2:$DC2))} \n",
      "In this formula you check which customers match the 0 in the column label at H1, and \n",
      "when they do match, you sum whether or not they took the ﬁ rst deal by checking H2:DC2 \n",
      "on the Matrix tab. Note that you use TRANSPOSE in order to orient everything vertically. \n",
      "This means you have to make the calculation an array formula.\n",
      "Note that you’ve used absolute references on the customer community assignments, \n",
      "the header rows, and the purchase matrix columns. This allows you to drag the formula \n",
      "to the right and down, giving you a full picture of the popular purchases for each cluster \n",
      "(see Figure 5-36).\n",
      "Just as in Chapter 2, you need to apply ﬁ  ltering to the sheet and sort by descending \n",
      "deal count on community 0 in column H. This gives you Figure 5-37, the low-volume\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "195Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "customer community (your clusters may vary in their order and composition depending \n",
      "on the solution Solver terminated with at each step).\n",
      "Figure 5-36: TopDealsByCluster with completed purchase counts\n",
      "Figure 5-37: Top deals for community 0\n",
      "Sorting by community 1, you get what appears to be the high-volume French Champagne \n",
      "cluster (see Figure 5-38). Fascinating.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "196 Data Smart\n",
      "Figure 5-38:  Poppin’ bottles in community 1\n",
      "As for community 2, it looks similar to community 0, except that the March Espumante \n",
      "deal is the main driver (see Figure 5-39).\n",
      "Figure 5-39: People who liked the March Espumante deal\n",
      "And for community 3, it’s the Pinot Noir folks. Haven’t you ever heard of Cabernet \n",
      "Sauvignon, people!? Admittedly, I have a terrible palate for wine. See Figure 5-40.\n",
      "That’s it! You have four clusters, and honestly, three of them make perfect sense, \n",
      "although I suppose it’s possible that you have a group of people who really just love \n",
      "Espumante in March. And you may get that in your work—some indecipherable outlier \n",
      "clusters.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "197Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "Figure 5-40:  Pinot peeps\n",
      "Note how similar this solution is to the clusters found in Chapter 2, however. In \n",
      "Chapter 2, you used a whole diff erent methodology by keeping each customer’s deal vector \n",
      "in the mix and using it to measure their distances from a cluster center. Here, there’s no \n",
      "concept of a center and even which deals a customer has purchased have been obfuscated. \n",
      "What’s important is the distance to other customers.\n",
      "There and Back Again: A Gephi Tale\n",
      "Now that you’ve gone through the entire clustering process, I’d like to show you that same \n",
      "process in Gephi. In Figure 5-20, you examined a laid out export of the r-Neighborhood \n",
      "graph into Gephi, which I return to in this section.\n",
      "This next step is going to make you envious, but here it goes. In Excel you had to solve \n",
      "for the optimal graph modularity using divisive clustering. In Gephi, there’s a Modularity \n",
      "button. You’ll ﬁ nd it on the right side of the window in the Network Overview section of \n",
      "the Statistics tab. \n",
      "When you press the Modularity button, a settings window opens. You needn’t use edge \n",
      "weights since you exported an adjacency matrix (see Figure 5-41 for the Gephi modular-\n",
      "ity settings window).\n",
      "Press OK. The modularity optimization will run using an approximation algorithm \n",
      "that’s blindingly fast. A report is then displayed with a total modularity score of 0.549 \n",
      "as well as the size of each detected cluster (see Figure 5-42). Note that if you run this in \n",
      "Gephi, the solution may come out diff erent since the calculation is randomized.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "198 Data Smart\n",
      "Figure 5-41:  Gephi modularity settings\n",
      "Figure 5-42:  Modularity score from Gephi\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "199Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "Once you have your clusters from Gephi, you can do a few things with them. \n",
      "First, you can recolor the graph using the modularity. Just as you resized the Friends \n",
      "graph using node degree, you can navigate to the Ranking window in the upper left of \n",
      "window in Gephi and go into the Nodes section. From there, you can select Modularity \n",
      "Class from the drop-down menu, pick any color scheme you want, and press Apply to \n",
      "recolor the graph (see Figure 5-43).\n",
      "Figure 5-43:  Customer graph recolored to show modularity clusters\n",
      "Cool! You can now see that the two “tumor-esque” parts of the graph are indeed com-\n",
      "munities. The spread-out middle section of the graph was divided into three clusters. And \n",
      "poor Parker was placed in his own cluster, unconnected to anyone. How lonely and sad.\n",
      "The second thing you can do with the modularity information is export it back into \n",
      "Excel to examine it, just as you did with your own clusters. To accomplish this, go into \n",
      "the Data Laboratory tab you visited earlier in Gephi. You’ll notice that the modularity \n",
      "classes have already been populated as a column in the Nodes data table. Pressing the \n",
      "Export Table button, you can select the label and modularity class columns to dump to \n",
      "a CSV ﬁ le (see Figure 5-44).\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "200 Data Smart\n",
      "Figure 5-44:  Exporting modularity classes back to Excel\n",
      "Press OK on the export window to export your modularity classes to a CSV wherever \n",
      "you like and then open that ﬁ  le in Excel. From there, you can create a new tab in the \n",
      "main workbook called CommunitiesGephi, where you can paste the classes Gephi has \n",
      "found for you (see Figure 5-45). You’ll need to use the ﬁ lter capability in Excel to sort your \n",
      "customers by name just as they are in the rest of the workbook.\n",
      "Just for kicks, let’s conﬁ rm that this clustering really does beat the original score in \n",
      "column C. You’re not bound by linear modeling constraints anymore, so you can total \n",
      "each customer’s modularity scores using the following formula (shown here using our \n",
      "favorite customer, Adams, in cell C2):\n",
      "{=SUMPRODUCT(IF($B$2:$B$101=B2,1,0),TRANSPOSE(Scores!B2:CW2))} \n",
      "The formula merely checks for customers in the same cluster using an IF statement, \n",
      "gives those customers 1s and all else 0s, and then uses a SUMPRODUCT to sum their modu-\n",
      "larity scores.\n",
      "You can double-click this formula to send it down column C. Summing the column in \n",
      "cell E2 and dividing through by the total stub count from 'r-NeighborhoodAdj'!CX102,\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "201Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "you do indeed get a modularity score of 0.549 (see Figure 5-46). So Gephi’s heuristic has \n",
      "beat out the divisive clustering heuristic by 0.003. Oh well! Pretty close. (If you used \n",
      "OpenSolver, you may actually be able to beat Gephi.)\n",
      "Figure 5-45:  Gephi modularity classes back in Excel\n",
      "Figure 5-46:  Reproducing the modularity score for the communities detected by Gephi\n",
      "Let’s see which clusters Gephi actually came up with. To start, let’s make a copy of the \n",
      "TopDealsByCluster tab, which you should rename TopDealsByClusterGephi. Once you’ve \n",
      "made a copy, sort the deals back in order by column A and drop the ﬁ  ltering placed on \n",
      "the table. Now, in Gephi’s clustering, you have six clusters with labels 0 through 5 (your \n",
      "results may be diff erent since Gephi uses a randomized algorithm), so let’s add 4 and 5 \n",
      "to the mix in columns L and M.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "202 Data Smart\n",
      "The formula in cell H2 need only be modified to reference column B on the  \n",
      "CommunitiesGephi tab instead of column D on the Communities tab. You can then drag \n",
      "this formula to the rest of the sheet, yielding Figure 5-47.\n",
      "Figure 5-47: Top purchases per cluster from Gephi\n",
      "If you sort once again by column, you see the all too familiar clusters—low volume, \n",
      "sparkling wine, Francophiles, Pinot people, high volume, and last but not least, Parker \n",
      "by himself.\n",
      "Wrapping Up\n",
      "In Chapter 2, you looked at k-means clustering. Using the same data in this chapter, you \n",
      "tackled network graphs and clustering via modularity maximization. You should feel \n",
      "pretty good about your data mining chops by now. In more detail, here are some items \n",
      "you learned:\n",
      "• How network graphs are visually represented as well as how they’re represented \n",
      "numerically using adjacency and affi  nity matrices\n",
      "• How to load a network graph into Gephi to augment Excel’s visualization deﬁ ciencies\n",
      "• How to prune edges from network graphs via the r-neighborhood graph. You also \n",
      "learned the concept of a kNN graph, which I recommend you go back and tinker \n",
      "with.\n",
      "• The deﬁ nitions of node degree and graph modularity and how to calculate modular-\n",
      "ity scores for grouping two nodes together\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "203Cluster Analysis Part II: Network Graphs and Community Detection  \n",
      "• How to maximize graph modularity using a linear optimization model and divisive \n",
      "clustering\n",
      "• How to maximize graph modularity in Gephi and export the results\n",
      "Now, you may be wondering, “John, why in the world did you take me through that \n",
      "graph modularity maximization process when Gephi does it for me?”\n",
      "Remember, the point of this book is not to press buttons blindly, without understanding \n",
      "what they do. Now you know how to construct and prep graph data for cluster detection. \n",
      "And you know how community detection on graph data works. You’ve done it. So next \n",
      "time you do this, even if you’re just pushing a button, you’ll know what’s going on behind \n",
      "the scenes, and that level of understanding and conﬁ dence in the process is invaluable.\n",
      "Although Gephi is one of the best places to do this analysis, if you’re looking for a place \n",
      "to code with graph data, the igraph library, which has hooks in R and Python, is excellent \n",
      "for working with network graphs. \n",
      "Also worth mentioning are the Neo4J and Titan graph databases. These databases are \n",
      "designed to store graph data for querying later, whether that query is something as simple \n",
      "as “get John’s friends’ favorite ﬁ lms” or as complex as “ﬁ nd the shortest path on Facebook \n",
      "between John and Kevin Bacon.”\n",
      "So that’s it. Go forth, graph, and ﬁ nd commu nities!\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "6\n",
      "Wait, What? You’re Pregnant?\n",
      "I\n",
      "n a recent Forbes article, it was reported that Target had created an artificial intelligence \n",
      "(AI) model that could predict when a customer was pregnant and use that information \n",
      "to start targeting them with pregnancy-related marketing and offers. New parents blow a \n",
      "lot of money on the accouterments of child rearing, and what better time to turn them into \n",
      "loyal customers than before the baby even shows up? They’ll be buying the store brand \n",
      "diapers for years!\n",
      "This story about Target is just one of many that have peppered the press recently. Watson \n",
      "won Jeopardy!. Netﬂ ix off ered a million dollar prize to improve its recommendation system. \n",
      "The Obama re-election campaign used artiﬁ cial intelligence to help direct ground, online, \n",
      "and on the air media and fundraising operations. And then there’s Kaggle.com, where \n",
      "competitions are popping up to predict everything from whether a driver is getting sleepy \n",
      "to how much a grocery shopper will spend on groceries. \n",
      "But those are only the headline-catching applications. AI is useful across nearly any \n",
      "industry you can think of. Your credit card company uses it to identify odd transactions \n",
      "on your account. The enemy in your shoot-em-up Xbox game runs on AI. There’s e-mail \n",
      "spam ﬁ ltering, tax fraud detection, spelling auto-correction, and friend recommendation \n",
      "on social networks. \n",
      "Quite simply, a good AI model can help a business make better decisions, market better, \n",
      "increase revenue, and decrease costs. An AI model can help your sales and support staff   \n",
      "prioritize leads and support calls. AI can help predict what off  ers will bring a customer \n",
      "back to your brick and mortar store. AI can identify applicants who lie on their online \n",
      "dating proﬁ le or are going to have a coronary in the next year. You name it; if there’s good \n",
      "historical data, a trained AI model can help.\n",
      "The Granddaddy of \n",
      "Supervised Artiﬁ cial \n",
      "Intelligence—\n",
      "Regression\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart206\n",
      "Don’t Kid Yourself\n",
      "Folks who don’t know how AI models work often experience some combination of awe and \n",
      "creepiness when hearing about how these models can predict the future. But to paraphrase \n",
      "the great 1992 ﬁ lm Sneakers, “Don’t kid yourself. It’s not that [intelligent].”\n",
      "Why? Because AI models are no smarter than the sum of their parts. At a simplistic level, you \n",
      "feed a supervised AI algorithm some historical data, purchases at Target for example, and \n",
      "you tell the algorithm, “Hey, these purchases were from pregnant people, and these other \n",
      "purchases were from not-so-pregnant people.” The algorithm munches on the data and \n",
      "out pops a model. In the future, you feed the model a customer’s purchases and ask, “Is \n",
      "this person pregnant?” and the model answers, “No, that’s a 26-year-old dude living in \n",
      "his mom’s basement.”\n",
      "That’s extremely helpful, but the model isn’t a magician. It just cleverly turns past data \n",
      "into a formula or set of rules that it uses to predict a future case. As we saw in the case \n",
      "of naïve Bayes in Chapter 3, it’s the AI model’s ability to recall this data and associated \n",
      "decision rules, probabilities, or coeffi  cients that make it so eff ective.\n",
      "We do this all the time in our own non-artiﬁ cially intelligent lives. For example, using \n",
      "personal historical data, my brain knows that when I eat a sub sandwich with brown-\n",
      "looking alfalfa sprouts on it, there’s a good chance I may be ill in a few hours. I’ve taken \n",
      "past data (I got sick) and trained my brain on it, so now I have a rule, formula, model, \n",
      "whatever you’d like to call it: brown sprouts = gastrointestinal nightmare.\n",
      "In this chapter, we’re going to implement two diff erent regression models just to see how \n",
      "straightforward AI can be. Regression is the granddaddy of supervised predictive modeling \n",
      "with research being done on it as early as the turn of the 19\n",
      "th century. It’s an oldie, but its \n",
      "pedigree contributes to its power—regression has had time to build up all sorts of rigor \n",
      "around it in ways that some newer AI techniques have not. In contrast to the MacGyver \n",
      "feel of naïve Bayes in Chapter 3, you’ll feel the weight of the statistical rigor of regression \n",
      "in this chapter, particularly when we investigate signiﬁ cance testing.\n",
      "Similarly to how we used the naïve Bayes model in Chapter 3, we’ll use these models \n",
      "for classiﬁ cation. However as you’ll see, the problem at hand is very diff  erent from the \n",
      "bag-of-words document classiﬁ cation problem we encountered earlier.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "207The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\n",
      "Predicting Pregnant Customers at RetailMart Using \n",
      "Linear Regression\n",
      "NOTE\n",
      "The Excel workbook used in this chapter, “RetailMart.xlsx,” is available for download \n",
      "at the book’s website at www.wiley.com/go/datasmart.This workbook includes all the \n",
      "initial data if you want to work from that. Or you can just read along using the sheets \n",
      "I’ve already put together in the workbook.\n",
      "Pretend you’re a marketing manager at RetailMart’s corporate headquarters in charge of \n",
      "infant merchandise. Your job is to help sell more diapers, formula, onesies, cribs, strollers, \n",
      "paciﬁ ers, etc. to new parents, but you have a problem.\n",
      "You know from focus groups that new parents get into habits with baby products. \n",
      "They ﬁ nd diaper brands they like early on and stores that have the best prices on their \n",
      "brands. They ﬁ nd the paciﬁ er that works with their baby, and they know where to go \n",
      "to get the cheap two-pack. You want RetailMart to be the ﬁ  rst store these new parents \n",
      "buy diapers at. You want to maximize RetailMart’s chances of being a parent’s go-to for \n",
      "baby purchases.\n",
      "But to do that, you need to market to these parents before they buy their ﬁ rst package \n",
      "of diapers somewhere else. You need to market to the parents before the baby shows up. \n",
      "That way, when the baby arrives, the parents have already received and possibly already \n",
      "used that coupon they got in the mail for diapers and ointment.\n",
      "Quite simply, you need a predictive model to help identify potential pregnant custom-\n",
      "ers for targeted direct marking. \n",
      "The Feature Set\n",
      "You have a secret weapon at your disposal for building this model: customer account \n",
      "data. You don’t have this data for every customer; no, you’re up the creek for the guy who \n",
      "lives in the woods and only pays cash. But for those who use a store credit card or have \n",
      "an online account tied to their major credit card, you can tie purchases not necessarily to \n",
      "an individual but at least to a household.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart208\n",
      "However, you can’t just feed an entire purchase history, unstructured, into an AI model \n",
      "and expect things to happen. You have to be smart about pulling relevant predictors out of \n",
      "the dataset. So the question you should ask yourself is which past purchases are predictive \n",
      "for or against a household being pregnant?\n",
      "The ﬁ rst purchase that comes to mind is a pregnancy test. If a customer buys a preg-\n",
      "nancy test, they’re more likely to be pregnant than the average customer. These predictors \n",
      "are often called model features or independent variables , while the thing we’re trying to \n",
      "predict “Pregnant (yes/no)?” would be the dependent variable  in the sense that its value is \n",
      "dependent on the independent variable data we’re pushing into the model.\n",
      "Pause a moment, and jot down your thoughts on possible features for the AI model. \n",
      "What purchase history should RetailMart consider?\n",
      "Here’s a list of example features that could be generated from a customer’s purchase \n",
      "records and associated account information:\n",
      "• Account holder is Male/Female/Unknown by matching surname to census data.\n",
      "• Account holder address is a home, apartment, or PO box.\n",
      "• Recently purchased a pregnancy test\n",
      "• Recently purchased birth control\n",
      "• Recently purchased feminine hygiene products\n",
      "• Recently purchased folic acid supplements\n",
      "• Recently purchased prenatal vitamins\n",
      "• Recently purchased prenatal yoga DVD\n",
      "• Recently purchased body pillow\n",
      "• Recently purchased ginger ale\n",
      "• Recently purchased Sea-Bands\n",
      "• Bought cigarettes regularly until recently, then stopped\n",
      "• Recently purchased cigarettes\n",
      "• Recently purchased smoking cessation products (gum, patch, etc.)\n",
      "• Bought wine regularly until recently, then stopped\n",
      "• Recently purchased wine\n",
      "• Recently purchased maternity clothing\n",
      "None of these predictors are perfect. Customers don’t buy everything at RetailMart; \n",
      "a customer might choose to buy their pregnancy test at the local drug store instead of \n",
      "RetailMart or their prenatal supplements might be prescription. Even if the customer did \n",
      "buy everything at RetailMart, pregnant households can still have a smoker or a drinker. \n",
      "Maternity clothing is often worn by non-pregnant folks, especially when the Empire waist \n",
      "is in style—thank goodness RetailMart doesn’t exist in a Jane Austen novel. Ginger ale \n",
      "may help nausea, but it’s also great with bourbon. You get the picture.\n",
      "None of these predictors are going to cut it, but the hope is that with their powers com-\n",
      "bined Captain-Planet-style, the model will be able to classify customers reasonably well.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "209The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\n",
      "Assembling the Training Data\n",
      "Six percent of RetailMart’s customer households are pregnant at any given time according \n",
      "to surveys the company has conducted. You need to grab some examples of this group \n",
      "from the RetailMart database and assemble your modeling features on their purchase \n",
      "history before they gave birth. Likewise, you need to assemble these features for a sample \n",
      "of customers who aren’t pregnant.\n",
      "Once you assemble these features for a bunch of pregnant and non-pregnant households, \n",
      "you can use these known examples to train an AI model. \n",
      "But how should you go about identifying past pregnant households in the data? \n",
      "Surveying customers to build a training set is always an option. You’re just building a \n",
      "prototype, so perhaps approximating households who just had a baby by looking at buying \n",
      "habits is good enough. For customers who suddenly began buying newborn diapers and \n",
      "continued to buy diapers of increasing size on and off  for at least a year, you can reason-\n",
      "ably assume the customer’s household has a new baby.\n",
      "So by looking at the purchase history for the customer before the diaper-buying event, \n",
      "you can assemble the features listed previously for a pregnant household. Imagine you \n",
      "pull 500 examples of pregnant households and assemble their feature data from the \n",
      "RetailMart database.\n",
      "As for non-pregnant customers, you can assemble purchase history from a random selec-\n",
      "tion of customers in RetailMart’s database that don’t meet the “ongoing diaper purchasing” \n",
      "criteria. Sure, one or two pregnant people might slip into the not-pregnant category, but \n",
      "because pregnant households only make up a small percentage of the RetailMart popu-\n",
      "lation (and that’s before excluding diaper-buyers), this random sample should be clean \n",
      "enough. Imagine you grab another 500 examples of these non-pregnant customers.\n",
      "If you plopped the 1,000 rows (500 preggers, 500 not) into a spreadsheet it’d look like \n",
      "Figure 6-1.\n",
      "Figure 6-1: Raw training data\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart210\n",
      "RESOLVING CLASS IMBALANCE\n",
      "Now, you know that only 6 percent of our customer population in the wild is preg-\n",
      "nant at any given time, but the training set you’ve assembled is 50/50. This is called \n",
      "over-sampling. Pregnancy would be the “minority” or rare class in the data, and by \n",
      "balancing the sample, the classiﬁ er you’re going to train won’t become overwhelmed \n",
      "by non-pregnant customers. After all, if you left the sample at a natural 6/94 split, \n",
      "then just labeling everyone as not pregnant leads to a 94 percent accuracy rate. \n",
      "That’s dangerous since pregnancy, while in the minority, is actually the class you \n",
      "care about marketing to.\n",
      "This rebalancing of the training data will introduce a bias to the model—it’ll think \n",
      "pregnancy is more common than it really is. But that’s ﬁ ne, because you don’t need to \n",
      "get actual probabilities of being pregnant out of the model. As you’ll see later in this \n",
      "chapter, you just need to ﬁ nd the sweet spot for pregnancy scores coming out of the \n",
      "model that balances the true positives and false positives.\n",
      "In the ﬁ rst two columns of the training dataset, you have categorical data for gender \n",
      "and address type. The rest of the features are binary where a 1 means TRUE. So for \n",
      "example, if you look at the ﬁ rst row in the spreadsheet, you can see that this customer \n",
      "was conﬁ rmed pregnant (column S). That’s the column you’re going to train the model to \n",
      "predict. And if you look at this customer’s past purchasing history, you can see that they \n",
      "purchased a pregnancy test and some prenatal vitamins. Also, they have not purchased \n",
      "cigarettes or wine recently.\n",
      "If you scroll through the data, you’ll see all types of customers, some with lots of indi-\n",
      "cators and some with little. Just as expected, pregnant households will occasionally buy \n",
      "cigarettes and wine, while non-pregnant households will buy products associated with \n",
      "pregnancy.\n",
      "Creating Dummy Variables\n",
      "You can think of an AI model as nothing more than a formula that takes numbers in, \n",
      "chews on them a bit, and spits out a prediction that should look something like the 1s \n",
      "(pregnant) and 0s (not) in column S of the spreadsheet. \n",
      "But the problem with this data is that the ﬁ  rst two columns aren’t numbers, now are \n",
      "they? They’re letters standing for categories, like male and female. \n",
      "This issue, handling categorical data, that is, data that’s grouped by a ﬁ nite number of \n",
      "labels without inherent numeric equivalents, is one that constantly nips at data miners’\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "211The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\n",
      "heels. If you send out a survey to your customers and they have to report back what line \n",
      "of work they’re in, their marital status, the country they live in, the breed of dog they \n",
      "own, or their favorite episode of Gilmore Girls, then you’re going to be stuck dealing with \n",
      "categorical data. \n",
      "This is in contrast to quantitative data , which is already numeric and ready to be \n",
      "devoured by data mining techniques.\n",
      "So what do you do to handle categorical data? Well, in short you need to make it \n",
      "quantitative.\n",
      "Sometimes, your categorical data may have a natural ordering that you can use to \n",
      "assign each category a value. For instance, if you had a variable in your dataset where \n",
      "folks reported whether they drove a Scion, a Toyota, or a Lexus, maybe you could just \n",
      "make those responses 1, 2, and 3. Voila, numbers.\n",
      "But more frequently, there is no ordering, such as with gender. For example, male, \n",
      "female, and unknown are distinct labels without a notion of ordering. In this case, it’s \n",
      "common to use a technique called dummy coding  to convert your categorical data to \n",
      "quantitative data.\n",
      "Dummy coding works by taking a single categorical column (consider the Implied \n",
      "Gender column) and turning it into multiple binary columns. You could take the Implied \n",
      "Gender column and instead have one column for male, another for female, and another \n",
      "for unknown gender. If a value in the original column were “M,” that instead could be \n",
      "coded as a 1 in the male column, a 0 in the female column, and a 0 in the unknown \n",
      "gender column.\n",
      "This is actually overkill, because if the male and female columns were both 0, then the \n",
      "unknown gender is already implied. You don’t need a third column.\n",
      "In this way, when dummy coding a categorical variable, you always need one less \n",
      "column than you have category values—the last category is always implied by the other \n",
      "values. In stats-speak, you’d say that the gender categorical variable has only two degrees \n",
      "of freedom, because the degrees of freedom are always one less than the possible values \n",
      "the variable can take.\n",
      "In this particular example, start by creating a copy of the Training Data sheet called \n",
      "Training Data w Dummy Vars . You’re going to split the ﬁ rst two predictors into two \n",
      "columns each, so go ahead and clear out column A and B and insert another two blank \n",
      "columns to the left of column A.\n",
      "Label these four empty columns Male, Female, Home, and Apt (unknown gender and \n",
      "PO box become implied). As shown in Figure 6-2, you should now have four empty col-\n",
      "umns to house the dummy coding of your two categorical variables.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart212\n",
      "Figure 6-2: Training Data w Dummy Vars tab with new columns for the dummy variables\n",
      "Consider the ﬁ rst row of training data. To turn the “M” in the gender column into \n",
      "dummy encoded data, you place a 1 in the Male column and a 0 in the Female column. \n",
      "(The 1 in the Male column naturally implies that the gender is not Unknown.)\n",
      "In cell A2 on the Training Data w Dummy Vars tab, check the old category on the \n",
      "Training Data tab and set a 1 if the category was set to “M”:\n",
      "=IF(‘Training Data’!A2=”M”,1,0)\n",
      "Same goes for values “F” in the Female column, “H” in the Home column, and “A” in \n",
      "the Apt column. To copy these four formulas down through all the rows of the training \n",
      "data, you can either drag them, or better yet, as explained in Chapter 1, highlight all four \n",
      "formulas and then double-click the bottom right corner of D2. That’ll ﬁ  ll in the sheet \n",
      "with the converted values through D1001. Once you’ve converted these two categorical \n",
      "columns into four binary dummy variables (see Figure 6-3), you’re ready to get modeling.\n",
      "Figure 6-3: Training data with dummy variables populated\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "213The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\n",
      "Let’s Bake Our Own Linear Regression\n",
      "Every time I say this, a statistician loses its wings, but I’m going to say it anyway—If \n",
      "you’re ever shoved a trendline through a cloud of points on a scatter plot, then you’ve \n",
      "built an AI model.\n",
      "You’re probably thinking, “But there’s no way! I would’ve known had I created a robot \n",
      "that could travel back in time to stop John Conner!”\n",
      "The Simplest of Linear Models\n",
      "Let me explain by showing some simple data in Figure 6-4.\n",
      "Figure 6-4:  Cat ownership versus me sneezing\n",
      "In the pictured table, you have the number of cats in a house in the ﬁ rst column and the \n",
      "likelihood that I’ll sneeze inside that house in the second column. No cats? Three percent \n",
      "of the time I sneeze any way just because I know a Platonic cat exists somewhere. Five \n",
      "cats? Well, then my sneezing is just about guaranteed. Now, we can scatter plot this data \n",
      "in Excel and look at it as shown in Figure 6-5 (For more on inserting plots and charts \n",
      "see Chapter 1).\n",
      "0\n",
      "0%\n",
      "20%\n",
      "40%\n",
      "60%\n",
      "80%\n",
      "100%\n",
      "12\n",
      "# of cats\n",
      "Likelihood I’ll sneeze in your home\n",
      "Likelihood of sneezing\n",
      "345\n",
      "Figure 6-5:  Scatter plot of cats versus sneezing\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart214\n",
      "By right-clicking on the data points in the graph (you have to right-click an actual data \n",
      "point, not just the graph itself) and selecting Add Trendline from the menu, you can select \n",
      "a linear regression model to add to the graph. Under the “Options” section of the “Format \n",
      "Trendline” window, you can select to “Display equation on chart.” Pressing OK, you can \n",
      "now see the trendline and formula for the line (Figure 6-6).\n",
      "0\n",
      "0%\n",
      "20%\n",
      "40%\n",
      "60%\n",
      "80%\n",
      "100%\n",
      "y = 0.1529x + 0.0362\n",
      "12\n",
      "# of cats\n",
      "Likelihood I’ll sneeze in your home\n",
      "Likelihood of sneezing\n",
      "345\n",
      "Figure 6-6:  Linear model displayed on the graph\n",
      "The trendline in the graph rightly shows the relationship between cats and sneezing \n",
      "with a formula of:\n",
      "Y = 0.1529x + 0.0362 \n",
      "In other words, when x is 0, the linear model thinks I’ve got about a 3-4 percent chance \n",
      "of sneezing, and the model gives me an extra 15 percent chance per cat.\n",
      "That baseline of 3-4 percent is called the intercept of the model, and the 15 percent per \n",
      "cat is called a coeffi  cient for the cats variable. Making a prediction with a linear model \n",
      "like this requires nothing more than taking my future data and combining it with the \n",
      "coeffi  cients and the intercept of the model.\n",
      "In fact, you can copy the formula \n",
      "=0.1529x+0.0362 out of the graph if you like and paste \n",
      "it in a cell to make predictions by replacing the x with an actual number., For example, \n",
      "if in the future I went into a home with three and a half cats (poor Timmy lost his hind \n",
      "paws in a boating accident), then I’d take a “linear combination” of the coeffi  cients and \n",
      "my data, add in the intercept, and get my prediction:\n",
      "0.1529*3.5 cats + 0.0362 = 0.57\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "215The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\n",
      "A 57 percent chance of sneezing! This is an AI model in the sense that we’ve taken an \n",
      "independent variable (cats) and a dependent variable (sneezing) and asked the computer \n",
      "to describe their relationship as a formula that best ﬁ ts our historical data.\n",
      "Now, you might wonder how the computer ﬁ gured this trendline out from the data. \n",
      "It looks good, but how’d it know where to put it? Basically, the computer looked for a \n",
      "trendline that best ﬁ t the data, where by best ﬁ t I mean the trendline that minimizes the \n",
      "sum of squared error with the training data.\n",
      "To get a handle on what the sum of squared error means, if you evaluate the trendline \n",
      "for one cat you get:\n",
      "0.1529*1 cat + 0.0362 = 0.1891\n",
      "But the training data gives a likelihood of 20 percent, not 18.91 percent. So then your \n",
      "error at this point on the trendline is 1.09 percent. This error value is squared to make \n",
      "sure it a positive value, regardless of whether the trendline is above or below the data \n",
      "point. 1.09 percent squared is 0.012 percent. Now if you summed each of these squared \n",
      "error values for the points in our training data, you’d get the sum of the squared error \n",
      "(often just called the sum of squares). And that’s what Excel minimized when ﬁ tting the \n",
      "trendline to the sneeze graph.\n",
      "Although your RetailMart data has way too many dimensions to toss into a scatter \n",
      "plot, in these next sections, you’ll ﬁ t the exact same type of line to the data from scratch.\n",
      "Back to the RetailMart Data\n",
      "OK, so it’s time to build a linear model like the Kitty Sneeze model on the RetailMart \n",
      "dataset. First, create a new tab called Linear Model, and paste the values from the Training \n",
      "Data w Dummy Vars tab, except when you paste it, start in column B to save room for \n",
      "some row labels in column A and on row 7 to leave space at the top of the sheet for the \n",
      "linear model’s coeffi  cients and other evaluative data you’ll be tracking.\n",
      "Paste the header row for your dependent variables again on row 1 to stay organized. \n",
      "And in column U, add the label Intercept because your linear model will need a baseline \n",
      "just like in the previous example. Furthermore, to incorporate the intercept into the model \n",
      "easier, ﬁ ll in your intercept column (U8:U1007) with 1s. This will allow you to evaluate \n",
      "the model by taking a \n",
      "SUMPRODUCT of the coeffi  cient row with a data row that will incor-\n",
      "porate the intercept value.\n",
      "All the coeffi  cients for this model are going to go on row 2 of the spreadsheet, so label \n",
      "row 2 as Model Coeffi  cients and place a starting value of 1 in each cell. You can also lay\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart216\n",
      "on some conditional formatting on the coeffi  cient row so you can see diff erences in them \n",
      "once they’re set. \n",
      "Your dataset now looks like Figure 6-7.\n",
      "Figure 6-7: Linear modeling setup\n",
      "Once the coeffi  cients in row 2 are set, you can take a linear combination (formula \n",
      "SUMPRODUCT ) of the coefficients with a row of customer data and get a pregnancy \n",
      "prediction.\n",
      "You have too many columns here, to build a linear model by graphing it the way I did \n",
      "with the cats, so instead you’re going to train the model yourself. The ﬁ rst step is to add \n",
      "a column to the spreadsheet with a prediction on one of the rows of data.\n",
      "In column W, next to the customer data, add the column label Linear Combination  \n",
      "(Prediction) to row 7 and below it take a linear combination of coeffi  cients and customer \n",
      "data (intercept column included). The formula you plug into row 8 to do this for your \n",
      "ﬁ rst customer is:\n",
      "=SUMPRODUCT(B$2:U$2,B8:U8)\n",
      "The absolute reference should be placed on row 2, so that you can drag this formula \n",
      "down to all the other customers without the coeffi  cient row changing.\n",
      "TIP\n",
      "Also, you may want to highlight column W, right-click, select “Format Cells…,” and \n",
      "format the values as a number with two decimal places just to keep your eyes from \n",
      "bleeding at the sight of so many decimals.\n",
      "Once you’ve added this column, your data will look like Figure 6-8.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "217The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\n",
      "Figure 6-8:  The prediction column for a linear model\n",
      "Ideally, the prediction column (column W) would look identical to what we know to be \n",
      "the truth (column V), but using coeffi  cients of 1 for every variable, it’s easy to see you’re \n",
      "way off . The ﬁ rst customer gets a prediction of 5 even though pregnancy is indicated with \n",
      "a 1 and non-pregnancy with a 0. What’s a 5? Really, really pregnant?\n",
      "Adding in an Error Calculation\n",
      "You need to get the computer to set these model coeffi  cients for you, but in order for it to \n",
      "know how to do that, you need to let the machine know when a prediction is right and \n",
      "when it’s wrong.\n",
      "To that end, add an error calculation in column X. Use squared error, which is just the square \n",
      "of the distance of the value of PREGNANT (column V) from the predicted value (column W).\n",
      "Squaring the error allows each error calculation to be positive, so that you can sum \n",
      "them together to get a sense of overall error of the model. You don’t want positive and \n",
      "negative errors canceling each other out. So for the ﬁ rst customer in the sheet, you’d have \n",
      "the following formula:\n",
      "=(V8-W8)^2\n",
      "You can drag that cell down the rest of the column to give each prediction its own \n",
      "error calculation.\n",
      "Now, add a cell above the predictions in cell X1 (labeled in W1 as Sum Squared Error) \n",
      "where you’ll sum the squared error column using the formula:\n",
      "=SUM(X8:X1007)\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart218\n",
      "Your spreadsheet looks like Figure 6-9:\n",
      "Figure 6-9: Predictions and sum of squared error\n",
      "Training with Solver\n",
      "Now you’re ready to train your linear model. You want to set the coeffi  cients for each vari-\n",
      "able such that the sum of squared error is as low as it can be. If this sounds like a job for \n",
      "Solver to you, you’re right. Just as you did in Chapters 2, 4, and 5, you’re going to open up \n",
      "Solver and get the computer to ﬁ nd the best coeffi  cients for you.\n",
      "The objective function will be the Sum Squared Error value from cell X1, which you’ll \n",
      "want to minimize “by changing variable cells” B2 through U2, which are your model \n",
      "coeffi  cients.\n",
      "Now, squared error is a quadratic function of your decision variables, the coeffi  cients, \n",
      "so you can’t use Simplex-LP as the solving method like you used extensively in Chapter \n",
      "4. Simplex is super-fast and guarantees ﬁ  nding the best answer, but it requires that the \n",
      "model only consider linear combinations of the decisions. You’ll need to use the evolu-\n",
      "tionary algorithm in Solver.\n",
      "REFERENCE\n",
      "For more on non-linear optimization models and the inner workings of the evolution-\n",
      "ary optimization algorithm, see Chapter 4. If you like, you can also play with the other \n",
      "non-linear optimization algorithm Excel off ers called GRG.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "219The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\n",
      "Basically, Solver is going to sniff   around for coeffi  cient values that make the sum of \n",
      "squares fall until it feels like it’s found a really good solution. But in order to use the evo-\n",
      "lutionary algorithm eff ectively, you need to set upper and lower bounds on each of the \n",
      "coeffi  cients you’re trying to set.\n",
      "I urge you to play around with these upper and lower bounds. The tighter they are \n",
      "(without getting too tight!), the better the algorithm works. For this model, I’ve set them \n",
      "to be between -1 and 1.\n",
      "Once you’ve completed these items, your Solver setup should look like Figure 6-10.\n",
      "Figure 6-10:  Solver setup for linear model\n",
      "Press the Solve button and wait! As the Evolutionary Solver tries out various coeffi  cients \n",
      "for the model, you’ll see the values change. The conditional formatting on the cells will \n",
      "give you a sense of magnitude. Furthermore, the sum of the squared error should bounce \n",
      "around but generally decrease over time. Once Solver ﬁ nishes, it will tell you the problem \n",
      "is optimized. Click OK, and you’ll have your model back.\n",
      "In Figure 6-11, you’ll see that the Solver run ﬁ  nished with a 135.52 sum of squared \n",
      "error. If you’re following along and would like to run Solver yourself, be aware that two \n",
      "runs of the evolutionary algorithm don’t have to end up in the same place—your sum of \n",
      "squares might end up being higher or lower than the book’s, with slightly diff erent ﬁ nal \n",
      "model coeffi  cients. The optimized linear model is pictured in Figure 6-11.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart220\n",
      "Figure 6-11: Optimized linear model\n",
      "USING THE LINEST() FORMULA FOR LINEAR REGRESSION\n",
      "Some readers may be aware that Excel has its own linear regression formula called \n",
      "LINEST(). In one stroke, this formula can, indeed, do what you just did by hand. It \n",
      "craps out at 64 features, however, so for truly large regressions, you’ll need to roll \n",
      "your own anyway.\n",
      "Feel free to try it out on this dataset. But beware! Read the Excel help documentation \n",
      "on the formula. In order to get all your coeffi  cients out of it, you’ll need to use it as an \n",
      "array formula (see Chapter 1). Also, it spits the coeffi  cients out in reverse order (Male \n",
      "will be the ﬁ nal coeffi  cient before the intercept), which is truly annoying.\n",
      "Where LINEST() comes in super handy is that it automatically computes many of \n",
      "the values needed for performing statistical testing on your linear model, such as the \n",
      "dreaded coeffi  cient standard error calculation that you’ll see in the next section.\n",
      "But in this chapter, you’re going to do everything by hand so that you’ll know a great \n",
      "deal about what LINEST() (and other software packages’ linear modeling functions) is \n",
      "doing and will feel comfortable leaning on it in the future. Also, doing things by hand \n",
      "will aid the transition into logistic regression, which Excel does not support.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "221The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\n",
      "USING MEDIAN REGRESSION TO BETTER HANDLE OUTLIERS\n",
      "In median regression, you minimize the sum of the absolute values of the errors instead \n",
      "of the sum of the squared errors. That’s the only change from linear regression.\n",
      "What does it get you?\n",
      "In linear regression, outliers (values that are markedly distant from the rest of the data) \n",
      "in your training set have more pull and can throw off  the model ﬁ tting process. When \n",
      "an outlier’s error values are large, the linear regression will chase them more, striking \n",
      "a diff erent balance between a large error and a bunch of other normal points’ smaller \n",
      "errors than the balance that is struck in median regression. In median regression, the \n",
      "line that’s ﬁ t to the data will stay close to the typical, inlying data points rather than \n",
      "chase the outliers so much.\n",
      "While I won’t work through median regression in this chapter, it’s not hard to try \n",
      "out on your own. Just swap the squared error term for the absolute value (Excel has the \n",
      "ABS function) and you’re off  and running.\n",
      "That said, if you’re on Windows and have OpenSolver installed (see Chapter 1), then \n",
      "here’s a huge bonus problem!\n",
      "Since in median regression, you’re minimizing error, and since an absolute value can \n",
      "also be thought of as a max function (the max of a value and -1 times that value), try to \n",
      "linearize the median regression as a minimax-esque optimization model (see Chapter 4 \n",
      "for more on minimax optimization models). Hint: You’ll need to create one variable per \n",
      "row of training data, which is why you need OpenSolver—regular Solver can’t handle \n",
      "a thousand decisions and two thousand constraints.\n",
      "Good luck!\n",
      "Linear Regression Statistics: R-Squared, F Tests, t Tests\n",
      "NOTE\n",
      "This next section is the heaviest statistical section in the whole book. Indeed, this section \n",
      "arguably houses the most complex calculation in this entire book—the calculation of \n",
      "model coeffi  cient standard error. I’ve tried to describe everything as intuitively as pos-\n",
      "sible, but some of the calculations defy explanation at a level appropriate for the text. \n",
      "And I don’t want to get sidetracked teaching a linear algebra course here.\n",
      "Try to understand these concepts as best you can. Practice them. And if you want to \n",
      "know more, grab an intro level stats textbook (for example, Statistics in Plain English  \n",
      "by Timothy C. Urdan [Routledge, 2010]).\n",
      "If you get bogged down, know that this section is self-contained. Skip it and come \n",
      "back if you need to.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart222\n",
      "You have a linear model now that you ﬁ t by minimizing the sum of squares. Glancing \n",
      "at the predictions in Column Y, they look all right to the eye. For example, the pregnant \n",
      "customer on row 27 who bought a pregnancy test, prenatal vitamins, and maternity clothes \n",
      "gets a score of 1.07 while the customer on row 996 who’s only ever bought wine gets a \n",
      "score of 0.15. That said, questions remain:\n",
      "• How well does the regression actually ﬁ t the data from a quantitative, non-eyeball \n",
      "perspective?\n",
      "• Is this overall ﬁ t by chance or is it statistically signiﬁ cant?\n",
      "• How useful are each of the features to the model?\n",
      "To answer these questions for a linear regression, you can compute the R-squared, an \n",
      "overall F test, and t tests for each of your coeffi  cients.\n",
      "R-Squared—Assessing Goodness of Fit\n",
      "If you knew nothing about a customer in the training set (columns B through T were \n",
      "missing) but you were forced to make a prediction on pregnancy anyway, the best way to \n",
      "minimize the sum of squared error in that case would be to just put the average of column \n",
      "V in the sheet for each prediction. In this case the average is 0.5 given the 500/500 split in \n",
      "the training data. And since each actual value is either a 0 or 1, each error would be 0.5, \n",
      "making each squared error 0.25. At 1000 predictions then, this strategy of predicting the \n",
      "average, would give a sum of squares of 250.\n",
      "This value is called the total sum of squares. It’s the sum of squared deviations of each \n",
      "value in column V from the average of column V. And Excel off  ers a nifty formula for \n",
      "calculating it in one step, \n",
      "DEVSQ.\n",
      "In X2, you can calculate the total sum of squares as:\n",
      "=DEVSQ(V8:V1007)\n",
      "But while putting the mean for every prediction would yield a sum of squared error of \n",
      "250, the sum of the squared error given by the linear model you ﬁ t earlier is far less than \n",
      "that. Only 135.52. \n",
      "That means 135.52 out of the total 250 sum of squares remains unexplained after you \n",
      "ﬁ t your regression (in this context, the sum of squared error is often called the residual \n",
      "sum of squares). \n",
      "Flipping this value around, the explained sum of squares (which is exactly what it says—\n",
      "the amount you explained with your model) is 250 – 135.52. Put this in X3 as:\n",
      "=X2–X1\n",
      "This gives 114.48 for the explained sum of squares (if you didn’t obtain a sum of squared \n",
      "error of 135.52 when you ﬁ t your regression, then your results might vary slightly).\n",
      "So how good of a ﬁ t is this?\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "223The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\n",
      "Generally, this is answered by looking at the ratio of the explained sum of squares to the \n",
      "total sum of squares. This value is called the R-squared. We can calculate the ratio in X4:\n",
      "=X3/X2\n",
      "As shown in Figure 6-12, this gives an R-squared of 0.46. If the model ﬁ  t perfectly, \n",
      "you’d have 0 squared error, the explained sum of squares would equal the total, and the \n",
      "R-squared would be a perfect 1. If the model didn’t ﬁ t at all, the R-squared would be closer \n",
      "to 0. So then in the case of this model, given the training data’s inputs, the model can do \n",
      "an okay-but-not-perfect job of replicating the training data’s independent variable (the \n",
      "Pregnancy column).\n",
      "Figure 6-12: R-squared of 0.46 for the linear regression\n",
      "Now, keep in mind that the R-squared calculation only works in ﬁ nding linear relation-\n",
      "ships between data. If you have a funky, non-linear relationship (maybe a V or U shape) \n",
      "between a dependent and independent variable in a model, the R-squared value could not \n",
      "capture that relationship.\n",
      "The F Test—Is the Fit Statistically Signiﬁ cant?\n",
      "Oftentimes, people stop at R-squared when analyzing the ﬁ t of a regression. \n",
      "“Hey, the ﬁ t looks good! I’m done.”\n",
      "Don’t do that.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart224\n",
      "The R-squared only tells you how well the model ﬁ ts the data. What it doesn’t tell you \n",
      "is whether this ﬁ t is statistically signiﬁ cant. \n",
      "It is easy, especially with sparse datasets (only a few observations), to get a model that \n",
      "ﬁ ts quite well but whose ﬁ  t is statistically insigniﬁ  cant, meaning that the relationship \n",
      "between the features and the independent variable may not actually be real. \n",
      "Is your model’s ﬁ t due to chance? Some stroke of luck? For a model to be statistically \n",
      "signiﬁ cant, you must reject this ﬁ t-by-ﬂ uke hypothesis. So assume for a moment, that your \n",
      "model’s ﬁ t is a complete ﬂ uke. That the entire ﬁ t is due to luck of the draw on the random \n",
      "1,000 observations you pulled from the RetailMart database. This devil’s advocate assump-\n",
      "tion is called the null hypothesis.\n",
      "The standard practice is to reject the null hypothesis if given it were true, the prob-\n",
      "ability of obtaining a ﬁ t at least this good is less than 5 percent. This probability is often \n",
      "called a p value.\n",
      "To calculate that probability, we perform an F test. An F test takes three pieces of \n",
      "information about our model and runs them through a probability distribution called the \n",
      "F distribution (for an explanation of the term probability distribution, see Chapter 4’s \n",
      "discussion of the normal distribution). Those three pieces of information are:\n",
      "• Number of model coeffi  cients—This is 20 in our case (19 features plus an intercept).\n",
      "• Degrees of freedom—This is the number of training data observations minus the \n",
      "number of model coeffi  cients.\n",
      "• The F statistic—The F statistic is the ratio of explained to unexplained squared error \n",
      "(X3/X1 in the sheet) times the ratio of degrees of freedom to dependent variables.\n",
      "The larger the F statistic, the lower the null hypothesis probability is. And given the \n",
      "explanation of the F statistic above, how do you make it larger? Make one of the two ratios \n",
      "in the calculation larger. You can either explain more of the data (i.e., get a better ﬁ  t) or \n",
      "you can get more data for the same number of variables (i.e., make sure your ﬁ t holds in \n",
      "a larger sample).\n",
      "Returning then to the sheet, we need to count up the number of observations and the \n",
      "number of model coeffi  cients we have. \n",
      "Label Y1 as Observation Count and in Z1 count up all the pregnancy values in column V:\n",
      "=COUNT(V8:V1007)\n",
      "You should, as you’d expect, get 1,000 observations.\n",
      "In Z2, get the Model Coeffi  cient Count by counting them on row 2:\n",
      "=COUNT(B2:U2)\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "225The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\n",
      "You should get 20 counting the intercept. You can then calculate the Degrees of Freedom \n",
      "in Z3 by subtracting the model coeffi  cient count from the observation count:\n",
      "=Z1-Z2\n",
      "You’ll get a value of 980 degrees of freedom. \n",
      "Now for the F statistic in Z4. As noted above, this is just the ratio of explained to \n",
      "unexplained squared error (X3/X1) times the ratio of degrees of freedom to dependent \n",
      "variables (Z3/(Z2-1)):\n",
      "=(X3/X1)*(Z3/(Z2-1))\n",
      "We can then plug these values into the F distribution in Z5 using the Excel function \n",
      "FDIST. Label the cell F Test P Value. FDIST takes the F statistic, the number of dependent \n",
      "variables in the model, and the degrees of freedom:\n",
      "=FDIST(Z4,Z2-1,Z3)\n",
      "As shown in Figure 6-13, the probability of getting a ﬁ t like this given the null hypoth-\n",
      "esis is eff ectively 0. Thus, you may reject the null hypothesis and conclude that the ﬁ t is \n",
      "statistically signiﬁ cant.\n",
      "Figure 6-13: The result of the F test\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart226\n",
      "Coefﬁ cient t Tests—Which Variables Are Signiﬁ cant?\n",
      "WARNING: MATRIX MATH AHEAD!\n",
      "While the previous two statistics weren’t hard to compute, performing a t test on a \n",
      "multiple linear regression requires matrix multiplication and inversion. If you don’t \n",
      "remember how these operations work from high school or intro college math, check \n",
      "out a linear algebra or calculus book. Or just read up on Wikipedia. And use the \n",
      "workbook that’s available for download with this chapter to make sure your math is \n",
      "correct.\n",
      "In Excel, matrix multiplication uses the \n",
      "MMULT  function while inversion uses the \n",
      "MINVERSE function. Since a matrix is nothing more than a rectangular array of numbers, \n",
      "these formulas are array formulas (see Chapter 1 for using array formulas in Excel).\n",
      "While the F test veriﬁ ed that the entire regression was signiﬁ cant, you can also check \n",
      "the signiﬁ cance of individual variables. By testing the signiﬁ cance of single features, you \n",
      "can gain insight into what’s driving your model’s results. Statistically insigniﬁ cant vari-\n",
      "ables might be able to be eliminated, or if you’re sure in your gut that the insigniﬁ  cant \n",
      "variable should matter, then you might investigate if there are data cleanliness issues in \n",
      "your training set.\n",
      "This test for model coeffi  cient signiﬁ cance is called a t test. When performing a t test, \n",
      "much like an F test, you assume that the model coeffi  cient you’re testing is worthless and \n",
      "should be 0. Given that assumption, the t test calculates the probability of obtaining a \n",
      "coeffi  cient as far from 0 as what you actually obtained from your sample.\n",
      "When performing a t test on a dependent variable, the ﬁ rst value you should calculate is \n",
      "the prediction standard error. This is the sample standard deviation of the prediction error \n",
      "(see Chapter 4 for more on standard deviation), meaning that it’s a measure of variability \n",
      "in the model’s prediction errors.\n",
      "You can calculate the prediction standard error in X5 as the square root of the sum of \n",
      "squared error (X1) divided by the degrees of freedom (Z3):\n",
      "=SQRT(X1/Z3)\n",
      "This gives us the sheet shown in Figure 6-14.\n",
      "Using this value, you can then calculate the model’s coeffi  cient standard errors. Think \n",
      "of the standard error of a coeffi  cient as the standard deviation of that coeffi  cient if you \n",
      "kept drawing new thousand-customer samples from the RetailMart database and ﬁ  tting \n",
      "new linear regressions to those training sets. You wouldn’t get the same coeffi  cients each \n",
      "time; they’d vary a bit. And the coeffi  cient standard error quantiﬁ es the variability you’d \n",
      "expect to see.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "227The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\n",
      "Figure 6-14: The prediction standard error for the linear regression\n",
      "To start this calculation, create a new tab in the workbook called \n",
      "ModelCoeffi  cientStandardError. Now, the thing that makes computing the standard error \n",
      "so diffi  cult is that we need to understand both how the training data for a coeffi  cient var-\n",
      "ies by itself and in concert  with the other variables. The ﬁ  rst step in nailing that down \n",
      "is multiplying the training set as one gigantic matrix (often called the design matrix  in \n",
      "linear regression) by itself.\n",
      "This product of the design matrix (B8:U1007) with itself forms what’s called a sum of \n",
      "squares and cross products (SSCP) matrix. To see what this looks like, ﬁ rst paste the row \n",
      "headers for the training data in the ModelCoeffi  cientStdError tab in B1:U1 and transposed \n",
      "down the rows in A2:A21. This includes the Intercept header.\n",
      "To multiply the design matrix times itself, you feed it into the Excel’s MMULT function, \n",
      "ﬁ rst transposed, then right-side up:\n",
      "{=MMULT(TRANSPOSE(‘Linear Model’!B8:U1007),’Linear Model’!B8:U1007)}\n",
      "Since this function returns a variables-by-variables sized matrix, you actually have to \n",
      "highlight the entire range of B2:U21 on the ModelCoeffi  cientStdError tab and execute the \n",
      "function as an array formula (see Chapter 1 for more on array formulas). \n",
      "This yields the tab shown in Figure 6-15.\n",
      "Note the values in the SSCP matrix. Along the diagonal, you’re counting matches of \n",
      "each variable with itself—the same as just summing up the 1s in each column of the \n",
      "design matrix. The intercept gets 1000, for example, in cell U21, because in the original \n",
      "training data, that column is made up of 1000 ones.\n",
      "In the off -diagonal cells, you end up with counts of the matches between diff  erent \n",
      "predictors. While Male and Female obviously never match by design, Pregnancy Test and \n",
      "Birth Control appear together in six customer rows in the training data.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart228\n",
      "Figure 6-15: The SSCP matrix\n",
      "The SSCP matrix then gives you a glimpse into the magnitudes of each variable and \n",
      "how much they overlap and move with each other. \n",
      "The coeffi  cient standard error calculation uses the inverse of the SSCP matrix. To obtain \n",
      "the inverse, paste the variable headers again below the SSCP matrix in B24:U24 and in \n",
      "A25:A44. The inverse of the SSCP matrix in B2:U21 is then calculated by highlighting \n",
      "B25:U44 and employing the \n",
      "MINVERSE function as an array formula:\n",
      "{=MINVERSE(B2:U21)}\n",
      "This yields the sheet shown in Figure 6-16.\n",
      "The values required in the coeffi   cient standard error calculation are those on the \n",
      "diagonal of the SSCP inverse matrix. Each coeffi  cient standard error is calculated as the \n",
      "prediction standard error for the entire model (calculated as 0.37 on the Linear Model \n",
      "tab earlier in cell X5) scaled by the square root of the appropriate value from the SSCP \n",
      "inverse diagonal.\n",
      "For example, the coeffi  cient standard error for Male would be the square root of its \n",
      "Male-to-Male entry in the inverse SSCP matrix (square root of 0.0122) times the predic-\n",
      "tion standard error.\n",
      "To calculate this for all variables, number each variable starting with 1 in B46 through 20 \n",
      "in U46. The appropriate diagonal value can then be read for each predictor using the \n",
      "INDEX \n",
      "formula. For example, INDEX(ModelCoefficientStdError!B25:B44,ModelCoefficientSt\n",
      "dError!B46) returns the Male-to-Male diagonal entry (see more on the INDEX formula in \n",
      "Chapter 1).\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "229The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\n",
      "Figure 6-16: The inverse of the SSCP matrix\n",
      "Taking the square root of this value and multiplying it times the prediction standard \n",
      "error, the Male coeffi  cient standard error is calculated in cell B47 as:\n",
      "=’Linear Model’!$X5*SQRT(INDEX(ModelCoefficientStdError!B25:B44,\n",
      "ModelCoefficientStdError!B46))\n",
      "This comes out to 0.04 for the model ﬁ t in the book.\n",
      "Drag this formula through column U to obtain all the coeffi  cient standard error values \n",
      "as shown in Figure 6-17.\n",
      "Figure 6-17: The standard error of each model coefﬁ  cient\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart230\n",
      "On the Linear Model tab, label A3 as Coeffi  cient Standard Error. Copy the coeffi  cient \n",
      "standard errors, and paste their values back on the Linear Model tab in row 3 (B3:U3).\n",
      "Phew! It’s downhill from here. No more matrix math for the rest of the book. I swear.\n",
      "Now you have everything you need to calculate each coeffi  cient’s t statistic (similar to \n",
      "the entire model’s F statistic from the previous section). You will be performing what’s \n",
      "called a two-tailed t test, meaning that you’ll be calculating the probability of obtaining \n",
      "a coeffi  cient at least as large in either the positive or negative direction if, in reality, there’s \n",
      "no relationship between the feature and the dependent variable. \n",
      "The t statistic for the test can be calculated in row 4 as the absolute value of the coef-\n",
      "ﬁ cient normalized by the coeffi  cient’s standard error. For the Male feature this is:\n",
      "=ABS(B2/B3)\n",
      "Copy this through column U to all the variables.\n",
      "The t test can then be called by evaluating the t distribution (another statistical distri-\n",
      "bution like the normal distribution introduced in Chapter 4) at the value of the t statistic \n",
      "for your particular degrees of freedom value. Label row 5 then as t Test p Value, and in \n",
      "B5 use the formula TDIST to calculate the probability of a coeffi   cient at least this large \n",
      "given the null hypothesis:\n",
      "=TDIST(B4,$Z3,2)\n",
      "The two in the formula indicates you’re performing the two-tailed t test. Copying this \n",
      "formula across to all variables and applying conditional formatting to cells over 0.05 \n",
      "(5percent probability), you can see which features are not statistically signiﬁ cant. While \n",
      "your results may vary based on the ﬁ  t of your model, in the workbook shown in Figure \n",
      "6-18, the Female, Home, and Apt columns are shown to be insigniﬁ cant.\n",
      "Figure 6-18:  Female, Home, and Apt are insigniﬁ  cant predictors according to the test\n",
      "You could remove these columns from your model in future training runs.\n",
      "Now that you’ve learned how to evaluate the model using statistical tests, let’s change gears \n",
      "and look at measuring the model’s performance by making actual predictions on a test set.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "231The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\n",
      "Making Predictions on Some New Data and Measuring \n",
      "Performance\n",
      "That last section was all statistics. Lab work you could say. It’s not the most fun you’ve \n",
      "ever had, but validating goodness of ﬁ t and signiﬁ cance are important skills to have. But \n",
      "now it’s time to take this model to the racetrack and have some fun!\n",
      "How do you know your linear model actually will predict well in the real world? After \n",
      "all, your training set does not encapsulate every possible customer record, and your coef-\n",
      "ﬁ cients have been purpose built to ﬁ  t the training set (although if you’ve done your job \n",
      "right, the training set, very nearly, resembles the world at large).\n",
      "To get a better sense of how the model will perform in the real world, you should run some \n",
      "customers through the model that were not used in the training process. You’ll see this sepa-\n",
      "rate set of examples used for testing a model often called a validation set, test set, or holdout set. \n",
      "To assemble your test set, you can just return to the customer database and select \n",
      "another set of data from random customers (paying special attention to not pull the same \n",
      "customers used in training). Now, as noted earlier, 6 percent of RetailMart’s customers are \n",
      "pregnant, so if you randomly selected a thousand customers from the database, roughly \n",
      "60 of them would be pregnant.\n",
      "While you oversampled the pregnant class in training the model, for testing you’ll leave \n",
      "the ratio of pregnant households at 6 percent so that our measurements of the precision \n",
      "of the model are accurate for how the model would perform in a live setting.\n",
      "In the RetailMart spreadsheet available for download that accompanies this chapter, \n",
      "you’ll ﬁ nd a tab called Test Set, which is populated with a thousand rows of data identi-\n",
      "cal to the training data. The ﬁ rst 60 customers are pregnant, while the other 940 are not \n",
      "(see Figure 6-19).\n",
      "Figure 6-19: Test set data\n",
      "Just as you did on the Linear Model tab, run this new data through the model by tak-\n",
      "ing a linear combination of customer data and coeffi  cients and adding in the intercept.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart232\n",
      "Placing this prediction in column V, you have the following formula for the ﬁ rst customer \n",
      "on row 2 (since the test set doesn’t have an Intercept column, you add it in separately): \n",
      "=SUMPRODUCT(‘Linear Model’!B$2:T$2,’Test Set’!A2:S2)+’Linear Model’!U$2\n",
      "Copy this calculation down to all the customers. The resulting spreadsheet looks as \n",
      "shown in Figure 6-20.\n",
      "Figure 6-20: Predictions on the test set\n",
      "You can see in Figure 6-20 that the model has identiﬁ ed many of the pregnant house-\n",
      "holds with predictions closer to 1 than they are to 0. The highest prediction values are \n",
      "for households that bought a product clearly related to pregnancy, such as folic acid or \n",
      "prenatal vitamins.\n",
      "On the other hand, out of the 60 pregnant households, there are some who never bought \n",
      "anything to indicate they were pregnant. Of course, they didn’t buy alcohol or tobacco, but \n",
      "as their low pregnancy scores indicate, not buying something doesn’t mean a whole lot.\n",
      "Conversely, if you look at the predictions for non-pregnant folks there are some misses. \n",
      "For instance if you’re following along in the workbook, on row 154 a non-pregnant cus-\n",
      "tomer bought maternity clothing and stopped buying cigarettes, and the model gave them \n",
      "a score of 0.76.\n",
      "It’s clear then that if you are going to use these predictions in real marketing eff  orts, \n",
      "you need to set a score threshold for when you can assume someone is pregnant and reach\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "233The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\n",
      "out to that person with marketing materials. Perhaps you only send someone marketing \n",
      "materials if they’re scored at  0.8 or above. Perhaps that cutoff   should be 0.95, so that \n",
      "you’re extra sure.\n",
      "In order to set this classiﬁ  cation threshold, you need to look at trade-off  s in model \n",
      "performance metrics. Most predictive model performance metrics are based on counts \n",
      "and ratios of four values that come from the predictions on our test set:\n",
      "• True positives—Labeling a pregnant customer as pregnant\n",
      "• True negatives—Labeling a not pregnant customer as not pregnant\n",
      "• False positives (also called type I error)—Calling a not-so-pregnant customer preg-\n",
      "nant. In my experience, this speciﬁ c false positive is very insulting face-to-face. Do \n",
      "not try this at home.\n",
      "• False negatives (also called type II error)—Failing to identify a pregnant customer \n",
      "as such. This is not nearly as insulting in my experience. \n",
      "As you’ll see, while there are lots of diff erent performance metrics for a predictive model, \n",
      "they all feel a bit like Tex Mex food—they’re all basically combinations of the same four \n",
      "ingredients listed above.\n",
      "Setting Up Cutoff Values\n",
      "Create a new sheet called Performance. The lowest value that could practically be used as \n",
      "a cutoff  between pregnant and not pregnant is the lowest prediction value from the test \n",
      "set. Label A1 as Min Prediction and in A2, you can calculate this as:\n",
      "=MIN(‘Test Set’!V2:V1001)\n",
      "Similarly, the highest cutoff  value would be the max prediction from the test set. Label \n",
      "A4 as Max Prediction, and in A5, you can calculate this as:\n",
      "=MAX(‘Test Set’!V2:V1001)\n",
      "The values given back are -0.35 and 1.25 respectively. Keep in mind that your linear \n",
      "regression can make predictions below 0 and above 1 because it’s not actually returning \n",
      "class probabilities (we’ll address this with another model later).\n",
      "In column B, then, add the header Probability Cutoff  for Pregnant Classiﬁ cation and \n",
      "below that specify a range of cutoff  values starting with -0.35. In the sheet shown in Figure \n",
      "6-21, the cutoff  values have been chosen to increase in increments of 0.05 all the way to \n",
      "the max of 1.25 (just enter the ﬁ rst three by hand, highlight them, and drag down to ﬁ  ll \n",
      "in the rest).\n",
      "Alternatively, you could specify every single prediction value from the test set as a cutoff  \n",
      "if you wanted to be thorough. No more than that would be needed.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart234\n",
      "Precision (Positive Predictive Value)\n",
      "Let’s now ﬁ ll in some model performance metrics for each of these cutoff  values using the \n",
      "Test Set data predictions starting with precision, also known as positive predictive value.\n",
      "Precision is the measure of how many pregnant households we correctly identify out \n",
      "of all the households the model says are pregnant. In business-speak, precision is the \n",
      "percent of ﬁ sh in your net that are tuna and not dolphins.\n",
      "Label column C as Precision. Consider the cutoff  score in B2 of -0.35. What’s the preci-\n",
      "sion of our model if we consider anyone scoring at least a -0.35 to be pregnant? \n",
      "To calculate that, we can go to the “Test Set” tab and count the number of cases where \n",
      "a pregnant household scored greater than or equal to -0.35 divided by the number of total \n",
      "rows with a score over -0.35. Using the \n",
      "COUNTIFS formula to check actuals and predictions, \n",
      "the formula in cell C2 would look as follows:\n",
      "=COUNTIFS(‘Test Set’!$V$2:$V$1001,”>=” & B2,\n",
      "‘Test Set’!$U$2:$U$1001,”=1”)/COUNTIF(‘Test Set’!$V$2:$V$1001,”>=” & B2)\n",
      "Figure 6-21: Cutoff values for the pregnancy classiﬁ  cation\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "235The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\n",
      "The ﬁ rst COUNTIFS  statement in the formula matches both on actual pregnancy and \n",
      "model prediction, while the COUNTIF in the denominator just cares about only those who \n",
      "scored higher that -0.35 regardless of pregnancy. You can copy this formula to all the \n",
      "thresholds you’re evaluating.\n",
      "As seen in Figure 6-22, the precision of the model increases with the cutoff  value, and \n",
      "at a cutoff  value of 1, the model becomes completely precise. A completely precise model \n",
      "identiﬁ es only pregnant customers as pregnant.\n",
      "Figure 6-22: Precision calculations on the test set\n",
      "Speciﬁ city (True Negative Rate)\n",
      "Another performance metric that increases with the cutoff   value is called Speciﬁ city. \n",
      "Speciﬁ city, also called the Tr ue Negative Rate is a count of how many not pregnant cus-\n",
      "tomers are correctly predicted as such (true negatives) divided by the total number of not \n",
      "pregnant cases.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart236\n",
      "Labeling column D as Speciﬁ city/True Negative Rate , you can calculate it in D2 by \n",
      "using COUNTIFS in the numerator to count true negatives, and COUNTIF in the denominator \n",
      "to count total customers who aren’t pregnant:\n",
      "=COUNTIFS(‘Test Set’!$V$2:$V$1001,”<” & B2,\n",
      "‘Test Set’!$U$2:$U$1001,”=0”)/COUNTIF(‘Test Set’!$U$2:$U$1001,”=0”) \n",
      "Copying this calculation down through the other cutoff   values, you should see it \n",
      "increase (see Figure 6-23). Once a cutoff   value of 0.85 is reached, 100 percent of not \n",
      "pregnant customers in the test set are appropriately predicted.\n",
      "Figure 6-23: Speciﬁ  city calculations on the test set\n",
      "False Positive Rate\n",
      "The false positive rate is a common metric looked at to understand model performance. \n",
      "And since you already have the true negative rate, this can quickly be calculated as one \n",
      "minus the true negative rate. Label column E as False Positive Rate/(1 – Speciﬁ city) and \n",
      "ﬁ ll in the cells as one minus the value in the adjacent cell in D. For E2, that’s written as:\n",
      "=1-D2\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "237The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\n",
      "Copying this formula down, you can see that as the cutoff  value increases, you get less \n",
      "false positives. In other words, you’re committing fewer type I errors (calling customers \n",
      "pregnant who aren’t).\n",
      "True Positive Rate/Recall/Sensitivity\n",
      "The ﬁ nal metric you can calculate on your model’s performance is call true positive rate. \n",
      "And recall. And sensitivity. Geez. They should just pick one name and stick with it.\n",
      "The true positive rate is the ratio of correctly identiﬁ ed pregnant women divided by the \n",
      "total of actual pregnant women in the test set. Label column F as True Positive Rate/Recall/\n",
      "Sensitivity. In F2 then you can calculate the true positive rate of a cutoff  value of -0.35 as:\n",
      "=COUNTIFS(‘Test Set’!$V$2:$V$1001,”>=” & B2,\n",
      "‘Test Set’!$U$2:$U$1001,”=1”)/COUNTIF(‘Test Set’!$U$2:$U$1001,”=1”)\n",
      "Looking back at the true negative rate column, this calculation is exactly the same \n",
      "except “<” becomes “>=” and 0s become 1s.\n",
      "Copying this metric down, you can see that as the cutoff  increases, some of the pregnant \n",
      "women cease to be identiﬁ ed as such (these are type II errors) and the true positive rate \n",
      "falls. Figure 6-24 shows the false and true positive rates in columns E and F.\n",
      "Figure 6-24: The false positive rate and the true positive rate\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart238\n",
      "Evaluating Metric Trade-Offs and the Receiver Operating Characteristic Curve\n",
      "When choosing a threshold value for a binary classiﬁ  er, it’s important to select the best \n",
      "balance of these performance metrics. The higher the cutoff , the more precise the model \n",
      "but the lower the recall, for example. One of the most common visualizations used to \n",
      "assess these performance trade-off s is the receiver operating characteristic (ROC) curve. \n",
      "The ROC curve is just a plot of the False Positive Rate versus the True Positive Rate (col-\n",
      "umns E and F in the Performance sheet). \n",
      "WHY IS IT CALLED THE RECEIVER OPERATING CHARACTERISTIC?\n",
      "The reason why such a simple graph has such a complex name is that it was devel-\n",
      "oped during World War II by radar engineers rather than by marketers predicting \n",
      "when customers are pregnant.\n",
      "These folks were using signals to detect enemies and their equipment in the battle-\n",
      "ﬁ eld, and they wanted to better visualize the trade-off  between correctly and incorrectly \n",
      "identifying something as a foe.\n",
      "To insert this graph, simply highlight the data in columns E and F and select the straight \n",
      "lined scatter plot in Excel (see Chapter 1 for more on inserting charts and graphs). With a \n",
      "little formatting (setting the axes between 0 and 1, bumping up the font), the ROC curve \n",
      "looks as shown in Figure 6-25.\n",
      "0%\n",
      "0% 10% 20% 30% 40% 50% 60% 70% 80% 90%100%\n",
      "True Positive Rate\n",
      "False Positive Rate\n",
      "ROC Curve\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "90%\n",
      "100%\n",
      "Figure 6-25: The ROC curve for the linear regression\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "239The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\n",
      "This curve allows you to quickly assess the false positive rate that’s associated with a \n",
      "true positive rate in order to understand your options. For example, in Figure 6-25, you \n",
      "can see that the model is capable of identifying 40 percent of pregnant customers using \n",
      "a cutoff  of 0.85 without a single fal se positive. Nice! \n",
      "And if you were okay with occasionally sending a not pregnant household some \n",
      "pregnancy-related coupons, the model could achieve a 75 percent true positive rate with \n",
      "only a 9 percent false positive rate.\n",
      "Where you decide to set the threshold for acting on someone’s pregnancy score is a \n",
      "business decision , not purely an analytic one. If there were little downside to predicting \n",
      "someone was pregnant, then a low precision might be a ﬁ ne trade-off  for a high true posi-\n",
      "tive rate. But if you’re predicting likelihood of default for loan applications, you’re going \n",
      "to want speciﬁ city and precision to be a bit higher, right? On the extreme end, if a model \n",
      "like this were being used to validate the legitimacy of overseas threats based on a body of \n",
      "intelligence, then you’d hope that the operator of the model would want a very high level \n",
      "of precision before calling in a drone strike.\n",
      "So whether we’re talking sending coupons in the mail, approving loans, or dropping \n",
      "bombs, the balance you strike between these performance metrics is a strategic decision.\n",
      "COMPARING ONE MODEL TO ANOTHER\n",
      "As we’ll see a bit later, the ROC curve is also good for choosing one predictive model \n",
      "over another. Ideally, the ROC curve would jump straight up to 1 on the y-axis as \n",
      "fast as possible and stay there all the way across the graph. So the model that looks \n",
      "most like that (also said to have the highest area under the curve or AUC) is often \n",
      "considered superior.\n",
      "All right! So now you’ve run the model on some test data, made some predictions, \n",
      "computed its performance on the test set for diff erent cutoff  values, and visualized that \n",
      "performance with the ROC curve.\n",
      "But in order to compare model performance, you need another model to race against.\n",
      "Predicting Pregnant Customers at RetailMart Using \n",
      "Logistic Regression\n",
      "If you look at the predicted values coming out your linear regression, it’s clear that while \n",
      "the model is useful for classiﬁ cation, the prediction values themselves are certainly in no \n",
      "way class probabilities. You can’t be pregnant with 125 percent probability or -35 percent \n",
      "probability.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart240\n",
      "So is there a model whose predictions are actually class probabilities? Once such model \n",
      "that we can build is called a logistic regression.\n",
      "First You Need a Link Function\n",
      "Think about the predictions currently coming out of your linear model. Is there a formula \n",
      "you can shove these numbers through that will make them stay between 0 and 1? It turns \n",
      "out, this kind of function is called a link function, and there’s a great one for doing just that:\n",
      "exp(x)/(1 + exp(x))\n",
      "In this formula, x is our linear combination from column W on the Linear Model tab, \n",
      "and exp is the exponential function. The exponential function exp(x) is just the math-\n",
      "ematical constant e (2.71828…it’s like pi, but a little lower) raised to the power of x.\n",
      "Look at a graph of the function pictured in Figure 6-26.\n",
      "–5 –4 –3 –2 –10\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1\n",
      "1234\n",
      "Any value can go in\n",
      "Link function for pregnant/not pregnant\n",
      "Values between 0 and 1 come out\n",
      "5\n",
      "Figure 6-26: The link function\n",
      "This link function looks like a really wide S. It takes in any values given from multiply-\n",
      "ing the model coeffi  cients times a row of customer data, and it outputs a number between \n",
      "0 and 1. But why does this odd function look like this?\n",
      "Well, just round e to 2.7 real quick and think about the case where the input to this \n",
      "function is pretty big, say 10. Then the link function is:\n",
      "exp(x)/(1 + exp(x)) = 2.7^10 / (1+ 2.7^10) = 20589/20590\n",
      "Well, that’s basically 1, so we can see that as x gets larger, that 1 in the denominator \n",
      "just doesn’t matter much. But as x goes negative? Look at -10:\n",
      "exp(x)/(1 + exp(x)) = 2.7^-10 / (1+ 2.7^-10) = 0.00005/1.00005\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "241The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\n",
      "Well, that’s just 0 for the most part. In this case the 1 in the denominator means every-\n",
      "thing and the teeny numbers are more or less 0s.\n",
      "Isn’t that handy? In fact, this link function has been so useful that someone gave it a \n",
      "name along the way. It’s called the “logistic” function.\n",
      "Hooking Up the Logistic Function and Reoptimizing\n",
      "Now create a copy of the Linear Model tab in the spreadsheet and call it Logistic Link \n",
      "Model. Delete all of the statistical testing data from the sheet since that was primarily \n",
      "applicable to linear regression. Speciﬁ  cally, highlight and delete rows 3 through 5, and \n",
      "clear out all the values at the top of columns W through Z except for the Sum Squared \n",
      "Error placeholder. Also, clear out the squared error column and rename it Prediction (after \n",
      "Link Function). See Figure 6-27 to see what the sheet should look like.\n",
      "Figure 6-27: The initial logistic model sheet\n",
      "You’re going use column X to suck in the linear combination of coeffi  cients and data \n",
      "from column W and put it through your logistic function. For example, the ﬁ  rst row of \n",
      "modeled customer data would be sent through the logistic function by putting this for-\n",
      "mula in cell X5:\n",
      "=EXP(W5)/(1+EXP(W5))\n",
      "If you copy this formula down the column, you can see that the new values are all \n",
      "between 0 and 1 (see Figure 6-28).\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart242\n",
      "NOTE\n",
      "Your sheet might have slightly diff erent values in columns W and X to start since the \n",
      "model coeffi  cients are coming from the evolutionary algorithm run on the previous tab.\n",
      "Figure 6-28: Values through the logistic function\n",
      "However, most of the predictions appear to be middling, between 0.4 and 0.7. Well, \n",
      "that’s because we didn’t optimize our coeffi  cients in the “Linear Model” tab for this new \n",
      "kind of model. We need to optimize again.\n",
      "So add back in a squared error column to column Y, although this time, the error cal-\n",
      "culation will use the predictions coming out of the link function in column X:\n",
      "=(V5-X5)^2\n",
      "Which you’ll again sum up just as in the linear model in cell X1 as:\n",
      "=SUM(Y5:Y1004)\n",
      "You can then minimize the sum of squares in this new model using the exact same \n",
      "Solver setup (see Figure 6-29) as in the linear model, except if you experiment with the \n",
      "variable bounds, you’ll ﬁ nd it’s best to broaden them a bit for a logistic model. In Figure \n",
      "6-29, the bounds have been set to keep each coeffi  cient between -5 and 5.\n",
      "Once you’ve reoptimized for the new link function, you can see that your predictions \n",
      "on the training data now all fall between 0 and 1 with many predictions conﬁ dently being\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "243The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\n",
      "committed to either a 0 or a 1. As you can see in Figure 6-30, from an aesthetic perspec-\n",
      "tive, these predictions feel nicer than those from the linear regression.\n",
      "Figure 6-29: Identical Solver setup for logistic model\n",
      "Figure 6-30:  Fitted logistic model\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart244\n",
      "Baking an Actual Logistic Regression\n",
      "The truth is that in order to do an actual logistic regression that gives accurate, unbiased \n",
      "class probabilities, you can’t, for reasons outside the scope of this book, minimize the \n",
      "sum of squared error.\n",
      "Instead, you ﬁ t the model by ﬁ nding the model coeffi  cients that maximize the joint \n",
      "probability (see Chapter 3 for more on joint probability) of you having pulled this training \n",
      "set from the RetailMart database given that the model accurately explains reality. \n",
      "So what is the likelihood of a training row given a set of logistic model parameters? For \n",
      "a given row in the training set, let p stand in for the class probability your logistic model \n",
      "is giving in column X. Let y stand for the actual pregnancy value housed in column V. \n",
      "The likelihood of that training row, given the model parameters is:\n",
      "py(1-p)(1-y)\n",
      "For a pregnant customer (column V is 1) with a prediction of 1 (column X has a 1 in \n",
      "it), this likelihood calculation is, likewise, 1. But if the prediction were 0 for a pregnant \n",
      "customer, then the above calculation would be 0 (plug in the numbers and check it). Thus, \n",
      "the likelihood of each row is maximized when the predictions and actuals all line up. \n",
      "Assuming each row of data is independent (see Chapter 3 for more on independence) as \n",
      "is the case in any good random pull from a database, then you can calculate the log of the \n",
      "joint probability of the data by taking the log of each of these likelihoods and summing \n",
      "them up. The log of the above equation, using the same rules you saw in the ﬂ oating-point \n",
      "underﬂ ow section in Chapter 3, is:\n",
      "y*ln(p)+(1-y)*ln(1-p)\n",
      "The log likelihood is near 0 when the previous formula is near 1 (i.e., when the model \n",
      "ﬁ ts well).\n",
      "Rather than minimize the sum of the squared error then, you can calculate this \n",
      "log-likelihood value on each prediction and sum them up instead. The model coeffi  cients \n",
      "that maximize the joint likelihood of the data will be the best ones.\n",
      "To start, make a copy of the Logistic Link Model tab and call it Logistic Regression. In \n",
      "column Y, change the squared error column to read Log Likelihood. In cell Y5, the ﬁ  rst \n",
      "log likelihood can be calculated as:\n",
      "=IFERROR(V5*LN(X5)+(1-V5)*LN(1-X5),0)\n",
      "The entire log likelihood calculation is wrapped in an IFERROR formula, because when \n",
      "the model coeffi  cients generate a prediction very, very near the actual 0/1 class value, you \n",
      "can get numerical instability. In that case, it’s fair just to set the log-likelihood to a perfect \n",
      "match score of 0.\n",
      "Copy this formula down column Y, and in X1, sum the log likelihoods. Optimizing, \n",
      "you get a set of coeffi  cients that look similar to the sum of squares coeffi  cients with some \n",
      "small shifts here and there. See Figure 6-31.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "245The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\n",
      "Figure 6-31: The Logistic Regression sheet\n",
      "If you check the sum of squared error associated with your actual logistic regression, \n",
      "it’s nearly optimal for that metric anyway.\n",
      "STATISTICAL TESTS ON A LOGISTIC REGRESSION\n",
      "Analogous statistical concepts to the R-squared, F test, and t test are available in \n",
      "logistic regression. Computations such as pseudo R-squared, model deviance, and \n",
      "the Wald statistic lend logistic regression much of the same rigor as linear regres-\n",
      "sion. For more information, see Applied Logistic Regression by David W. Hosmer, Jr., \n",
      "Stanley Lemeshow, and Rodney X. Sturdivant (John Wiley & Sons, 2013).\n",
      "Model Selection—Comparing the Performance of the Linear \n",
      "and Logistic Regressions\n",
      "Now that you have a second model, you can run it on the test set and compare its perfor-\n",
      "mance to that of your linear regression. Predictions using the logistic regression are made in \n",
      "exactly the same way they were modeled in the Logistic Regression tab in columns W and X. \n",
      "In cell W2 on the Test Set tab, take the linear combination of model coeffi  cients and \n",
      "test data as:\n",
      "=SUMPRODUCT(‘Logistic Regression’!B$2:T$2,’Test Set’!A2:S2)+\n",
      "‘Logistic Regression’!U$2\n",
      "In X2, run this through the link function to get your class probability:\n",
      "=EXP(W2)/(1+EXP(W2))\n",
      "Copy these cells down through the test set to obtain the sheet shown in Figure 6-32.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart246\n",
      "Figure 6-32: Logistic regression predictions on the test set\n",
      "To see how the predictions stack up, make a copy of the Performance tab and call it \n",
      "Performance Logistic . Changing the minimum and maximum prediction formulas to \n",
      "point to column X from the Test Set tab, the values come back as 0 and 1, just as you’d \n",
      "expect now that your model is giving actual class probabilities unlike the linear regression.\n",
      "NOTE\n",
      "While the logistic regression returns class probabilities (actual predictions between 0 \n",
      "and 1), these probabilities are based on the 50/50 split of pregnant and not pregnant \n",
      "customers in the rebalanced training set. \n",
      "This is ﬁ ne if all you care about is binary classiﬁ  cation at some cutoff  value rather \n",
      "than using the actual probabilities.  \n",
      "Choose cutoff  values from 0 to 1 in 0.05 increments (actually, you may need to make \n",
      "1 a 0.999 or so to keep the precision formula from dividing by 0). Everything below row \n",
      "22 can be cleared, and the performance metrics need only be changed to check column X \n",
      "on the Test Set tab instead of V. This yields the sheet shown in Figure 6-33.\n",
      "You can set the ROC curve up in exactly the same way as before, however, in order \n",
      "to compare the logistic regression to the linear regression, add in a data series for each \n",
      "model’s performance metrics (right-click the chart and choose Select Data to add another \n",
      "series). In Figure 6-34, it’s apparent that the ROC curves for the two models are almost \n",
      "exactly on top of each other.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "247The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\n",
      "Figure 6-33:  The Performance Logistic tab\n",
      "0%\n",
      "0% 10% 20% 30% 40% 50% 60% 70% 80% 90%100%\n",
      "True Positive Rate\n",
      "False Positive Rate\n",
      "ROC Curve\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "Logistic Regression\n",
      "Linear Regression\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "90%\n",
      "100%\n",
      "Figure 6-34:  The linear and logistic regression ROC curves graphed together\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart248\n",
      "Given that the models’ performances are nearly identical, you might consider using \n",
      "the logistic regression if for no other reason than the practicality of getting actual class \n",
      "probabilities bounded between 0 and 1 from the model. It’s prettier if nothing else.\n",
      "A WORD OF CAUTION\n",
      "You may hear a lot about model selection out there in the real world. Folks may ask, \n",
      "“Why didn’t you use support vector machines or neural nets or random forests or \n",
      "boosted trees?” There are numerous types of AI models, all with their strengths and \n",
      "weaknesses. And I would encourage you to read about them, and if in your work you \n",
      "happen to use an AI model, then you should try some of these models head-to-head.\n",
      "But. \n",
      "Trying diff erent AI models is not the most important part of an AI modeling project. \n",
      "It’s the last step, the icing on the cake. This is where sites like Kaggle.com (an AI model-\n",
      "ing competition website) have it all wrong.\n",
      "You get more bang for your buck spending your time on selecting good data and \n",
      "features than models. For example, in the problem I outlined in this chapter, you’d be \n",
      "better served testing out possible new features like “customer ceased to buy lunch meat \n",
      "for fear of listeriosis” and making sure your training data was perfect than you would \n",
      "be testing out a neural net on your old training data.\n",
      "Why? Because the phrase “garbage in, garbage out” has never been more applicable \n",
      "to any ﬁ eld than AI. No AI model is a miracle worker; it can’t take terrible data and \n",
      "magically know how to use that data. So do your AI model a favor and give it the best \n",
      "and most creative features you can ﬁ nd.\n",
      "For More Information\n",
      "If you just love supervised AI, and this chapter wasn’t enough for you, then let me make \n",
      "some reading suggestions:\n",
      "• Data Mining with R  by Luis Torgo (Chapman & Hall/CRC, 2010) is a great next \n",
      "step. The book covers machine learning in the programming language, R. R is a \n",
      "programming language beloved by statisticians everywhere, and it’s not hard to \n",
      "pick up for AI modeling purposes. In fact, if you were going to productionalize \n",
      "something like the model in this chapter, R would be a great place to train up and \n",
      "run that production model.\n",
      "• The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome \n",
      "Friedman (Springer, 2009) takes an academic look at various AI models. At times\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "249The Granddaddy of Supervised Artiﬁ cial Intelligence—Regression\n",
      "a slog, the book can really up your intellectual game. A free copy can be found on \n",
      "Hastie’s Stanford website.\n",
      "For discussion with other practitioners, I usually head to the CrossValidated forum at \n",
      "StackExchange (stats.stackexchange.com). Oftentimes, someone has already asked your \n",
      "question for you, so this forum makes for an excellent knowledge base.\n",
      "Wrapping Up\n",
      "Congratulations! You just built a classiﬁ  cation model in a spreadsheet. Two of them \n",
      "actually. Maybe even two and a half. And if you took me up on my median regression \n",
      "challenge, then you’re a beast.\n",
      "Let’s recap some of the things we covered:\n",
      "• Feature selection and assembling training data, including creating dummy variables \n",
      "out of categorical predictors\n",
      "• Training a linear regression model by minimizing the sum of squared error\n",
      "• Calculating R-squared, showing a model is statistically signiﬁ cant using an F test, \n",
      "and showing model coeffi  cients are individually signiﬁ cant using a t test\n",
      "• Evaluating model performance on a holdout set at various classiﬁ cation cutoff  values \n",
      "by calculating precision, speciﬁ city, false positive rate, and recall\n",
      "• Graphing a ROC curve\n",
      "• Adding a logistic link function to a general linear model and reoptimizing\n",
      "• Maximizing likelihood in a logistic regression\n",
      "• Comparing models with the ROC curve\n",
      "And while I’ll be the ﬁ rst to admit that the data in this chapter is fabricated from whole \n",
      "cloth, let me assure you that the power of such a logistic model is not to be scoff  ed at. \n",
      "You could use something like it in a production decision support or automated marketing \n",
      "system for your business. \n",
      "If you’d like to keep going with AI, in the next chapter, I’m going to introduce a diff er-\n",
      "ent approach to AI called the ensemble model.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "7\n",
      "O\n",
      "n the American version of the popular TV show The Office, the boss, Michael Scott, \n",
      "buys pizza for his employees. Everyone groans when they learn that he has unfortu-\n",
      "nately bought pizza from Pizza by Alfredo instead of Alfredo’s Pizza. Although it’s cheaper, \n",
      "apparently pizza from Pizza by Alfredo is awful.\n",
      "In response to their protests, Michael asks his employees a question: is it better to have \n",
      "a small amount of really good pizza or a lot of really bad pizza?\n",
      "For many practical artiﬁ cial intelligence implementations, the answer is arguably the \n",
      "latter. In the previous chapter, you built a single, good model for predicting pregnant \n",
      "households shopping at RetailMart. What if instead, you got democratic? What if you \n",
      "built a bunch of admittedly crappy models and let them vote on whether a customer was \n",
      "pregnant? The vote tally would then be used as a single prediction.\n",
      "This type of approach is called ensemble  modeling, and as you’ll see, it turns simple \n",
      "observations into gold. \n",
      "You’ll be going over a type of ensemble model called bagged decision stumps, which is \n",
      "very close to an approach used constantly in industry called the random forest model. In \n",
      "fact, it’s very nearly the approach I use daily in my own life here at MailChimp.com to \n",
      "predict when a user is about to send some spam.\n",
      "After bagging, you’ll investigate another awesome technique called boosting. Both of \n",
      "these techniques ﬁ nd creative ways to use the training data over and over and over again \n",
      "to train up an entire ensemble of classiﬁ ers. There’s an intuitive feel to these approaches \n",
      "that’s reminiscent of naïve Bayes—a stupidity that, in aggregate, is smart.\n",
      "Ensemble Models: A \n",
      "Whole Lot of Bad Pizza\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart252\n",
      "Using the Data from Chapter 6\n",
      "NOTE\n",
      "The Excel workbook used in this chapter, “Ensemble.xlsm,” is available for down-\n",
      "load at the book’s website at www.wiley.com/go/datasmart. This workbook includes \n",
      "all the initial data if you want to work from that. Or you can just read along using \n",
      "the sheets I’ve already put together in the workbook.\n",
      "This chapter’s gonna move quickly, because you’ll use the RetailMart data from \n",
      "Chapter 6. Using the same data will give you a sense of the diff erences in these two mod-\n",
      "els’ implementations from the regression models in the previous chapter. The modeling \n",
      "techniques demonstrated in this chapter were invented more recently. They’re somewhat \n",
      "more intuitive, and yet, are some of the most powerful off   the shelf AI technologies we \n",
      "have today.\n",
      "Also, we’ll be building ROC curves identical to those from Chapter 6, so I won’t be \n",
      "spending much time explaining performance metric calculations. See Chapter 6 if you \n",
      "really want to understand concepts like precision and recall.\n",
      "Starting off , the workbook available for download has a sheet called TD which includes \n",
      "the training data from Chapter 6 with the dummy variables already set up properly (for \n",
      "more on this see Chapter 6). Also, the features have been numbered 0 to 18 in row 2. This \n",
      "will come in handy with recordkeeping later (see Figure 7-1). \n",
      "The workbook also includes the Test Set tab from Chapter 6.\n",
      "Figure 7-1: The TD tab houses the data from Chapter 6.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "253Ensemble Models: A Whole Lot of Bad Pizza \n",
      "You will try to do exactly what you did in Chapter 6 with this data—predict the values \n",
      "in the PREGNANT column using the data to the left of it. Then you’ll verify the accuracy on \n",
      "the holdout set.\n",
      "MISSING VALUE IMPUTATION\n",
      "In the RetailMart example introduced in Chapter 6 and continued here, you’re work-\n",
      "ing with a dataset that doesn’t have holes in it. For many models built off  of trans-\n",
      "actional business data, this is often the case. But there will be situations in which \n",
      "elements are missing from some of the rows in a dataset.\n",
      "For example, if you were building a recommendation AI model for a dating site and \n",
      "you asked users in their proﬁ le questionnaire if they listened to the symphonic heavy \n",
      "metal band Evanescence, you might expect that question to be left blank on occasion.\n",
      "So how do you train a model if some of the folks in your training set leave the \n",
      "Evanescence question blank?\n",
      "There are all sorts of ways around this issue, but really quickly I’ll list some places \n",
      "to start:\n",
      "• Just drop the rows with missing values. If the missing values are more or less \n",
      "random, losing some rows of training data isn’t going to kill you. In the dating \n",
      "site example, these blanks are more likely intentional than random, so dropping \n",
      "the rows could cause the training data to get a skewed view of reality.\n",
      "• If the column is numeric, ﬁ  ll in the missing value with the median of those \n",
      "records that have values. Filling in missing values is often called imputation. If \n",
      "the column is categorical, use the most common category value. Once again, in \n",
      "the case of ashamed Evanescence fans, the most common value is probably No, so \n",
      "ﬁ lling in with the most common value can be the wrong way to go when people \n",
      "are censoring themselves.\n",
      "• On top of the previous option, you can add another indicator column that has a \n",
      "0 in it unless you had a missing value in your original column and a 1 otherwise. \n",
      "That way, you’ve ﬁ lled in the missing value as best you could, but you’ve told \n",
      "the model not to quite trust it.\n",
      "• Instead of just using the median, you can train a model like the general linear \n",
      "model presented in Chapter 6 to predict the missing value using the data from \n",
      "the other columns. This is a fair bit of work, but it’s worth it if you have a small \n",
      "dataset and can’t aff ord to lose accuracy or throw away rows. \n",
      "continues\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "254 Data Smart\n",
      "Bagging: Randomize, Train, Repeat\n",
      "Bagging is a technique used to train multiple classiﬁ ers (an ensemble if you will) without \n",
      "them all being trained on the exact same set of training data. Because if you trained the \n",
      "classiﬁ ers on the same data, they’d look identical; you want a variety of models, not a \n",
      "bunch of copies of the same model. Bagging lets you introduce some variety in a set of \n",
      "classiﬁ ers where there otherwise wouldn’t be.\n",
      "Decision Stump Is an Unsexy Term for a Stupid Predictor\n",
      "In the bagging model you’ll be building, the individual classiﬁ ers will be decision stumps. A \n",
      "decision stump is nothing more than a single question you ask about the data. Depending \n",
      "on the answer, you say that the household is either pregnant or not. A simple classiﬁ  er \n",
      "such as this is often called a weak learner.\n",
      "For example, in the training data, if you count the number of times a pregnant house-\n",
      "hold purchased folic acid by highlighting H3:H502 and summing with the summary bar, \n",
      "you’d ﬁ nd that 104 pregnant households made the purchase before giving birth. On the \n",
      "other hand, only two not-pregnant customers bought folic acid.\n",
      "So there’s a relationship between buying folic acid supplements and being pregnant. \n",
      "You can use that simple relationship to construct the following weak learner: \n",
      "Did the household buy folic acid? If yes, then assume they’re pregnant. If no, then assume \n",
      "they’re not pregnant.\n",
      "(continued)\n",
      "• Unfortunately, this last approach (like all others mentioned in this note) feels a \n",
      "bit overly conﬁ dent. It treats the imputed data point as if it’s a ﬁ rst-class citizen \n",
      "once it’s predicted from the regression line. To get around this, statisticians will \n",
      "often use statistical models to generate multiple regression lines. The empty data \n",
      "will be ﬁ lled in multiple times using these regression models, each creating a \n",
      "new imputed dataset. Any analysis will be run on each of the imputed datasets \n",
      "and any results will be combined at the end of the analysis. This is called multiple \n",
      "imputation.\n",
      "• Another approach worth trying is called k nearest neighbors imputation. Using \n",
      "distance (see Chapter 2) or affi  nity matrices (Chapter 5), calculate the k nearest \n",
      "neighbors to an entry with missing data. Take a weighted average by distance \n",
      "(or the most common value if you prefer) of the neighbors’ values to impute the \n",
      "missing data.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "255Ensemble Models: A Whole Lot of Bad Pizza \n",
      "This predictor is visualized in Figure 7-2. \n",
      "Customer Data\n",
      "TRUE\n",
      "Purchased\n",
      "Folic Acid?\n",
      "Not\n",
      "Pregnant Pregnant\n",
      "FALSE\n",
      "Figure 7-2: The folic acid decision stump\n",
      "Doesn’t Seem So Stupid to Me!\n",
      "The stump in Figure 7-2 divides the set of training records into two subsets. Now, you \n",
      "might be thinking that that decision stump makes perfect sense, and you’re right, it does. \n",
      "But it ain’t perfect. After all, there are nearly 400 pregnant households in the training data \n",
      "that didn’t buy folic acid but who would be classiﬁ ed incorrectly by the stump.\n",
      "It’s still better than not having a model at all, right? \n",
      "Undoubtedly. But the question is how much better is the stump than not having a model. \n",
      "One way to evaluate that is through a measurement called node impurity.\n",
      "Node impurity measures how often a chosen customer record would be incorrectly \n",
      "labeled as pregnant or not-pregnant if it were assigned a label randomly, according to the \n",
      "distribution of customers in its decision stump subset.\n",
      "For instance, you could start by shoving all 1,000 training records into the same subset, \n",
      "which is to say, start without a model.\n",
      "The probability that you’ll pull a pregnant person from the heap is 50 percent. And \n",
      "if you label them randomly according to the 50/50 distribution, you have a 50 percent \n",
      "chance of guessing the label correctly.\n",
      "Thus, you have a 50%*50% = 25 percent chance of pulling a pregnant customer and \n",
      "appropriately guessing they’re pregnant. Similarly, you have a 25 percent chance of pulling\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "256 Data Smart\n",
      "a not-pregnant customer and guessing they’re not pregnant. Everything that’s not those \n",
      "two cases is just some version of an incorrect guess.\n",
      "That means I have a 100% – 25% – 25% = 50 percent chance of incorrectly labeling a \n",
      "customer. So you would say that the impurity of my single starting node is 50 percent.\n",
      "The folic acid stump splits this set of 1,000 cases into two groups—894 folks who didn’t \n",
      "buy folic acid and 106 folks who did. Each of those subsets will have its own impurity, \n",
      "so if you average the impurities of those two subsets (adjusting for their size diff erence), \n",
      "you can tell how much the decision stump has improved your situation.\n",
      "For those 894 customers placed into the not-pregnant bucket, 44 percent of them are \n",
      "pregnant and 56 percent are not. This gives an impurity calculation of 100% – 44%^2 – \n",
      "56%^2 = 49 percent. Not a whole lot of improvement.\n",
      "But for the 106 customers placed in the pregnant category, 98 percent of them are \n",
      "pregnant and 2 percent are not. This gives an impurity calculation of 100% – 98%^2 – \n",
      "2%^2 = 4 percent. Very nice. Averaging those together, you ﬁ nd that the impurity for the \n",
      "entire stump is 44 percent. That’s better than a coin ﬂ ip!\n",
      "Figure 7-3 shows the impurity calculation.\n",
      "SPLITTING A FEATURE WITH MORE THAN TWO VALUES\n",
      "In the RetailMart example, all the independent variables are binary. You never have \n",
      "to decide how to split the training data when you create a decision tree—the 1s go \n",
      "one way and the 0s go the other. But what if you have a feature that has all kinds \n",
      "of values?\n",
      "For example, at MailChimp one of the things we predict is whether an e-mail address \n",
      "is alive and can receive mail. One of the metrics we use to do this is how many days have \n",
      "elapsed since someone sent an e-mail to that address. (We send about 7 billion e-mails \n",
      "a month, so we pretty much have data on everyone ...)\n",
      "This feature isn’t anywhere close to being binary! So when we train a decision tree \n",
      "that uses this feature, how do we determine what value to split it on so that some of the \n",
      "training data can go one direction and the rest the other direction?\n",
      "It’s actually really easy.\n",
      "There’s only a ﬁ nite number of values you can split on. At max, it’s one unique value \n",
      "per record in your training set. And there’s probably some addresses in your training \n",
      "set that have the exact same number of days since you last sent to them. \n",
      "You need to consider only these values. If you have four unique values to split on \n",
      "from your training records (say 10 days, 20 days, 30 days, and 40 days), splitting on 35 \n",
      "is no diff erent than splitting on 30. So you just check the impurity scores you get if you \n",
      "chose each value to split on, and you pick the one that gives you the least impurity. Done!\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "257Ensemble Models: A Whole Lot of Bad Pizza \n",
      "Customer Data\n",
      "TRUE\n",
      "Purchased\n",
      "Folic Acid?\n",
      "Not\n",
      "Pregnant Pregnant\n",
      "FALSE\n",
      "500 Pregnant/500 Not\n",
      "Impurity = 100% – (50%2) – (50%2) = 50%\n",
      "396 Pregnant/498 Not (89% of rows)\n",
      "Impurity = 100% – (44%2) – (56%2) = 49%\n",
      "104 Pregnant/2 Not (11% of rows)\n",
      "Impurity = 100% – (98%2) – (2%2) = 4%\n",
      "Average Impurity:\n",
      ".89*.49 + .11*.04 = 44%\n",
      "Figure 7-3: Node impurity for the folic acid stump\n",
      "You Need More Power!\n",
      "A single decision stump isn’t enough. What if you had scads of them, each trained on \n",
      "diff erent pieces of data and each with an impurity slightly lower than 50 percent? Then \n",
      "you could allow them to vote. Based on the percentage of stumps that vote pregnant, you \n",
      "could decide to call a customer pregnant.\n",
      "But you need more stumps.\n",
      "Well, you’ve trained one on the Folic Acid column. Why not just do the same thing on \n",
      "every other feature?\n",
      "You have only 19 features, and frankly, some of those features, like whether the cus-\n",
      "tomer’s address is an apartment, are pretty terrible. So you’d be stuck with 19 stumps of \n",
      "dubious quality.\n",
      "It turns out that through bagging, you can make as many decision stumps as you like. \n",
      "Bagging will go something like this:\n",
      " 1. First, bite a chunk out of the dataset. Common practice is to take roughly the square \n",
      "root of the feature count (four random columns in our case) and a random two \n",
      "thirds of the rows.\n",
      " 2. Build a decision stump for each of those four features you chose using only the \n",
      "random two thirds of the data you picked.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "258 Data Smart\n",
      " 3. Out of those four stumps, single out the purest stump. Keep it. Toss everything back \n",
      "into the big pot and train a new stump.\n",
      " 4. Once you have a load of stumps, grab them all, make them vote, and call them a \n",
      "single model.\n",
      "Let’s Train It\n",
      "You need to be able to select a random set of rows and columns from the training data. \n",
      "And the easiest way to do that is to shuffl  e the rows and columns like a deck of cards and \n",
      "then select what you need from the top left of the table.\n",
      "To start, copy A2:U1002 from the TD tab into the top of a new tab called TD_BAG \n",
      "(you won’t need the feature names, just their index values from row 2). The easiest way to \n",
      "shuffl  e TD_BAG will be to add an extra column and an extra row next to the data ﬁ  lled \n",
      "with random numbers (using the \n",
      "RAND() formula). Sorting by the random values from top \n",
      "to bottom and left to right and then skimming the amount you want off  the upper left of \n",
      "the table gives you a random sample of rows and features.\n",
      "Getting the Random Sample\n",
      "Insert a row above the feature indexes and add the RAND() formula to row 1 (A1:S1) and \n",
      "to column V (V3:V1002). The resulting spreadsheet then looks like Figure 7-4. Note that \n",
      "I’ve titled column V as RANDOM.\n",
      "Figure 7-4: Adding random numbers to the top and side of the data \n",
      "Sort the columns and rows randomly. Start with the columns, because side-to-side \n",
      "sorting is kind of funky. To shuffl  e the columns, highlight columns A through S. Don’t \n",
      "highlight the PREGNANT column, because that’s not a feature; it’s the dependent variable.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "259Ensemble Models: A Whole Lot of Bad Pizza \n",
      "Open the custom sort window (see Chapter 1 for a discussion on custom sorting). \n",
      "From the Sort window (Figure 7-5), press the Options button and select to sort left to \n",
      "right in order to sort the columns. Make sure Row 1, which is the row with the random \n",
      "numbers, is selected as the row to sort by. Also, conﬁ rm that the My List Has Headers box \n",
      "is unchecked since you have no headers in the horizontal direction. \n",
      "Figure 7-5: Sorting from left to right\n",
      "Press OK. You’ll see the columns on the sheet reorder themselves. \n",
      "Now you need to do the same thing to the rows. This time around, select the range \n",
      "A2:V1002, including the PREGNANT column so that it remains tied to its data while \n",
      "excluding the random numbers at the top of the sheet. \n",
      "Access the Custom Sort window again, and under the Options section, select to sort \n",
      "from top to bottom this time.\n",
      "Make sure the My List Has Headers box is checked this time around, and then select \n",
      "the RANDOM column from the drop-down. The Sort window should look like Figure 7-6.\n",
      "Figure 7-6: Sorting from top to bottom\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "260 Data Smart\n",
      "Now that you’ve sorted your training data randomly, the ﬁ rst four columns and the ﬁ rst \n",
      "666 rows form a rectangular random sample that you can grab. Create a new tab called \n",
      "RandomSelection. To pull out the random sample, you point the cell in A1 to the following:\n",
      "=TD_BAG!A2\n",
      "And then copy that formula through D667.\n",
      "You can get the PREGNANT  values next to the sample, by mapping them straight into \n",
      "column E. E1 points to cell U2 from the previous tab:\n",
      "=TD_BAG!U2\n",
      "Just double-click that formula to send it down the sheet. Once you complete this, you’re \n",
      "left with nothing but the random sample from the data (see Figure 7-7). Note that since \n",
      "the data is sorted randomly, you’ll likely end up with four diff erent feature columns.\n",
      "And what’s cool is that if you go back to the TD_BAG tab and sort again, this sample \n",
      "will automatically update!\n",
      "Figure 7-7: Four random columns and a random two-thirds of the rows\n",
      "Getting a Decision Stump Out of the Sample\n",
      "When looking at any one of these four features, there are only four things that can happen \n",
      "between a single feature and the dependent PREGNANT variable:\n",
      "• The feature can be 0 and PREGNANT can be 1.\n",
      "• The feature can be 0 and PREGNANT can be 0.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "261Ensemble Models: A Whole Lot of Bad Pizza \n",
      "• The feature can be 1 and PREGNANT can be 1.\n",
      "• The feature can be 1 and PREGNANT can be 0.\n",
      "You need to get a count of the number of training rows that fall into each of these cases \n",
      "in order to build a stump on the feature similar to that pictured in Figure 7-2. To do this, \n",
      "enumerate the four combinations of 0s and 1s in G2:H5. Set I1:L1 to equal the column \n",
      "indexes from A1:D1.\n",
      "The spreadsheet then looks like Figure 7-8.\n",
      "Figure 7-8: Four possibilities for the training data\n",
      "Once you’ve set up this small table, you need to ﬁ ll it in by getting counts of the train-\n",
      "ing rows whose values match the combination of predictor and pregnant values speciﬁ ed \n",
      "to the left. For the upper-left corner of the table (the ﬁ  rst feature in my random sample \n",
      "ended up being number 15), you can count the number of training rows where feature 15 \n",
      "is a 0 and the PREGNANT column is a 1 using the following formula:\n",
      "=COUNTIFS(A$2:A$667,$G2,$E$2:$E$667,$H2)\n",
      "The COUNTIFS() formula allows you to count rows that match multiple criteria, hence \n",
      "the S at the end of IFS. The ﬁ rst criterion looks at the feature number 15 range (A2:A667) \n",
      "and checks for rows that are identical to the value in G2 (0), whereas the second criterion \n",
      "looks at the \n",
      "PREGNANT range (E2:E667) and checks for rows that are identical to the value \n",
      "in H2 (1). \n",
      "Copy this formula into the rest of the cells in the table to get counts for each case (see \n",
      "Figure 7-9).\n",
      "If you were going to treat each of these features as a decision stump, which value for \n",
      "the feature would indicate pregnancy? It’d be the value with the highest concentration of \n",
      "pregnant customers in the sample.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "262 Data Smart\n",
      "So in row 6 below the count values you can compare these two ratios. In I6 place the \n",
      "formula:\n",
      "=IF(I2/(I2+I3)>I4/(I4+I5),0,1)\n",
      "Figure 7-9: Feature/response pairings for each of the features in the random sample\n",
      "If the ratio of pregnant customers associated with the 0 value for the feature ( I2/\n",
      "(I2+I3)) is larger than that associated with 1 (I4/(I4+I5)), then 0 is predictive of preg-\n",
      "nancy in this stump. Otherwise, 1 is. Copy this formula across through column L. This \n",
      "gives the sheet shown in Figure 7-10.\n",
      "Figure 7-10: Calculating which feature value is associated with pregnancy\n",
      "Using the counts in rows 2 through 5, you can calculate the impurity values for the \n",
      "nodes of each decision stump should you choose to split on that feature.\n",
      "Let’s insert the impurity calculations on row 8 below the case counts. Just as in Figure \n",
      "7-3, you need to calculate an impurity value for the training cases that had a feature value \n",
      "of 0 and average it with those that had a value of 1.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "263Ensemble Models: A Whole Lot of Bad Pizza \n",
      "If you use the ﬁ rst feature (number 15 for me), 299 pregnant folks and 330 not-pregnant \n",
      "folks ended up in the 0 node, so the impurity is 100% – (299/629)^2 – (330/629)^2, which \n",
      "can be entered in the sheet in cell I8 as follows:\n",
      "=1-(I2/(I2+I3))^2-(I3/(I2+I3))^2\n",
      "Likewise, the impurity for the 1 node can be written as follows:\n",
      "=1-(I4/(I4+I5))^2-(I5/(I4+I5))^2\n",
      "They are combined in a weighted average by multiplying each impurity times the \n",
      "number of training cases in its node, summing them, and dividing by the total number \n",
      "of training cases, 666:\n",
      "=(I8*(I2+I3)+I9*(I4+I5))/666\n",
      "You can then drag these impurity calculations across all four features yielding com-\n",
      "bined impurity values for each of the possible decision stumps, as shown in Figure 7-11.\n",
      "Figure 7-11: Combined impurity values for four decision stumps\n",
      "Looking over the impurity values, for my workbook (yours will likely be diff erent due \n",
      "to the random sort), the winning feature is number 8 (looking back at the TD sheet, this \n",
      "is Prenatal Vitamins) with an impurity of 0.450.\n",
      "Recording the Winner\n",
      "All right, so prenatals won on this sample for me. You probably got a diff  erent winner, \n",
      "which you should record somewhere.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "264 Data Smart\n",
      "Label cells N1 and N2 as Winner and Pregnant Is. You’ll save the winning stump in \n",
      "column O. Start with saving the winning column number in cell O1. This would be the \n",
      "value in I1:L1 that has the lowest impurity (in my case that’s 8). You can combine the \n",
      "MATCH and INDEX formulas to do this lookup (see Chapter 1 for more on these formulas):\n",
      "=INDEX(I1:L1,0,MATCH(MIN(I10:L10),I10:L10,0))\n",
      "MATCH(MIN(I10:L10),I10:L10,0) ﬁ nds which column has the minimum impurity on \n",
      "row 10 and hands it to INDEX. INDEX locates the appropriate winning feature label.\n",
      "Similarly, in O2 you can put whether 0 or 1 is associated with pregnancy by ﬁ  nding \n",
      "the value on row 6 from the column with the minimum impurity:\n",
      "=INDEX(I6:L6,0,MATCH(MIN(I10:L10),I10:L10,0))\n",
      "The winning decision stump and its pregnancy-associated node are then called out, as \n",
      "pictured in Figure 7-12.\n",
      "Figure 7-12: The winner’s circle for the four decision stumps\n",
      "Shake Me Up, Judy!\n",
      "Phew! I know that was a lot of little steps to create one stump. But now that all the formulas \n",
      "are in place, creating the next couple hundred will be a lot easier.\n",
      "You can create a second one real quick. But before you do, save the stump you just \n",
      "made. To do that, just copy and paste the values in O1:O2 over to the right into P1:P2.\n",
      "Then to create a new stump, ﬂ  ip back to the TD_BAG tab and shuffl  e the rows and \n",
      "columns again.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "265Ensemble Models: A Whole Lot of Bad Pizza \n",
      "Click back on the RandomSelection tab. Voila! The winner has changed. In my case, \n",
      "it’s folic acid, and the value associated with pregnancy is 1 (see Figure 7-13). The previous \n",
      "stump is saved over to the right.\n",
      "Figure 7-13: Reshufﬂ  ing the data yields a new stump.\n",
      "To save this second stump, right-click column P and select Insert to shift the ﬁ  rst \n",
      "stump to the right. Then paste the new stump’s values in column P. The ensemble now \n",
      "looks like Figure 7-14.\n",
      "Figure 7-14: And then there were two.\n",
      "Well, that second one sure took less time than the ﬁ rst. So here’s the thing ...\n",
      "Let’s say you want to shoot for 200 stumps in the ensemble model. All you have to do \n",
      "is repeat these steps another 198 times. Not impossible, but annoying.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "266 Data Smart\n",
      "Why don’t you just record a macro of yourself doing it and then play the macro back? \n",
      "As it turns out, this shuffl  ing operation is perfect for a macro.\n",
      "For those of you who have never recorded a macro, it’s nothing more than recording \n",
      "a series of repetitive button presses so you can play them back later instead of giving \n",
      "yourself carpal tunnel syndrome.\n",
      "So hop on up to View ➪ Macros (Tools ➪ Macro in Mac OS) and select Record New \n",
      "Macro.\n",
      "Pressing Record will open a window where you can name your macro something like \n",
      "GetBaggedStump. And for convenience sake, let’s associate a shortcut key with the macro. \n",
      "I’m on a Mac so my shortcut keys begin with Option+Cmd, and I’m going to throw in a \n",
      "z into the shortcut box, because that’s the kind of mood I’m in today (see Figure 7-15).\n",
      "Figure 7-15: Getting ready to record a macro\n",
      "Press OK to get recording. Here are the steps that’ll record a full decision stump:\n",
      " 1. Click the TD_BAG tab.\n",
      " 2. Highlight columns A through S.\n",
      " 3. Custom-sort the columns.\n",
      " 4. Highlight rows 2 through 1002.\n",
      " 5. Custom-sort the rows.\n",
      " 6. Click over to the RandomSelection tab.\n",
      " 7. Right-click column P and insert a new blank column.\n",
      " 8. Select and copy the winning stump in O1:O2.\n",
      " 9. Paste Special the values into P1:P2.\n",
      "Go to View ➪ Macro ➪ Stop Recording (Tools ➪ Macro ➪ Stop Recording in Excel \n",
      "2011 for Mac) to end the recording.\n",
      "You should now be able to generate a new decision stump with a single shortcut key \n",
      "press to activate the macro. Hold on while I go click this thing about 198 hundred times . . .\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "267Ensemble Models: A Whole Lot of Bad Pizza \n",
      "Evaluating the Bagged Model\n",
      "That’s bagging! All you do is shuffl  e the data, grab a subset, train a simple classiﬁ  er, and \n",
      "go again. And once you have a bunch of classiﬁ  ers in your ensemble, you’re ready to \n",
      "make predictions.\n",
      "Once you’ve run the decision stump macro a couple hundred times, the RandomSelection \n",
      "sheet should look like Figure 7-16 (your stumps will likely diff er).\n",
      "Figure 7-16: The 200 decision stumps\n",
      "Predictions on the Test Set\n",
      "Now that you have your stumps, it’s time to send your test set data through the model. \n",
      "Create a copy of the Test Set tab and name it TestBag.\n",
      "Moving over to the TestBag tab, insert two blank rows at the top of the sheet to make \n",
      "room for your stumps.\n",
      "Paste the stump values from the RandomSelection tab (P1:HG2 if you’ve got 200 of them) \n",
      "onto the TestBag tab starting in column W. This gives the sheet shown in Figure 7-17.\n",
      "Figure 7-17: Stumps added to the TestBag tab\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "268 Data Smart\n",
      "You can run each row in the Test Set through each stump. Start by running the ﬁ rst row \n",
      "of data (row 4) through the ﬁ rst stump in column W. You can use the OFFSET formula \n",
      "to look up the value from the stump column listed in W1, and if that value equals the \n",
      "one in W2, then the stump predicts a pregnant customer. Otherwise, the stump predicts \n",
      "non-pregnancy. The formula looks like this:\n",
      "=IF(OFFSET($A4,0,W$1)=W$2,1,0)\n",
      "This formula can be copied across all stumps and down the sheet (note the absolute \n",
      "references). This gives the sheet shown in Figure 7-18.\n",
      "Figure 7-18: Stumps evaluated on the TestBag set\n",
      "In column V, take the average of the rows to the left in order to obtain a class probability \n",
      "for pregnancy. For example, in V4 if you have 200 stumps, you’d use:\n",
      "=AVERAGE(W4:HN4) \n",
      "Copy this down column V to get predictions for each row in the test set as shown in \n",
      "Figure 7-19.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "269Ensemble Models: A Whole Lot of Bad Pizza \n",
      "Figure 7-19: Predictions for each row\n",
      "Performance\n",
      "You can evaluate these predictions using the same performance measures used in \n",
      "Chapter 6. I won’t dwell on these calculations since the technique is exactly the same as \n",
      "that in Chapter 6. First, create a new tab called PerformanceBag. In the ﬁ rst column, just \n",
      "as in Chapter 6, calculate the maximum and minimum predictions. For my 200 stumps, \n",
      "that range comes out to 0.02 to 0.75.\n",
      "In column B, place a range of cutoff  values from the minimum to the maximum (in my \n",
      "case, I incremented by 0.02). Precision, speciﬁ city, false positive rate, and recall can all then \n",
      "be calculated in the same way as Chapter 6 (ﬂ ip back to Chapter 6 for the precise details).\n",
      "This gives the sheet shown in Figure 7-20. \n",
      "Note that for a prediction cutoff  of 0.5, that is, with half of the stumps voting pregnant, \n",
      "you can identify 33 percent of pregnant customers with only a 1 percent false positive \n",
      "rate (your mileage may vary due to the random nature of the algorithm). Pretty sweet for \n",
      "some simple stumps!\n",
      "You can also insert a ROC curve using the false positive rate and true positive rate \n",
      "(columns E and F) just as you did in Chapter 6. For my 200 stumps, I got Figure 7-21.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "270 Data Smart\n",
      "Figure 7-20: Performance metrics for bagging\n",
      "0%\n",
      "0% 10%20%30%40%50% 60%70%80%90%100%\n",
      "True Positive Rate\n",
      "False Positive Rate\n",
      "ROC Curve\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "90%\n",
      "100%\n",
      "Figure 7-21: The ROC Curve for Bagged Stumps\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "271Ensemble Models: A Whole Lot of Bad Pizza \n",
      "Beyond Performance\n",
      "While this bagged stumps model is supported by industry standard packages like R’s \n",
      "randomForest package, it’s important to call out two diff erences between this and typical \n",
      "random forest modeling settings:\n",
      "• Vanilla random forests usually sample with replacement, meaning that the same row \n",
      "from the training data can be pulled into the random sample more than once. When \n",
      "you sample with replacement, you can sample the same number of records as the \n",
      "actual training set rather than limiting it to two thirds. In practice, while sampling \n",
      "with replacement has nicer statistical properties, if you’re working with a large \n",
      "enough dataset, there’s virtually no diff erence between the two sampling methods.\n",
      "• Random forests by default grow full classiﬁ cation trees rather than stumps. A full tree \n",
      "is one where once you’ve split the data into two nodes, you pick some new features \n",
      "to split those nodes apart, and on and on until you hit some stopping criteria. Full \n",
      "classiﬁ cation trees are better than stumps when there are interactions between the \n",
      "features that can be modeled.\n",
      "Moving the conversation beyond model accuracy, here are some advantages to the \n",
      "bagging approach:\n",
      "• Bagging is resistant to outliers and tends not to overﬁ t the data. Overﬁ tting occurs \n",
      "when the model ﬁ ts more than just the signal in your data and actually ﬁ  ts the \n",
      "noise as well.\n",
      "• The training process can be parallelized since training an individual weak learner \n",
      "is not dependent on the training of a previous weak learner.\n",
      "• This type of model can handle tons of decision variables.\n",
      "The models we use at MailChimp for predicting spam and abuse are random forest \n",
      "models, which we train in parallel using around 10 billion rows of raw data. That’s not \n",
      "going to ﬁ t in Excel, and I sure as heck wouldn’t use a macro to do it! \n",
      "No, I use the R programming language with the \n",
      "randomForest package, which I would \n",
      "highly recommend learning about as a next step if you want to take one of these models \n",
      "into production at your organization. Indeed, the model in this chapter can be achieved \n",
      "by the \n",
      "randomForest package merely by turning off  sampling with replacement and set-\n",
      "ting the maximum nodes in the decision trees to 2 (see Chapter 10).\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "272 Data Smart\n",
      "Boosting: If You Get It Wrong, Just Boost and \n",
      "Try Again\n",
      "What was the reason behind doing bagging, again? \n",
      "If you trained up a bunch of decision stumps on the whole dataset over and over again, \n",
      "they’d be identical. By taking random selections of the dataset, you introduce some variety \n",
      "to your stumps and end up capturing nuances in the training data that a single stump \n",
      "never could.\n",
      "Well, what bagging does with random selections, boosting does with weights. Boosting \n",
      "doesn’t take random portions of the dataset. It uses the whole dataset on each training \n",
      "iteration. Instead, with each iteration, boosting focuses on training a decision stump that \n",
      "resolves some of the sins committed by the previous decision stumps. It works like this:\n",
      "• At ﬁ rst, each row of training data counts exactly the same. They all have the same \n",
      "weight. In your case, you have 1000 rows of training data, so they all start with a \n",
      "weight of 0.001. This means the weights sum up to 1.\n",
      "• Evaluate each feature on the entire dataset to pick the best decision stump. Except \n",
      "when it comes to boosting instead of bagging, the winning stump will be the one that \n",
      "has the lowest weighted error. Each wrong prediction for a possible stump is given a \n",
      "penalty equal to that row’s weight. The sum of those penalties is the weighted error. \n",
      "Choose the decision stump that gives the lowest weighted error.\n",
      "• The weights are adjusted. If the chosen decision stump accurately predicts a row, \n",
      "then that row’s weight decreases. If the chosen decision stump messes up on a row, \n",
      "then that row’s weight increases.\n",
      "• A new stump is trained using these new weights. In this way, as the algorithm rolls \n",
      "on, it concentrates more on the rows in the training data that previous stumps \n",
      "haven’t gotten right. Stumps are trained until the weighted error exceeds a threshold.\n",
      "Some of this may seem a bit vague, but the process will become abundantly clear in a \n",
      "spreadsheet. Off  to the data!\n",
      "Training the Model—Every Feature Gets a Shot\n",
      "In boosting, each feature is a possible stump on every iteration. You won’t be selecting \n",
      "from four features this time.\n",
      "To start, create a tab called BoostStumps. And on it, paste the possible feature/response \n",
      "value combinations from G1:H5 of the RandomSelection tab.\n",
      "Next to those values, paste the feature index values (0–18) in row 1. This gives the \n",
      "sheet shown in Figure 7-22.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "273Ensemble Models: A Whole Lot of Bad Pizza \n",
      "Figure 7-22: The initial portions of the BoostStumps tab\n",
      "Below each index, just as in the bagging process, you must sum up the number of train-\n",
      "ing set rows that fall into each of the four combinations of feature value and independent \n",
      "variable value listed in columns A and B.\n",
      "Start in cell C2 (feature index 0) by summing the number of training rows that \n",
      "have a 0 for the feature value and also are pregnant. This can be counted using the \n",
      "COUNTIFS formula:\n",
      "=COUNTIFS(TD!A$3:A$1002,$A2,TD!$U$3:$U$1002,$B2)\n",
      "The use of absolute references allows you to copy this formula through U5. This gives \n",
      "the sheet shown in Figure 7-23.\n",
      "Figure 7-23: Counting up how each feature splits the training data\n",
      "And just as in the case of bagging, in C6 you can ﬁ  nd the value associated with preg-\n",
      "nancy for feature index 0 by looking at the pregnancy ratios associated with a feature \n",
      "value of 0 and a feature value of 1:\n",
      "=IF(C2/(C2+C3)>C4/(C4+C5),0,1)\n",
      "This too may be copied through column U.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "274 Data Smart\n",
      "Now, in column B enter in the weights for each data point. Begin in B9 with the label \n",
      "Current Weights, and below that through B1009 put in a 0.001 for each of the thousand \n",
      "training rows. Across row 9, paste the feature names from the TD sheet, just to keep track \n",
      "of each feature.\n",
      "This gives the sheet shown in Figure 7-24.\n",
      "For each of these possible decision stumps, you need to calculate its weighted error \n",
      "rate. This is done by locating the training rows that are miscategorized and penalizing \n",
      "each according to its weight.\n",
      "For instance in C10, you can look back at the ﬁ rst training row’s data for feature index 0 \n",
      "(A3 on the TD tab), and if it matches the pregnancy indicator in C6, then you get a penalty \n",
      "(the weight in cell B10) if the row is not pregnant. If the feature value does not match C6, \n",
      "then you get a penalty if the row is pregnant. This gives the following two \n",
      "IF statements:\n",
      "=IF(AND(TD!A3=C$6,TD!$U3=0),$B10,0)+IF(AND(TD!A3<>C$6,TD!$U3=1),$B10,0)\n",
      "The absolute references allow you to copy this formula through U1009. The weighted \n",
      "error for each possible decision stump may then be calculated in row 7. For cell C7 the \n",
      "calculation of the weighted error is:\n",
      "=SUM(C10:C1009)\n",
      "Figure 7-24: Weights for each training data row\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "275Ensemble Models: A Whole Lot of Bad Pizza \n",
      "Copy this across row 7 to get the weighted error of each decision stump (see \n",
      "Figure 7-25).\n",
      "Figure 7-25: The weighted error calculation for each stump\n",
      "Tallying Up the Winner\n",
      "Label cell W1 as the Winning Error, and in X1, ﬁ nd the minimum of the weighted error \n",
      "values:\n",
      "=MIN(C7:U7)\n",
      "Just as in the bagging section, in X2 combine the INDEX and MATCH formulas to grab the \n",
      "feature index of the winning stump:\n",
      "=INDEX(C1:U1,0,MATCH(X1,C7:U7,0))\n",
      "And in X3, you can likewise grab the value associated with pregnancy for the stump \n",
      "using INDEX and MATCH:\n",
      "=INDEX(C6:U6,0,MATCH(X1,C7:U7,0))\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "276 Data Smart\n",
      "This gives the sheet shown in Figure 7-26. Starting with equal weights for each data \n",
      "point, feature index 5 with a value of 0 indicating pregnancy is chosen as the top stump. \n",
      "Flipping back to the TD tab, you can see that this is the Birth Control feature.\n",
      "Figure 7-26: The ﬁ rst winning boosted stump\n",
      "Calculating the Alpha Value for the Stump\n",
      "Boosting works by giving weight to training rows that were misclassiﬁ  ed by previous \n",
      "stumps. Stumps at the beginning of the boosting process are then more generally eff ective, \n",
      "while the stumps at the end of the training process are more specialized—the weights \n",
      "have been altered to concentrate on a few annoying points in the training data.\n",
      "These stumps with specialized weights help ﬁ t the model to the strange points in the \n",
      "dataset. However in doing so, their weighted error will be larger than that of the initial \n",
      "stumps in the boosting process. As their weighted error rises, the overall improvement \n",
      "they contribute to the model falls. In boosting, this relationship is quantiﬁ ed with a value \n",
      "called alpha:\n",
      "alpha = 0.5 * ln((1 – total weighted error for the stump)/total weighted error for the stump)\n",
      "As the total weighted error of a stump climbs, the fraction inside the natural log func-\n",
      "tion grows smaller and closer to 1. Since the natural log of 1 is 0, the alpha value gets \n",
      "tinier and tinier. Take a look at it in the context of the sheet.\n",
      "Label cell W4 as Alpha and in X4 send the weighted error from call X1 through the \n",
      "alpha calculation:\n",
      "=0.5*LN((1-X1)/X1)\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "277Ensemble Models: A Whole Lot of Bad Pizza \n",
      "For this ﬁ rst stump, you end up with an alpha value of 0.207 (see Figure 7-27). \n",
      "Figure 7-27: Alpha value for the ﬁ  rst boosting iteration\n",
      "How exactly are these alpha values used? In bagging, each stump gave a 0/1 vote when \n",
      "predicting. When it comes time to predict with your boosted stumps, each classiﬁ er will \n",
      "instead give alpha if it thinks the row is pregnant and –alpha if not. So for this ﬁ rst stump, \n",
      "when used on the test set, it would give 0.207 points to any customer who had not bought \n",
      "birth control and -0.207 points to any customer who had. The ﬁ  nal prediction of the \n",
      "ensemble model is the sum of all these positive and negative alpha values.\n",
      "As you’ll see later on, to determine the overall pregnancy prediction coming from \n",
      "the model, a cutoff  is set for the sum of the individual stump scores. Since each stump \n",
      "returns either a positive or negative alpha value for its contribution to the prediction, it \n",
      "is customary to use 0 as the classiﬁ  cation threshold for pregnancy, however this can be \n",
      "tuned to suit your precision needs.\n",
      "Reweighting\n",
      "Now that you’ve completed one stump, it’s time to reweight the training data. And to \n",
      "do that, you need to know which rows of data this stump gets right and which rows it \n",
      "gets wrong. \n",
      "So in column V label V9 as Wrong. In V10, you can use the \n",
      "OFFSET formula in combi-\n",
      "nation with the winning stump’s column index (cell X2) to look up the weighted error \n",
      "for the training row. If the error is nonzero, then the stump is incorrect for that row, and \n",
      "Wrong is set to 1:\n",
      "=IF(OFFSET($C10,0,$X$2)>0,1,0)\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "278 Data Smart\n",
      "This formula can be copied down to all training rows (note the absolute references).\n",
      "Now, the original weights for this stump are in column B. To adjust the weights accord-\n",
      "ing to which rows are set to 1 in the Wrong column, boosting multiplies the original weight \n",
      "times exp(alpha * Wrong) (where exp is the exponential function you encountered when \n",
      "doing logistic regression in Chapter 6). \n",
      "If the value in the Wrong column is 0, then exp(alpha * Wrong) becomes 1, and the \n",
      "weight stays put. \n",
      "If Wrong is set to 1, then exp(alpha * Wrong) is a value larger than 1, so the entire \n",
      "weight is scaled up. Label column W as Scale by Alpha, and in W10, you can calculate \n",
      "this new weight as:\n",
      "=$B10*EXP($V10*$X$4)\n",
      "Copy this down through the dataset.\n",
      "Unfortunately, these new weights don’t sum up to one like your old weights. They need \n",
      "to be normalized (adjusted so that they sum to one). So label X9 as Normalize and in X10, \n",
      "divide the new, scaled weight by the sum of all the new weights:\n",
      "=W10/SUM(W$10:W$1009)\n",
      "This ensures that your new weights sum to one. Copy the formula down. This gives \n",
      "the sheet shown in Figure 7-28.\n",
      "Figure 7-28: The new weight calculation\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "279Ensemble Models: A Whole Lot of Bad Pizza \n",
      "Do That Again... and Again...\n",
      "Now you’re ready to build a second stump. First, copy the winning stump data from the \n",
      "previous iteration over from X1:X4 to Y1:Y4.\n",
      "Next, copy the new weight values from column X over to column B. The entire sheet \n",
      "will update to select the stump that’s best for the new set of weights. As shown in \n",
      "Figure 7-29, the second winning stump is index 7 (Folic Acid) where a 1 indicates \n",
      "pregnancy.\n",
      "You can train 200 of these stumps in much the same way as you did in the bagging \n",
      "process. Simply record a macro that inserts a new column Y, copies the values from X1:X4 \n",
      "into Y1:Y4, and pastes the weights over from column X to column B. \n",
      "After 200 iterations, your weighted error rate will have climbed very near to 0.5 while \n",
      "your alpha value will have fallen to 0.005 (see Figure 7-30). Consider that your ﬁ rst stump \n",
      "had an alpha value of 0.2. That means that these ﬁ nal stumps are 40 times less powerful \n",
      "in the voting process than your ﬁ rst stump.\n",
      "Figure 7-29: The second stump\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "280 Data Smart\n",
      "Figure 7-30: The 200 th stump\n",
      "Evaluating the Boosted Model\n",
      "That’s it! You’ve now trained an entire boosted decision stumps model. You can compare \n",
      "it to the bagged model by looking at its performance metrics. To make that happen, you \n",
      "must ﬁ rst make predictions using the model on the test set data.\n",
      "Predictions on the Test Set\n",
      "First make a copy of the Test Set called TestBoost and insert four blank rows at the top \n",
      "of it to make room for your winning decision stumps. Beginning in column W on the \n",
      "TestBoost tab, paste your stumps (all 200 in my case) at the top of the sheet. This gives \n",
      "the sheet shown in Figure 7-31.\n",
      "Figure 7-31: Decision stumps pasted to TestBoost\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "281Ensemble Models: A Whole Lot of Bad Pizza \n",
      "In W6, you can then evaluate the ﬁ rst stump on the ﬁ rst row of test data using OFFSET \n",
      "just as you did with the bagged model. Except this time, a pregnancy prediction returns \n",
      "the stump’s alpha value (cell W4) and a non-pregnancy prediction returns –alpha:\n",
      "=IF(OFFSET($A6,0,W$2)=W$3,W$4,-W$4)\n",
      "Copy this formula across to all the stumps and down through all the test rows (see \n",
      "Figure 7-32). To make a prediction for a row, you sum these values across all its individual \n",
      "stump predictions.\n",
      "Figure 7-32: Predictions on each row of test data from each stump\n",
      "Label V5 as Score. The score then for V6 is just the sum of the predictions to the right:\n",
      "=SUM(W6:HN6)\n",
      "Copy this sum down. You get the sheet shown in Figure 7-33. A score in column V \n",
      "above 0 means that more alpha-weighted predictions went in the pregnant direction than \n",
      "in the not pregnant direction (see Figure 7-33).\n",
      "Calculating Performance\n",
      "To measure the performance of the boosted model on the test set, simply create a copy of \n",
      "the PerformanceBag tab called PerformanceBoost, point the formulas at column V on the \n",
      "TestBoost tab, and set the cutoff  values to range from the minimum score to the maxi-\n",
      "mum score produced by the boosted model. In my case, I incremented the cutoff   values \n",
      "by 0.25 between a minimum prediction score of -8 and a maximum of 4.5. This gives the \n",
      "performance tab shown in Figure 7-34.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "282 Data Smart\n",
      "Figure 7-33: Final predictions from the boosted model\n",
      "With this model, you can see that a score cutoff   of 0 produces a true positive rate \n",
      "85 percent with only a 27 percent false positive rate. Not bad for 200 stupid stumps.\n",
      "Add the boosted model’s ROC curve to the bagged model’s ROC curve to compare the \n",
      "two just as you did in Chapter 6. As seen in Figure 7-35, at 200 stumps each, the boosted \n",
      "model outperforms the bagged model for many points on the graph.\n",
      "Figure 7-34: The performance metrics for boosted stumps\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "283Ensemble Models: A Whole Lot of Bad Pizza \n",
      "0%\n",
      "0% 10% 20% 30% 40% 50% 60% 70% 80% 90%100%\n",
      "True Positive Rate\n",
      "False Positive Rate\n",
      "ROC Curve\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "Boosted\n",
      "Bagged80%\n",
      "90%\n",
      "100%\n",
      "Figure 7-35: The ROC curves for the boosted and bagged models\n",
      "Beyond Performance\n",
      "In general, boosting requires fewer trees than bagging to produce a good model. It’s not \n",
      "as popular in practice as bagging, because there is a slightly higher risk of overﬁ tting the \n",
      "data. Since each reweighting of the training data is based on the misclassiﬁ  ed points in \n",
      "the previous iteration, you can end up in a situation where you’re training classiﬁ  ers to \n",
      "be overly-sensitive to a few noisy points in the data.\n",
      "Also, the iterative reweighting of the data means that boosting, unlike bagging, cannot \n",
      "be parallelized across multiple computers or CPU cores.\n",
      "That said, in a neck and neck contest between a well ﬁ  t boosted model and a well ﬁ t \n",
      "bagged model, it’s hard for the bagged model to win.\n",
      "Wrapping Up\n",
      "You’ve just seen how a bunch of simple models can be combined via bagging or boosting \n",
      "to form an ensemble model. These approaches were unheard of until about the mid-1990s, \n",
      "but today, they stand as two of the most popular modeling techniques used in business.\n",
      "And you can boost or bag any model that you want to use as a weak learner. These \n",
      "models don’t have to be decision stumps or trees. For example, there’s been a lot of talk \n",
      "recently about boosting naïve Bayes models like the one you encountered in Chapter 3.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "284 Data Smart\n",
      "In Chapter 10, you’ll implement some of what you’ve encountered in this chapter using \n",
      "the R programming language.\n",
      "If you’d like to learn more about these algorithms, I’d recommend reading about them \n",
      "in The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome \n",
      "Friedman (Springer, 2 009).\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "8\n",
      "A\n",
      "s you saw in Chapters 3, 6 and 7, supervised machine learning is about predicting a \n",
      "value or classifying an observation using a model trained on past data. Forecasting \n",
      "is similar. Sure, you can forecast without data (astrology, anyone?). But in quantitative \n",
      "forecasting, past data is used to predict a future outcome. Indeed, some of the same tech-\n",
      "niques, such as multiple regression (introduced in Chapter 6), are used in both disciplines.\n",
      "But where forecasting and supervised machine learning diff er greatly is in their canoni-\n",
      "cal problem spaces. Typical forecasting problems are about taking some data point over \n",
      "time (sales, demand, supply, GDP, carbon emissions, or population, for example) and \n",
      "projecting that data into the future. And in the presence of trends, cycles, and the occa-\n",
      "sional act of God, the future data can be wildly outside the bounds of the observed past.\n",
      "And that’s the problem with forecasting: unlike in Chapters 6 and 7 where pregnant \n",
      "women more or less keep buying the same stuff , forecasting is used in contexts where the \n",
      "future often looks nothing like the past. \n",
      "Just when you think you have a good projection for housing demand, the housing bubble \n",
      "bursts and your forecast is in the toilet. Just when you think you have a good demand \n",
      "forecast, a ﬂ ood disrupts your supply chain, limiting your supply, forcing you to raise \n",
      "prices, and throwing your sales completely out of whack. Future time series data can and \n",
      "will look diff erent than the data you’ve observed before.\n",
      "The only guarantee with forecasting is that your forecast is wrong. You hear that a lot in \n",
      "the world of forecasting. But that doesn’t mean you don’t try. When it comes to planning \n",
      "your business, you often need some projection. At MailChimp, we might continue to grow \n",
      "like gangbusters, or a hole might open up under Atlanta and swallow us. But we make an \n",
      "eff ort to forecast growth as best we can so that we can plan our infrastructure and HR \n",
      "pipelines. You don’t always want to be playing catch-up.\n",
      "And as you’ll see in this chapter, you can try forecasting the future, but you can also \n",
      "quantify the uncertainty around the forecast. And quantifying the forecast uncertainty \n",
      "by creating prediction intervals is invaluable and often ignored in the forecasting world.\n",
      "Forecasting: Breathe \n",
      "Easy; You Can’t Win\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart286\n",
      "As one wise forecaster said, “A good forecaster is not smarter than everyone else; they \n",
      "merely have their ignorance better organized.” \n",
      "So without further ado, let’s go organize some ignorance.\n",
      "The Sword Trade Is Hopping\n",
      "Imagine with me that you’re a rabid Lord of the Rings fan. Years ago when the ﬁ rst of the \n",
      "feature ﬁ lms came out, you strapped on some prosthetic hobbit feet and waited in line for \n",
      "hours to see the ﬁ rst midnight showing. Soon you were attending conventions and arguing \n",
      "on message boards about whether Frodo could have just ridden an eagle to Mount Doom.\n",
      "One day, you decided to give something back. You took a course at the local commu-\n",
      "nity college on metalwork and began handcrafting your own swords. Your favorite sword \n",
      "from the book was Anduril, the Flame of the West. You became an expert at hammering \n",
      "out those beefy broadswords in your homemade forge, and you started selling them on \n",
      "Amazon, eBay, and Etsy. These days, your replicas are the go-to swords for the discerning \n",
      "nerd; business is booming.\n",
      "In the past, you’ve found yourself scrambling to meet demand with the materials on \n",
      "hand. And so you’ve decided to forecast your future demand. So you dump your past sales \n",
      "data in a spreadsheet. But how do you take that past data and project it out?\n",
      "This chapter looks at a set of forecasting techniques called exponential smoothing meth-\n",
      "ods. They’re some of the simplest and most widely used techniques in business today. \n",
      "Indeed, I know a few Fortune 500s just off   the top of my head that forecast with these \n",
      "techniques, because they’ve proven the most accurate for their data.\n",
      "This accuracy stems in part from the techniques’ simplicity—they resist over-ﬁ tting the \n",
      "often-sparse historical data used in forecasting. Furthermore, with these techniques, it’s \n",
      "relatively easy to compute prediction intervals around exponential smoothing forecasts, \n",
      "so you’re going to do a bit of that too.\n",
      "Getting Acquainted with Time Series Data\n",
      "NOTE\n",
      "The Excel workbook used in this chapter, “SwordForecasting.xlsm,” is available for \n",
      "download at the book’s website at www.wiley.com/go/datasmart . This workbook \n",
      "includes all the initial data if you want to work from that. Or you can just read along \n",
      "using the sheets I’ve already put together in the workbook.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "287Forecasting: Breathe Easy; You Can’t Win \n",
      "The workbook for this chapter includes the last 36 months of sword demand starting from \n",
      "January three years ago. The data is shown in the Timeseries tab in Figure 8-1. As men-\n",
      "tioned earlier in this chapter, data like this—observations over regular time intervals—is \n",
      "called time series data. The time interval can be whatever is appropriate for the problem \n",
      "at hand, whether that’s yearly population ﬁ gures or daily gas prices.\n",
      "Figure 8-1: Time series data\n",
      "In this case, you have monthly sword demand data, and the ﬁ rst thing you should do \n",
      "with it is plot it, as shown in Figure 8-2. To insert a plot like this, just highlight columns \n",
      "A and B in Excel and pick Scatter from the charts section of the Excel ribbon (Charts tab \n",
      "on Mac, Insert Tab on Windows). You can adjust the range of your axes by right-clicking \n",
      "them and selecting the Format option. \n",
      "So what do you see in Figure 8-2? The data ranges from the 140s three years ago to \n",
      "304 last month. That’s a doubling of demand in three years—so maybe there’s an upward \n",
      "trend? You’ll come back to this thought in a bit.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "288 Data Smart\n",
      "Figure 8-2: Scatter plot of time series data\n",
      "There are a few ups and downs that may be indicative of some seasonal pattern. For \n",
      "instance, months 12, 24, and 36, which are all Decembers, are the highest demand months \n",
      "for each of their years. But that could just be chance or due to the trend. Let’s ﬁ nd out.\n",
      "Starting Slow with Simple Exponential Smoothing\n",
      "Exponential smoothing techniques base a future forecast off  of past data where the most \n",
      "recent observations are weighted more than older observations. This weighting is done \n",
      "through smoothing constants . The ﬁ rst exponential smoothing method you’re going to \n",
      "tackle is called simple exponential  smoothing (SES), and it uses only one smoothing con-\n",
      "stant, as you’ll see.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "289Forecasting: Breathe Easy; You Can’t Win \n",
      "Simple exponential smoothing assumes that your time series data is made up of two \n",
      "components: a level (or mean) and some error around that level. There’s no trend, no \n",
      "seasonality, just a level around which the demand hovers with little error jitters here \n",
      "and there. By preferring recent observations, SES can account for shifts in this level. In \n",
      "formula-speak then, you have:\n",
      "  Demand  at time t = level + random error around the level at time t\n",
      "And the most current estimate of the level serves as a forecast for future time periods. \n",
      "If you’re at month 36, what’s a good estimate of demand at time period 38? The most \n",
      "recent level estimate. And time 40? The level. Simple—hence the name simple exponen-\n",
      "tial smoothing.\n",
      "So how do you get an estimate of the level? \n",
      "If you assume that all your historical values are of equal importance, you just take a straight \n",
      "average. \n",
      "This mean would give you a level, and you’d forecast the future by just saying, “Demand \n",
      "in the future is the average of the past demand.” And there are companies that do this. I’ve \n",
      "seen monthly forecasts at companies where future months were equal to the average of \n",
      "those same months over the past few years. Plus a “fudge factor” for kicks. Yes, forecast-\n",
      "ing is often done so hand-wavily that even at huge, public companies words like “fudge \n",
      "factor” are still used. Eek.\n",
      "But when the level shifts over time, you don’t want to give equal weight to each histori-\n",
      "cal point in the way that an average does. Should 2008 through 2013 all carry the same \n",
      "weight when forecasting 2014? Maybe, but for most businesses, probably not. So you want \n",
      "a level estimate that gives more weight to your recent demand observations.\n",
      "So let’s think about calculating the level, instead, by rolling over the data points in \n",
      "order, updating the level calculation as you go. To start, say the initial estimate of the level \n",
      "is the average of some of the earliest data points. In this case, pick the ﬁ  rst year’s worth \n",
      "of data. Call this initial estimate of the level, level\n",
      "0:\n",
      "  level 0 = average of the ﬁ rst year’s demand (months 1 – 12)\n",
      "That’s 163 for the sword demand.\n",
      "Now, the way exponential smoothing works is that even though you know demand for \n",
      "months 1 through 36, you’re going to take your most recent forecast components and use \n",
      "them to forecast one month ahead through the entire series.\n",
      "So you use level\n",
      "0 (163) as the forecast for demand in month 1. \n",
      "Now that you’ve forecasted period 1, you take a step forward in time from period 0 to \n",
      "period 1. The actual demand was 165, so you were off  by two swords. You should update\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "290 Data Smart\n",
      "the estimate of the level then to account for this error. Simple exponential smoothing \n",
      "uses this equation:\n",
      "  level 1 = level0 + some percentage * (demand1 – level0)\n",
      "Note that (demand1 - level0) is the error you get when you forecast period one with the \n",
      "initial level estimate. Rolling forward:\n",
      "  level 2 = level1 + some percentage * (demand2 – level1)\n",
      "And again:\n",
      "  level 3 = level2 + some percentage * (demand3 – level2)\n",
      "Now, the percentage of the error you want to fold back into the level is the smoothing \n",
      "constant, and for the level, it’s historically been called alpha. It can be any value between \n",
      "0 and 100 percent (0 and 1). \n",
      "If you set alpha to 1, you’re accounting for all the error, which just means the level of \n",
      "the current period is the demand of the current period. \n",
      "If you set alpha to 0, you conduct absolutely no error correction on that ﬁ  rst level \n",
      "estimate.\n",
      "You’ll likely want something in between those two extremes, but you’ll learn how to \n",
      "pick the best alpha value later.\n",
      "So you can roll this calculation forward through time:\n",
      "  level current period = levelprevious period + alpha * (demandcurrent period – levelprevious period)\n",
      "Eventually you end up with a ﬁ nal level estimate, level36, where the last demand obser-\n",
      "vations count for more because their error adjustments haven’t been multiplied by alpha \n",
      "a zillion times:\n",
      "  level 36 = level35 + alpha * (demand36 – level35)\n",
      "This ﬁ nal estimate of the level is what you’ll use as the forecast of future months. The \n",
      "demand for month 37? Well, that’s just level36. And the demand for month 40? level36. \n",
      "Month 45? level36. You get the picture. The ﬁ nal level estimate is the best one you have \n",
      "for the future, so that’s what you use.\n",
      "Let’s take a look at it in a spreadsheet.\n",
      "Setting Up the Simple Exponential Smoothing Forecast\n",
      "The ﬁ rst thing you’ll do is create a new worksheet in the workbook called SES. Paste the \n",
      "time series data in columns A and B starting at row 4 to leave some room at the top of the\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "291Forecasting: Breathe Easy; You Can’t Win \n",
      "sheet for an alpha value. You can put the number of months you have in your data (36) \n",
      "in cell A2, and an initial swag at the alpha value in C2. I’m going with 0.5, because it’s in \n",
      "between 0 and 1, and that’s just how I roll.\n",
      "Now, in column C, you place the level calculations. You’ll need to insert a new row 5 \n",
      "into the time series data at the top for the initial level estimate at time 0. In C5, use the \n",
      "following calculation:\n",
      "=AVERAGE(B6:B17)\n",
      "This averages the ﬁ rst year’s worth of data to give the initial level. The spreadsheet then \n",
      "looks as shown in Figure 8-3.\n",
      "Figure 8-3: Initial level estimate for simple exponential smoothing\n",
      " Adding in the One-Step Forecast and Error\n",
      "Now that you’ve added the ﬁ rst level value into the sheet, you can roll forward in time \n",
      "using the SES formula laid out in the previous section. To do this, you’ll need to add two \n",
      "columns: a one-step forecast column (D) and a forecast error column (E). The one-step\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "292 Data Smart\n",
      "forecast for time period 1 is just level0 (cell C5), and the error calculation is then the actual \n",
      "demand minus the forecast:\n",
      "=B6-D6\n",
      "The level estimate then for period 1 is the previous level adjusted by alpha times the \n",
      "error, which is:\n",
      "=C5+C$2*E6\n",
      "Note that I’ve placed a $ in front of the alpha value so that when you drag the formula \n",
      "down the sheet, the absolute row reference leaves alpha be. This yields the sheet shown \n",
      "in Figure 8-4.\n",
      "Figure 8-4:  Generating the one-step forecast, error, and level calculation for period 1\n",
      "Drag That Stuff Down!\n",
      "Humorously enough, you’re pretty much done here. Just drag C6:E6 down through all 36 \n",
      "months, and voila, you have level36.\n",
      "Let’s add months 37–48 to column A. The forecast for these next 12 months is just \n",
      "level36. So in B42, you can just add: \n",
      "=C$41\n",
      "as the forecast and drag it down for the next year.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "293Forecasting: Breathe Easy; You Can’t Win \n",
      "This gives you a forecast of 272, as shown in Figure 8-5. \n",
      "Figure 8-5: Simple exponential smoothing forecast with alpha of 0.5\n",
      "But is that the best you can do? Well, the way you optimize this forecast is by setting \n",
      "alpha—the larger alpha is, the less you care about the old demand points.\n",
      "Optimizing for One-Step Error\n",
      "Similar to how you minimized the sum of squared error when ﬁ  tting the regression in \n",
      "Chapter 6, you can ﬁ nd the best smoothing constant for the forecast by minimizing the \n",
      "sum of the squared error for the one-step ahead forecasts.\n",
      "Let’s add a squared error calculation into column F that’s just the value from column \n",
      "E squared, drag that calculation through all 36 months, and sum it in cell E2 as the sum \n",
      "of squared error (SSE). This yields the sheet shown in Figure 8-6.\n",
      "Also, you’re going to add the standard error to the spreadsheet in cell F2. The standard \n",
      "error is just the square root of the SSE divided by 35 (36 months minus the number of \n",
      "smoothing parameters in the model, which for simple exponential smoothing is 1).\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "294 Data Smart\n",
      "Figure 8-6:  The sum of squared error for simple exponential smoothing\n",
      "The standard error is an estimate of the standard deviation of the one-step ahead error. \n",
      "You saw the standard deviation ﬁ rst in Chapter 4. It’s just a measure of the spread of the \n",
      "error.\n",
      "If you have a nicely ﬁ tting forecast model, its error will have a mean of 0. This is to \n",
      "say the forecast is unbiased. It over-estimates demand as often as it underestimates. The \n",
      "standard error quantiﬁ es the spread around 0 when the forecast is unbiased.\n",
      "So in cell F2, you can calculate the standard error as:\n",
      "=SQRT(E2/(36-1))\n",
      "For an alpha value of 0.5, it comes out to 20.94 (see Figure 8-7). And if you’ll recall the \n",
      "68-95-99.7 rule from the normal distribution discussed in Chapter 4, this is saying that 68 \n",
      "percent of the one-step forecast errors should be less than 20.94 and greater than -20.94.\n",
      "Now, what you want to do is shrink that spread down as low as you can by ﬁ nding the \n",
      "appropriate alpha value. You could just try a bunch of diff erent values of alpha. But you’re \n",
      "going to use Solver for the umpteenth time in this book.\n",
      "The Solver setup for this is super easy. Just open Solver, set the objective to the standard \n",
      "error in F2, set the decision variable to alpha in C2, add a constraint that C2 be less than \n",
      "1, and check the box that the decision be non-negative. The recursive level calculations \n",
      "that go into making each forecast error are highly non-linear, so you’ll need to use the \n",
      "evolutionary algorithm to optimize alpha.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "295Forecasting: Breathe Easy; You Can’t Win \n",
      "Figure 8-7: The standard error calculation\n",
      "The Solver formulation should look like what’s shown in Figure 8-8. Pressing Solve, \n",
      "you get an alpha value of 0.73, which gives a new standard error of 20.39. Not a ton of \n",
      "improvement.\n",
      "Figure 8-8:  Solver formulation for optimizing alpha\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "296 Data Smart\n",
      "Let’s Graph It \n",
      "The best way to “gut check” a forecast is to graph it alongside your historical demand \n",
      "and see how the predicted demand takes off  from the past. You can select the historical \n",
      "demand data and the forecast and plot them. I like the look of Excel’s straight-lined scat-\n",
      "ter. To start, select A6:B41, which is just the historical data, and choose the straight-line \n",
      "scatter plot from Excel’s chart section.\n",
      "Once you’ve added that chart, right-click the center of the chart, choose Select Data, \n",
      "and add a new series to the chart with just the forecasted values of A42:B53. You can also \n",
      "add some labels to the axes if you like, after which you should have something similar \n",
      "to Figure 8-9.\n",
      "Figure 8-9: Graphing the ﬁ  nal simple exponential smoothing forecast\n",
      "You Might Have a Trend\n",
      "Just looking at that graph, a few things stand out. First, simple exponential smoothing is \n",
      "just a ﬂ at line—the level. But when you look at the demand data from the past 36 months, \n",
      "it’s on the rise. There appears to be a trend upward, especially at the end.\n",
      "Not to denigrate the human eyeball, but how do you prove it?\n",
      "You prove it by ﬁ tting a linear regression to the demand data and performing a t test \n",
      "on the slope of that trendline, just as you did in Chapter 6.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "297Forecasting: Breathe Easy; You Can’t Win \n",
      "If the slope of the line is nonzero and statistically signiﬁ  cant (has a p value less than \n",
      "0.05 in the t test), you can be conﬁ  dent that the data has a trend. If that last sentence \n",
      "makes absolutely no sense to you, check out the statistical testing section in Chapter 6.\n",
      "Flip back to the Timeseries tab in the workbook to perform the trend test.\n",
      "Now, in Chapter 6 you proved your mettle by performing both an F test and a t test by \n",
      "hand. No one wants to subject you to that again.\n",
      "In this chapter, you’ll use Excel’s built-in \n",
      "LINEST function to ﬁ t a linear regression, pull \n",
      "the slope, standard error of the slope coeffi  cient, and degrees of freedom (see Chapter 6 \n",
      "to understand these terms). Then you can calculate your t statistic and run it through the \n",
      "TDIST function just as in Chapter 6. \n",
      "If you’ve never used \n",
      "LINEST before, Excel’s help documentation on the function is very \n",
      "good. You provide LINEST with the dependent variable data (demand in column B) and \n",
      "the independent variable data (you only have one independent variable and it’s time in \n",
      "column A). \n",
      "You also have to provide a ﬂ ag of \n",
      "TRUE to let the function know to ﬁ t an intercept as part \n",
      "of the regression line, and you have to provide a second ﬂ ag of TRUE to get back detailed \n",
      "stats like standard error and R-squared. For the Timeseries tab data then, a linear regres-\n",
      "sion can be run as:\n",
      "=LINEST(B2:B37,A2:A37,TRUE,TRUE)\n",
      "This call will only return the slope of the regression line however, because LINEST  \n",
      "is an array formula. LINEST returns back all the regression stats in an array, so you can \n",
      "either run LINEST as an array formula to dump everything out into a selected range in a \n",
      "sheet, or you can run LINEST through the INDEX formula and pull off  just the values you \n",
      "care about one by one.\n",
      "For instance, the ﬁ rst components of a regression line that LINEST gives are the regres-\n",
      "sion coeffi  cients, so you can pull the slope for the regression in cell B39 on the Timeseries \n",
      "tab by feeding LINEST through INDEX:\n",
      "=INDEX(LINEST(B2:B37,A2:A37,TRUE,TRUE),1,1)\n",
      "You get back a slope of 2.54, meaning the regression line is showing an upward trend \n",
      "of 2.54 additional demanded swords per month. So there is a slope. But is it statistically \n",
      "signiﬁ cant?\n",
      "To run a t test on the slope, you need to pull the standard error for the slope and the \n",
      "degrees of freedom for the regression. \n",
      "LINEST parks the standard error value in row 2, \n",
      "column 1 of its array of results. So in B40, you can pull it as:\n",
      "=INDEX(LINEST(B2:B37,A2:A37,TRUE,TRUE),2,1)\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "298 Data Smart\n",
      "The only change from pulling the slope is that in the INDEX formula you pull row 2, \n",
      "column 1 for the standard error instead of row 1 column 1 for the slope. \n",
      "The standard error of the slope is given as 0.34. This gives the sheet shown in \n",
      "Figure 8-10.\n",
      "Figure 8-10:  The slope and standard error for a regression line ﬁ  tted to the historical demand\n",
      "Similarly, Excel’s LINEST documentation notes that degrees of freedom for the regres-\n",
      "sion are returned at the fourth row and second column value in the result array. So in B41 \n",
      "you can pull it as follows:\n",
      "=INDEX(LINEST(B2:B37,A2:A37,TRUE,TRUE),4,2)\n",
      "You should get 34 for the degrees of freedom (as noted in Chapter 6, this is calculated \n",
      "as 36 data points minus 2 coeffi  cients from the linear regression).\n",
      "You now have the three values you need to perform a t test on the statistical signiﬁ  -\n",
      "cance of your ﬁ tted trend. Just as in Chapter 6, you can calculate the test statistic as the \n",
      "absolute value of the slope divided by the standard error for the slope. You can pull the p \n",
      "value for this statistic from the t distribution with 34 degrees of freedom using the TDIST \n",
      "function in B42:\n",
      "=TDIST(ABS(B39/B40),B41,2)\n",
      "This returns a p value near 0 implying that if the trend were nonexistent in reality \n",
      "(slope of 0), there’s no chance we would have gotten a slope so extreme from our regres-\n",
      "sion. This is shown in Figure 8-11.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "299Forecasting: Breathe Easy; You Can’t Win \n",
      "Figure 8-11: Your trend is legit\n",
      "All right! So you have a trend. Now you just need to incorporate it into your forecast.\n",
      " Holt’s Trend-Corrected Exponential Smoothing\n",
      "Holt’s Trend-Corrected Exponential  Smoothing expands simple exponential smoothing to \n",
      "create a forecast from data that has a linear trend. It’s often called double exponential  \n",
      "smoothing, because unlike SES, which has one smoothing parameter alpha and one non-\n",
      "error component, double exponential smoothing has two. \n",
      "If the time series has a linear trend, you can write it as:\n",
      "  Demand  at time t = level + t*trend + random error around the level at time t\n",
      "The most current estimates of the level and trend (times the number of periods out) \n",
      "serve as a forecast for future time periods. If you’re at month 36, what’s a good estimate of \n",
      "demand at time period 38? The most recent level estimate plus two months of the trend. \n",
      "And time 40? The level plus four months of the trend. Not as simple as SES but pretty close.\n",
      "Now, just as in simple exponential smoothing, you need to get some initial estimates \n",
      "of the level and trend values, called level\n",
      "0 and trend0. One common way to get them is \n",
      "just to plot the ﬁ rst half of your demand data and send a trendline through it (just like \n",
      "you did in Chapter 6 in the cat allergy example). The slope of the line is trend0 and the \n",
      "y-intercept is level0.\n",
      "Holt’s Trend-Corrected Smoothing has two update equations, one for the level as you roll \n",
      "through time and one for the trend. The level equation still uses a smoothing parameter\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "300 Data Smart\n",
      "called alpha, whereas the trend equation uses a parameter often called gamma. They’re \n",
      "exactly the same—just values between 0 and 1 that regulate how much one-step forecast-\n",
      "ing error is incorporated back into the estimates.\n",
      "So, here’s the new level update equation:\n",
      "  level\n",
      "1 = level0 + trend0 + alpha *  (demand1 – (level0 + trend0) )\n",
      "Note that (level0 + trend0) is just the one-step ahead forecast from the initial values to \n",
      "month 1, so (demand1 – (level0 + trend0) ) is the one-step ahead error. This equation looks \n",
      "identical to the level equation from SES except you account for one time period’s worth \n",
      "of trend whenever you count forward a slot. Thus, the general equation for the level esti-\n",
      "mate is:\n",
      "  level\n",
      "current  period  = levelprevious  period  + trendprevious  period  + alpha * ( demand current  period  – \n",
      "(levelprevious period + trendprevious period) )\n",
      "Under this new smoothing technique, you also need a trend update equation. For the \n",
      "ﬁ rst time slot it’s:\n",
      "  trend 1 = trend0 +gamma * alpha * (demand1 – (level0 + trend0) )\n",
      "So the trend equation is similar to the level update equation. You take the previous \n",
      "trend estimate and adjust it by gamma times the amount of error incorporated into the \n",
      "accompanying level update (which makes intuitive sense because only some of the error \n",
      "you’re using to adjust the level would be attributable to poor or shifting trend estimation).\n",
      "Thus, the general equation for the trend estimate is:\n",
      "  trend\n",
      "current  period  = trend previous  period  + gamma  * alpha  * ( demand current  period  – \n",
      "(levelprevious period + trendprevious period) )\n",
      "Setting Up Holt’s Trend-Corrected Smoothing in a Spreadsheet\n",
      "To start, create a new tab called Holt’sTrend-Corrected. On this tab, just as with the simple \n",
      "exponential smoothing tab, paste the time series data on row 4 and insert an empty row \n",
      "5 for the initial estimates.\n",
      "Column C will once again contain the level estimates, and you’ll put the trend estimates \n",
      "in column D. So at the top of those two columns you’ll put the alpha and gamma values. \n",
      "You’re going to be optimizing them with Solver in a second, but for now, just toss in some \n",
      "0.5s. This gives the sheet shown in Figure 8-12.\n",
      "For the initial values of level and trend that go in C5 and D5, let’s scatter plot the ﬁ rst \n",
      "18 months of data and add a trendline to it with the equation (if you don’t know how to \n",
      "add a trendline to a scatterplot, see Chapter 6 for an example). This gives an initial trend \n",
      "of 0.8369 and an initial level (intercept of the trendline) of 155.88.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "301Forecasting: Breathe Easy; You Can’t Win \n",
      "Figure 8-12: Starting with smoothing parameters set to 0.5\n",
      "Adding these to D5 and C5 respectively, you get the sheet shown in Figure 8-13.\n",
      "Figure 8-13: The initial level and trend values\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "302 Data Smart\n",
      "Now in Columns E and F, add the one-step ahead forecast and forecast error columns. \n",
      "If you look at row 6, the one-step ahead forecast is merely the previous level plus one \n",
      "month’s trend using the previous estimate—that’s C5+D5. And the forecast error is the \n",
      "same as in simple exponential smoothing; F6 is just actual demand minus the one-step \n",
      "forecast—B6-E6.\n",
      "You can then update the level in cell C6 as the previous level plus the previous trend \n",
      "plus alpha times the error:\n",
      "=C5+D5+C$2*F6\n",
      "The trend in D6 is updated as the previous trend plus gamma times alpha times the \n",
      "error:\n",
      "=D5+D$2*C$2*F6\n",
      "Note that you need to use absolute references on both alpha and gamma in order to \n",
      "drag the formulas down. You’ll do that now—drag C6:F6 down through month 36. This \n",
      "is shown in Figure 8-14.\n",
      "Figure 8-14: Dragging down the level, trend, forecast, and error calculations\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "303Forecasting: Breathe Easy; You Can’t Win \n",
      "Forecasting Future Periods\n",
      "To forecast out from month 36, you add the ﬁ nal level (which for an alpha and gamma of \n",
      "0.5 is 281) to the number of months out you’re forecasting times the ﬁ nal trend estimate. \n",
      "You can calculate the number of months between month 36 and the month you care about \n",
      "by subtracting one month in column A from the other.\n",
      "For example, forecasting month 37 in cell B42, you’d use:\n",
      "=C$41+(A42-A$41)*D$41\n",
      "By using absolute references for month 36, the ﬁ nal trend, and the ﬁ nal level, you can \n",
      "drag the forecast down through month 48, giving the sheet shown in Figure 8-15.\n",
      "Figure 8-15: Forecasting future months with Holt’s Trend-Corrected Exponential Smoothing\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "304 Data Smart\n",
      "Just as on the simple exponential smoothing tab, you can graph the historical demand \n",
      "and the forecast as two series on a straight-line scatter plot, as shown in Figure 8-16.\n",
      "With an alpha and gamma of 0.5, that forecast sure looks a bit nutty, doesn’t it? It’s taking \n",
      "off  where the ﬁ nal month ends and increasing at a rather rapid rate from there. Perhaps \n",
      "you should optimize the smoothing parameters.\n",
      "Figure 8-16: Graph of the forecast with default alpha and gamma values\n",
      "Optimizing for One-Step Error\n",
      "As you did for simple exponential smoothing, add the squared forecast error in column \n",
      "G. In F2 and G2, you can calculate the sum of the squared error and the standard error \n",
      "for the one-step forecast exactly as earlier. Except, this time the model has two smoothing \n",
      "parameters so you’ll divide the SSE by 36 – 2 before taking the square root:\n",
      "=SQRT(F2/(36-2))\n",
      "This gives you the sheet shown in Figure 8-17.\n",
      "The optimization setup is identical to simple exponential smoothing except this time \n",
      "around you’re optimizing both alpha and gamma together, as shown in Figure 8-18.\n",
      "When you solve, you get an optimal alpha value of 0.66 and an optimal gamma value of \n",
      "0.05. The optimal forecast is shown in the straight-line scatter in Figure 8-19.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "305Forecasting: Breathe Easy; You Can’t Win \n",
      "Figure 8-17: Calculating the SSE and standard error\n",
      "Figure 8-18: Optimization setup for Holt’s Trend-Corrected Exponential Smoothing\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "306 Data Smart\n",
      "Figure 8-19: Graph of optimal Holt’s forecast\n",
      "The trend you’re using from the forecast is an additional ﬁ ve swords sold per month. \n",
      "The reason why this trend is double the one you found using the trendline on the previ-\n",
      "ous tab is because trend-corrected smoothing favors recent points more, and in this case, \n",
      "the most recent demand points have been very “trendy.”\n",
      "Note how this forecast starts very near the SES forecast for month 37 – 290 versus \n",
      "292. But pretty quickly the trend-corrected forecast begins to grow just like you’d expect \n",
      "with a trend.\n",
      "So Are You Done? Looking at Autocorrelations\n",
      "All right. Is this the best you can do? Have you accounted for everything?\n",
      "Well, one way to check if you have a good model for the forecast is to check the one-\n",
      "step ahead errors. If those errors are random, you’ve done your job. But if there’s a pattern \n",
      "hidden in the error—some kind of repeated behavior at a regular interval—there may be \n",
      "something seasonal in the demand data that is unaccounted for.\n",
      "And by a “pattern in the error,” I mean that if you took the error and lined it up \n",
      "with itself shifted by a month or two months or twelve months, would it move in sync? \n",
      "This concept of the error being correlated with the time-shifted version of itself is called \n",
      "autocorrelation (auto means “self” in Greek. It’s also a good preﬁ x for ditching vowels in \n",
      "Scrabble). \n",
      "So to start, create a new tab called Holt’s Autocorrelation. And in that tab, paste months \n",
      "1 through 36 along with their one-step errors from the Holt’s forecast into columns A and B. \n",
      "Underneath the errors in B38, calculate the average error. This gives the sheet shown \n",
      "in Figure 8-20.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "307Forecasting: Breathe Easy; You Can’t Win \n",
      "Figure 8-20: Months and associated one-step forecast errors\n",
      "In column C, calculate the deviations of each error in column B from the average in \n",
      "B38. These deviations in the one-step error from the average are where patterns are going \n",
      "to rear their ugly head. For instance, maybe every December the forecast error is sub-\n",
      "stantially above average—that type of seasonal pattern would show up in these numbers. \n",
      "In cell C2, then, the deviation of the error in B2 from the mean would be:\n",
      "=B2-B$38\n",
      "You can then drag this formula down to give all the mean deviations. In cell C38, cal-\n",
      "culate the sum of squared deviations as:\n",
      "=SUMPRODUCT($C2:$C37,C2:C37)\n",
      "This gives you the sheet shown in Figure 8-21.\n",
      "Now, in column D “lag” the error deviations by one month. Label column D with a 1. \n",
      "You can leave cell D2 blank and set cell D3 to:\n",
      "=C2\n",
      "And then just drag the formula down until D37 equals C36. This gives you Figure 8-22.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "308 Data Smart\n",
      "Figure 8-21: Sum of squared mean deviations of Holt’s forecast errors\n",
      "Figure 8-22: One month lagged error deviations \n",
      "To lag by two months, just select D1:D37 and drag it into column E. Similarly, to lag up \n",
      "to 12 months, just drag the selection through column O. Easy! This gives you a cascading \n",
      "matrix of lagged error deviations, as shown in Figure 8-23.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "309Forecasting: Breathe Easy; You Can’t Win \n",
      "Figure 8-23: A beautiful cascading matrix of lagged error deviations ﬁ  t for a king\n",
      "Now that you have these lags, think about what it means for one of these columns to \n",
      "“move in sync” with column C. For instance, take the one-month lag in column D. If these \n",
      "two columns were in sync then when one goes negative, the other should. And when one \n",
      "is positive, the other should be positive. That means that the product of the two columns \n",
      "would result in a lot of positive numbers (a negative times a negative or a positive times \n",
      "a positive results in a positive number). \n",
      "You can sum these products, and the closer this \n",
      "SUMPRODUCT of the lagged column with \n",
      "the original deviations gets to the sum of squared deviations in C38, the more in sync, \n",
      "the more correlated, the lagged errors are with the originals. \n",
      "You can also get negative autocorrelation where the lagged deviations go negative \n",
      "whenever the originals are positive and vice versa. The \n",
      "SUMPRODUCT in this case will be a \n",
      "larger negative number.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "310 Data Smart\n",
      "To start, drag the SUMPRODUCT($C2:$C37,C2:C37) in cell C38 across through column O. \n",
      "Note how the absolute reference to column C will keep the column in place, so you get the \n",
      "SUMPRODUCT of each lag column with the original, as shown in Figure 8-24.\n",
      "Figure 8-24: SUMPRODUCT of lagged deviations with originals\n",
      "You calculate the autocorrelation for a given month lag as the SUMPRODUCT  of lagged \n",
      "deviations times original deviations divided by the sum of squared deviations in C38.\n",
      "For example, you can calculate the autocorrelation of a one-month lag in cell D40 as:\n",
      "=D38/$C38\n",
      "And dragging this across, you can get the autocorrelations for each lag.\n",
      "Highlighting D40:O40, you can insert a bar chart into the sheet as shown in Figure \n",
      "8-25 (Right-click and format the series’ ﬁ ll to be slightly transparent if you want to read \n",
      "the month labels under the negative values). This bar chart is called a correlogram, and it \n",
      "shows the autocorrelations for each month lag up to a year. (As a personal note, I think \n",
      "the word correlogram is really cool.)\n",
      "All right. So which autocorrelations matter? Well, the convention is that you only worry \n",
      "about the autocorrelations larger than 2/sqrt(number of data points), which in this case \n",
      "is 2/sqrt(36) = 0.333. You should also care about ones with a negative autocorrelation less \n",
      "than -0.333. \n",
      "You can just eyeball your chart for autocorrelations that are above or below these \n",
      "critical values. But it’s typical in forecasting to plot some dashed lines at these critical values \n",
      "on the correlogram. For the sake of a pretty picture, I’ll show you how to do that here.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "311Forecasting: Breathe Easy; You Can’t Win \n",
      "Figure 8-25: This is my correlogram; there are many like it but this is mine.\n",
      "In D42, add =2/SQRT(36) and drag it across through O. Do the same in D43 only with \n",
      "the negative value =-2/SQRT(36) and drag that across through O. This gives you the criti-\n",
      "cal points for the autocorrelations, as shown in Figure 8-26.\n",
      "Figure 8-26: Critical points for the autocorrelations\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "312 Data Smart\n",
      "Right-click the correlation bar chart and choose Select Data. From the window that \n",
      "appears, press the Add button to create a new series. \n",
      "For one series select the range D42:O42 as the y-values. Add a third series using \n",
      "D43:O43. This will add two more sets of bars to the graph. \n",
      "Right-clicking each of these new bar series, you can select Change Series Chart Type \n",
      "and select the Line chart to turn it into a solid line instead of bars. Right-click these lines \n",
      "and select Format Data Series. Then navigate to the Line (Line Style in some Excel ver-\n",
      "sions) option in the window. In this section, you can set the line to dashed, as shown in \n",
      "Figure 8-27.\n",
      "Figure 8-27: Changing the critical values for bars into a dashed line\n",
      "This yields a correlogram with plotted critical values, as shown in Figure 8-28.\n",
      "And what do you see? \n",
      "There’s exactly one autocorrelation that’s above the critical value, and that’s at 12 \n",
      "months. \n",
      "The error shifted by a year is correlated with itself. That indicates a 12-month seasonal cycle. \n",
      "This shouldn’t be too surprising. If you look at the plot of the demand on the Timeseries \n",
      "tab, it’s apparent that there are spikes each Christmas and dips around April/May.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "313Forecasting: Breathe Easy; You Can’t Win \n",
      "You need a forecasting technique that can account for seasonality. And wouldn’t you \n",
      "know it—there’s an exponential smoothing technique for that.\n",
      "Figure 8-28: Correlogram with critical values\n",
      "Multiplicative Holt-Winters Exponential Smoothing\n",
      "Multiplicative Holt-Winters Smoothing is the logical extension of Holt’s Trend-Corrected \n",
      "Smoothing. It accounts for a level, a trend, and the need to adjust the demand up or down \n",
      "on a regular basis due to seasonal ﬂ uctuations. Note that the seasonal ﬂ uctuation needn’t \n",
      "be every 12 months like in this example. In the case of MailChimp, we have periodic \n",
      "demand ﬂ uctuations every Thursday (people seem to think Thursday is a good day to send \n",
      "marketing e-mail). Using Holt-Winters, we could account for this 7-day cycle.\n",
      "Now, in most situations you can’t just add or subtract a ﬁ xed amount of seasonal demand \n",
      "to adjust the forecast. If your business grows from selling 200 to 2,000 swords each month, \n",
      "you wouldn’t adjust the Christmas demand in both those contexts by adding 20 swords. \n",
      "No, seasonal adjustments usually need to be multipliers. Instead of adding 20 swords \n",
      "maybe it’s multiplying the forecast by 120 percent. That’s why it’s called Multiplicative  \n",
      "Holt-Winters. Here’s how this forecast conceives of demand:\n",
      "  Demand  at time t = (level +t*trend) * seasonal adjustment for time t * whatever irregular \n",
      "adjustments are left we can’t account for\n",
      "So you still have the identical level and trend structure you had in Holt’s Trend-\n",
      "Corrected Smoothing, but the demand is adjusted for seasonality. And since you can’t \n",
      "account for irregular variations in the demand, such as acts of God, you’re not going to.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "314 Data Smart\n",
      "Holt-Winters is also called triple exponential  smoothing, because, you guessed it, there \n",
      "are three smoothing parameters this time around. There are still alpha and gamma param-\n",
      "eters, but this time you have a seasonal adjustment factor with an update equation and \n",
      "a factor called delta.\n",
      "Now, the three error adjustment equations are slightly more complex than what you’ve \n",
      "seen so far, but you’ll recognize bits. \n",
      "Before you get started, I want to make one thing clear—so far you’ve used levels and \n",
      "trends from the previous period to forecast the next and adjust. But with seasonal adjust-\n",
      "ments, you don’t look at the previous period. Instead, you look at the previous estimate \n",
      "of the adjustment factor for that point in the cycle. In this case, that’s 12 periods prior \n",
      "rather than one. \n",
      "That means that if you’re at month 36 and you’re forecasting three months forward to \n",
      "39, that forecast is going to look like:\n",
      "  Forecast  for month 39 = (level\n",
      "36+3*trend36)*seasonality27\n",
      "Yep, you’re seeing that seasonality27 correctly. It’s the most recent estimate for the March \n",
      "seasonal adjustment. You can’t use seasonality36, because that’s for December.\n",
      "All right, so that’s how the future forecast works. Let’s dig into the update equations, \n",
      "starting with the level. You need only an initial level0 and trend0, but you actually need \n",
      "twelve initial seasonality factors, seasonality-11 through seasonality0. \n",
      "For example, the update equation for level1 relies on an initial estimate of the January \n",
      "seasonality adjustment: \n",
      "  level 1 = level 0 + trend 0 + alpha  * ( demand 1 – ( level 0 + trend 0)*seasonality -11)/\n",
      "seasonality-11\n",
      "You have lots of familiar components here in this level calculation. The current level is \n",
      "the previous level plus the previous trend (just as in double exponential smoothing) plus \n",
      "alpha times the one-step ahead forecast error (demand\n",
      "1 – (level0 + trend0)*seasonality-11), \n",
      "where the error gets a seasonal adjustment by being divided by seasonality-11.\n",
      "And so as you walk forward in time, the next month would be:\n",
      "  level 2 = level 1 + trend 1 + alpha  * ( demand 2 – ( level 1 + trend 1)*seasonality -10)/\n",
      "seasonality-10\n",
      "So in general then the level is calculated as:\n",
      "  level current  period = levelprevious  period + trendprevious  period + alpha * ( demand current  period – \n",
      "(levelprevious period + trendprevious period)*seasonalitylast relevant period)/seasonalitylast relevant period\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "315Forecasting: Breathe Easy; You Can’t Win \n",
      "The trend is updated in relation to the level in exactly the same way as in double expo-\n",
      "nential smoothing:\n",
      "  trend current  period  = trend previous  period  + gamma  * alpha  * ( demand current  period  – \n",
      "(levelprevious period + trendprevious period)*seasonalitylast relevant period)/seasonalitylast relevant period\n",
      "Just as in double exponential smoothing, the current trend is the previous trend plus \n",
      "gamma times the amount of error incorporated into the level update equation.\n",
      "And now for the seasonal factor update equation. It’s a lot like the trend update equa-\n",
      "tion, except that it adjusts the last relevant seasonal factor using delta times the error that \n",
      "the level and trend updates ignored:\n",
      "  seasonality current  period  = seasonality last  relevant  period  + delta  * ( 1-alpha ) * \n",
      "(demandcurrent period – (levelprevious period + trendprevious period)*seasonality last relevant period)/ \n",
      "(levelprevious period + trendprevious period)\n",
      "In this case you’re updating the seasonality adjustment with the corresponding factor \n",
      "from 12 months prior, but you’re folding in delta times whatever error was left on the cut-\n",
      "ting room ﬂ oor from the level update. Except, note that rather than seasonally adjusting \n",
      "the error here, you’re dividing through by the previous level and trend values. By “level \n",
      "and trend adjusting” the one-step ahead error, you’re putting the error on the same mul-\n",
      "tiplier scale as the seasonal factors.\n",
      "Setting the Initial Values for Level, Trend, and Seasonality\n",
      "Setting the initial values for SES and double exponential smoothing was a piece of cake. \n",
      "But now you have to tease out what’s trend and what’s seasonality from the time series. \n",
      "And that means that setting the initial values for this forecast (one level, one trend, and \n",
      "12 seasonal adjustment factors) is a little tough. There are simple (and wrong!) ways of \n",
      "doing this. I’m going to show you a good way to initialize Holt-Winters, assuming you \n",
      "have at least two seasonal cycles’ worth of historical data. In this case, you have three \n",
      "cycles’ worth.\n",
      "Here’s what you’re going to do:\n",
      " 1. Smooth out the historical data using what’s called a 2 × 12 moving average.\n",
      " 2. Compare a smoothed version of the time series to the original to estimate seasonality.\n",
      " 3. Using the initial seasonal estimates, deseasonalize the historical data.\n",
      " 4. Estimate the level and trend using a trendline on the deseasonalized data.\n",
      "To start, create a new tab called HoltWintersInitial and paste the time series data into \n",
      "its ﬁ rst two columns. Now you need to smooth out some of the time series data using a\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "316 Data Smart\n",
      "moving average. Because the seasonality is in 12-month cycles, it makes sense to use a \n",
      "12-month moving average on the data. \n",
      "What do I mean by a 12-month moving average?\n",
      "For a moving average, you take the demand for a particular month as well as the \n",
      "demand around that month in both directions and average them. This tamps down any \n",
      "weird spikes in the series.\n",
      "But there’s a problem with a 12-month moving average. Twelve is an even number. If \n",
      "you’re smoothing out the demand for month 7, should you average it as the demand of \n",
      "months 1 through 12 or the demand of months 2 through 13? Either way, month 7 isn’t \n",
      "quite in the middle. There is no middle!\n",
      "To accommodate this, you’re going to smooth out the demand with a “2 × 12 moving \n",
      "average,” which is the average of both those possibilities—months 1 through 12 and 2 \n",
      "through 13. (The same goes for any other even number of time periods in a cycle. If your \n",
      "cycle has an odd number of periods, the “2x” part of the moving average is unnecessary \n",
      "and you can just do a simple moving average.)\n",
      "Now note for the ﬁ rst six months of data and the last six months of data, this isn’t even \n",
      "possible. They don’t have six months of data on either side of them. You can only smooth \n",
      "the middle months of the dataset (in this case it’s months 7–30). This is why you need at \n",
      "least two years’ worth of data, so that you get one year of smoothed data.\n",
      "So starting with month 7, use the following formula:\n",
      "=(AVERAGE(B3:B14)+AVERAGE(B2:B13))/2\n",
      "This is the average of month 7 with the 12 months around it, except that months 1 and \n",
      "13 count for half of what the other months count for, which makes sense; since months \n",
      "1 and 13 are in the same month if they were each counted twice then you’d have January \n",
      "over-represented in the moving average.\n",
      "Dragging this formula down through month 30 and graphing both the original and \n",
      "smoothed data in a straight-line scatter plot, you get the sheet shown in Figure 8-29. In \n",
      "my chart I’ve labeled the two series smoothed and unsmoothed. It’s apparent looking at \n",
      "the smoothed line that any seasonal variation present in the data has, more or less, been \n",
      "smoothed out.\n",
      "Now, in column D, you can divide the original value by the smoothed value to get an \n",
      "estimate of the seasonal adjustment factor. Starting at month 7, you have for cell D8:\n",
      "B8/C8\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "317Forecasting: Breathe Easy; You Can’t Win \n",
      "And you can drag this down through month 30. Note how in both months 12 and 24 \n",
      "(December) you get spikes around 20 percent of normal, whereas you get dips in the spring.\n",
      "Figure 8-29: The smoothed demand data\n",
      "This smoothing technique has given you two point estimates for each seasonality fac-\n",
      "tor. In column E, let’s average these two points together into a single value that will be \n",
      "the initial seasonal factor used in Holt-Winters.\n",
      "For example, in E2, which is January, you average the two January points in column \n",
      "D, which are D14 and D26. Since the smoothed data starts in the middle of the year in \n",
      "column D, you can’t drag this average down. In E8, which is July, you have to take the \n",
      "average of D8 and D20 for instance.\n",
      "Once you have these 12 adjustment factors in column E, you can subtract 1 from each \n",
      "of them in column F and format the cells as percentages (highlight the range and right-\n",
      "click Format Cells) to see how these factors move the demand up or down each month. \n",
      "You can even insert a bar chart of these skews into the sheet, as shown in Figure 8-30.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "318 Data Smart\n",
      "Figure 8-30:  A bar chart of estimated seasonal variations\n",
      "Now that you have these initial seasonal adjustments, you can use them to \n",
      "deseasonalize the time series data. Once the entire series is deseasonalized, you can toss \n",
      "a trendline through it and use the slope and intercept as the initial level and trend.\n",
      "To start, paste the appropriate seasonal adjustment values for each month in G2 through \n",
      "G37. Essentially, you’re just pasting E2:E13 three times in a row down column G (make \n",
      "sure to paste values only). In column H you can then divide the original series in column \n",
      "B by the seasonal factors in G to remove the estimated seasonality present in the data. \n",
      "This sheet is shown in Figure 8-31.\n",
      "Next, as you’ve done on previous tabs, insert a scatter plot of column H and toss a \n",
      "trendline through it. Displaying the trendline equation on the graph, you get an initial \n",
      "trend estimate of 2.29 additional sword sales per month and an initial level estimate of \n",
      "144.42 (see Figure 8-32).\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "319Forecasting: Breathe Easy; You Can’t Win \n",
      "Figure 8-31: The deseasonalized time series\n",
      "Getting Rolling on the Forecast\n",
      "Now that you have the initial values for all the parameters, create a new tab called \n",
      "HoltWintersSeasonal, where you’ll start by pasting the time series data on row 4 just as \n",
      "you did for the previous two forecasting techniques.\n",
      "In columns C, D, and E next to the time series you’re going to put the level, trend, and \n",
      "seasonal values, respectively. And in order to start, unlike on previous tabs where you only \n",
      "needed to insert one new blank row 5, this time around you need to insert blank rows 5 \n",
      "through 16 and label them as time slots -11 through 0 in column A. You can then paste \n",
      "the initial values from the previous tab in their respective spots, as shown in Figure 8-33.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "320 Data Smart\n",
      "Figure 8-32: Initial level and trend estimates via a trendline on the deseasonalized series\n",
      "Figure 8-33: All of the initial Holt-Winters values in one place\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "321Forecasting: Breathe Easy; You Can’t Win \n",
      "In column F you’ll do a one-step ahead forecast. So for time period 1, it’s the previous \n",
      "level in C16 plus the previous trend in D16. But both of those are adjusted by the appro-\n",
      "priate January seasonality estimate 12 rows up in E5. Thus, F17 is written as:\n",
      "=(C16+D16)*E5\n",
      "The forecast error in G17 may then be calculated as:\n",
      "=B17-F17\n",
      "Now you’re ready to get started with calculating the level, trend, and seasonality rolling \n",
      "forward. So in cells C2:E2, put the alpha, gamma, and delta values (as always I’m going to \n",
      "start with 0.5). Figure 8-34 shows the worksheet.\n",
      "Figure 8-34: Worksheet with smoothing parameters and ﬁ  rst one-step forecast and error \n",
      "The ﬁ rst item you’ll calculate as you roll through the time periods is a new level esti-\n",
      "mate for period 1 in cell C17:\n",
      "=C16+D16+C$2*G17/E5\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "322 Data Smart\n",
      "Just as you saw in the previous section, the new level equals the previous level plus \n",
      "the previous trend plus alpha times the deseasonalized forecast error. And the updated \n",
      "trend in D17 is quite similar:\n",
      "=D16+D$2*C$2*G17/E5\n",
      "You have the previous trend plus gamma times the amount of deseasonalized error \n",
      "incorporated into the level update.\n",
      "And for the January seasonal factor update you have:\n",
      "=E5+E$2*(1-C$2)*G17/(C16+D16) \n",
      "That’s the previous January factor adjusted by delta times the error ignored by the level \n",
      "correction scaled like the seasonal factors by dividing through by the previous level and \n",
      "trend.\n",
      "Note that in all three of these formulas alpha, gamma, and delta are referenced via \n",
      "absolute references, so that as you drag the calculations down they don’t move. Dragging \n",
      "C17:G17 down through month 36, you get the sheet shown in Figure 8-35.\n",
      "Figure 8-35: Taking the update equations through month 36\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "323Forecasting: Breathe Easy; You Can’t Win \n",
      "And now that you have your ﬁ nal level, trend, and seasonal estimates, you can forecast \n",
      "the next year’s worth of demand. Starting in month 37 in cell B53 you have:\n",
      "=(C$52+(A53-A$52)*D$52)*E41\n",
      "Just as in Holt’s Trend-Corrected Smoothing, you’re taking the last level estimate \n",
      "and adding to it the trend times the number of elapsed months since the most recent \n",
      "trend estimate. The only diff  erence is you’re scaling the whole forecast by the most \n",
      "up-to-date seasonal multiplier for January, which is in cell E41. And while the level in \n",
      "C$52 and the trend in D$52 use absolute references so that they won’t shift as you drag \n",
      "the forecast down, the seasonal reference in E41 must move down as you drag the \n",
      "forecast through the next 11 months. And so, dragging the calculation down, you get \n",
      "the forecast shown in Figure 8-36.\n",
      "Figure 8-36: Getting the Holt-Winters forecast for future months\n",
      "You can graph this forecast using Excel’s straight-line scatter plot just as in the previous \n",
      "two techniques (see Figure 8-37).\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "324 Data Smart\n",
      "Figure 8-37: Graphing the Holt-Winters forecast\n",
      "And...Optimize!\n",
      "You thought you were done, but no. Time to set those smoothing parameters. So just as in \n",
      "the previous two techniques, toss the SSE in cell G2, and place the standard error in H2.\n",
      "The only diff erence this time around is that you have three smoothing parameters, so \n",
      "the standard error is calculated as:\n",
      "=SQRT(G2/(36-3))\n",
      "This gives the sheet shown in Figure 8-38.\n",
      "As for the Solver setup (shown in Figure 8-39), this time around you’re optimizing H2 \n",
      "by varying the three smoothing parameters. You’re able to achieve a standard error almost \n",
      "half that of previous techniques. The forecast plot (see Figure 8-40) looks good to the \n",
      "eye, doesn’t it? You’re tracking with the trend and the seasonal ﬂ uctuations. Very nice.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "325Forecasting: Breathe Easy; You Can’t Win \n",
      "Figure 8-38:  Adding SSE and standard error\n",
      "Figure 8-39: The Solver setup for Holt-Winters\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "326 Data Smart\n",
      "Figure 8-40:  The optimized Holt-Winters forecast\n",
      "Please Tell Me We’re Done Now!!!\n",
      "You now need to check the autocorrelations on this forecast. Since you’ve already set up \n",
      "the autocorrelation sheet, this time around you just need to make a copy of it and paste \n",
      "in the new error values. \n",
      "Make a copy of the Holt’s Autocorrelation tab and call it HW Autocorrelation. Then \n",
      "you need only paste special the values from the error column G into the autocorrelation \n",
      "sheet in column B. This gives the correlogram shown in Figure 8-41.\n",
      "Figure 8-41:  Correlogram for the Holt-Winters model\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "327Forecasting: Breathe Easy; You Can’t Win \n",
      "Bam! Since there are no autocorrelations above the critical value of 0.33, you know that \n",
      "the model is doing a nice job at capturing the structure in the demand values.\n",
      "Putting a Prediction Interval around the Forecast\n",
      "All right, so you have a forecast that ﬁ  ts well. How do you put some lower and upper \n",
      "bounds around it that you can use to set realistic expectations with the boss?\n",
      "You’re going to do this through Monte Carlo simulation, which you’ve already seen \n",
      "in Chapter 4. Essentially, you’re going to generate future scenarios of what the demand \n",
      "might look like and determine the band that 95 percent of those scenarios fall into. The \n",
      "question is how do you even begin to simulate future demand? It’s actually quite easy.\n",
      "Start by making a copy of the HoltWintersSeasonal tab and calling it PredictionIntervals. \n",
      "Delete all the graphs in the tab. They’re unnecessary. Furthermore, clear out the forecast \n",
      "in cells B53:B64. You’ll be putting “actual” (but simulated) demand in those spots.\n",
      "Now, like I said at the beginning of this chapter, the forecast is always wrong. There \n",
      "will always be error. But you know how this error will be distributed. You have a well-\n",
      "ﬁ tting forecast that you can assume has mean 0 one-step error (unbiased) with a standard \n",
      "deviation of 10.37, as calculated on the previous tab.\n",
      "Just as in Chapter 4, you can generate a simulated error using the \n",
      "NORMINV function. In \n",
      "future months, you can just feed the NORMINV function the mean (0), the standard devia-\n",
      "tion (10.37 in cell H$2), and a random number between 0 and 1, and it’ll pull an error \n",
      "from the bell curve. (See the discussion on cumulative distribution functions in Chapter \n",
      "4 for more on how this works.)\n",
      "Okay, so toss a simulated one-step error into cell G53:\n",
      "=NORMINV(RAND(),0,H$2)\n",
      "Drag it down through G64 to get 12 months of simulated errors in the one-step fore-\n",
      "cast. This gives you the sheet shown in Figure 8-42 (yours will have diff  erent simulated \n",
      "values from these).\n",
      "But now that you have the forecast error, you have everything you need to update the \n",
      "level, trend, and seasonality estimates going forward as well as the one-step forecast. So \n",
      "grab cells C52:F52 and drag them down through row 64.\n",
      "Here’s where things get analytically badass. You now have a simulated forecast error \n",
      "and a one-step ahead forecast. So if you add the error in G to the forecast in F, you can \n",
      "actually back out a simulated demand for that time period.\n",
      "Thus, B53 would simply be:\n",
      "=F53+G53\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "328 Data Smart\n",
      "Figure 8-42:  Simulated one-step errors\n",
      "And you can drag that down through B64 to get all 12 months’ demand values (see \n",
      "Figure 8-43).\n",
      "Once you have that one scenario, by simply refreshing the sheet, the demand values \n",
      "change. So you can generate multiple future demand scenarios merely by copy-pasting \n",
      "one of the scenarios elsewhere and watching the sheet refresh itself.\n",
      "To start then, label cell A69 as Simulated Demand  and label A70:L70 as months 37 \n",
      "through 48. You can do this by copying A53:A64 and doing a paste special with trans-\n",
      "posed values into A70:L70.\n",
      "Similarly, paste special the transposed values of the ﬁ rst demand scenario into A71:L71. \n",
      "To insert a second scenario, simply right-click row 71 and select Insert to insert a new \n",
      "blank row 71. Then paste special some more simulated demand values (they should have \n",
      "updated when you pasted the last set).\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "329Forecasting: Breathe Easy; You Can’t Win \n",
      "You can just keep doing this operation to generate as many future demand scenarios \n",
      "as you want. That’s tedious though. Instead, you can record a quick macro. \n",
      "Figure 8-43:  Simulated future demand\n",
      "Just as in Chapter 7, record the following steps into a macro:\n",
      " 1. Insert a blank row 71.\n",
      " 2. Copy B53:B64. \n",
      " 3. Paste special transposed values into row 71.\n",
      " 4. Press the Stop recording button.\n",
      "Once you’ve recorded those keystrokes, you can hammer on whatever macro shortcut \n",
      "key you selected (see Chapter 7) over and over until you get a ton of scenarios. You can \n",
      "even hold the shortcut key down—1,000 scenarios should do it. (If the idea of holding a \n",
      "button down is abhorrent to you, you can read up on how to put a loop around your macro \n",
      "code using Visual Basic for Applications. Just Google for it.)\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "330 Data Smart\n",
      "When it’s all said and done, your sheet should look like Figure 8-44. \n",
      "Figure 8-44:  I have 1,000 demand scenarios\n",
      "Once you have your scenarios for each month, you can use the PERCENTILE function to \n",
      "get the upper and lower bounds on the middle 95 percent of scenarios to create a predic-\n",
      "tion interval.\n",
      "For instance, above month 37 in A66 you can place the formula:\n",
      "=PERCENTILE(A71:A1070,0.975)\n",
      "This gives you the 97.5th percentile of demand for this month. In my sheet it comes out \n",
      "to about 264. And in A67 you can get the 2.5th percentile as:\n",
      "=PERCENTILE(A71:A1070,0.025)\n",
      "Note that I’m using A71:A1070 because I have 1,000 simulated demand scenarios. You \n",
      "may have more or less depending on the dexterity of your index ﬁ nger. For me, this lower \n",
      "bound comes out to around 224. \n",
      "That means that although the forecast for month 37 is 245, the 95 percent prediction \n",
      "interval is 224 to 264.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "331Forecasting: Breathe Easy; You Can’t Win \n",
      "You can drag these percentile equations across through month 48 in column L to get \n",
      "the entire interval (see Figure 8-45). So now you can provide your superiors with a con-\n",
      "servative range plus a forecast if you like! And feel free to swap out the 0.025 and 0.975 \n",
      "with 0.05 and 0.95 for a 90 percent interval or with 0.1 and 0.9 for an 80 percent interval, \n",
      "and so on.\n",
      "Figure 8-45:  The forecast interval for Holt-Winters\n",
      "Creating a Fan Chart for Effect\n",
      "Now, this last step isn’t necessary, but forecasts with prediction intervals are often shown \n",
      "in something called a fan chart. You can create such a chart in Excel.\n",
      "To start, create a new tab called Fan Chart and in that tab, paste months 37 through 48 \n",
      "on row 1 and then paste the values of the upper bound of the prediction interval from row \n",
      "66 of the PredictionIntervals tab on row 2. On row 3, paste special the transposed values \n",
      "for the actual forecast from the HoltWintersSeasonal tab. And on row 4, paste the values of \n",
      "the lower bound of the prediction interval from row 67 of the intervals sheet.\n",
      "So you have the months, the upper bound of the interval, the forecast, and the lower \n",
      "bound all right in a row (see Figure 8-46).\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "332 Data Smart\n",
      "Figure 8-46:  The forecast sandwiched by the prediction interval\n",
      "By highlighting A2:L4 and selecting Area Chart from the charts menu in Excel, you \n",
      "get three solid area charts laid over each other. Right-click one of the series and choose \n",
      "Select Data. You can change the Category (X) axis labels for one of the series to be A1:L1 \n",
      "in order to add in the correct monthly labels to the graph.\n",
      "Now, right-click the lower bound series and format it to have a white ﬁ  ll. You should \n",
      "also remove grid lines from the graph for consistency’s sake. Feel free to add axis labels \n",
      "and a title. This yields the fan chart shown in Figure 8-47.\n",
      "Figure 8-47: The fan chart is a thing of beauty\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "333Forecasting: Breathe Easy; You Can’t Win \n",
      "The cool thing about this fan chart is that it conveys both the forecast and the intervals \n",
      "in one simple picture. Heck, you could actually layer on an 80 percent interval too if you \n",
      "wanted more shades of gray. There are two interesting items that stand out in the chart:\n",
      "• The error gets wider as time goes on. This makes sense. The uncertainty from month \n",
      "to month gets compounded.\n",
      "• Similarly, there is more error in absolute terms during periods of high seasonal \n",
      "demand. When demand dips in a trough, the error bounds tighten up. \n",
      "Wrapping Up\n",
      "This chapter covered a ton of content:\n",
      "• Simple exponential smoothing (SES)\n",
      "• Performing a t test on a linear regression to verify a linear trend in the time series\n",
      "• Holt’s Trend-Corrected Exponential Smoothing\n",
      "• Calculating autocorrelations and graphing a correlogram with critical values\n",
      "• Initializing Holt-Winters Multiplicative Exponential Smoothing using a 2 x 12 mov-\n",
      "ing average\n",
      "• Forecasting with Holt-Winters\n",
      "• Creating prediction intervals around the forecast using Monte Carlo simulation\n",
      "• Graphing the prediction intervals as a fan chart\n",
      "If you made it through the entire chapter, bravo. Seriously, that’s a lot of forecasting \n",
      "for one chapter.\n",
      "Now if you want to go further with forecasting, there are some excellent textbooks out \n",
      "there. I really like Forecasting, Time Series, and Regression by Bowerman et al. (Cengage \n",
      "Learning, 2004). Hyndman has a free forecasting textbook online at http://otexts.com/\n",
      "fpp/, and his blog (awesomely called “Hyndsight”) is an excellent resource. For questions, \n",
      "http://stats.stackexchange.com/ is the community to go to.\n",
      "When it comes to forecasting in a production setting, there are countless products out \n",
      "there. For light jobs, feel free to stay in Excel. If you have tons of products or SKUs, using \n",
      "some code would be helpful.\n",
      "SAS and R both have excellent packages for forecasting. The ones in R were written by \n",
      "Hyndman himself (see Chapter 10), who came up with the statistical underpinnings for \n",
      "how to do prediction intervals on the exponential smoothing techniques.\n",
      "And that’s it! I hope you now feel empowered to go forth and “organize your ign orance!”\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "9\n",
      "O\n",
      "utliers are the odd points in a dataset—the ones that don’t fit somehow. Historically, \n",
      "that’s meant extreme values, meaning quantities that were either too large or small \n",
      "to have come naturally from the same process as the other observations in the dataset. \n",
      "The only reason people used to care about outliers was because they wanted to get rid \n",
      "of them. Statisticians a hundred years ago had a lot in common with the Borg: a data point \n",
      "needed to assimilate or die. However, this was done with good reason (in the case of the \n",
      "statistician)—outliers can move averages and mess with spread measurements in the data. \n",
      "A good example of outlier removal is in gymnastics, where the highest and lowest judges’ \n",
      "scores are always trimmed from the data before taking the average score.\n",
      "Outliers have a knack for messing up machine learning models. For example, in \n",
      "Chapters 6 and 7 you looked at predicting pregnant customers based on their purchase \n",
      "data. What if a store miscoded some items on the shelves of the pharmacy and were \n",
      "registering multi-vitamin purchases as folic acid purchases? The customers with those \n",
      "faulty purchase vectors are outliers that shift the relationship of pregnancy-to-folic-acid-\n",
      "purchasing in a way that harms the AI model’s understanding.\n",
      "Once upon a time when I consulted for the government, my company found a water \n",
      "storage facility that the United States had in Dubai that had been valued at billions and \n",
      "billions of dollars. The property value was an outlier that was throwing off  the results of \n",
      "our analysis—turns out someone had typed it into the database with too many zeroes.\n",
      "So that’s one reason to care about outliers: to facilitate cleaner data analysis and modeling.\n",
      "But there’s another reason to care about outliers. They’re interesting for their own sake!\n",
      "Outliers Are (Bad?) People, Too\n",
      "Consider when your credit card company calls you after you make a transaction that is \n",
      "potentially fraudulent. What’s your credit card company doing? They’re detecting that \n",
      "transaction as being an outlier based on your past behavior. Rather than ignoring the \n",
      "Outlier Detection: Just \n",
      "Because They’re Odd \n",
      "Doesn’t Mean They’re \n",
      "Unimportant\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "336 Data Smart\n",
      "transaction because it’s an outlier, they’re purposefully ﬂ agging the potential fraud and \n",
      "acting on it.\n",
      "At MailChimp when we predict spammers before they send, we’re predicting outliers. \n",
      "These spammers are a small group of people whose behavior lies outside of what we as a \n",
      "company consider normal. We use supervised models similar to those in Chapters 6 and \n",
      "7 to predict based on past occurrences when a new user is going to send spam.\n",
      "So in the case of MailChimp, then, an outlier is no more than a small but understood \n",
      "class of data in the population that can be predicted using training data. But what about \n",
      "the cases when you don’t know what you’re looking for? Like those mislabeled folic acid \n",
      "shoppers? Fraudsters often change their behavior so that the only thing you can expect \n",
      "from them is something unexpected. If that error has never happened before, how do you \n",
      "ﬁ nd those odd points for the ﬁ rst time?\n",
      "This type of outlier detection is an example of unsupervised learning and data mining. \n",
      "It’s the intuitive ﬂ ip side of the analysis performed in Chapters 2 and 5 of this book where \n",
      "you detected clusters of points. In cluster analysis, you look for a data point’s group of \n",
      "friends and analyze that group. In outlier detection, you care about data points that diff er \n",
      "from the groups. They’re odd or exceptional in some way.\n",
      "This chapter starts with a simple, standard way of calculating outliers in normal-like \n",
      "one-dimensional data. Then it moves on to using k nearest neighbor (kNN) graphs to \n",
      "detect outliers in multidimensional data, similar to how you used r-neighborhood graphs \n",
      "to create clusters in Chapter 5.\n",
      "The Fascinating Case of Hadlum v. Hadlum\n",
      "NOTE\n",
      "The Excel workbook used in this section, “Pregnancy Duration.xlsx,” is available for \n",
      "download at the book’s website at www.wiley.com/go/datasmart. Later in this chap-\n",
      "ter, you’ll be diving into a larger spreadsheet, “SupportCenter.xlsx,” also available on \n",
      "the same website.\n",
      "Back in the 1940s, a British guy named Mr. Hadlum went off  to war. Some days later, 349 \n",
      "of them in fact, his wife Mrs. Hadlum gave birth. Now, the average pregnancy lasts about \n",
      "266 days. That places Mrs. Hadlum almost 12 weeks past her due date. I can’t think of a \n",
      "single woman who’d stand for that added discomfort these days, but back then, inducing \n",
      "pregnancy wasn’t as common.\n",
      "Now, Mrs. Hadlum claimed she had nothing more than an exceptionally long preg-\n",
      "nancy. Fair enough.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "337Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \n",
      "But Mr. Hadlum concluded her pregnancy must have been the result of another man \n",
      "while he’d been away—that a 349-day pregnancy was an anomaly that couldn’t be justi-\n",
      "ﬁ ed given the distribution of typical birth durations.\n",
      "So, if you had some pregnancy data, what’s a quick-and-dirty way to decide whether \n",
      "Mrs. Hadlum’s pregnancy should be considered an outlier? \n",
      "Well, studies have found that gestation length is more or less a normally distributed \n",
      "random variable with a mean of 266 days after conception, with a standard deviation of \n",
      "about 9. So you can evaluate the normal cumulative distribution function (CDF) intro-\n",
      "duced in Chapter 4 to get the probability of a value less than 349 occurring. In Excel, this \n",
      "is evaluated using the \n",
      "NORMDIST function:\n",
      "=NORMDIST(349,266,9,TRUE)\n",
      "The NORMDIST function is supplied with the value whose cumulative probability you \n",
      "want, the mean, the standard deviation, and a ﬂ ag set to TRUE, which sets the function to \n",
      "provide the cumulative value.\n",
      "This formula returns a value of 1.000 all the way out as far as Excel tracks decimals. \n",
      "This means that nearly all babies born from here to eternity are going to be born at or \n",
      "under 349 days. Subtracting this value from 1:\n",
      "=1-NORMDIST(349,266,9,TRUE)\n",
      "You get 0.0000000 as far as the eye can see. In other words, it’s nearly impossible for \n",
      "a human baby to gestate this long.\n",
      "We’ll never know for sure, but I’d bet good money Mrs. Hadlum had a little something \n",
      "else going on. Funny thing is, the court ruled in her favor, stating that such a long preg-\n",
      "nancy, although highly unlikely, was still possible.\n",
      "Tukey Fences\n",
      "This concept of outliers being unlikely points when sampled from the bell curve has led \n",
      "to a rule of thumb for outlier detection called Tukey fences. Tukey fences are easy to check \n",
      "and easy to code. They are used by statistical packages the world over for identifying and \n",
      "removing spurious data points from any set of data that ﬁ ts in a normal bell curve.\n",
      "Here’s the Tukey fences technique in its entirety:\n",
      "• Calculate the 25th and 75th percentiles in any dataset you’d like to ﬁ nd outliers in. \n",
      "These values are also called the ﬁ rst quartile and the third quartile. Excel calculates \n",
      "values these using the PERCENTILE function.\n",
      "• Subtract the ﬁ rst quartile from the third quartile to get a measure of the spread of \n",
      "the data, which is called the Interquartile Range (IQR). The IQR is cool because it’s\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "338 Data Smart\n",
      "relatively robust against extreme values as a measure of spread, unlike the typical \n",
      "standard deviation calculation you’ve used to measure spread in previous chapters \n",
      "of this book.\n",
      "• Subtract 1.5*IQR from the ﬁ rst quartile to get the lower inner fence. Add 1.5*IQR \n",
      "to the third quartile to get the upper inner fence.\n",
      "• Likewise, subtract 3*IQR from the ﬁ rst quartile to get the lower outer fence. Add \n",
      "3*IQR to the third quartile to get the upper outer fence.\n",
      "• Any value less than a lower fence or greater than an upper fence is extreme. In \n",
      "normally distributed data, you’d see about 1 in every 100 points outside the inner \n",
      "fence, but only 1 in every 500,000 points outside the outer fence.\n",
      "Applying Tukey Fences in a Spreadsheet\n",
      "I’ve included a sheet called PregnancyDuration.xlsx for download off  the book’s website \n",
      "so that you can apply this technique to some actual data. If you open it, you’ll see a tab \n",
      "called Pregnancies, with a sample of 1,000 durations in column A.\n",
      "Mrs. Hadlum’s gestation period of 349 days is in cell A2. In column D, place all of the \n",
      "summary statistics and fences. Start with the median (the middle value), which is a more \n",
      "robust statistic of centrality than the average value (averages can be skewed by outliers). \n",
      "Label C1 as Median and in D1, calculate the median as follows:\n",
      "=PERCENTILE(A2:A1001,0.5)\n",
      "That would be the 50th percentile. Below the median, you can calculate the ﬁ  rst and \n",
      "third quartiles as:\n",
      "=PERCENTILE(A2:A1001,0.25)\n",
      "=PERCENTILE(A2:A1001,0.75)\n",
      "And the interquartile range is the diff erence between them:\n",
      "=D3-D2\n",
      "Tacking on 1.5 and 3 times the IQR to the ﬁ rst and third quartile respectively, you can \n",
      "then calculate all the fences:\n",
      "=D2-1.5*D4\n",
      "=D3+1.5*D4\n",
      "=D2-3*D4\n",
      "=D3+3*D4\n",
      "If you label all these values, you’ll get the sheet shown in Figure 9-1.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "339Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \n",
      "Figure 9-1: Tukey fences for some pregnancy durations\n",
      "Now you can apply some conditional formatting to the sheet and see who falls outside \n",
      "these fences. Start with the inner fence. To highlight the extreme values, select Conditional \n",
      "Formatting from the Home tab, choose Highlight Cells Rules, and select Less Than, as \n",
      "shown in Figure 9-2. \n",
      "Figure 9-2: Adding conditional formatting for outliers\n",
      "Specifying the lower inner fence, feel free to choose a highlight color that tickles your \n",
      "fancy (I’m going to choose a yellow ﬁ ll for inner fences and a red for outer, because I like \n",
      "traffi  c lights). Similarly, add formatting for the other three fences (if you’re using Excel \n",
      "2011 for the Mac you can use the Not Between rule to add the formatting with two rules \n",
      "rather than four).\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "340 Data Smart\n",
      "As shown in Figure 9-3, Mrs. Hadlum turns red, meaning her pregnancy was radically \n",
      "extreme. Scrolling down, you’ll ﬁ nd no other red pregnancies, but there are nine yellows. \n",
      "This matches up closely with the roughly 1 out of 100 points you’d expect to be ﬂ agged \n",
      "in normal data by the rule.\n",
      "Figure 9-3: Uh oh, Mrs. Hadlum. What say you to this conditional formatting?\n",
      "The Limitations of This Simple Approach\n",
      "Tukey fences work only when three things are true:\n",
      "• The data is vaguely normally distributed. It doesn’t have to be perfect, but it should \n",
      "be Bell-curve shaped and hopefully symmetric without some long tail jutting out \n",
      "one side of it.\n",
      "• The deﬁ nition of an outlier is an extreme value on the perimeter of a distribution.\n",
      "• You’re looking at one-dimensional data.\n",
      "Let’s look at an example of an outlier that violates the ﬁ rst two of these assumptions.\n",
      "In The Fellowship of the Ring, when the adventurers ﬁ nally form a single company (the \n",
      "fellowship for which the book is named), they all stand in a little group as the leader of \n",
      "the elves, Elrond, pronounces who they are and what their mission is.\n",
      "This group contains four tall people: Gandalf, Aragorn, Legolas, and Boromir. There \n",
      "are also four short people. The hobbits themselves: Frodo, Merry, Pippin, and Sam. \n",
      "And in between them, there’s a single dwarf: Gimli. Gimli is shorter than the men by \n",
      "a couple heads and taller than the hobbits by about the same (see Figure 9-4).\n",
      "In the movie, when we see this group presented to us for the ﬁ  rst time, Gimli is the \n",
      "clear outlier by height. He belongs to neither group.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "341Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \n",
      "Guess\n",
      "I’m an\n",
      "outlier\n",
      "Hobbits Gimli Tall Folks\n",
      "Figure 9-4:  Gimli, son of Gloin, Dwarven outlier\n",
      "But how is he the outlier? His height is neither the least nor the greatest. In fact, his \n",
      "height is the closest to the average of the group’s.\n",
      "You see, this height distribution isn’t anywhere near normal. If anything, you could call \n",
      "it “multi-modal” (a distribution with multiple peaks). And Gimli is an outlier not because \n",
      "his height is extreme, but because it’s between these two peaks. And these types of data \n",
      "points can be even harder to spot when you’re looking over several dimensions.\n",
      "This kind of outlier crops up in fraud pretty frequently. Someone who’s too ordinary \n",
      "to actually be ordinary. Bernie Madoff  is a great example of this. Although most Ponzi \n",
      "schemes off er outlier rates of return of 20-plus percent, Madoff   off ered reliably modest \n",
      "returns that blended into the noise each year—he wasn’t jumping any Tukey fences. But \n",
      "across years, his multiyear returns in their reliability became a multi-dimensional outlier. \n",
      "So how do you ﬁ nd outliers in the case of multi-model, multi-dimensional data (you \n",
      "just as easily could call it “real-world data”)?\n",
      "One awesome way to approach this is to treat the data like a graph, just as you did in \n",
      "Chapter 5 to ﬁ nd clusters in the data. Think about it. What deﬁ  nes Gimli as an outlier \n",
      "is his relationship to the other data points; his distance from them in relation to their \n",
      "distance from each other. \n",
      "All of those distances, each point from every other point, deﬁ  nes edges on a graph. \n",
      "Using this graph, you can tease out the isolated points. To do that, you start by creating \n",
      "a k nearest neighbor (kNN) graph and going from there.\n",
      "Terrible at Nothing, Bad at Everything\n",
      "For this next section, imagine that you manage a large customer support call center. Each \n",
      "call, e-mail, or chat from a customer creates a ticket, and each member of the support team \n",
      "is required to handle at least 140 tickets daily. At the end of each interaction, a customer\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "342 Data Smart\n",
      "is given the opportunity to rate the support employee on a ﬁ  ve-star scale. Support staff  \n",
      "are required to keep an average rating above 2, or they are ﬁ red. \n",
      "High standards, I know.\n",
      "The company keeps track of plenty of other metrics on each employee as well. How \n",
      "many times they’ve been tardy over the past year. How many graveyard and weekend \n",
      "shifts they’ve taken for the team. How many sick days they’ve taken, and out of those, \n",
      "how many have been on Friday. The company even tracks how many hours the employee \n",
      "uses to take internal training courses (they get up to 40 hours paid) and how many times \n",
      "they’ve put in a request for a shift swap or been a good Samaritan and fulﬁ  lled another \n",
      "employee’s request.\n",
      "You have all this data for all 400 call center employees in a spreadsheet. And the ques-\n",
      "tion is which employees are outliers, and what do they teach you about being a call center \n",
      "employee? Are there some baddies slipping through who don’t get culled by the ticket \n",
      "requirements and minimum customer ratings? Perhaps the outliers will teach you how \n",
      "to write better rules.\n",
      "If you open the spreadsheet for this section of the chapter (SupportCenter.xlsx available \n",
      "for download on the book’s website at \n",
      "www.wiley.com/go/datasmart), you’ll ﬁ nd all this \n",
      "tracked performance data on the SupportPersonnel sheet (see Figure 9-5).\n",
      "Figure 9-5: Multi-dimensional employee performance data\n",
      "Preparing Data for Graphing\n",
      "There’s a problem with this performance data. You can’t measure the distance between \n",
      "employees in order to ﬁ gure out who’s “on the outside” when each column is scaled so \n",
      "diff erently. What does it mean to have a diff erence of 5 between two employees on their\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "343Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \n",
      "average tickets versus a diff erence of 0.2 in customer rating? You need to standardize each \n",
      "column so that the values are closer to the same center and spread.\n",
      "The way that columns of data are usually standardized is:\n",
      " 1. Subtract the mean of a column from each observation.\n",
      " 2. Divide each observation by the standard deviation of the column.\n",
      "For normally distributed data, this centers the data at 0 (gives it a mean of 0) and gives \n",
      "it a standard deviation of 1. Indeed, a normal distribution with mean 0 and standard \n",
      "deviation 1 is called the standard normal distribution. \n",
      "STANDARDIZING USING ROBUST MEASURES OF CENTRALITY AND SCALE\n",
      "Not all data you’ll want to scale is normally distributed to begin with. Subtracting \n",
      "out the mean and dividing through by the standard deviation tends to work well \n",
      "anyway. But outliers can screw up mean and standard deviation calculations, so \n",
      "sometimes folks like to standardize by subtracting more robust statistics of centrality \n",
      "(the “middle” of the data) and dividing through by more robust measures of scale/\n",
      "statistical dispersion (the spread of the data).\n",
      "Here are some centrality calculations that work better against one-dimensional outli-\n",
      "ers than the mean:\n",
      "• Median—Yep, just the 50th percentile\n",
      "• Midhinge—The average of the 25th and 75th percentiles\n",
      "• Trimean—The average of the median and the midhinge. I like this one, because \n",
      "it sounds intelligent.\n",
      "• Trimmed/truncated mean—The mean, but you throw away the top and bottom \n",
      "N points or top and bottom percentage of points. You see this one in sports a \n",
      "lot (think gymnastics where they throw out the top and bottom scores). If you \n",
      "throw away the top and bottom 25 percent and average the middle 50 percent of \n",
      "the data, that has its own name: the interquartile mean (IQM).\n",
      "• Winsorized mean—Like the trimmed mean, but instead of throwing away points \n",
      "that are too large or too small, you replace them with a limit.\n",
      "As for robust measures of scale, here are some others worth using instead of the \n",
      "standard deviation:\n",
      "• Interquartile range—You saw this one earlier in the chapter. It’s just the 75th \n",
      "percentile minus the 25th percentile in the data. You can use other n-tiles too. For \n",
      "example, if you use the 90th and 10th percentiles, you get the interdecile range.\n",
      "• Median absolute deviation (MAD)—Take the median of the data. Then take the \n",
      "absolute value of the diff erence of each point from the median. The median of \n",
      "these deviations is the MAD. It’s kinda like the median’s answer to the standard \n",
      "deviation.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "344 Data Smart\n",
      "To start then, calculate the mean and standard deviation of each column at the bot-\n",
      "tom of the SupportPersonnel sheet. The ﬁ rst value you’ll want in B402 is the mean of the \n",
      "tickets taken per day, which you can write as:\n",
      "=AVERAGE(B2:B401)\n",
      "And below that you take the standard deviation of the column as:\n",
      "=STDEV(B2:B401)\n",
      "Copying those two formulas through column K, you get the sheet shown in Figure 9-6.\n",
      "Figure 9-6:  Mean and standard deviation for each column\n",
      "Create a new tab called Standardized and copy the column labels from row 1 as well \n",
      "as the employee IDs from column A. You can start standardizing the values in cell B2 \n",
      "using Excel’s \n",
      "STANDARDIZE formula. This formula just takes the original value, a center, \n",
      "and a spread measure and returns the value with the center subtracted out divided by the \n",
      "spread. So in B2 you would have:\n",
      "=STANDARDIZE(SupportPersonnel!B2,\n",
      "             SupportPersonnel!B$402,SupportPersonnel!B$403)\n",
      "Note that you’re using absolute references on the rows only for the mean and standard \n",
      "deviation, so that they stay put when you copy the formula down. However, when you \n",
      "copy the formula across, the column will change.\n",
      "Copy and paste B2 across through K2, highlight the range, and then double-click it to \n",
      "send the calculations down through K401. This yields the standardized set of data shown \n",
      "in Figure 9-7.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "345Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \n",
      "Figure 9-7: The standardized set of employee performance data\n",
      "Creating a Graph\n",
      "A graph is nothing more than some nodes and edges. In this case, each employee is a node, \n",
      "and to start, you can just draw edges between everybody. The length of the edge is the \n",
      "Euclidean distance between the two employees using their standardized performance data.\n",
      "As you saw in Chapter 2, the Euclidean (as-the-crow-ﬂ ies) distance between two points \n",
      "is the square root of the sum of the squared diff erences of each column value for the two.\n",
      "In a new sheet called Distances, create an employee-by-employee distance matrix in \n",
      "the exact same way as in Chapter 2, by using the \n",
      "OFFSET formula.\n",
      "To start, number the employees 0 through 399 starting at A3 going down and at C1 \n",
      "going across. (Tip: Type 0, 1, and 2 in the ﬁ rst three cells and then highlight those cells \n",
      "and drag down or across. Excel will ﬁ  ll in the rest for you, because it’s smart like that.) \n",
      "Next to these off set values, paste the employee IDs (Paste Special values transposed for \n",
      "the columns). This creates the empty matrix shown in Figure 9-8.\n",
      "To ﬁ ll in this matrix, let’s start in the ﬁ rst distance cell C3. This is the distance between \n",
      "employee 144624 and themselves. \n",
      "Now, for all these distance calculations, you’re going to use the OFFSET formula anchored \n",
      "on the ﬁ rst row of standardized employee data:\n",
      "OFFSET(Standardized!$B$2:$K$2,Some number of rows, 0 columns)\n",
      "In the case of cell C3, Standardized!$B$2:$K$2 is the actual row you want for employee \n",
      "144624, so you can take the diff erences between this employee and themselves using the \n",
      "off set formula as:\n",
      "OFFSET(Standardized!$B$2:$K$2,Distances!$A3,0)-\n",
      "OFFSET(Standardized!$B$2:$K$2,Distances!C$1,0)\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "346 Data Smart\n",
      "Figure 9-8:  Empty employee distance matrix\n",
      "In the ﬁ rst off set formula, you’re moving rows using the value in $A3, while in the \n",
      "second off set formula you use the value in C$1 to move the OFFSET formula to another \n",
      "employee. Absolute references are used on these values in the appropriate places so that \n",
      "as you copy the formula around the sheet, you’re still reading row off  sets from column \n",
      "A and row 1.\n",
      "This diff erence calculation needs to be squared, summed, and then square rooted to \n",
      "get the full Euclidean distance:\n",
      "{=SQRT(SUM((OFFSET(Standardized!$B$2:$K$2,Distances!$A3,0)\n",
      "   -OFFSET(Standardized!$B$2:$K$2,Distances!C$1,0))^2))}\n",
      "Note that this calculation is an array formula due to the diff erence of entire rows from \n",
      "each other. So you have to press Ctrl+Shift+Enter (Command+Return on a Mac) to make \n",
      "it work.\n",
      "The Euclidean distance of employee 144624 from his/herself is, naturally, 0. This for-\n",
      "mula can be copy and pasted through OL2. Then highlight this range and double-click \n",
      "the bottom corner to send the calculation down through cell OL402. This gives you the \n",
      "sheet shown in Figure 9-9.\n",
      "And that’s it! Now you have an employee-by-employee graph. You could export it to \n",
      "Gephi like you did in Chapter 5 and take a peak at it, but since it has 16,000 edges and \n",
      "only 400 nodes, it would be a mess.\n",
      "Similarly to how in Chapter 5 you constructed an r-neighborhood graph out of the \n",
      "distance matrix, in this chapter you’re going to focus on only the nearest k neighbors of \n",
      "each employee in order to ﬁ nd the outliers.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "347Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \n",
      "The first step is ranking the distance of each employee in relation to each other \n",
      "employee. This ranking will yield the ﬁ rst and most basic method for highlighting outli-\n",
      "ers on the graph.\n",
      "Figure 9-9: The employee distance matrix\n",
      "Getting the k Nearest Neighbors\n",
      "Create a new tab called Rank. Paste the employee IDs starting down at A2 and across at \n",
      "B1 to form a grid, as on the previous tab.\n",
      "Now you need to rank each employee going across the top according to his or her dis-\n",
      "tance to each employee in column A. Start the rankings at 0, just so that rank 1 will go \n",
      "to an actual other employee, and all the 0s will stay on the diagonal of the graph (due to \n",
      "self-distances always being the smallest). \n",
      "Starting in B2, the ranking of employee 144624 in relation to him/herself is written \n",
      "using the \n",
      "RANK formula:\n",
      "=RANK(Distances!C3,Distances!$C3:$OL3,1)-1\n",
      "This -1 at the end of the formula gives this self-distance a rank of 0 instead of 1. Note \n",
      "that you lock down columns C through OL on the Distances tab with absolute references, \n",
      "which allows you to copy this formula to the right.\n",
      "Copying this formula one to the right, C2, you are now ranking employee 142619 in \n",
      "relationship to their distance from employee 144624:\n",
      "=RANK(Distances!D3,Distances!$C3:$OL3,1)-1\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "348 Data Smart\n",
      "This returns a rank of 194 out of 400, so these two folks aren’t exactly buds (see \n",
      "Figure 9-10).\n",
      "Figure 9-10:  Employee 142619 ranked by distance in relation to 144624\n",
      "Copy this formula throughout the sheet. You’ll get the full ranking matrix pictured in \n",
      "Figure 9-11.\n",
      "Figure 9-11: Each employee on the column ranked in relation to each row\n",
      "Graph Outlier Detection Method 1: Just Use the Indegree\n",
      "If you wanted to assemble a k nearest neighbors (kNN) graph using the Distances and \n",
      "Rank sheets, all you’d need to do is delete any edge in the Distances sheet (set its cell to\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "349Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \n",
      "blank) whose rank was greater than k. For k = 5, you’d drop all the distances with a rank \n",
      "on the Rank sheet that was 6 or over.\n",
      "What would it mean to be an outlier in this context? Well, an outlier wouldn’t get picked \n",
      "all that often as a “nearest neighbor,” now would it?\n",
      "Say you created a 5NN graph, so you kept only those edges with a rank of 5 or less. If \n",
      "you scroll down a column, such as column B for employee 144624, how many times does \n",
      "this employee end up in the top-ﬁ ve ranks for all the other employees? That is, how many \n",
      "employees choose 144624 as one of their top ﬁ ve neighbors? Not many. I’m eyeballing none, \n",
      "in fact, except for its self-distance on the diagonal with a rank of 0, which you can ignore. \n",
      "How about if you made a 10NN? Well, in that case employee 139071 on row 23 hap-\n",
      "pens to consider 144624 its ninth nearest neighbor. This means that in the 5NN graph \n",
      "employee 144624 has an indegree of 0, whereas in the 10NN graph employee 144624 has \n",
      "an indegree of 1. \n",
      "The indegree is the count of the number of edges going into any node on a graph. The lower \n",
      "the indegree, the more of an outlier you are, because no one wants to be your neighbor.\n",
      "At the bottom of column B on the Rank sheet, count up the indegree for employee \n",
      "144624 for the cases of 5, 10, and 20 nearest neighbor graphs. You can do this using a \n",
      "simple \n",
      "COUNTIF formula (subtracting out 1 for the self-distance on the diagonal which \n",
      "you’re ignoring). So, for example, to count up the indegree for employee 144624 in a 5NN \n",
      "graph, you’d use the following formula in cell B402:\n",
      "=COUNTIF(B2:B401,”<=5”)–1\n",
      "Similarly below it, you could calculate the employee’s indegree if you made a 10NN \n",
      "graph:\n",
      "=COUNTIF(B2:B401,”<=10”)-1\n",
      "And below that for a 20NN:\n",
      "=COUNTIF(B2:B401,”<=20”)-1\n",
      "Indeed, you could pick any k you wanted between 1 and the number of employees \n",
      "you have. But you can stick with 5, 10, and 20 for now. Using the conditional formatting \n",
      "menu, you can highlight cells whose counts are 0 (which means there are no inbound \n",
      "edges to the node for a graph of that size). This calculation on employee 144624 yields \n",
      "the tab shown in Figure 9-12.\n",
      "Highlighting B402:B404, you can drag the calculations to the right through column \n",
      "OK. Scrolling through the results, you can see that some employees may be considered \n",
      "outliers at the 5NN mark but not necessarily at the 10NN mark (if you deﬁ  ne an outlier \n",
      "as an employee with a 0 indegree—you could use another number if you liked).\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "350 Data Smart\n",
      "Figure 9-12: The indegree counts for three different nearest neighbor graphs\n",
      "There are only two employees who even at the 20NN graph level still have no inbound \n",
      "edges. No one considers them even in the top 20 closest of neighbors. That’s pretty distant!\n",
      "Those two employee IDs are 137155 and 143406. Flipping back to the SupportPersonnel \n",
      "tab, you can investigate. Employee 137155 is on row 300 (see Figure 9-13). They have a \n",
      "high ticket average, high customer rating, and they appear to be a good Samaritan. They’ve \n",
      "taken lots of weekend shifts, graveyard shifts, and they’ve off ered on seven occasions to \n",
      "swap shifts with an employee who needed it. Nice! This is someone who across multiple \n",
      "dimensions is exceptional enough that they’re not even in the top 20 distances to any other \n",
      "employee. That’s pretty amazing. Maybe this employee deserves a pizza party or something.\n",
      "Figure 9-13: The performance data for employee 137155\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "351Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \n",
      "What about the other employee—143406? They’re on row 375, and they’re an interesting \n",
      "contrast to the previous employee (see Figure 9-14). No metric by itself is enough to ﬁ re \n",
      "them, but that said, their ticket number is two standard deviations below the average, their \n",
      "customer rating is likewise a couple of standard deviations down the distribution. Their \n",
      "tardies are above average, and they’ve taken ﬁ ve out of six sick days on a Friday. Hmmm. \n",
      "This employee has taken plenty of employee development, which is a plus. But maybe \n",
      "that’s because they just enjoy getting out of taking tickets. Perhaps employee dev should \n",
      "start being graded. And they’ve requested four shift swaps without off ering to swap with \n",
      "someone else. \n",
      "This employee feels like they’re working the system. While meeting the minimum \n",
      "requirements for employment (note they’re not jumping any Tukey fences here), they seem \n",
      "to be skating by at the bad end of every distribution.\n",
      "Figure 9-14: The performance data for employee 143406\n",
      "Graph Outlier Detection Method 2: Getting Nuanced with \n",
      "k-Distance\n",
      "One of the drawbacks of the previous method is that for a given kNN graph you either get \n",
      "an inbound edge from someone or you don’t. And that means that you get large shifts in \n",
      "who’s an outlier and who’s not one, depending on the value of k you pick. This example \n",
      "ended up trying 5, 10, and 20 before you were left with just two employees. And of those \n",
      "two employees, which one was the biggest outlier? Beats me! They both had an indegree \n",
      "of 0 on the 20NN, so they were kinda tied, right?\n",
      "What would be nice is to have a calculation that assigned an employee a continuous \n",
      "degree of outlying-ness. The next two methods you’ll look at attempt to do just that. First, \n",
      "you’ll look at ranking outliers using a quantity called the k-distance.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "352 Data Smart\n",
      "The k-distance is the distance from an employee to their kth neighbor. \n",
      "Nice and simple, but since it’s giving back a distance rather than a count, you can get \n",
      "a nice ranking out of the value. Create a new tab in the workbook called K-Distance to \n",
      "take a look.\n",
      "For k, use 5, which means you’ll grab everyone’s distance to their ﬁ fth closest neighbor. \n",
      "One way to think of this is that if the neighborhood where I live has ﬁ  ve neighbors and \n",
      "myself, how much land does that neighborhood sit on? If I have to walk 30 minutes to \n",
      "make it to my ﬁ fth neighbor’s house, then maybe I live in the boonies. \n",
      "So label A1 as How many employees are in my neighborhood? and put a 5 in B1. This \n",
      "is your k value.\n",
      "Starting in A3, label the column Employee ID and paste the employee IDs down. Then \n",
      "you’ll start calculating the k-distance with that of employee 144624 in cell B4.\n",
      "Now, how do you calculate the distance between 144624 and his ﬁ fth closest neighbor? \n",
      "The ﬁ fth closest employee will be ranked 5 on row 2 (144624’s row) of the Rank tab. So \n",
      "you can just use an \n",
      "IF statement to set that value to 1 in a vector of all 0s, and then mul-\n",
      "tiply that vector times the distances row for 144624 on the Distances tab. Finally, sum \n",
      "everything up. \n",
      "Thus, in B4 you’d have:\n",
      "{=SUM(IF(Rank!B2:OK2=$B$1,1,0)*Distances!C3:OL3)}\n",
      "Note that the k value in cell B1 is locked down with absolute references, so you can \n",
      "copy the formula down. Also, this is an array formula since the IF statement is checking \n",
      "an entire array of values.\n",
      "Double-click the formula to send it down the sheet and apply some conditional for-\n",
      "matting to highlight the large distances. Once again, the two outliers from the previous \n",
      "section rise to the top (see Figure 9-15).\n",
      "Figure 9-15: Employee 143406 has a high 5-distance\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "353Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \n",
      "This time around, you get a little more nuance. You can see in this single list that the \n",
      "bad employee, 143406, is substantially more distant than 137155, and both of those values \n",
      "are substantially larger than the next largest value of 3.53.\n",
      "But there’s a drawback to this approach, which is visualized in Figure 9-16. Merely using \n",
      "k-distance gives you a sense of global outlying-ness, that is, you can highlight points that \n",
      "are farther away from their neighbors than any other points. But when you look at Figure \n",
      "9-16, the triangular point is clearly the outlier, and yet, its k-distance is going to be less \n",
      "than that of some of the diamond shape points.\n",
      "Are those diamonds really weirder than that triangle? Not to my eyes!\n",
      "The issue here is that the triangle is not a global outlier, so much as it is a local outlier. \n",
      "The reason why your eyeballs pick it up as the odd point out is that it’s nearest to the tight \n",
      "cluster of circles. If the triangle were among the spaced-out diamonds, it’d be ﬁ ne. But it’s \n",
      "not. Instead, it looks nothing like its circular neighbors.\n",
      "This leads to a cutting-edge technique called local outlier factors (LOF).\n",
      "Figure 9-16: k-distance fails on local outliers\n",
      "Graph Outlier Detection Method 3: Local Outlier Factors Are \n",
      "Where It’s At\n",
      "Just like using k-distance, local outlier factors provide a single score for each point. The \n",
      "larger the score, the more of an outlier they are. But LOF gives you something a little \n",
      "cooler than that: The closer the score is to 1, the more ordinary the point is locally. As\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "354 Data Smart\n",
      "the score increases, the point should be considered less typical and more like an outlier. \n",
      "And unlike k-distance, this “1 is typical” fact doesn’t change no matter the size or scale \n",
      "of your graph, which is really cool.\n",
      "At a high level here’s how it works: You are an outlier if your k nearest neighbors consider \n",
      "you farther away than their neighbors consider them. The algorithm cares about a point’s \n",
      "friends and friends-of-friends. That’s how it deﬁ nes “local.”\n",
      "Looking back at Figure 9-16 this is exactly what makes the triangle an outlier, isn’t it? \n",
      "It may not have the highest k-distance, but the ratio of the triangle’s distance to its nearest \n",
      "neighbors over their distance to each other is quite high (see Figure 9-17). \n",
      "Figure 9-17: The triangle is not nearly as reachable by its neighbors as the neighbors are by each other\n",
      "Starting with Reach Distance\n",
      "Before you can put together your local outlier factors for each employee, you need to cal-\n",
      "culate one more set of numbers, called reachability distances. \n",
      "The reachability distance of employee A with respect to employee B is just their ordinary \n",
      "distance, unless A is within B’s k-distance neighborhood, in which case the reachability distance \n",
      "is just B’s k-distance.\n",
      "In other words, if A is inside B’s neighborhood, you round up A’s distance to B to the \n",
      "size of B’s neighborhood; otherwise, you leave it alone.\n",
      "Using reachability distance rather than ordinary distance for LOF helps stabilize the \n",
      "calculation a bit. \n",
      "Create a new tab called Reach-dist and replace the distances from the Distances tab \n",
      "with the new reach distances.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "355Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \n",
      "First thing you’ll want to do is Paste Special the transposed values from the K-Distance \n",
      "tab across the top of the tab, and then paste the employee-by-employee grid, like on the \n",
      "Distances tab starting in row 3. This gives you the empty sheet shown in Figure 9-18.\n",
      "Figure 9-18: The skeleton of the reach distance tab\n",
      "Starting in cell B4, you’re going to slide in the distance of 144624 to itself from the \n",
      "Distances tab (Distances!C3) unless it’s less than the k-distance above in B1. It’s a simple \n",
      "MAX formula:\n",
      "=MAX(B$1,Distances!C3)\n",
      "The absolute reference on the k-distance allows you to copy the formula around the \n",
      "sheet. Copying the formula through OK4, you can then highlight the calculations on row 4 \n",
      "and double-click them to send them through row 403. This ﬁ lls in all the reach distances, \n",
      "as shown in Figure 9-19.\n",
      "Putting Together the Local Outlier Factors\n",
      "Now you’re ready to calculate each employee’s local outlier factor. To start, create a new \n",
      "tab called LOF and paste the employee IDs down column A.\n",
      "As stated earlier, local outlier factors gauge how a point is viewed by its neighbors versus \n",
      "how those neighbors are viewed by their neighbors. If I’m 30 miles outside of town, my \n",
      "closest neighbors may view me as a redneck, whereas they are viewed by their neighbors \n",
      "as members of the community. That means that locally I’m viewed more as an outlier than \n",
      "my neighbors are. You want to capture that phenomenon.\n",
      "These values hinge on the average reachability of each employee with respect to his k \n",
      "nearest neighbors.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "356 Data Smart\n",
      "Figure 9-19:  All reach distances\n",
      "Consider employee 144624 on row 2. You’ve already set k to 5, so the question is, what \n",
      "is the average reachability distance of 144624 with respect to that employee’s ﬁ ve nearest \n",
      "neighbors? \n",
      "To calculate this, pull a vector of 1s from the Rank tab for the ﬁ ve employees closest to \n",
      "144624 and 0s for everyone else (similar to what you did on the K-Distance tab). Such a \n",
      "vector can be created using \n",
      "IF formulas to grab the top-ranked neighbors while exclud-\n",
      "ing the actual employee:\n",
      "IF(Rank!B2:OK2<=’K-Distance’!B$1,1,0)*IF(Rank!B2:OK2>0,1,0)\n",
      "Multiply this indicator vector times 144624’s reach distances, sum up the product, and \n",
      "divide them by k=5. In cell B2, then, you have:\n",
      "=SUM(IF(Rank!B2:OK2<=’K-Distance’!B$1,1,0)*\n",
      "IF(Rank!B2:OK2>0,1,0)*\n",
      "‘Reach-dist’!B4:OK4)/’K-Distance’!B$1}\n",
      "Just as when you calculated k-distance, this is an array formula. You can send this \n",
      "formula down the sheet by double-clicking it (see Figure 9-20).\n",
      "So this column indicates how the ﬁ ve nearest neighbors of each employee view them.\n",
      "The local outlier factor then for an employee is the average of the ratios of the employee’s \n",
      "average reachability distance divided by the average reachability distances of each of their k \n",
      "neighbors.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "357Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \n",
      "Figure 9-20: Average reachability for each employee with respect to his neighbors\n",
      "You will tackle the LOF calculation for employee 144624 in cell C2 ﬁ  rst. Just as in \n",
      "previous calculations, the following IF statements give you a vector of 1s for 144624’s top \n",
      "ﬁ ve nearest neighbors:\n",
      "IF(Rank!B2:OK2<=’K-Distance’!B$1,1,0)*IF(Rank!B2:OK2>0,1,0)\n",
      "You then multiply the ratio of 144624’s average reachability divided by each neighbor’s \n",
      "average reachability as:\n",
      "IF(Rank!B2:OK2<=’K-Distance’!B$1,1,0)\n",
      "  *IF(Rank!B2:OK2>0,1,0)*B2/TRANSPOSE(B$2:B$401)\n",
      "Note that the neighbors’ reachability distances referenced in range B2:B401 on the \n",
      "bottom of the ratio are transposed so that the column is turned into a row, just like the \n",
      "vectors coming out of the \n",
      "IF statements in the equation.\n",
      "You can average these ratios by summing them and dividing by k:\n",
      "{=SUM(IF(Rank!B2:OK2<=\n",
      "   ‘K-Distance’!B$1,1,0)\n",
      "   *IF(Rank!B2:OK2>0,1,0)\n",
      "   *B2/TRANSPOSE(B$2:B$401))/’K-Distance’!B$1}\n",
      "Note the curly braces since this is an array formula. Press Control+Shift+Enter \n",
      "(Command+Return on Mac) to get back the LOF factor for 144624.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "358 Data Smart\n",
      "It’s 1.34, which is somewhat over a value of 1, meaning that this employee is a bit of a \n",
      "local outlier.\n",
      "You can send this formula down the sheet by double-clicking and then check out \n",
      "the other employees. Conditional formatting is helpful to highlight the most signiﬁ  cant \n",
      "outliers.\n",
      "Lo and behold, when you scroll down you ﬁ  nd that employee 143406, the resident \n",
      "slacker, is the most outlying point with an LOF of 1.97 (see Figure 9-21). His neighbors \n",
      "view him as twice as distant as they are viewed by their neighbors. That’s pretty far out-\n",
      "side the community.\n",
      "Figure 9-21: LOFs for the employees. Somebody is knocking on the door of 2.\n",
      "And that’s it! You now have a single value assigned to each employee that ranks them \n",
      "as a local outlier and is scaled the same no matter the size of the graph. Pretty ﬂ  ippin’ \n",
      "awesome.\n",
      "Wrapping Up\n",
      "Between the graph modularity chapter and this chapter on outlier detection, you’ve been \n",
      "exposed to the power of analyzing a dataset by “graphing” your data, that is, assigning \n",
      "distances and edges between your observations. \n",
      "Although in the clustering chapters, you mined groups of related points for insights, \n",
      "here you mined the data for points outside of communities. You saw the power of some-\n",
      "thing as simple as indegree to demonstrate who’s inﬂ uential and who’s isolated.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "359Outlier Detection: Just Because They’re Odd Doesn’t Mean They’re Unimportant \n",
      "For more on outlier detection, check out the 2010 survey put together by Kriegel, Kroger, \n",
      "and Zimek at http://www.siam.org/meetings/sdm10/tutorial3.pdf for the 2010 SIAM \n",
      "conference. All the techniques in this chapter show up there along with a number of others.\n",
      "Note that these techniques don’t require any kind of arbitrarily long-running process \n",
      "the way optimization models might. There are a ﬁ nite number of steps to get LOFs, so this \n",
      "kind of thing can be coded in production on top of a database quite easily. \n",
      "If you’re looking for a good programming language to do this stuff  in, R is the way to \n",
      "go. The bplot function in R provides box plots of data with Tukey fences built in. The \n",
      "ability to plot Tukey fences graphically is something so painful in Excel that I didn’t even \n",
      "bother putting it in this book, so the bplot function is a huge plus for R.\n",
      "Also in R, the DMwR package (which accompanies the excellent Data Mining with R \n",
      "book by Torgo [Chapman and Hall, 2010]) includes an implementation of LOF in a func-\n",
      "tion called lofactor. To construct and analyze the degree of nodes in a graph, the igraph \n",
      "package in Python and R is the way  to go.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "10\n",
      "A\n",
      "fter spending the previous nine chapters injecting Excel directly into your veins, I’m \n",
      "now going to tell you to drop it. Well, not for everything, but let’s be honest, Excel \n",
      "is not ideal for all analytics tasks.\n",
      "Excel is awesome for learning analytics, because you can touch and see your data in \n",
      "every state as an algorithm changes it from input into output. But you came, you saw, \n",
      "you learned. Do you really need to go through all those steps manually every time? For \n",
      "example, do you really need to bake up your own optimization formulation to ﬁ t your own \n",
      "logistic regressions? Do you need to input the deﬁ nitions of cosine similarity all yourself? \n",
      "Now that you’ve learned it, you’re allowed to cheat and have someone else do that for \n",
      "you! Think of yourself as Wolfgang Puck. Does he cook everything at all his restaurants? \n",
      "I sure hope not; otherwise, his skills vary wildly from airport to real world. Now that \n",
      "you’ve learned this stuff , you too should feel comfortable using other folks’ implementa-\n",
      "tions of these algorithms.\n",
      "And that, among many other things (for example, referencing a whole table of data using \n",
      "one word) is why moving from Excel into the analytics-focused programming language \n",
      "called R is worth doing.\n",
      "This chapter runs some of the previous chapters’ analyses in R rather than Excel—same \n",
      "data, same algorithms, diff erent environment. You’ll see how easy this stuff  can be!\n",
      "Now, just as a warning, this chapter is not an intro tutorial of R. I’m going to be \n",
      "moving at a thousand miles an hour to hit a few algorithms in a single chapter. If you \n",
      "want a more comprehensive introduction, check out the books I recommend at the end \n",
      "of this chapter.\n",
      "And if you haven’t read the previous chapters to this point, this isn’t going to make \n",
      "a lick of sense, because I’m going to assume that you are already familiar with the data, \n",
      "problems, and techniques from earlier chapters. This ain’t a “choose your own adventure” \n",
      "novel. Read everything else and come back!\n",
      "Moving from \n",
      "Spreadsheets into R\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "362 Data Smart\n",
      "Getting Up and Running with R\n",
      "You can download R from the R website at www.r-project.org. Just click the download \n",
      "link, pick a mirror nearest you, and download the installer for your OS.\n",
      "Run through the installer (on Windows it’s nice to install the software as the admin-\n",
      "istrator) and then open the application. On Windows and Mac, the R console is going to \n",
      "load. It looks something like Figure 10-1.\n",
      "Figure 10-1:  The R console on Mac OS\n",
      "Inside the R console, you type commands into the > prompt and press Return to get \n",
      "the system to do anything. Here’s a couple for you:\n",
      "> print(\"No regrets. Texas forever.\")\n",
      "[1] \"No regrets. Texas forever.\"\n",
      "> 355/113\n",
      "[1] 3.141593\n",
      "You can call the print  function to get the system to print out text. You can also \n",
      "type in arithmetic directly to make calculations. Now , my standard workflow for \n",
      "using R is:\n",
      " 1. Bring data into an R.\n",
      " 2. Do data-sciency things with data.\n",
      " 3. Dump results out of R where some other person or process can use them.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "363Moving from Spreadsheets into R \n",
      "When it comes to the ﬁ rst step, bringing data in R, there are all sorts of options, but in \n",
      "order to understand variables and datatypes, you’ll start simply by entering data manually.\n",
      "Some Simple Hand-Jamming\n",
      "The simplest way to get data in R is the same way you get it into Excel. By typing it with \n",
      "your ﬁ ngers and storing those keystrokes somewhere. You can start by storing a single \n",
      "value in a variable:\n",
      "> almostpi <- 355/113\n",
      "> almostpi\n",
      "[1] 3.141593\n",
      "> sqrt(almostpi)\n",
      "[1] 1.772454\n",
      "In this little bit of code, you are storing 355/113 in a variable called almostpi. Then \n",
      "by typing the variable back into the console and pressing Return, you can print its con-\n",
      "tents. You can then act on that variable with a variety of functions (this example calls \n",
      "the square root). \n",
      "For a quick reference of many of the built-in functions R has (functions available without \n",
      "loading packages ... something you’re building toward), check out the R reference card at \n",
      "http://cran.r-project.org/doc/contrib/Short-refcard.pdf.\n",
      "To understand what a function does, just type a question mark before it when you put \n",
      "it into the console:\n",
      "> ?sqrt\n",
      "This will pop open a Help window on the function (see Figure 10-2 for the Help win-\n",
      "dow on sqrt). \n",
      "You can also type two question marks in front of functions to do a search for informa-\n",
      "tion, like the following:\n",
      "> ??log\n",
      "The log search yields the results shown in Figure 10-3.\n",
      "NOTE\n",
      "There are all sorts of great resources for ﬁ nding out what functions and packages are avail-\n",
      "able to you in R besides the whole ?? rigmarole. For example, rseek.org is a great search \n",
      "engine for R-related content. And you can post speciﬁ c questions to stackoverflow.com \n",
      "(see http://stackoverflow.com/questions/tagged/r) and the R mailing list (see http://\n",
      "www.r-project.org/mail.html).\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "364 Data Smart\n",
      "Figure 10-2: The Help window for the square root function\n",
      "Vector Math and Factoring\n",
      "You can insert a vector of numbers using the c() function (the c stands for “combine”). \n",
      "Toss some primes into a variable:\n",
      "> someprimes <- c(1,2,3,5,7,11)\n",
      "> someprimes\n",
      "[1]  1  2  3  5  7 11\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "365Moving from Spreadsheets into R \n",
      "Figure 10-3:  Search results for the word log\n",
      "Using the Length() function, you can count the number of elements you have in your \n",
      "vector:\n",
      "> length(someprimes)\n",
      "[1] 6\n",
      "You can also reference single values in the vector using bracket notation:\n",
      "> someprimes[4]\n",
      "[1] 5\n",
      "This gives back the fourth value in the vector, which happens to be 5. You can provide \n",
      "vectors of indices using the c() function or a : character to specify a range:\n",
      "> someprimes[c(4,5,6)]\n",
      "[1]  5  7 11\n",
      "> someprimes[4:6]\n",
      "[1]  5  7 11\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "366 Data Smart\n",
      "In both of these cases, you’re grabbing the fourth through sixth values of the vector. \n",
      "You can also use logical statements to pull out values. For instance, if you only wanted \n",
      "primes less than seven, you could use the \n",
      "which() function to return their indices:\n",
      "> which(someprimes<7)\n",
      "[1] 1 2 3 4\n",
      "> someprimes[which(someprimes<7)]\n",
      "[1] 1 2 3 5\n",
      "Once you’ve placed your data in a variable, you can perform operations on the entire \n",
      "dataset and store the results in a new variable. For example, you can multiply all the data \n",
      "by two:\n",
      "> primestimes2 <- someprimes*2\n",
      "> primestimes2\n",
      "[1]  2  4  6 10 14 22\n",
      "Think about how you do this in Excel. You enter the formula in the adjacent column \n",
      "and copy it down. R lets you name that column or row of data and operate on that variable \n",
      "as a single entity, which is neat.\n",
      "One useful function for checking your data for wonky entries is the \n",
      "summary function:\n",
      "> summary(someprimes)\n",
      "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
      "  1.000   2.250   4.000   4.833   6.500  11.000\n",
      "And you can work with text data too:\n",
      "> somecolors <- c(\"blue\",\"red\",\"green\",\"blue\",\n",
      "\"green\",\"yellow\",\"red\",\"red\")\n",
      "> somecolors\n",
      "[1] \"blue\"   \"red\"    \"green\"  \"blue\"   \"green\"  \"yellow\" \"red\"    \"red\"\n",
      "If you summarize somecolors, all you get is a little bit of descriptive data:\n",
      "> summary(somecolors)\n",
      "   Length     Class      Mode \n",
      "        8 character character\n",
      "But you can treat these colors as categories and make this vector into categorical data \n",
      "by “factoring” it:\n",
      "> somecolors <- factor(somecolors)\n",
      "> somecolors\n",
      "[1] blue   red    green  blue   green  yellow red    red   \n",
      "Levels: blue green red yellow\n",
      "Now when you summarize the data, you get back counts for each “level” (a level is \n",
      "essentially a category):\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "367Moving from Spreadsheets into R \n",
      "> summary(somecolors)\n",
      "  blue  green    red yellow \n",
      "     2      2      3      1\n",
      "Two-Dimensional Matrices\n",
      "The vectors you’ve been playing with so far are one-dimensional. Something more akin \n",
      "to a spreadsheet in R might be a matrix, which is a two-dimensional array of numbers. \n",
      "You can construct one with the \n",
      "matrix function:\n",
      "> amatrix <- matrix(data=c(someprimes,primestimes2),nrow=2,ncol=6)\n",
      "> amatrix\n",
      "     [,1] [,2] [,3] [,4] [,5] [,6]\n",
      "[1,]    1    3    7    2    6   14\n",
      "[2,]    2    5   11    4   10   22\n",
      "You can count columns and rows:\n",
      "> nrow(amatrix)\n",
      "[1] 2\n",
      "> ncol(amatrix)\n",
      "[1] 6\n",
      "If you want to transpose the data (just as you did throughout the book using Excel’s \n",
      "Paste Special transpose functionality), you use the t() function:\n",
      "> t(amatrix)\n",
      "     [,1] [,2]\n",
      "[1,]    1    2\n",
      "[2,]    3    5\n",
      "[3,]    7   11\n",
      "[4,]    2    4\n",
      "[5,]    6   10\n",
      "[6,]   14   22\n",
      "To grab individual records or ranges, you use the same bracket notation, except you \n",
      "separate column and row references with a comma:\n",
      "> amatrix[1:2,3]\n",
      "[1]  7 11\n",
      "This gives back rows 1 through 2 for column 3. But you need not reference row 1 and \n",
      "2 since that’s all the rows you have—you can instead leave that portion of the bracket \n",
      "blank and all the rows will be printed:\n",
      "> amatrix[,3]\n",
      "[1]  7 11\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "368 Data Smart\n",
      "Using the rbind() and cbind() functions, you can smush new rows and columns of \n",
      "data into the matrix:\n",
      "> primestimes3 <- someprimes*3\n",
      "> amatrix <- rbind(amatrix,primestimes3)\n",
      "> amatrix\n",
      "             [,1] [,2] [,3] [,4] [,5] [,6]\n",
      "                1    3    7    2    6   14\n",
      "                2    5   11    4   10   22\n",
      "primestimes3    3    6    9   15   21   33\n",
      "Here you’ve created a new row of data (primestimes3) and used rbind() on the amatrix \n",
      "variable to tack primestimes3 onto it and assign the result back into amatrix.\n",
      "The Best Datatype of Them All: The Dataframe\n",
      "A dataframe is the ideal way to work with real world, database table-style data in R. A \n",
      "dataframe in R is a speciﬁ c version of the “list” datatype. So what’s a list? A list is a col-\n",
      "lection of objects in R that can be of diff erent types. For instance, here’s a list with some \n",
      "info about yours truly:\n",
      "> John <- list(gender=\"male\", age=\"ancient\", height = 72,\n",
      "               spawn = 3, spawn_ages = c(.5,2,5))\n",
      "> John\n",
      "$gender\n",
      "[1] \"male\"\n",
      "$age\n",
      "[1] \"ancient\"\n",
      "$height\n",
      "[1] 72\n",
      "$spawn\n",
      "[1] 3\n",
      "$spawn_ages\n",
      "[1] 0.5 2.0 5.0\n",
      "A dataframe is a type of list that looks eerily similar to an Excel sheet. Essentially, it’s a \n",
      "two-dimensional column-oriented sheet of data where columns can be treated as numeric \n",
      "or categorical vectors. You can create a dataframe by calling the \n",
      "data.frame() function \n",
      "on arrays of imported or jammed-in data. The following example uses data from James \n",
      "Bond ﬁ lms to illustrate. First, create some vectors:\n",
      "> bondnames <- c(\"connery\",\"lazenby\",\"moore\",\"dalton\",\"brosnan\",\"craig\")\n",
      "> firstyear <- c(1962,1969,1973,1987,1995,2006)\n",
      "> eyecolor <- c(\"brown\",\"brown\",\"blue\", \"green\", \"blue\", \"blue\")\n",
      "> womenkissed <- c(17,3,20,4,12,4)\n",
      "> countofbondjamesbonds <- c(3,2,10,2,5,1)\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "369Moving from Spreadsheets into R \n",
      "So at this point you have ﬁ ve vectors—some text, some numeric—and all are the same \n",
      "length. You can combine them into a single dataframe called bonddata like so:\n",
      "> bonddata <- data.frame(bondnames,firstyear,eyecolor,womenkissed,\n",
      "countofbondjamesbonds)\n",
      "> bonddata\n",
      "  bondnames firstyear eyecolor womenkissed countofbondjamesbonds\n",
      "1   connery      1962    brown          17                     3\n",
      "2   lazenby      1969    brown           3                     2\n",
      "3     moore      1973     blue          20                    10\n",
      "4    dalton      1987    green           4                     2\n",
      "5   brosnan      1995     blue          12                     5\n",
      "6     craig      2006     blue           4                     1\n",
      "The data.frame function is going to take care of recognizing which of these columns \n",
      "are factors and which are numeric. You can see this diff  erence by calling the str() and \n",
      "summary() functions (the str stands for “structure”):\n",
      "> str(bonddata)\n",
      "'data.frame’: 6 obs. of  5 variables:\n",
      "$ bondnames            : Factor w/ 6 levels \"brosnan\",\"connery\",..: \n",
      "2 5 6 4 1 3\n",
      " $ firstyear            : num  1962 1969 1973 1987 1995 ...\n",
      " $ eyecolor             : Factor w/ 3 levels \"blue\",\"brown\",..: \n",
      "2 2 1 3 1 1\n",
      " $ womenkissed          : num  17 3 20 4 12 4\n",
      " $ countofbondjamesbonds: num  3 2 10 2 5 1\n",
      "> summary(bonddata)\n",
      "bondnames  firstyear     eyecolor womenkissed    countofbondjamesbonds\n",
      "brosnan:1  Min.   :1962  blue :3  Min.   : 3.00  Min.   : 1.000       \n",
      "connery:1  1st Qu.:1970  brown:2  1st Qu.: 4.00  1st Qu.: 2.000       \n",
      "craig  :1  Median :1980  green:1  Median : 8.00  Median : 2.500       \n",
      "dalton :1  Mean   :1982           Mean   :10.00  Mean   : 3.833       \n",
      "lazenby:1  3rd Qu.:1993           3rd Qu.:15.75  3rd Qu.: 4.500       \n",
      "moore  :1  Max.   :2006           Max.   :20.00  Max.   :10.000 \n",
      "Note that the year is being treated as a number. You could factorize this column using \n",
      "the factor() function if you wanted it treated categorically instead. \n",
      "And one of the awesome things about dataframes is that you can reference each column \n",
      "using a $ character plus the column name, as shown:\n",
      "> bonddata$firstyear <- factor(bonddata$firstyear)\n",
      "> summary(bonddata)\n",
      "   bondnames firstyear  eyecolor  womenkissed    countofbondjamesbonds\n",
      " brosnan:1   1962:1    blue :3   Min.   : 3.00   Min.   : 1.000       \n",
      " connery:1   1969:1    brown:2   1st Qu.: 4.00   1st Qu.: 2.000       \n",
      " craig  :1   1973:1    green:1   Median : 8.00   Median : 2.500       \n",
      " dalton :1   1987:1              Mean   :10.00   Mean   : 3.833       \n",
      " lazenby:1   1995:1              3rd Qu.:15.75   3rd Qu.: 4.500       \n",
      " moore  :1   2006:1              Max.   :20.00   Max.   :10.000\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "370 Data Smart\n",
      "Thus, when you run the summary function, the years are rolled up by category counts \n",
      "instead of by distribution data. Also, keep in mind that whenever you transpose a \n",
      "dataframe, the result is a good old two-dimensional matrix rather than another dataframe. \n",
      "This makes sense since the transposed version of the Bond data would not have consistent \n",
      "datatypes in each column.\n",
      "Reading Data into R\n",
      "NOTE\n",
      "The CSV ﬁ le used in this section, “WineKMC.csv,” is available for download at the \n",
      "book’s website, www.wiley.com/go/datasmart.\n",
      "Okay, so you’ve learned how to shove data into various datatypes by hand, but how \n",
      "do you read data in from ﬁ  les? The ﬁ rst thing you need to understand is the working \n",
      "directory. The working directory is the folder in which you can put data so that the R \n",
      "console can ﬁ nd it and read it in. The \n",
      "getwd() function displays the current working \n",
      "directory:\n",
      "> getwd()\n",
      "[1] \"/Users/johnforeman/RHOME\"\n",
      "If you don’t like the present working directory, you can change it with the setwd() \n",
      "command. Keep in mind, even on Windows machines R expects directory paths to be \n",
      "speciﬁ ed with forward slashes. For example:\n",
      "> setwd(\"/Users/johnforeman/datasmartfiles\")\n",
      "Use this command to set your working directory to a place where you’re happy to toss \n",
      "some data. You’ll start by placing the downloaded WineKMC.csv ﬁ  le in that directory. \n",
      "This comma-delimited ﬁ le has the data from the Matrix tab in the k-means clustering \n",
      "workbook from Chapter 2. Read it in and take a look.\n",
      "To read in data, you use the read.csv() function:\n",
      "> winedata <- read.csv(\"WineKMC.csv\")\n",
      "This data should look exactly like the Matrix tab from Chapter 2, so when you print \n",
      "the ﬁ rst few columns (I’ve chosen nine to ﬁ t on this page) you see descriptive data about \n",
      "each of the 32 off ers followed by some customers’ click vectors in columns:\n",
      "> winedata[,1:9]\n",
      "   Offer  Mth   Varietal MinQty Disc    Origin PastPeak Adams Allen\n",
      "1      1  Jan     Malbec     72   56    France    FALSE    NA    NA\n",
      "2      2  Jan Pinot Noir     72   17    France    FALSE    NA    NA\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "371Moving from Spreadsheets into R \n",
      "3      3  Feb  Espumante    144   32    Oregon     TRUE    NA    NA\n",
      "4      4  Feb  Champagne     72   48    France     TRUE    NA    NA\n",
      "5      5  Feb Cab. Sauv.    144   44        NZ     TRUE    NA    NA\n",
      "6      6  Mar   Prosecco    144   86     Chile    FALSE    NA    NA\n",
      "7      7  Mar   Prosecco      6   40 Australia     TRUE    NA    NA\n",
      "8      8  Mar  Espumante      6   45 S. Africa    FALSE    NA    NA\n",
      "9      9  Apr Chardonnay    144   57     Chile    FALSE    NA     1\n",
      "10    10  Apr   Prosecco     72   52        CA    FALSE    NA    NA\n",
      "11    11  May  Champagne     72   85    France    FALSE    NA    NA\n",
      "12    12  May   Prosecco     72   83 Australia    FALSE    NA    NA\n",
      "13    13  May     Merlot      6   43     Chile    FALSE    NA    NA\n",
      "14    14  Jun     Merlot     72   64     Chile    FALSE    NA    NA\n",
      "15    15  Jun Cab. Sauv.    144   19     Italy    FALSE    NA    NA\n",
      "16    16  Jun     Merlot     72   88        CA    FALSE    NA    NA\n",
      "17    17  Jul Pinot Noir     12   47   Germany    FALSE    NA    NA\n",
      "18    18  Jul  Espumante      6   50    Oregon    FALSE     1    NA\n",
      "19    19  Jul  Champagne     12   66   Germany    FALSE    NA    NA\n",
      "20    20  Aug Cab. Sauv.     72   82     Italy    FALSE    NA    NA\n",
      "21    21  Aug  Champagne     12   50        CA    FALSE    NA    NA\n",
      "22    22  Aug  Champagne     72   63    France    FALSE    NA    NA\n",
      "23    23 Sept Chardonnay    144   39 S. Africa    FALSE    NA    NA\n",
      "24    24 Sept Pinot Noir      6   34     Italy    FALSE    NA    NA\n",
      "25    25  Oct Cab. Sauv.     72   59    Oregon     TRUE    NA    NA\n",
      "26    26  Oct Pinot Noir    144   83 Australia    FALSE    NA    NA\n",
      "27    27  Oct  Champagne     72   88        NZ    FALSE    NA     1\n",
      "28    28  Nov Cab. Sauv.     12   56    France     TRUE    NA    NA\n",
      "29    29  Nov  P. Grigio      6   87    France    FALSE     1    NA\n",
      "30    30  Dec     Malbec      6   54    France    FALSE     1    NA\n",
      "31    31  Dec  Champagne     72   89    France    FALSE    NA    NA\n",
      "32    32  Dec Cab. Sauv.     72   45   Germany     TRUE    NA    NA\n",
      "It’s all in! But you’ll notice that the blank spaces in purchase vectors (which Excel treats \n",
      "as zeroes) have become NA values. You need to make those NA values 0, which you can do \n",
      "using the is.na() function inside of brackets:\n",
      "> winedata[is.na(winedata)] <- 0\n",
      "> winedata[1:10,8:17]\n",
      "   Adams Allen Anders Bailey Baker Barnes Bell Bennett Brooks Brown\n",
      "1      0     0      0      0     0      0    0       0      0     0\n",
      "2      0     0      0      0     0      0    1       0      0     0\n",
      "3      0     0      0      0     0      0    0       0      1     0\n",
      "4      0     0      0      0     0      0    0       0      0     0\n",
      "5      0     0      0      0     0      0    0       0      0     0\n",
      "6      0     0      0      0     0      0    0       0      0     0\n",
      "7      0     0      0      1     1      0    0       0      0     1\n",
      "8      0     0      0      0     0      0    0       1      1     0\n",
      "9      0     1      0      0     0      0    0       0      0     0\n",
      "10     0     0      0      0     1      1    0       0      0     0\n",
      "Bam! NA becomes 0.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "372 Data Smart\n",
      "Doing Some Actual Data Science\n",
      "At this point you’ve learned how to work with variables and datatypes, hand-jam data, \n",
      "and read it in from a CSV. But how do you actually use the algorithms you learned earlier \n",
      "in this book? Since you already have the wine data loaded up, you’ll start with a little \n",
      "k-means clustering.\n",
      "Spherical K-Means on Wine Data in Just a Few Lines\n",
      "In this section, you’ll cluster based on cosine similarity (also called spherical k-means). \n",
      "And in R, there’s a spherical k-means package you can load, called skmeans. But skmeans \n",
      "doesn’t come baked into R; it’s written by a third party as a package that you can load into \n",
      "R and use. Essentially, these geniuses have done all the work for you, and you just have \n",
      "to stand on their shoulders.\n",
      "Like most R packages, you can read up on it and install it from the Comprehensive R \n",
      "Archive Network (CRAN). CRAN is a repository of many of the useful packages that can \n",
      "be loaded into R to extend its functionality. A list of all the packages you can download \n",
      "from CRAN is available here: \n",
      "http://cran.r-project.org/web/packages/.\n",
      "Just search for “spherical k means” in rseek.org and a PDF explaining the package \n",
      "comes up as the ﬁ rst result. There’s a function called skmeans() that you want.\n",
      "R is initially set up to download packages from CRAN, so to get the skmeans package \n",
      "you need only use the install.packages()  function (R may ask to set up a personal \n",
      "library the ﬁ rst time you do this):\n",
      "> install.packages(\"skmeans\",dependencies = TRUE)\n",
      "trying URL 'http://mirrors.nics.utk.edu/cran/bin/macosx/leopard/\n",
      "                   contrib/2.15/skmeans_0.2-3.tgz’\n",
      "Content type 'application/x-gzip’ length 224708 bytes (219 Kb)\n",
      "opened URL\n",
      "==================================================\n",
      "downloaded 219 Kb\n",
      "The downloaded binary packages are in\n",
      "   /var/…/downloaded_packages\n",
      "You can see in the code that I set dependencies = TRUE  in the installation call. This \n",
      "ensures that if the skmeans package is dependent on any other packages, R downloads \n",
      "those packages as well. The call downloads the appropriate package for my R installation \n",
      "(version 2.15 on Mac) from a mirror and puts it where it needs to go. \n",
      "You can then load the package using the \n",
      "library() function:\n",
      "> library(skmeans)\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "373Moving from Spreadsheets into R \n",
      "You can look up how to use the skmeans() function using the ? call. The documenta-\n",
      "tion speciﬁ es that skmeans() accepts a matrix where each row corresponds to an object \n",
      "to cluster. \n",
      "Your data on the other hand is column-oriented with a bunch of deal descriptors at the \n",
      "beginning that the algorithm isn’t gonna want to see. So you need to transpose it (note \n",
      "that the transpose function coerces a matrix out of the dataframe). \n",
      "Using the \n",
      "ncol() function, you can see that the customer columns go out to column 107, \n",
      "so you can isolate just the purchase vectors as rows for each customer by transposing the \n",
      "data from column 8 to 107 and shoving it in a new variable called \n",
      "winedata.transposed:\n",
      "> ncol(winedata)\n",
      "[1] 107\n",
      "> winedata.transposed <- t(winedata[,8:107])\n",
      "> winedata.transposed[1:10,1:10]\n",
      "        [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n",
      "Adams      0    0    0    0    0    0    0    0    0     0\n",
      "Allen      0    0    0    0    0    0    0    0    1     0\n",
      "Anders     0    0    0    0    0    0    0    0    0     0\n",
      "Bailey     0    0    0    0    0    0    1    0    0     0\n",
      "Baker      0    0    0    0    0    0    1    0    0     1\n",
      "Barnes     0    0    0    0    0    0    0    0    0     1\n",
      "Bell       0    1    0    0    0    0    0    0    0     0\n",
      "Bennett    0    0    0    0    0    0    0    1    0     0\n",
      "Brooks     0    0    1    0    0    0    0    1    0     0\n",
      "Brown      0    0    0    0    0    0    1    0    0     0\n",
      "Then you can call skmeans on the dataset, specifying ﬁ ve means and the use of a genetic \n",
      "algorithm (much like the algorithm you used in Excel). You’ll assign the results back to \n",
      "an object called \n",
      "winedata.clusters:\n",
      "> winedata.clusters <- skmeans(winedata.transposed, 5, method=\"genetic\")\n",
      "Typing the object back into the console, you can get a summary of its contents (your \n",
      "results may vary due to the optimization algorithm):\n",
      "> winedata.clusters\n",
      "A hard spherical k-means partition of 100 objects into 5 classes.\n",
      "Class sizes: 16, 17, 15, 29, 23\n",
      "Call: skmeans(x = winedata.transposed, k = 5, method = \"genetic\")\n",
      "Calling str() on the clusters object shows you that the actual cluster assignments are \n",
      "stored within the “cluster” list of the object:\n",
      "> str(winedata.clusters)\n",
      "List of 7\n",
      " $ prototypes: num [1:5, 1:32] 0.09 0.153 0 0.141 0 ...\n",
      "  ..- attr(*, \"dimnames\")=List of 2\n",
      "  .. ..$ : chr [1:5] \"1\" \"2\" \"3\" \"4\" ...\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "374 Data Smart\n",
      "  .. ..$ : NULL\n",
      " $ membership: NULL\n",
      " $ cluster   : int [1:100] 5 4 1 5 2 2 1 3 3 5 ...\n",
      " $ family    :List of 7\n",
      "  ..$ description: chr \"spherical k-means\"\n",
      "  ..$ D          :function (x, prototypes)  \n",
      "  ..$ C          :function (x, weights, control)  \n",
      "  ..$ init       :function (x, k)  \n",
      "  ..$ e          : num 1\n",
      "  ..$ .modify    : NULL\n",
      "  ..$ .subset    : NULL\n",
      "  ..- attr(*, \"class\")= chr \"pclust_family\"\n",
      " $ m         : num 1\n",
      " $ value     : num 38\n",
      " $ call      : language skmeans(x = winedata.transposed,\n",
      "                                k = 5, method = \"genetic\")\n",
      " - attr(*, \"class\")= chr [1:2] \"skmeans\" \"pclust\"\n",
      "So for instance, if you wanted to pull back the cluster assignment for row 4, you’d just \n",
      "use the matrix notation on the cluster vector:\n",
      "> winedata.clusters$cluster[4]\n",
      "[1] 5\n",
      "Now, each row is labeled with a customer’s name (because they were labeled when you \n",
      "read them in with the read.csv() function), so you can also pull assignments by name \n",
      "using the row.names() function combined with the which() function:\n",
      "> winedata.clusters$cluster[\n",
      "which(row.names(winedata.transposed)==\"Wright\")\n",
      "]\n",
      "[1] 4\n",
      "Cool! Furthermore, you can write out all these cluster assignments using the \n",
      "write.csv()  function if you cared to. Use ? to learn how to use it. Spoiler: It’s like \n",
      "read.csv() . \n",
      "Now, the main way you understood the clusters in Excel was by understanding the \n",
      "patterns in the descriptors of the deals that deﬁ ned them. You counted up the total deals \n",
      "taken in each cluster and sorted. How do you do something similar in R?\n",
      "To perform the counts, you just use the aggregate() function where in the “by” ﬁ eld \n",
      "you specify the cluster assignments—meaning “aggregate purchases by assignment.” And \n",
      "you also need to specify that the type of aggregation you want is a sum as opposed to a \n",
      "mean, min, max, median, and so on:\n",
      "aggregate(winedata.transposed,by=list(winedata.clusters$cluster),sum)\n",
      "You’ll use transpose to store these counts back as ﬁ  ve columns (just as they were \n",
      "in Excel) and you’ll lop off the first row of the aggregation, which just gives back\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "375Moving from Spreadsheets into R \n",
      "the cluster assignment names. Then, store all this back as a variable called winedata\n",
      ".clustercounts :\n",
      "> winedata.clustercounts <-t(aggregate(winedata.transposed,by=list\n",
      "           (winedata.clusters$cluster),sum)[,2:33])\n",
      "> winedata.clustercounts\n",
      "    [,1] [,2] [,3] [,4] [,5]\n",
      "V1     2    5    0    3    0\n",
      "V2     7    3    0    0    0\n",
      "V3     0    2    3    0    1\n",
      "V4     0    5    1    6    0\n",
      "V5     0    0    0    4    0\n",
      "V6     0    8    1    3    0\n",
      "V7     0    3    1    0   15\n",
      "V8     0    1   15    0    4\n",
      "V9     0    2    0    8    0\n",
      "V10    1    4    1    0    1\n",
      "V11    0    7    1    4    1\n",
      "V12    1    3    0    0    1\n",
      "V13    0    0    2    0    4\n",
      "V14    0    3    0    6    0\n",
      "V15    0    3    0    3    0\n",
      "V16    1    1    0    3    0\n",
      "V17    7    0    0    0    0\n",
      "V18    0    1    4    0    9\n",
      "V19    0    4    1    0    0\n",
      "V20    0    2    0    4    0\n",
      "V21    0    1    1    1    1\n",
      "V22    0   17    2    2    0\n",
      "V23    1    1    0    3    0\n",
      "V24   12    0    0    0    0\n",
      "V25    0    3    0    3    0\n",
      "V26   12    0    0    3    0\n",
      "V27    1    4    1    3    0\n",
      "V28    0    5    0    0    1\n",
      "V29    0    1    4    0   12\n",
      "V30    0    4    4    1   13\n",
      "V31    0   16    1    0    0\n",
      "V32    0    2    0    2    0\n",
      "All right, so there are your counts of deals by cluster. Let’s slap those seven columns of \n",
      "descriptive data back on to the deals using the column bind function cbind():\n",
      "> winedata.desc.plus.counts <- \n",
      "cbind(winedata[,1:7],winedata.clustercounts)\n",
      "> winedata.desc.plus.counts\n",
      "    Offer  Mth   Varietal MinQty Disc    Origin PastPeak  1  2  3 4  5\n",
      "V1      1  Jan     Malbec     72   56    France    FALSE  2  5  0 3  0\n",
      "V2      2  Jan Pinot Noir     72   17    France    FALSE  7  3  0 0  0\n",
      "V3      3  Feb  Espumante    144   32    Oregon     TRUE  0  2  3 0  1\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "376 Data Smart\n",
      "V4      4  Feb  Champagne     72   48    France     TRUE  0  5  1 6  0\n",
      "V5      5  Feb Cab. Sauv.    144   44        NZ     TRUE  0  0  0 4  0\n",
      "V6      6  Mar   Prosecco    144   86     Chile    FALSE  0  8  1 3  0\n",
      "V7      7  Mar   Prosecco      6   40 Australia     TRUE  0  3  1 0 15\n",
      "V8      8  Mar  Espumante      6   45 S. Africa    FALSE  0  1 15 0  4\n",
      "V9      9  Apr Chardonnay    144   57     Chile    FALSE  0  2  0 8  0\n",
      "V10    10  Apr   Prosecco     72   52        CA    FALSE  1  4  1 0  1\n",
      "V11    11  May  Champagne     72   85    France    FALSE  0  7  1 4  1\n",
      "V12    12  May   Prosecco     72   83 Australia    FALSE  1  3  0 0  1\n",
      "V13    13  May     Merlot      6   43     Chile    FALSE  0  0  2 0  4\n",
      "V14    14  Jun     Merlot     72   64     Chile    FALSE  0  3  0 6  0\n",
      "V15    15  Jun Cab. Sauv.    144   19     Italy    FALSE  0  3  0 3  0\n",
      "V16    16  Jun     Merlot     72   88        CA    FALSE  1  1  0 3  0\n",
      "V17    17  Jul Pinot Noir     12   47   Germany    FALSE  7  0  0 0  0\n",
      "V18    18  Jul  Espumante      6   50    Oregon    FALSE  0  1  4 0  9\n",
      "V19    19  Jul  Champagne     12   66   Germany    FALSE  0  4  1 0  0\n",
      "V20    20  Aug Cab. Sauv.     72   82     Italy    FALSE  0  2  0 4  0\n",
      "V21    21  Aug  Champagne     12   50        CA    FALSE  0  1  1 1  1\n",
      "V22    22  Aug  Champagne     72   63    France    FALSE  0 17  2 2  0\n",
      "V23    23 Sept Chardonnay    144   39 S. Africa    FALSE  1  1  0 3  0\n",
      "V24    24 Sept Pinot Noir      6   34     Italy    FALSE 12  0  0 0  0\n",
      "V25    25  Oct Cab. Sauv.     72   59    Oregon     TRUE  0  3  0 3  0\n",
      "V26    26  Oct Pinot Noir    144   83 Australia    FALSE 12  0  0 3  0\n",
      "V27    27  Oct  Champagne     72   88        NZ    FALSE  1  4  1 3  0\n",
      "V28    28  Nov Cab. Sauv.     12   56    France     TRUE  0  5  0 0  1\n",
      "V29    29  Nov  P. Grigio      6   87    France    FALSE  0  1  4 0 12\n",
      "V30    30  Dec     Malbec      6   54    France    FALSE  0  4  4 1 13\n",
      "V31    31  Dec  Champagne     72   89    France    FALSE  0 16  1 0  0\n",
      "V32    32  Dec Cab. Sauv.     72   45   Germany     TRUE  0  2  0 2  0\n",
      "And you can sort using the order()  function inside the brackets of the dataframe. \n",
      "Here’s a sort to discover the most popular deals for cluster 1 (note that I put a minus sign \n",
      "in front of the data to sort descending. Alternatively, you can set the \n",
      "decreasing=TRUE  \n",
      "ﬂ ag in the order() function.):\n",
      "> winedata.desc.plus.counts[order(-winedata.desc.plus.counts[,8]),]\n",
      "    Offer  Mth   Varietal MinQty Disc    Origin PastPeak  1  2  3 4  5\n",
      "V24    24 Sept Pinot Noir      6   34     Italy    FALSE 12  0  0 0  0\n",
      "V26    26  Oct Pinot Noir    144   83 Australia    FALSE 12  0  0 3  0\n",
      "V2      2  Jan Pinot Noir     72   17    France    FALSE  7  3  0 0  0\n",
      "V17    17  Jul Pinot Noir     12   47   Germany    FALSE  7  0  0 0  0\n",
      "V1      1  Jan     Malbec     72   56    France    FALSE  2  5  0 3  0\n",
      "V10    10  Apr   Prosecco     72   52        CA    FALSE  1  4  1 0  1\n",
      "V12    12  May   Prosecco     72   83 Australia    FALSE  1  3  0 0  1\n",
      "V16    16  Jun     Merlot     72   88        CA    FALSE  1  1  0 3  0\n",
      "V23    23 Sept Chardonnay    144   39 S. Africa    FALSE  1  1  0 3  0\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "377Moving from Spreadsheets into R \n",
      "V27    27  Oct  Champagne     72   88        NZ    FALSE  1  4  1 3  0\n",
      "V3      3  Feb  Espumante    144   32    Oregon     TRUE  0  2  3 0  1\n",
      "V4      4  Feb  Champagne     72   48    France     TRUE  0  5  1 6  0\n",
      "V5      5  Feb Cab. Sauv.    144   44        NZ     TRUE  0  0  0 4  0\n",
      "V6      6  Mar   Prosecco    144   86     Chile    FALSE  0  8  1 3  0\n",
      "V7      7  Mar   Prosecco      6   40 Australia     TRUE  0  3  1 0 15\n",
      "V8      8  Mar  Espumante      6   45 S. Africa    FALSE  0  1 15 0  4\n",
      "V9      9  Apr Chardonnay    144   57     Chile    FALSE  0  2  0 8  0\n",
      "V11    11  May  Champagne     72   85    France    FALSE  0  7  1 4  1\n",
      "V13    13  May     Merlot      6   43     Chile    FALSE  0  0  2 0  4\n",
      "V14    14  Jun     Merlot     72   64     Chile    FALSE  0  3  0 6  0\n",
      "V15    15  Jun Cab. Sauv.    144   19     Italy    FALSE  0  3  0 3  0\n",
      "V18    18  Jul  Espumante      6   50    Oregon    FALSE  0  1  4 0  9\n",
      "V19    19  Jul  Champagne     12   66   Germany    FALSE  0  4  1 0  0\n",
      "V20    20  Aug Cab. Sauv.     72   82     Italy    FALSE  0  2  0 4  0\n",
      "V21    21  Aug  Champagne     12   50        CA    FALSE  0  1  1 1  1\n",
      "V22    22  Aug  Champagne     72   63    France    FALSE  0 17  2 2  0\n",
      "V25    25  Oct Cab. Sauv.     72   59    Oregon     TRUE  0  3  0 3  0\n",
      "V28    28  Nov Cab. Sauv.     12   56    France     TRUE  0  5  0 0  1\n",
      "V29    29  Nov  P. Grigio      6   87    France    FALSE  0  1  4 0 12\n",
      "V30    30  Dec     Malbec      6   54    France    FALSE  0  4  4 1 13\n",
      "V31    31  Dec  Champagne     72   89    France    FALSE  0 16  1 0  0\n",
      "V32    32  Dec Cab. Sauv.     72   45   Germany     TRUE  0  2  0 2  0\n",
      "Looking at the top deals, it becomes clear that cluster 1 is the Pinot Noir cluster. (Your \n",
      "mileage may vary. The genetic algorithm doesn’t give the same answer each time.)\n",
      "So just to reiterate then, if you strip away all my pontiﬁ  cation, the following R code \n",
      "replicates much of Chapter 2 of this book:\n",
      "> setwd(\"/Users/johnforeman/datasmartfiles\")\n",
      "> winedata <- read.csv(\"WineKMC.csv\")\n",
      "> winedata[is.na(winedata)] <- 0\n",
      "> install.packages(\"skmeans\",dependencies = TRUE)\n",
      "> library(skmeans)\n",
      "> winedata.transposed <- t(winedata[,8:107])\n",
      "> winedata.clusters <- skmeans(winedata.transposed, 5, method=\"genetic\")\n",
      "> winedata.clustercounts <-\n",
      "t(aggregate(winedata.transposed,\n",
      "by=list(winedata.clusters$cluster),sum)[,2:33])\n",
      "> winedata.desc.plus.counts <- \n",
      "cbind(winedata[,1:7],winedata.clustercounts)\n",
      "> winedata.desc.plus.counts[order(-winedata.desc.plus.counts[,8]),]\n",
      "That’s it—from reading in the data all the way to analyzing the clusters. Pretty nuts! \n",
      "And that’s because the call to skmeans() pretty much isolates all the complexity of this \n",
      "method away from you. Terrible for learning, but awesome for working.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "378 Data Smart\n",
      "Building AI Models on the Pregnancy Data\n",
      "NOTE\n",
      "The CSV ﬁ les used in this section, “Pregnancy.csv” and “Pregnancy_Test.csv,” are avail-\n",
      "able for download at the book’s website, www.wiley.com/go/datasmart.\n",
      "In this section, you’re going to replicate some of the pregnancy prediction models you \n",
      "built in Chapters 6 and 7 of this book. Speciﬁ  cally, you’re going to build two classiﬁ ers \n",
      "using the glm() function (general linear model) with a logistic link function and using \n",
      "the randomForest() function (randomForest() bags trees, which may be anywhere from \n",
      "simple stumps to full decision trees).\n",
      "The training and test data are separated into two CSV ﬁ  les, called Pregnancy.csv and \n",
      "Pregnancy_Test.csv. Go ahead and save them into your working directory and then load \n",
      "them into a couple of dataframes:\n",
      "> PregnancyData <- read.csv(\"Pregnancy.csv\")\n",
      "> PregnancyData.Test <- read.csv(\"Pregnancy_Test.csv\")\n",
      "You can then run summary() and str() on the data to get a feel for it. It’s immediately \n",
      "apparent that the gender and address type data have been loaded as categorical data, but as \n",
      "you can see in the \n",
      "str() output, the response variable (1 for pregnant, 0 for not pregnant) \n",
      "has been treated as numeric instead of as two distinct classes:\n",
      "> str(PregnancyData)\n",
      "'data.frame’: 1000 obs. of  18 variables:\n",
      "$ Implied.Gender        : Factor w/ 3 levels \"F\",\"M\",\"U\": 2 2 2 3 1...\n",
      "$ Home.Apt..PO.Box      : Factor w/ 3 levels \"A\",\"H\",\"P\": 1 2 2 2 1...\n",
      "$ Pregnancy.Test        : int  1 1 1 0 0 0 0 0 0 0 ...\n",
      "$ Birth.Control         : int  0 0 0 0 0 0 1 0 0 0 ...\n",
      "$ Feminine.Hygiene      : int  0 0 0 0 0 0 0 0 0 0 ...\n",
      "$ Folic.Acid            : int  0 0 0 0 0 0 1 0 0 0 ...\n",
      "$ Prenatal.Vitamins     : int  1 1 0 0 0 1 1 0 0 1 ...\n",
      "$ Prenatal.Yoga         : int  0 0 0 0 1 0 0 0 0 0 ...\n",
      "$ Body.Pillow           : int  0 0 0 0 0 0 0 0 0 0 ...\n",
      "$ Ginger.Ale            : int  0 0 0 1 0 0 0 0 1 0 ...\n",
      "$ Sea.Bands             : int  0 0 1 0 0 0 0 0 0 0 ...\n",
      "$ Stopped.buying.ciggies: int  0 0 0 0 0 1 0 0 0 0 ...\n",
      "$ Cigarettes            : int  0 0 0 0 0 0 0 0 0 0 ...\n",
      "$ Smoking.Cessation     : int  0 0 0 0 0 0 0 0 0 0 ...\n",
      "$ Stopped.buying.wine   : int  0 0 0 0 1 0 0 0 0 0 ...\n",
      "$ Wine                  : int  0 0 0 0 0 0 0 0 0 0 ...\n",
      "$ Maternity.Clothes     : int  0 0 0 0 0 0 0 1 0 1 ...\n",
      "$ PREGNANT             : int  1 1 1 1 1 1 1 1 1 1 ...\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "379Moving from Spreadsheets into R \n",
      "It’s best for randomForest()  that you actually factorize this response variable into \n",
      "two classes (a 0 class and a 1 class) instead of treating the data as an integer. So you can \n",
      "factorize the data like so:\n",
      "PregnancyData$PREGNANT <- factor(PregnancyData$PREGNANT)\n",
      "PregnancyData.Test$PREGNANT <- factor(PregnancyData.Test$PREGNANT)\n",
      "Now if you summarize the PREGNANT column, you merely get back class counts as if 0 \n",
      "and 1 were categories:\n",
      "> summary(PregnancyData$PREGNANT)\n",
      "  0   1 \n",
      "500 500\n",
      "To build a logistic regression, you need the glm() function, which is in the built-in stats \n",
      "package for R. But for the randomForest() function, you’ll need the randomForest pack-\n",
      "age. Also, it’d be nice to build the ROC curves that you saw in Chapters 6 and 7. There’s a \n",
      "package speciﬁ cally built to give you those graphs, called \n",
      "ROCR. Go ahead and install and \n",
      "load up those two real quick:\n",
      "> install.packages(\"randomForest\",dependencies=TRUE)\n",
      "> install.packages(\"ROCR\",dependencies=TRUE)\n",
      "> library(randomForest)\n",
      "> library(ROCR)\n",
      "You now have the data in and the packages loaded. It’s time to get model building! Start \n",
      "with a logistic regression:\n",
      "> Pregnancy.lm <- glm(PREGNANT ~ .,\n",
      "data=PregnancyData,family=binomial(\"logit\"))\n",
      "The glm()function builds the linear model that you’ve speciﬁ ed as a logistic regression \n",
      "using the family=binomial(\"logit\") option. You supply data to the function using the \n",
      "data=PregnancyData ﬁ eld. Now, you’re probably wondering what PREGNANT ~ . means. \n",
      "This is a formula in R. It means “train my model to predict the PREGNANT column using all \n",
      "the other columns.” The ~ means “using” and the period means “all the other columns.” \n",
      "You can specify a subset of columns as well by typing their column names:\n",
      "> Pregnancy.lm <- glm(PREGNANT ~ \n",
      "Implied.Gender + \n",
      "Home.Apt..PO.Box + \n",
      "Pregnancy.Test + \n",
      "Birth.Control,\n",
      "data=PregnancyData,family=binomial(\"logit\"))\n",
      "But you’re using the PREGNANT~. notation because you want to use all of the columns \n",
      "to train the model.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "380 Data Smart\n",
      "Once the linear model is built, you can view the coeffi  cients and analyze which vari-\n",
      "ables are statistically signiﬁ  cant (similar to the t tests you conducted in Chapter 6) by \n",
      "summarizing the model:\n",
      "> summary(Pregnancy.lm)\n",
      "Call:\n",
      "glm(formula = PREGNANT ~ ., family = binomial(\"logit\"), \n",
      "data = PregnancyData)\n",
      "Deviance Residuals: \n",
      "    Min       1Q   Median       3Q      Max  \n",
      "-3.2012  -0.5566  -0.0246   0.5127   2.8658  \n",
      "Coefficients:\n",
      "                    Estimate Std. Error z value Pr(>|z|)    \n",
      "(Intercept)        -0.343597   0.180755  -1.901 0.057315 .  \n",
      "Implied.GenderM    -0.453880   0.197566  -2.297 0.021599 *  \n",
      "Implied.GenderU     0.141939   0.307588   0.461 0.644469    \n",
      "Home.Apt..PO.BoxH  -0.172927   0.194591  -0.889 0.374180    \n",
      "Home.Apt..PO.BoxP  -0.002813   0.336432  -0.008 0.993329    \n",
      "Pregnancy.Test      2.370554   0.521781   4.543 5.54e-06 ***\n",
      "Birth.Control      -2.300272   0.365270  -6.297 3.03e-10 ***\n",
      "Feminine.Hygiene   -2.028558   0.342398  -5.925 3.13e-09 ***\n",
      "Folic.Acid          4.077666   0.761888   5.352 8.70e-08 ***\n",
      "Prenatal.Vitamins   2.479469   0.369063   6.718 1.84e-11 ***\n",
      "Prenatal.Yoga       2.922974   1.146990   2.548 0.010822 *  \n",
      "Body.Pillow         1.261037   0.860617   1.465 0.142847    \n",
      "Ginger.Ale          1.938502   0.426733   4.543 5.55e-06 ***\n",
      "Sea.Bands           1.107530   0.673435   1.645 0.100053    \n",
      "Stopped.buying.cig  1.302222   0.342347   3.804 0.000142 ***\n",
      "Cigarettes         -1.443022   0.370120  -3.899 9.67e-05 ***\n",
      "Smoking.Cessation   1.790779   0.512610   3.493 0.000477 ***\n",
      "Stopped.buying.win  1.383888   0.305883   4.524 6.06e-06 ***\n",
      "Wine               -1.565539   0.348910  -4.487 7.23e-06 ***\n",
      "Maternity.Clothes   2.078202   0.329432   6.308 2.82e-10 ***\n",
      "---\n",
      "Signif. codes: 0 '***’ 0.001 '**’ 0.01 '*’ 0.05 '.’ 0.1 ' ' 1\n",
      "Those coeffi  cients without at least one * next to them are of dubious worth.\n",
      "Similarly, you can train a random forest model using the randomForest() function:\n",
      "> Pregnancy.rf <- \n",
      "randomForest(PREGNANT~.,data=PregnancyData,importance=TRUE)\n",
      "This is the same basic syntax as the glm() call (execute ?randomForest to learn more \n",
      "about tree count and depth). Note the importance=TRUE  in the call. This allows you to\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "381Moving from Spreadsheets into R \n",
      "graph variable importance using another function, varImpPlot(), which will allow you \n",
      "to understand which variables are important and which are weak. \n",
      "The randomForest package allows you to look at how much each variable contributes \n",
      "to decreasing node impurity on average. The more a variable contributes, the more useful \n",
      "it is. You can use this to select and pare down the variables you might want to feed into \n",
      "another model. To look at this data, use the \n",
      "varImpPlot() function with type=2 to pull \n",
      "rankings based on the node impurity calculation introduced in Chapter 7 (feel free to use \n",
      "the \n",
      "? command to read up on the diff erence between type=1 and type=2):\n",
      "> varImpPlot(Pregnancy.rf, type=2)\n",
      "This yields the ranking shown in Figure 10-4. Folic acid ranks ﬁ rst with prenatal vita-\n",
      "mins and birth control trailing. \n",
      "Figure 10-4:  A variable importance plot in R\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "382 Data Smart\n",
      "Now that you’ve built the models, you can predict with them using the predict()  \n",
      "function in R. Call the function and save the results to two diff  erent variables, so you \n",
      "can compare models. The way the predict() function generally works is that it accepts \n",
      "a model, a dataset to predict on, and any model-speciﬁ c options:\n",
      "> PregnancyData.Test.lm.Preds <-\n",
      "predict(Pregnancy.lm,PregnancyData.Test,type=\"response\")\n",
      "> PregnancyData.Test.rf.Preds <-\n",
      "predict(Pregnancy.rf,PregnancyData.Test,type=\"prob\")\n",
      "You can see in the two predict calls, that each is provided with a diff erent model, the \n",
      "test data, and the type parameters that those models need. In the case of a linear model, \n",
      "type=\"response\" sets the values returned from the prediction to be between 0 and 1 just \n",
      "like the original PREGNANT values. In the case of the random forest, the type=\"prob\" ensures \n",
      "that you get back class probabilities—two columns of data, one probability of pregnancy \n",
      "and one probability of no pregnancy.\n",
      "These outputs are slightly diff erent, but then again, they use diff erent algorithms, diff er-\n",
      "ent models, and so on. It’s important to play with these things and read the documentation.\n",
      "Here’s a summary of the prediction output:\n",
      "> summary(PregnancyData.Test.lm.Preds)\n",
      "    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n",
      "0.001179 0.066190 0.239500 0.283100 0.414300 0.999200 \n",
      "> summary(PregnancyData.Test.rf.Preds)\n",
      "       0                1         \n",
      " Min.   :0.0000   Min.   :0.0000  \n",
      " 1st Qu.:0.7500   1st Qu.:0.0080  \n",
      " Median :0.9500   Median :0.0500  \n",
      " Mean   :0.8078   Mean   :0.1922  \n",
      " 3rd Qu.:0.9920   3rd Qu.:0.2500  \n",
      " Max.   :1.0000   Max.   :1.0000  \n",
      "The second column from the random forest predictions then is the probability associ-\n",
      "ated with pregnancy (as opposed to a non-pregnancy), so that’s the column that’s akin to \n",
      "the logistic regression predictions. Using the bracket notation, you can pull out individual \n",
      "records or sets of records and look at their input data and predictions (I’ve transposed the \n",
      "row to make it print prettier):\n",
      "> t(PregnancyData.Test[1,])\n",
      "                       1  \n",
      "Implied.Gender         \"U\"\n",
      "Home.Apt..PO.Box       \"A\"\n",
      "Pregnancy.Test         \"0\"\n",
      "Birth.Control          \"0\"\n",
      "Feminine.Hygiene       \"0\"\n",
      "Folic.Acid             \"0\"\n",
      "Prenatal.Vitamins      \"0\"\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "383Moving from Spreadsheets into R \n",
      "Prenatal.Yoga          \"0\"\n",
      "Body.Pillow            \"0\"\n",
      "Ginger.Ale             \"0\"\n",
      "Sea.Bands              \"1\"\n",
      "Stopped.buying.ciggies \"0\"\n",
      "Cigarettes             \"0\"\n",
      "Smoking.Cessation      \"0\"\n",
      "Stopped.buying.wine    \"1\"\n",
      "Wine                   \"1\"\n",
      "Maternity.Clothes      \"0\"\n",
      "PREGNANT               \"1\"\n",
      "> t(PregnancyData.Test.lm.Preds[1])\n",
      "             1\n",
      "[1,] 0.6735358\n",
      "> PregnancyData.Test.rf.Preds[1,2]\n",
      "[1] 0.504\n",
      "Note that in printing the input row, I leave the column index blank in the square brack-\n",
      "ets [1,] so that all columns’ data is printed. This particular customer has an unknown \n",
      "gender, lives in an apartment, and has bought sea bands and wine, but then stopped buying \n",
      "wine. The logistic regression gives them a score of 0.67 while the random forest is right \n",
      "around 0.5. The truth is that she is pregnant—chalk one up for the logistic regression!\n",
      "Now that you have the two vectors of class probabilities, one for each mode, you \n",
      "can compare the models in terms of true positive rate and false positive rate just as you \n",
      "did earlier in the book. Luckily for you, though, in R the \n",
      "ROCR  package can compute \n",
      "and plot the ROC curves so you don’t have to. Since you’ve already loaded the ROCR  \n",
      "package, the ﬁ rst thing you need to do is create two ROCR prediction objects (using the \n",
      "ROCR prediction()  function), which simply count up the positive and negative class \n",
      "predictions at various cutoff  levels in the class probabilities:\n",
      "> pred.lm <-\n",
      "prediction(PregnancyData.Test.lm.Preds,\n",
      "PregnancyData.Test$PREGNANT)\n",
      "> pred.rf <-\n",
      "prediction(PregnancyData.Test.rf.Preds[,2],\n",
      "PregnancyData.Test$PREGNANT)\n",
      "Note in the second call that you hit the second column of class probabilities from the \n",
      "random forest object just as discussed earlier. You can then turn these prediction objects \n",
      "into \n",
      "ROCR performance objects by running them through the performance() function. A \n",
      "performance object takes the classiﬁ cations given by the model on the test set for various \n",
      "cutoff  values and uses them to assemble a curve of your choosing (in this case a ROC \n",
      "curve):\n",
      "> perf.lm <- performance(pred.lm,\"tpr\",\"fpr\")\n",
      "> perf.rf <- performance(pred.rf,\"tpr\",\"fpr\")\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "384 Data Smart\n",
      "NOTE\n",
      "If you’re curious, performance() provides other options besides the tpr and fpr values, \n",
      "such as prec for precision and rec for recall. Read the ROCR package documentation \n",
      "for more detail.\n",
      "You can then plot these curves using R’s plot() function. First, the linear model curve \n",
      "(the xlim and ylim ﬂ ags are used to set the upper and lower bounds on the x and y axes \n",
      "in the graph):\n",
      "> plot(perf.lm,xlim=c(0,1),ylim=c(0,1))\n",
      "You can add the random forest curve in using the add=TRUE ﬂ ag to overlay it and the lty=2 \n",
      "ﬂ ag (lty stands for “line type”; check out ?plot to learn more) to make this line dashed:\n",
      "> plot(perf.rf,xlim=c(0,1),ylim=c(0,1),lty=2,add=TRUE)\n",
      "This overlays the two curves with the random forest performance as a dashed line, \n",
      "as shown in Figure 10-5. For the most part, the logistic regression is superior with the \n",
      "random forest pulling ahead brieﬂ y on the far right of the graph.\n",
      "Figure 10-5:  Recall and precision graphed in R\n",
      "All right, so to recap here, you trained two diff erent predictive models, used them on \n",
      "a test set, and compared their precision versus recall using the following code:\n",
      "> PregnancyData <- read.csv(\"Pregnancy.csv\")\n",
      "> PregnancyData.Test <- read.csv(\"Pregnancy_Test.csv\")\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "385Moving from Spreadsheets into R \n",
      "> PregnancyData$PREGNANT <- factor(PregnancyData$PREGNANT)\n",
      "> PregnancyData.Test$PREGNANT <- factor(PregnancyData.Test$PREGNANT)\n",
      "> install.packages(\"randomForest\",dependencies=TRUE)\n",
      "> install.packages(\"ROCR\",dependencies=TRUE)\n",
      "> library(randomForest)\n",
      "> library(ROCR)\n",
      "> Pregnancy.lm <- glm(PREGNANT ~ .,\n",
      "data=PregnancyData,family=binomial(\"logit\"))\n",
      "> summary(Pregnancy.lm)\n",
      "> Pregnancy.rf <-\n",
      "randomForest(PREGNANT~.,data=PregnancyData,importance=TRUE)\n",
      "> PregnancyData.Test.rf.Preds <-\n",
      "predict(Pregnancy.rf,PregnancyData.Test,type=\"prob\")\n",
      "> varImpPlot(Pregnancy.rf, type=2)\n",
      "> PregnancyData.Test.lm.Preds <-\n",
      "predict(Pregnancy.lm,PregnancyData.Test,type=\"response\")\n",
      "> PregnancyData.Test.rf.Preds <-\n",
      "predict(Pregnancy.rf,PregnancyData.Test,type=\"prob\")\n",
      "> pred.lm <-\n",
      "prediction(PregnancyData.Test.lm.Preds,\n",
      "PregnancyData.Test$PREGNANT)\n",
      "> pred.rf <- \n",
      "prediction(PregnancyData.Test.rf.Preds[,2],\n",
      "PregnancyData.Test$PREGNANT)\n",
      "> perf.lm <- performance(pred.lm,\"tpr\",\"fpr\")\n",
      "> perf.rf <- performance(pred.rf,\"tpr\",\"fpr\")\n",
      "> plot(perf.lm,xlim=c(0,1),ylim=c(0,1))\n",
      "> plot(perf.rf,xlim=c(0,1),ylim=c(0,1),lty=2,add=TRUE)\n",
      "Pretty straightforward, really. Compared to Excel, look at how easy it was to compare \n",
      "two diff erent models. That’s quite nice.\n",
      "Forecasting in R\n",
      "NOTE\n",
      "The CSV ﬁ le used in this section, “SwordDemand.csv,” is available for download at the \n",
      "book’s website, www.wiley.com/go/datasmart.\n",
      "This next section is nuts. Why? Because you’re going to regenerate the exponential \n",
      "smoothing forecast from Chapter 8 so fast it’s going to make your head spin.\n",
      "First, load in the sword demand data from SwordDemand.csv and print it to the console:\n",
      "> sword <- read.csv(\"SwordDemand.csv\")\n",
      "> sword\n",
      "SwordDemand\n",
      "1          165\n",
      "2          171\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "386 Data Smart\n",
      "3          147\n",
      "4          143\n",
      "5          164\n",
      "6          160\n",
      "7          152\n",
      "8          150\n",
      "9          159\n",
      "10         169\n",
      "11         173\n",
      "12         203\n",
      "13         169\n",
      "14         166\n",
      "15         162\n",
      "16         147\n",
      "17         188\n",
      "18         161\n",
      "19         162\n",
      "20         169\n",
      "21         185\n",
      "22         188\n",
      "23         200\n",
      "24         229\n",
      "25         189\n",
      "26         218\n",
      "27         185\n",
      "28         199\n",
      "29         210\n",
      "30         193\n",
      "31         211\n",
      "32         208\n",
      "33         216\n",
      "34         218\n",
      "35         264\n",
      "36         304\n",
      "All right, so you have 36 months of demand loaded up, nice and simple. The ﬁ rst thing \n",
      "you need to do is tell R that this is time series data. There’s a function called ts() that is \n",
      "used for this purpose:\n",
      "sword.ts <- ts(sword,frequency=12,start=c(2010,1))\n",
      "In this call, you provide the ts() function with the data, a frequency value (the number \n",
      "of observations per unit of time, which in this case is 12 per year), and a starting point \n",
      "(this example uses January 2010).\n",
      "When you print \n",
      "sword.ts by typing it in the terminal, R now knows to print it in a \n",
      "table by month:\n",
      "> sword.ts\n",
      "     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "387Moving from Spreadsheets into R \n",
      "2010 165 171 147 143 164 160 152 150 159 169 173 203\n",
      "2011 169 166 162 147 188 161 162 169 185 188 200 229\n",
      "2012 189 218 185 199 210 193 211 208 216 218 264 304\n",
      "Nice!\n",
      "You can plot the data too:\n",
      " > plot(sword.ts)\n",
      "This gives the graph shown in Figure 10-6.\n",
      "Figure 10-6:  Graph of sword demand\n",
      "At this point, you’re ready to forecast, which you can do using the excellent forecast \n",
      "package. Feel free to look it up on CRAN (http://cran.r-project.org/package=forecast) \n",
      "or watch the author talk about it in this YouTube video: http://www.youtube.com/\n",
      "watch?v=1Lh1HlBUf8k.\n",
      "To forecast using the forecast  package, you just feed a time series object into the \n",
      "forecast() function. The forecast() call has been set up to detect the appropriate tech-\n",
      "nique to use. Remember how you ran through a few techniques earlier in the book? The \n",
      "forecast() function is gonna do all that stuff  for you:\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "388 Data Smart\n",
      "> install.packages(\"forecast\",dependencies=TRUE)\n",
      "> library(forecast)\n",
      "> sword.forecast <- forecast(sword.ts)\n",
      "And that’s it. Your forecast is saved in the sword.forecast object. Now you can print it:\n",
      "> sword.forecast\n",
      "         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n",
      "Jan 2013       242.9921 230.7142 255.2699 224.2147 261.7695\n",
      "Feb 2013       259.4216 246.0032 272.8400 238.8999 279.9433\n",
      "Mar 2013       235.8763 223.0885 248.6640 216.3191 255.4334\n",
      "Apr 2013       234.3295 220.6882 247.9709 213.4669 255.1922\n",
      "May 2013       274.1674 256.6893 291.6456 247.4369 300.8980\n",
      "Jun 2013       252.5456 234.6894 270.4019 225.2368 279.8544\n",
      "Jul 2013       257.0555 236.7740 277.3370 226.0376 288.0734\n",
      "Aug 2013       262.0715 238.9718 285.1711 226.7436 297.3993\n",
      "Sep 2013       279.4771 252.0149 306.9392 237.4774 321.4768\n",
      "Oct 2013       289.7890 258.1684 321.4097 241.4294 338.1487\n",
      "Nov 2013       320.5914 281.9322 359.2506 261.4673 379.7155\n",
      "Dec 2013       370.3057 321.2097 419.4018 295.2198 445.3917\n",
      "Jan 2014       308.3243 263.6074 353.0413 239.9357 376.7130\n",
      "Feb 2014       327.6427 275.9179 379.3675 248.5364 406.7490\n",
      "Mar 2014       296.5754 245.8459 347.3049 218.9913 374.1594\n",
      "Apr 2014       293.3646 239.2280 347.5013 210.5698 376.1595\n",
      "May 2014       341.8187 274.0374 409.5999 238.1562 445.4812\n",
      "Jun 2014       313.6061 247.0271 380.1851 211.7823 415.4299\n",
      "Jul 2014       317.9789 245.9468 390.0109 207.8153 428.1424\n",
      "Aug 2014       322.9807 245.1532 400.8081 203.9538 442.0075\n",
      "Sep 2014       343.1975 255.4790 430.9160 209.0436 477.3513\n",
      "Oct 2014       354.6286 258.7390 450.5181 207.9782 501.2790\n",
      "Nov 2014       391.0099 279.4304 502.5893 220.3638 561.6559\n",
      "Dec 2014       450.1820 314.9086 585.4554 243.2992 657.0648\n",
      "You get a forecast with prediction intervals built-in! And you can print the actual \n",
      "forecasting technique used by printing the method value in the sword.forecast object:\n",
      "> sword.forecast$method\n",
      "[1] \"ETS(M,A,M)\"\n",
      "The MAM stands for multiplicative error, additive trend, multiplicative seasonality. The \n",
      "forecast()  function has actually chosen to run Holt-Winters exponential smoothing! \n",
      "And you didn’t even have to do anything. When you plot it, as shown in Figure 10-7, you \n",
      "automatically get a fan chart:\n",
      "> plot(sword.forecast)\n",
      "To recap, here’s the code that replicated Chapter 8:\n",
      "> sword <- read.csv(\"SwordDemand.csv\")\n",
      "> sword.ts <- ts(sword,frequency=12,start=c(2010,1))\n",
      "> install.packages(\"forecast\",dependencies=TRUE)\n",
      "> library(forecast)\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "389Moving from Spreadsheets into R \n",
      "> sword.forecast <- forecast(sword.ts)\n",
      "> plot(sword.forecast)\n",
      "Figure 10-7: Fan chart of the demand forecast\n",
      "Crazy. But that’s the beauty of using packages other folks have written specially to do \n",
      "this stuff .\n",
      "Looking at Outlier Detection\n",
      "NOTE\n",
      "The CSV ﬁ les used in this section, “PregnancyDuration.csv” and “CallCenter.csv,” are \n",
      "available for download at the book’s website, www.wiley.com/go/datasmart.\n",
      "In this section, you’ll do one more of the chapters from this book in R, just to drive home \n",
      "the ease of this stuff . To start, read in the pregnancy duration data in PregnancyDuration\n",
      ".csv available from the book’s website:\n",
      "> PregnancyDuration <- read.csv(\"PregnancyDuration.csv\")\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "390 Data Smart\n",
      "In Chapter 9, you calculated the median, ﬁ  rst quartile, third quartile, and inner and \n",
      "outer Tukey fences. You can get the quartiles just from summarizing the data:\n",
      "> summary(PregnancyDuration)\n",
      " GestationDays  \n",
      " Min.   :240.0  \n",
      " 1st Qu.:260.0  \n",
      " Median :267.0  \n",
      " Mean   :266.6  \n",
      " 3rd Qu.:272.0  \n",
      " Max.   :349.0     \n",
      "That makes the interquartile range equal to 272 minus 260 (alternatively, you can call \n",
      "the built-in IQR() function on the GestationDays column):\n",
      "> PregnancyDuration.IQR <- 272 - 260\n",
      "> PregnancyDuration.IQR <- IQR(PregnancyDuration$GestationDays)\n",
      "> PregnancyDuration.IQR\n",
      "[1] 12\n",
      "You can then calculate the lower and upper Tukey fences:\n",
      "> LowerInnerFence <- 260 - 1.5*PregnancyDuration.IQR\n",
      "> UpperInnerFence <- 272 + 1.5*PregnancyDuration.IQR\n",
      "> LowerInnerFence\n",
      "[1] 242\n",
      "> UpperInnerFence\n",
      "[1] 290\n",
      "Using R’s which()  function, it’s easy to determine the points and their indices that \n",
      "violate the fences. For example:\n",
      "> which(PregnancyDuration$GestationDays > UpperInnerFence)\n",
      "[1]   1 249 252 338 345 378 478 913\n",
      "> PregnancyDuration$GestationDays[\n",
      "which(PregnancyDuration$GestationDays > UpperInnerFence)\n",
      "]\n",
      "[1] 349 292 295 291 297 303 293 296\n",
      "Of course, one of the best ways to do this analysis is to use R’s boxplot() function. The \n",
      "boxplot() function will graph the median, ﬁ  rst and third quartiles, Tukey fences, and \n",
      "any outliers. To use it, you simply toss the GestationDays column inside the function:\n",
      "> boxplot(PregnancyDuration$GestationDays)\n",
      "This yields the visualization shown in Figure 10-8.\n",
      "The Tukey fences can be modiﬁ ed to be “outer” fences by changing the range ﬂ ag in \n",
      "the boxplot call (it defaults to 1.5 times the \n",
      "IQR). If you set range=3, then the Tukey fences \n",
      "are drawn at the last point inside three times the IQR instead:\n",
      "> boxplot(PregnancyDuration$GestationDays, range=3)\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "391Moving from Spreadsheets into R \n",
      "As shown in Figure 10-9, note now that you have only one outlier, which is \n",
      "Mrs. Hadlum’s pregnancy duration of 349 days. \n",
      "Figure 10-8:  A boxplot of the pregnancy \n",
      "duration data\n",
      "Figure 10-9:  A boxplot with Tukey fences \n",
      "using three times the IQR\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "392 Data Smart\n",
      "You can also pull this data out of the boxplot in the console rather than plot it. Printing \n",
      "the stats list, you get the fences and quartiles:\n",
      "> boxplot(PregnancyDuration$GestationDays,range=3)$stats\n",
      "     [,1]\n",
      "[1,]  240\n",
      "[2,]  260\n",
      "[3,]  267\n",
      "[4,]  272\n",
      "[5,]  303\n",
      "Printing the out list, you get a list of outlier values:\n",
      "> boxplot(PregnancyDuration$GestationDays,range=3)$out\n",
      "[1] 349\n",
      "Okay, so that’s a bit on the pregnancy duration problem. Let’s move on to the harder \n",
      "problem of ﬁ  nding outliers in the call center employee performance data. It’s in the \n",
      "CallCenter.csv sheet on the book’s website. Loading it up and summarizing, you get:\n",
      "> CallCenter <- read.csv(\"CallCenter.csv\")\n",
      "> summary(CallCenter)\n",
      "       ID             AvgTix          Rating         Tardies     \n",
      " Min.   :130564   Min.   :143.1   Min.   :2.070   Min.   :0.000  \n",
      " 1st Qu.:134402   1st Qu.:153.1   1st Qu.:3.210   1st Qu.:1.000  \n",
      " Median :137906   Median :156.1   Median :3.505   Median :1.000  \n",
      " Mean   :137946   Mean   :156.1   Mean   :3.495   Mean   :1.465  \n",
      " 3rd Qu.:141771   3rd Qu.:159.1   3rd Qu.:3.810   3rd Qu.:2.000  \n",
      " Max.   :145176   Max.   :168.7   Max.   :4.810   Max.   :4.000  \n",
      "   Graveyards       Weekends         SickDays     PercSickOnFri   \n",
      " Min.   :0.000   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n",
      " 1st Qu.:1.000   1st Qu.:1.0000   1st Qu.:0.000   1st Qu.:0.0000  \n",
      " Median :2.000   Median :1.0000   Median :2.000   Median :0.2500  \n",
      " Mean   :1.985   Mean   :0.9525   Mean   :1.875   Mean   :0.3522  \n",
      " 3rd Qu.:2.000   3rd Qu.:1.0000   3rd Qu.:3.000   3rd Qu.:0.6700  \n",
      " Max.   :4.000   Max.   :2.0000   Max.   :7.000   Max.   :1.0000  \n",
      " EmployeeDevHrs  ShiftSwapsReq   ShiftSwapsOffered\n",
      " Min.   : 0.00   Min.   :0.000   Min.   :0.00     \n",
      " 1st Qu.: 6.00   1st Qu.:1.000   1st Qu.:0.00     \n",
      " Median :12.00   Median :1.000   Median :1.00     \n",
      " Mean   :11.97   Mean   :1.448   Mean   :1.76     \n",
      " 3rd Qu.:17.00   3rd Qu.:2.000   3rd Qu.:3.00     \n",
      " Max.   :34.00   Max.   :5.000   Max.   :9.00     \n",
      "Just as in Chapter 9, you need to scale and center the data. To do so, you need only use \n",
      "the scale() function:\n",
      "> CallCenter.scale <- scale(CallCenter[2:11])\n",
      "> summary(CallCenter.scale)\n",
      "    AvgTix              Rating            Tardies          Graveyards      \n",
      "Min.   :-2.940189   Min.   :-3.08810   Min.   :-1.5061   Min.   :-2.4981\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "393Moving from Spreadsheets into R \n",
      "1st Qu.:-0.681684   1st Qu.:-0.61788   1st Qu.:-0.4781   1st Qu.:-1.2396  \n",
      "Median :-0.008094   Median : 0.02134   Median :-0.4781   Median : 0.0188  \n",
      "Mean   : 0.000000   Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.0000  \n",
      "3rd Qu.: 0.682476   3rd Qu.: 0.68224   3rd Qu.: 0.5500   3rd Qu.: 0.0188  \n",
      "Max.   : 2.856075   Max.   : 2.84909   Max.   : 2.6062   Max.   : 2.5359  \n",
      "    Weekends           SickDays        PercSickOnFri     EmployeeDevHrs     \n",
      "Min.   :-1.73614   Min.   :-1.12025   Min.   :-0.8963   Min.   :-1.60222  \n",
      "1st Qu.: 0.08658   1st Qu.:-1.12025   1st Qu.:-0.8963   1st Qu.:-0.79910  \n",
      "Median : 0.08658   Median : 0.07468   Median :-0.2601   Median : 0.00401  \n",
      "Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.00000  \n",
      "3rd Qu.: 0.08658   3rd Qu.: 0.67215   3rd Qu.: 0.8088   3rd Qu.: 0.67328  \n",
      "Max.   : 1.90930   Max.   : 3.06202   Max.   : 1.6486   Max.   : 2.94879  \n",
      " ShiftSwapsReq     ShiftSwapsOffered\n",
      "Min.   :-1.4477   Min.   :-0.9710  \n",
      "1st Qu.:-0.4476   1st Qu.:-0.9710  \n",
      "Median :-0.4476   Median :-0.4193  \n",
      "Mean   : 0.0000   Mean   : 0.0000  \n",
      "3rd Qu.: 0.5526   3rd Qu.: 0.6841  \n",
      "Max.   : 3.5530   Max.   : 3.9942   \n",
      "Now that the data is prepped, you can send it through the lofactor() function that’s \n",
      "part of the DMwR package:\n",
      "> install.packages(\"DMwR\",dependencies=TRUE)\n",
      "> library(DMwR)\n",
      "To call the lofactor() function, you supply it the data and a k value (this example uses \n",
      "5, just like in Chapter 9), and the function spits out LOFs:\n",
      "> CallCenter.lof <- lofactor(CallCenter.scale,5)\n",
      "Data with the highest factors (LOFs usually hover around 1) are the oddest points. \n",
      "For instance, you can highlight the data associated with those employees whose LOF is \n",
      "greater than 1.5:\n",
      "> which(CallCenter.lof > 1.5)\n",
      "[1] 299 374\n",
      "> CallCenter[which(CallCenter.lof > 1.5),]\n",
      "        ID AvgTix Rating Tardies Graveyards Weekends SickDays\n",
      "299 137155  165.3   4.49       1          3        2        1\n",
      "374 143406  145.0   2.33       3          1        0        6\n",
      "    PercSickOnFri EmployeeDevHrs ShiftSwapsReq ShiftSwapsOffered\n",
      "299          0.00             30             1                 7\n",
      "374          0.83             30             4                 0\n",
      "These are the same two outlying employees discussed in Chapter 9. But what a huge \n",
      "diff erence in the number of lines of code it took to get this:\n",
      "> CallCenter <- read.csv(\"CallCenter.csv\")\n",
      "> install.packages(\"DMwR\",dependencies=TRUE)\n",
      "> library(DMwR)\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "394 Data Smart\n",
      "> CallCenter.scale <- scale(CallCenter[2:11])\n",
      "> CallCenter.lof <- lofactor(CallCenter.scale,5)\n",
      "That’s all it took!\n",
      "Wrapping Up\n",
      "Okay, this was a fast and furious run-through of some of what you can do in R merely by \n",
      "understanding three things:\n",
      "• Loading and working with data in R\n",
      "• Finding and installing relevant packages\n",
      "• Calling functions from those packages on your dataset\n",
      "Is this all you need to know how to do in R? Nope. I didn’t cover writing your own \n",
      "functions, a whole lot of plotting, connecting to databases, the slew of apply() functions \n",
      "available, and so on. But I hope this has given you a taste to learn more. If it has, there are \n",
      "scads of R books out there worth reading as a follow-up to this chapter. Here are a few:\n",
      "• Beginning R: The Statistical Programming Language by Mark Gardener (John Wiley \n",
      "& Sons, 2012)\n",
      "• R in a Nutshell, 2nd Edition by Joseph Adler (O’Reilly, 2012)\n",
      "• Data Mining with R: Learning with Case Studies by Luis Torgo (Chapman and Hall, \n",
      "2010)\n",
      "• Machine Learning for Hackers by Drew Conway and John Myles White (O’Reilly, \n",
      "2012)\n",
      "Go forth and tinker i n R!\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Where Am I? What Just Happened?\n",
      "Y\n",
      "ou may have started this book with a rather ordinary set of skills in math and spread-\n",
      "sheet modeling, but if you’re here, having made it through alive (and having not just \n",
      "skipped the first 10 chapters), then I imagine you’re now a spreadsheet modeling connois-\n",
      "seur with a good grasp of a variety of data science techniques.\n",
      "This book has covered topics ranging from classic operations research fodder (optimiza-\n",
      "tion, Monte Carlo, and forecasting) to unsupervised learning (outlier detection, clustering, \n",
      "and graphs) to supervised AI (regression, decision stumps, and naïve Bayes). You should \n",
      "feel conﬁ dent working with spreadsheet data at this higher level. \n",
      "I also hope that Chapter 10 showed you that now that you understand data science \n",
      "techniques and algorithms, it’s quite easy to use those techniques from within a program-\n",
      "ming language such as R.\n",
      "And if there’s a particular topic that really grabbed you in this book, dive deeper! Want \n",
      "more R, more optimization, more machine learning? Grab one of the sources I recom-\n",
      "mend in each relevant chapter’s conclusion and read on. There’s so much to learn. I’ve \n",
      "only scraped the surface of analytics practice in this book.\n",
      "But wait...\n",
      "Before You Go-Go\n",
      "I want to use this conclusion to off er up some thoughts about what it means to practice \n",
      "data science in the real world, because merely knowing the math isn’t enough. \n",
      "Anyone who knows me well knows that I’m not the sharpest knife in the drawer. My \n",
      "quantitative skills are middling, but I’ve seen folks much smarter than I fail mightily at \n",
      "working as analytics professionals. The problem is that while they’re brilliant, they don’t \n",
      "know the little things that can cause technical endeavors to fail within the business \n",
      "environment. So let’s cover these softer items that can mean the success or failure of your \n",
      "analytics project or career.\n",
      "Conclusion\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart396\n",
      "Get to Know the Problem\n",
      "My favorite movie of all time is the 1992 ﬁ  lm Sneakers. The movie centers on a band of \n",
      "penetration testers led by Robert Redford that steals a “black box” capable of cracking \n",
      "RSA encryption. Hijinks ensue. (If you haven’t watched it, I envy you, because you have \n",
      "an opportunity to see it for the ﬁ rst time!)\n",
      "There’s a scene where Robert Redford encounters an electronic keypad on a locked \n",
      "offi  ce door at a think tank, and he needs to break through.\n",
      "He reaches out to his team via his headset. They’re waiting in a van outside the building.\n",
      "“Anybody ever had to defeat an electronic keypad?” he asks.\n",
      "“Those things are impossible,” Sydney Poitier exclaims. But Dan Aykroyd, also wait-\n",
      "ing in the van, comes up with an idea. They explain its complexities to Redford over the \n",
      "comms.\n",
      "Robert Redford nods his head and says, “Okay, I’ll give it a shot.”\n",
      "He ignores the keypad and kicks in the door.\n",
      "You see, the problem wasn’t “defeating an electronic keypad” at all. The problem was \n",
      "getting inside the room. Dan Aykroyd understood this.\n",
      "This is the fundamental challenge of analytics: understanding what actually must be \n",
      "solved. You must learn the situation, the processes, the data, and the circumstances. You \n",
      "need to characterize everything around the problem as best you can in order to understand \n",
      "exactly what an ideal solution is.\n",
      "In data science, you’ll often encounter the “poorly posed problem”:\n",
      " 1. Someone else in the business encounters a problem.\n",
      " 2. They use their past experience and (lack of?) analytics knowledge to frame the \n",
      "problem.\n",
      " 3. They hand their conception of the problem to the analyst as if it were set in stone \n",
      "and well posed.\n",
      " 4. The analytics person accepts and solves the problem as-is.\n",
      "This can work. But it’s not ideal, because the problem you’re asked to solve is often not \n",
      "the problem that needs solving. If this problem is really about that problem then analytics \n",
      "professionals cannot be passive. \n",
      "You cannot accept problems as handed to you in the business environment. Never allow \n",
      "yourself to be the analyst to whom problems are “thrown over the fence.” Engage with the \n",
      "people whose challenges you’re tackling to make sure you’re solving the right problem. \n",
      "Learn the business’s processes and the data that’s generated and saved. Learn how folks \n",
      "are handling the problem now, and what metrics they use (or ignore) to gauge success. \n",
      "Solve the correct, yet often misrepresented, problem. This is something no mathe-\n",
      "matical model will ever say to you. No mathematical model can ever say, “Hey, good\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "397Conclusion\n",
      "job formulating this optimization model, but I think you should take a step back and \n",
      "change your business a little instead.” And that leads me to my next point: Learn how to \n",
      "communicate.\n",
      "We Need More Translators\n",
      "If you’ve ﬁ nished this book, it’s safe to say you now know a thing or two about analytics. \n",
      "You’re familiar with the tools that are available to you. You’ve prototyped in them. And \n",
      "that allows you to identify analytics opportunities better than most, because you know \n",
      "what’s possible. You needn’t wait for someone to bring an opportunity to you. You can \n",
      "potentially go out into the business and ﬁ nd them. \n",
      "But without the ability to communicate, it becomes diffi  cult to understand others’ chal-\n",
      "lenges, articulate what’s possible, and explain the work you’re doing. \n",
      "In today’s business environment, it is often unacceptable to be skilled at only one thing. \n",
      "Data scientists are expected to be polyglots who understand math, code, and the plain-\n",
      "speak (or sports analogy-ridden speak ...ugh) of business. And the only way to get good \n",
      "at speaking to other folks, just like the only way to get good at math, is through practice.\n",
      "Take any opportunity you can to speak with others about analytics, formally and \n",
      "informally. Find ways to discuss with others in your workplace what they do, what you \n",
      "do, and ways you might collaborate. Speak with others at local meet-ups about what you \n",
      "do. Find ways to articulate analytics concepts within your particular business context.\n",
      "Push your management to involve you in planning and business development discus-\n",
      "sions. Too often the analytics professional is approached with a project only after  that \n",
      "project has been scoped, but your knowledge of the techniques and data available makes \n",
      "you indispensable in early planning.\n",
      "Push to be viewed as a person worth talking to and not as an extension of some number-\n",
      "crunching machine that problems are thrown at from a distance. The more embedded \n",
      "and communicative an analyst is within an organization, the more eff ective he or she is.\n",
      "For too long analysts have been treated like Victorian women —separated from the \n",
      "ﬁ ner points of business, because they couldn’t possibly understand it all. Oh, please. \n",
      "Let people feel the weight of your well-rounded skill set—just because they can’t crunch \n",
      "numbers doesn’t mean you can’t discuss a PowerPoint slide. Get in there, get your hands \n",
      "dirty, and talk to folks. \n",
      "Beware the Three-Headed Geek-Monster: Tools, Performance, \n",
      "and Mathematical Perfection\n",
      "There are many things that can sabotage the use of analytics within the workplace. Politics \n",
      "and inﬁ ghting perhaps; a bad experience from a previous “enterprise, business intelligence,\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart398\n",
      "cloud dashboard” project; or peers who don’t want their “dark art” optimized or automated \n",
      "for fear that their jobs will become redundant. \n",
      "Not all hurdles are within your control as an analytics professional. But some are. There \n",
      "are three primary ways I see analytics folks sabotage their own work: overly-complex \n",
      "modeling, tool obsession, and ﬁ xation on performance.\n",
      "Complexity\n",
      "Many moons ago, I worked on a supply chain optimization model for a Fortune 500 com-\n",
      "pany. This model was pretty badass if I do say so myself. We gathered all kinds of busi-\n",
      "ness rules from the client and modeled their entire shipping process as a mixed-integer \n",
      "program. We even modeled normally distributed future demand into the model in a novel \n",
      "way that ended up getting published.\n",
      "But the model was a failure. It was dead out of the gate. By dead, I don’t mean that it \n",
      "was wrong, but rather that it wasn’t used. Frankly, once the academics left, there was no \n",
      "one left in that part of the company who could keep the cumulative forecast error means \n",
      "and standard deviations up to date. The boots on the ground just didn’t understand it, \n",
      "regardless of the amount of training we gave.\n",
      "This is a diff erence between academia and industry. In academia, success is not gauged \n",
      "by usefulness. A novel optimization model is valuable in its own right, even if it is too \n",
      "complex for a supply chain analyst to keep running. \n",
      "But in the industry, analytics is a results-driven pursuit, and models are judged by their \n",
      "practical value as much as by their novelty.\n",
      "In this case, I spent too much time using complex math to optimize the company’s \n",
      "supply chain but never realistically addressed the fact that no one would be able to keep \n",
      "the model up to date. \n",
      "The mark of a true analytics professional, much like the mark of a true artist, is in knowing \n",
      "when to edit. When do you leave some of the complexity of a solution on the cutting room \n",
      "ﬂ oor? To get all cliché on you, remember that in analytics great is the enemy of good. The \n",
      "best model is one that strikes the right balance between functionality and maintainability. \n",
      "If an analytics model is never used, it’s worthless.\n",
      "Tools\n",
      "Right now in the world of analytics (whether you want to call that “data science,” “big \n",
      "data,” “business intelligence,” “blah blah blah cloud,” and so on), people have become \n",
      "focused on tools and architecture. \n",
      "Tools are important. They enable you to deploy your analytics and data-driven prod-\n",
      "ucts. But when people talk about “the best tool for the job,” they’re too often focused on \n",
      "the tool and not on the job.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "399Conclusion\n",
      "Software and services companies are in the business of selling you solutions to problems \n",
      "you may not even have yet. And to make matters worse, many of us have bosses who read \n",
      "stuff  like the Harvard Business Review and then look at us and say, “We need to be doing \n",
      "this big data thing. Go buy something, and let’s get Hadoop-ing.” \n",
      "This all leads to a dangerous climate in business today where management looks \n",
      "to tools as proof that analytics are being done, and providers just want to sell us the \n",
      "tools that enable the analytics, but there’s little accountability that actual analytics is \n",
      "getting done.\n",
      "So here’s a simple rule: Identify the analytics opportunities you want to tackle in as much \n",
      "detail as possible before acquiring tools.\n",
      "Do you need Hadoop? Well, does your problem require a divide-and-conquer aggrega-\n",
      "tion of a lot of unstructured data? No? Then the answer may be no. Don’t put the cart \n",
      "before the horse and buy the tools (or the consultants who are needed to use the open \n",
      "source tools) only to then say, “Okay, now what do we do with this?”\n",
      "Performance\n",
      "If I had a nickel every time someone raised their eyebrows when I tell them MailChimp \n",
      "uses R in production for our abuse-prevention models, I could buy a Mountain Dew. \n",
      "People think the language isn’t appropriate for production settings. If I were doing high-\n",
      "performance stock trading, it probably wouldn’t be. I’d likely code everything up in C. \n",
      "But I’m not, and I won’t.\n",
      "For MailChimp, most of our time isn’t spent in R. It’s spent moving data to send through \n",
      "the AI model. It’s not spent running the AI model, and it’s certainly not spent training the \n",
      "AI model.\n",
      "I’ve met folks who are very concerned with the speed at which their software can train \n",
      "their artiﬁ cial intelligence model. Can the model be trained in parallel, in a low-level \n",
      "language, in a live environment?\n",
      "They never stop to ask themselves if any of this is necessary and instead end up spend-\n",
      "ing a lot of time gold-plating the wrong part of their analytics project. \n",
      "At MailChimp, we retrain our models offl  ine once a quarter, test them, and then promote \n",
      "them into production. In R, it takes me a few hours to train the model. And even though \n",
      "we as a company have terabytes of data, the model’s training set, once prepped, is only 10 \n",
      "gigabytes, so I can even train the model on my laptop. Crazy. \n",
      "Given that that’s the case, I don’t waste my time on R’s training speed. I focus on more \n",
      "important things, like model accuracy.\n",
      "I’m not saying that you shouldn’t care about performance. But keep your head on \n",
      "straight, and in situations where it doesn’t matter, feel free to let it go.\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Data Smart400\n",
      "You Are Not the Most Important Function of Your \n",
      "Organization\n",
      "Okay, so there are three things to watch out for. But more generally, keep in mind that \n",
      "most companies are not in the business of doing analytics. They make their money through \n",
      "other means, and analytics is meant to serve those processes.\n",
      "You may have heard elsewhere that data scientist  is the “sexiest job of the century!” \n",
      "That’s because of how data science serves an industry. Serves being the key word.\n",
      "Consider the airline industry. They’ve been doing big data analytics for decades to \n",
      "squeeze that last nickel out of you for that seat you can barely ﬁ t in. That’s all done through \n",
      "revenue optimization models. It’s a huge win for mathematics.\n",
      "But you know what? The most important part of their business is ﬂ ying. The products \n",
      "and services an organization sells matter more than the models that tack on pennies to \n",
      "those dollars. Your goals should be things like using data to facilitate better targeting, \n",
      "forecasting, pricing, decision-making, reporting, compliance, and so on. In other words, \n",
      "work with the rest of your organization to do better business , not to do data science for \n",
      "its own sake.\n",
      "Get Creative and Keep in Touch!\n",
      "That’s enough sage wisdom. If you’ve labored through the preceding chapters then you \n",
      "have a good base to begin dreaming up, prototyping, and implementing solutions to the \n",
      "analytics opportunities posed by your business. Talk with your coworkers and get cre-\n",
      "ative. Maybe there’s an analytical solution for something that’s been patched over with \n",
      "gut feelings and manual processes. Attack it.\n",
      "And as you go through the process of implementing these and other techniques in your \n",
      "work-a-day life, keep in touch. I’m on Twitter at \n",
      "@John4man. Reach out and tell me your tale. \n",
      "Or to give me hell about this book. I’ll take any feedback.\n",
      "Happy data wrangling !\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "A\n",
      "absolute references, Solver, 110\n",
      "absolute values of errors, median \n",
      "regression, 221\n",
      "additive smoothing, 86\n",
      "adjacency matrix, 158\n",
      "affi  nity matrix, 159\n",
      "agglomerative clustering, 185\n",
      "AI model\n",
      "Bayes rule and, 83–86\n",
      "dummy variables, 210–212\n",
      "feature set, 207–208\n",
      "versus optimization model, 101–102\n",
      "overview, 206–207\n",
      "pregnancy data, 378–385\n",
      "pregnant customers (See RetailMart \n",
      "(pregnant customers))\n",
      "training data, oversampling, 210\n",
      "AIMMS, 118\n",
      "algorithms, evolutionary, 115–116\n",
      "alpha value calculation, 276–277\n",
      "arrays, formulas, 19–20\n",
      "autocorrelations, 306–313\n",
      "B\n",
      "bag of words model, 79\n",
      "extraneous punctuation, 87–88\n",
      "spaces, 88–91\n",
      "bagged decision stumps, 251\n",
      "bagging, 254. See also decision stumps\n",
      "model evaluation, 267–271\n",
      "outliers and, 271\n",
      "random forest models, 271\n",
      "Bayes rule, 82\n",
      "AI model creation, 83–86\n",
      "Big M, 133–137\n",
      "binary tree, 193–197\n",
      "BINOMDIST function, 116\n",
      "blending model, 119\n",
      "boosting, 251\n",
      "model evaluation, 280–283\n",
      "model training, 272–275\n",
      "weighted errors, 272\n",
      "reweighting, 277–278\n",
      "C\n",
      "CDF (cumulative distribution function), \n",
      "146–148, 337\n",
      "mean deviation, 147–148\n",
      "standard deviation, 147–148\n",
      "scenarios from, 148–150\n",
      "cell formatting, 5–7\n",
      "central limit theorem, 146\n",
      "chain rule of probability, 81\n",
      "charts. See also graphs\n",
      "fan chart, 331–333\n",
      "inserting, in spreadsheets, 8–9\n",
      "classiﬁ ers, bagging, 254\n",
      "cluster analysis, 29\n",
      "cluster centers, solving for, 46–48\n",
      "cluster labels, 193–197\n",
      "clustering, 29–30\n",
      "agglomerative, 185\n",
      "cluster centroid, 31\n",
      "community detection, 155–156\n",
      "divisive, 185–192\n",
      "Index\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Index402\n",
      "hierarchical partitioning, 185\n",
      "image clustering, 30\n",
      "k-means, 30–35\n",
      "e-mail marketing, 35–66\n",
      "k groups, 30–35\n",
      "k-medians, 66–67\n",
      "cosine distance, 68–69\n",
      "Excel, 69–75\n",
      "Manhattan distance, 67–68\n",
      "network graphs, 155, 156–157\n",
      "edges, 156\n",
      "nodes, 156\n",
      "Solver, 34–35\n",
      "results, 49\n",
      "coeffi  cient, variables, 214\n",
      "coeffi  cient standard error, 226–227\n",
      "coeffi  cient tests, 226–230\n",
      "community detection, clustering and, \n",
      "155–156\n",
      "modularity maximization, 156\n",
      "Concessions.xlsx ﬁ le, 2\n",
      "conditional formatting, 6–7\n",
      "conditional probabilities, 80\n",
      "Bayes rule, 82\n",
      "naÏve Bayes model, 94–98\n",
      "token counting, 92–93\n",
      "constraints, 110–112\n",
      "copying\n",
      "data, 4–5\n",
      "formulas, 4–5\n",
      "correlogram, 310–313\n",
      "cosine distance, k-medians clustering, \n",
      "68–69\n",
      "cosine similarity matrix, 172–174\n",
      "COUNTIF function, 116\n",
      "COUNTIFS statement, 235\n",
      "CPLEX, 118\n",
      "CRAN (Comprehensive R Archive \n",
      "Network), 372–373\n",
      "critical values, 310–311\n",
      "cutoff  values, 233\n",
      "D\n",
      "data\n",
      "copying, 4–5\n",
      "merging, VLOOKUP and, 12\n",
      "Data Laboratory (Gephi), 168–170\n",
      "data mining, exploratory, 29–30\n",
      "data sources, k-means clustering, 37–38\n",
      "data standardization, 40\n",
      "dataframe, 368–370\n",
      "decision stumps, 254–257, 260–263\n",
      "alpha value calculation, 276–277\n",
      "macros, 266\n",
      "number of, 257–258\n",
      "dependent situations, probability theory, \n",
      "81–82\n",
      "dependent variables, 208\n",
      "design matrix (linear regression), 227\n",
      "SSCP, 227–228\n",
      "distribution\n",
      "CDF (cumulative distribution function), \n",
      "146–148, 337\n",
      "mean deviation, 147–148\n",
      "standard deviation, 147–148\n",
      "central limit theorem, 146\n",
      "Monte Carlo simulation, 149\n",
      "probability distribution, 145–146\n",
      "standard normal distribution, 343–344\n",
      "uniform distribution, 146\n",
      "divisive clustering, 185–192\n",
      "DocGraph, 156\n",
      "document classiﬁ cation, 77\n",
      "double exponential smoothing, \n",
      "299–313\n",
      "dummy variables, 210–212\n",
      "E\n",
      "edges, network graphs, 156, 158\n",
      "kNN (k nearest neighbors) graph, 176\n",
      "r-neighborhood graphs, 176\n",
      "ensemble modeling, 251\n",
      "Ensemble.xlsm, 252\n",
      "error in calculation column, 217–218\n",
      "Euclidean distance, 41–44, 345–347\n",
      "evolutionary algorithms, 115–116\n",
      "Excel\n",
      "constraints, 110–112\n",
      "GRG, 218\n",
      "k-medians clustering, 69–75\n",
      "silhouette, 57–60\n",
      "version diff erences, 1\n",
      "exploratory data mining, 29–30\n",
      "exponential smoothing, 288–290\n",
      "double exponential smoothing, \n",
      "299–313\n",
      "forecast setup, 290–296\n",
      "Holt’s Trend-Corrected Exponential \n",
      "Smoothing, 299–313\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Index 403\n",
      "Multiplicative Holt-Winters Smoothing, \n",
      "313–333\n",
      "trends, 296–299\n",
      "F\n",
      "F test, 223–225\n",
      "factoring, R, 364–367\n",
      "false positive rate, 236–237\n",
      "fan chart, 331–333\n",
      "features, independent variables, 208\n",
      "ﬁ lters, 13–16\n",
      "Find and Replace, 9–10\n",
      "ﬂ oating-point underﬂ ow, 86\n",
      "forecasting, 285\n",
      "autocorrelations, 306–313\n",
      "correlogram, 310–313\n",
      "critical values, 310–311\n",
      "future periods, 303–304\n",
      "graphing, 296\n",
      "one-step forecast column, 291–292\n",
      "error optimization, 293–295\n",
      "Holt’s Trend-Correct Exponential \n",
      "Smoothing, 304–306\n",
      "prediction intervals, 285, 327–331\n",
      "R, 385–389\n",
      "smoothing\n",
      "exponential, 288–299\n",
      "SES (simple exponential smoothing), \n",
      "288–290\n",
      "time series data, 286–287\n",
      "deseasonalizing, 318\n",
      "seasonality, 314–315\n",
      "Format Cells menu, 5–6\n",
      "formatting\n",
      "cells, 5–7\n",
      "conditional, 6–7\n",
      "formulas\n",
      "arrays, 19–20\n",
      "copying, 4–5\n",
      "INDEX, 298\n",
      "LINEST( ), 220\n",
      "SUMPRODUCT, 19–20\n",
      "values, locating, 10–11\n",
      "VLOOKUP, 12\n",
      "Freeze Panes, 3\n",
      "Freeze Top Row, 3\n",
      "functions\n",
      "BINOMDIST, 116\n",
      "COUNTIF, 116\n",
      "HLOOKUP, 116\n",
      "IF, 116\n",
      "INDEX, 116\n",
      "LARGE, 116\n",
      "LINEST, 297\n",
      "MATCH, 116\n",
      "MAX, 116\n",
      "MEDIAN, 116\n",
      "MIN, 116\n",
      "MINVERSE, 226\n",
      "MMULT, 226\n",
      "non-linear, 116\n",
      "NORMDIST, 116, 337\n",
      "OFFSET, 116\n",
      "PERCENTILE, 337–338\n",
      "SUMIF, 116\n",
      "SUMPRODUCT, 109\n",
      "TDIST, 297\n",
      "VLOOKUP, 116\n",
      "G\n",
      "Gephi, 158, 159\n",
      "Data Laboratory, 168–170\n",
      "graph layout, 162–164\n",
      "installation, 160–162\n",
      "modularity, 197–198\n",
      "node degrees, 165–166\n",
      "printing, 166–168\n",
      "global outliers, 353\n",
      "graphs. See also charts; network graphs\n",
      "data preparation, 342–345\n",
      "forecasting and, 296\n",
      "kNN (k nearest neighbors), 347–348\n",
      "modularity\n",
      "penalities, 179–183\n",
      "points, 179–183\n",
      "outlier detection and, 345–347\n",
      "indegree, 348–351\n",
      "k-distance, 351–353\n",
      "LOFs, 353–358\n",
      "GRG, 218\n",
      "Gurobi, 118\n",
      "H\n",
      "Hadlum versus Hadlum, 336–337\n",
      "hierarchical partitioning, 185\n",
      "high-level class probabilities, 84–85\n",
      "HLOOKUP function, 116\n",
      "Holt’s Trend-Corrected Exponential \n",
      "Smoothing, 299–313\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Index404\n",
      "I\n",
      "idiot’s Bayes. See naïve Bayes\n",
      "IF function, 114, 116\n",
      "image clustering, 30\n",
      "indegree (graphs), 166\n",
      "outlier detection, 348–351\n",
      "independent variables, 208\n",
      "INDEX formula, 298\n",
      "INDEX function, 116\n",
      "integer programming, switches, 133\n",
      "intercept of linear model, 214\n",
      "IQR (Interquartile Range), \n",
      "337–338\n",
      "J\n",
      "Joey Bag O’Donuts Wholesale Wine \n",
      "Emporium, 36\n",
      "joint probability, 80–81\n",
      "chain rule of probability, 81\n",
      "JuiceLand, 120–121\n",
      "Solver, 124–126\n",
      "K\n",
      "KDD (knowledge discovery in \n",
      "databases), 30\n",
      "k-distance, graph outlier detection, \n",
      "351–353\n",
      "k-means clustering, 30–35\n",
      "cluster centers, 46–48\n",
      "data source, 37–38\n",
      "distance, 44–46\n",
      "matrix, 55–56\n",
      "ﬁ ve clusters, 60–64\n",
      "four clusters, 41\n",
      "Joey Bag O’Donuts Wholesale Wine \n",
      "Emporium, 36\n",
      "k groups, 30–35\n",
      "PivotTables, 38–39\n",
      "silhouette, 53–60\n",
      "5-Means clustering, 64–66\n",
      "spherical k-means, 372–373\n",
      "k-medians clustering, 66–67\n",
      "cosine distance, 68–69\n",
      "Excel, 69–75\n",
      "Manhattan distance, 67–68\n",
      "kNN (k nearest neighbor), 336\n",
      "outlier detection and, \n",
      "347–348\n",
      "L\n",
      "LARGE function, 116\n",
      "law of total probability, 80\n",
      "layout, Gephi graph, 162–164\n",
      "level sets, 105–106\n",
      "lexical content, stop words and, 91\n",
      "LibreOffi  ce, 1\n",
      "linear programming, 102, 103–104\n",
      "Excel and, 108–117\n",
      "fractional solutions, 113\n",
      "level sets, 105–106\n",
      "polytopes, 103–105\n",
      "simplex method, 106–108\n",
      "linear regression\n",
      "coeffi  cient, 214\n",
      "cutoff  values, 233\n",
      "design matrix, 227\n",
      "SSCP, 227–228\n",
      "false positive rate, 236–237\n",
      "intercept, 214\n",
      "LINEST( ) formula, 220\n",
      "logistic regression comparison, 245–248\n",
      "metric trade-off s, 238–239\n",
      "positive predictive value, 234–235\n",
      "ROC (Receiver Operating \n",
      "Characteristic) curve, 238–239\n",
      "simple model, 213–215\n",
      "statistics, 221\n",
      "coeffi  cient standard error, 226–227\n",
      "coeffi  cient tests, 226–230\n",
      "F test, 223–225\n",
      "prediction standard error, 226\n",
      "R-squared, 222–223\n",
      "t distribution, 230\n",
      "t test, 226–230\n",
      "sum of squared error, 215\n",
      "training the model, 218–220\n",
      "true negative rate, 235–236\n",
      "true positive rate/recall/sensitivity, 237\n",
      "validation set, 231–233\n",
      "LINEST( ) formula, 220\n",
      "LINEST function, 297\n",
      "link function, 240–241\n",
      "link spam, 166\n",
      "local outliers, 353\n",
      "LOF (local outlier factors), 353–358\n",
      "logistic regression, 239–240\n",
      "linear regression comparison, 245–248\n",
      "link function, 240–241\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Index 405\n",
      "log-likelihood, 244–245\n",
      "reoptimizing, 241–243\n",
      "statistical tests, 245\n",
      "lower inner fence (Tukey fences), 338\n",
      "M\n",
      "machine learning, 30\n",
      "macros, recording, 266\n",
      "MailChimp.com, 29\n",
      "Mandrill.com, 77–79\n",
      "Mandrill.com, 77–79\n",
      "Mandrill.xlsx, 87\n",
      "Manhattan distance, 67–68\n",
      "MATCH function, 116\n",
      "matrix inversion, 226\n",
      "matrix multiplication, 226\n",
      "MAX function, 116\n",
      "mean deviation, CDF, 147–148\n",
      "measurement, Euclidean distance, 41–44\n",
      "MEDIAN function, 116\n",
      "median regression, 221\n",
      "merging, VLOOKUP and, 12\n",
      "MIN function, 116\n",
      "minimax formulation, 131–132\n",
      "MINVERSE function, 226\n",
      "missing values, 253–254\n",
      "MMULT function, 226\n",
      "modularity, Gephi, 197–198\n",
      "modularity maximization, 156\n",
      "penalities, 179–183\n",
      "points, 179–183\n",
      "Monte Carlo simulation, 149\n",
      "Multiplicative Holt-Winters Smoothing, \n",
      "313–333\n",
      "N\n",
      "naïve Bayes, 77\n",
      "bag of words model, 79\n",
      "conditional probability tables, 94–98\n",
      "rare words, 85–86\n",
      "navigation, Control button, 2–3\n",
      "network graphs, 155\n",
      "adjacency matrix, 158\n",
      "affi  nity matrix, 159\n",
      "binary tree, 193–197\n",
      "cosine similarity matrix, 172–174\n",
      "DocGraph, 156\n",
      "edges, 156, 158\n",
      "kNN (k nearest neighbors) graph, 176\n",
      "r-neighborhood graphs, 176\n",
      "Gephi, 158\n",
      "layout, 162–164\n",
      "node degrees, 165–166\n",
      "printing, 166–168\n",
      "indegree, 166\n",
      "link spam, 166\n",
      "nodes, 156, 158\n",
      "NodeXL, 158\n",
      "outdegree, 166\n",
      "outlier detection, 166\n",
      "r-Neighborhood graph, 174–185\n",
      "symmetry, 158\n",
      "undirected, 158\n",
      "visualizing, 157–158\n",
      "WineNetwork.xlsx, 170–172\n",
      "NLP (natural language processing), 87\n",
      "lexical content, 91\n",
      "stop words, 91\n",
      "node impurity, 255–256\n",
      "nodes, network graphs, 156, 158\n",
      "NodeXL, 158\n",
      "non-linear functions, 116\n",
      "NORMDIST function, 116, 337\n",
      "null hypothesis, 224\n",
      "O\n",
      "OFFSET function, 116\n",
      "one-step forecast column, 291–292\n",
      "error optimization, 293–295\n",
      "Holt’s Trend-Correct Exponential \n",
      "Smoothing, 304–306\n",
      "OpenSolver, 26–27, 118\n",
      "variables, multiplying, 137–144\n",
      "optimization, need for, 102–103\n",
      "Optimization Model tab, 127\n",
      "optimization models, 20–26, 121–124, \n",
      "127–128\n",
      "versus artiﬁ cial intelligence model, \n",
      "101–102\n",
      "OrangeJuiceBlending.xlsx, 118\n",
      "outdegree (graphs), 166\n",
      "outlier detection, 166, 335–336\n",
      "global outliers, 353\n",
      "graphing, 345–347\n",
      "data preparation, 342–345\n",
      "indegree, 348–351\n",
      "k-distance, 351–353\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Index406\n",
      "IQR (Interquartile Range), 337–338\n",
      "kNN (k nearest neighbor), 347–348\n",
      "local outliers, 353\n",
      "LOF (local outlier factors), 353–358\n",
      "R, 389–394\n",
      "Tukey fences, 337–338\n",
      "limitations, 340–341\n",
      "spreadsheets, 338–340\n",
      "unsupervised machine learning, 336\n",
      "outliers\n",
      "bagging and, 271\n",
      "overview, 335\n",
      "oversampling, 210\n",
      "P-Q\n",
      "p( ), 79–80\n",
      "partitioning, hierarchical, 185\n",
      "Paste Special, 7–8\n",
      "PERCENTILE function, 337–338\n",
      "PivotTables, 16–19\n",
      "k-means clustering, 38–39\n",
      "PivotTable Builder, 16–17\n",
      "polytopes, 103–105\n",
      "simplex method, 106–108\n",
      "positive predictive value, 234–235\n",
      "prediction intervals, 327–331\n",
      "prediction standard error, 226\n",
      "Pregnancy Duration.xlsx, 336\n",
      "pregnancy length, 336–337\n",
      "printing in Gephi, 166–168\n",
      "probability distribution, 145–146\n",
      "probability theory, 79–80\n",
      "Bayes rule, 82\n",
      "chain rule of probability, 81\n",
      "conditional probabilities, 80\n",
      "Bayes rule, 82\n",
      "token counting, 92–93\n",
      "dependent situations, 81–82\n",
      "ﬂ oating-point underﬂ ow, 86\n",
      "high-level class probabilities, 84–85\n",
      "independent events, 81\n",
      "joint probability, 80–81\n",
      "law of total probability, 80\n",
      "multiplication rule of probability, 81\n",
      "R\n",
      "R (programming language)\n",
      "aggregate( ) function, 374\n",
      "boxplot( ) function, 390–392\n",
      "c( ) function, 364–365\n",
      "cbind( ) function, 368, 375–376\n",
      "CRAN (Comprehensive R Archive \n",
      "Network), 372–373\n",
      "data input, 363\n",
      "dataframe, 368–370\n",
      "data.frame( ) function, 368–369\n",
      "downloading, 362\n",
      "factor( ) function, 369–370\n",
      "factoring, 364–367\n",
      "forecast( ) function, 387–388\n",
      "forecasting, 385–389\n",
      "functions, built-in, 363\n",
      "glm( ) function, 378\n",
      "installation, 362\n",
      "IQR( ) function, 390\n",
      "Length( ) function, 365\n",
      "library( ) function, 372–373\n",
      "lofactor( ) function, 393\n",
      "matrices, 367–368\n",
      "matrix function, 367\n",
      "order( ) function, 376–377\n",
      "outlier detection, 389–394\n",
      "packages, 363\n",
      "performance( ) function, 383\n",
      "plot( ) function, 384\n",
      "predict( ) function, 382\n",
      "print function, 362\n",
      "randomForest( ) function, 378\n",
      "rbind( ) function, 368\n",
      "read.csv( ) function, 374\n",
      "reading data into, 370–371\n",
      "row.names( ) function, 374\n",
      "scale( ) function, 392–393\n",
      "setwd( ) command, 370\n",
      "skmeans( ) function, 373\n",
      "skmeans package, 372\n",
      "spherical k-means, 372–373\n",
      "str( ) function, 373–374, 378\n",
      "summary( ) function, 378\n",
      "summary function, 370\n",
      "t function, 367\n",
      "ts( ) function, 386\n",
      "varImpPlot( ) function, 381\n",
      "vector math, 364–367\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Index 407\n",
      "which( ) function, 366, 374, 390\n",
      "working directory, 370–371\n",
      "write.csv( ) function, 374\n",
      "random forest model, 251\n",
      "replacement and, 271\n",
      "randomForest package, 271\n",
      "rare words, naÏve Bayes and, 85–86\n",
      "references, absolute, Solver, 110\n",
      "regression\n",
      "linear\n",
      "coeffi  cient, 214\n",
      "compared to logistic, 245–248\n",
      "cutoff  values, 233\n",
      "design matrix, 227–228\n",
      "false positive rate, 236–237\n",
      "intercept, 214\n",
      "LINEST( ) formula, 220\n",
      "metric trade-off s, 238–239\n",
      "positive predictive value, \n",
      "234–235\n",
      "ROC (Receiver Operating \n",
      "Characteristic) curve, 238–239\n",
      "simple model, 213–215\n",
      "statistics, 221–230\n",
      "sum of squared error, 215\n",
      "training the model, 218–220\n",
      "true negative rate, 235–236\n",
      "true positive rate/recall/sensitivity, \n",
      "237\n",
      "validation set, 231–233\n",
      "logistic, 239–240\n",
      "compared to linear, 245–248\n",
      "link function, 240–241\n",
      "log-likelihood, 244–245\n",
      "reoptimizing, 241–243\n",
      "statistical tests, 245\n",
      "median, 221\n",
      "residual sum of squares, 222\n",
      "RetailMart (pregnant customers)\n",
      "data, 215–217\n",
      "dummy variables, 210–212\n",
      "error in calculation column, \n",
      "217–218\n",
      "feature set, 207–208\n",
      "folic acid stump, 254–257\n",
      "linear regression, 213–239\n",
      "logistic regression, 239–248\n",
      "training data, 209–210\n",
      "reweighting weighted errors, \n",
      "277–278\n",
      "risk, 144–145\n",
      "distribution\n",
      "CDF (cumulative distribution \n",
      "function), 146–148\n",
      "central limit theorem, 146\n",
      "probability distribution, 145–146\n",
      "r-neighborhood graph, 174–185\n",
      "ROC (Receiver Operating Characteristic) \n",
      "curve, 238–239, 252\n",
      "rows, freezing, 3\n",
      "R-squared, linear regression, 222–223\n",
      "S\n",
      "scenarios, standard deviation, 148–150\n",
      "constraints, 151–153\n",
      "school dance analogy for clustering, \n",
      "31–35\n",
      "seasonality (forecasting), 314–315\n",
      "SES (simple exponential smoothing), \n",
      "288–290\n",
      "silhouette\n",
      "5-Means clustering, 64–66\n",
      "Excel and, 57–60\n",
      "k-means clustering, 53–60\n",
      "simplex method, 106–108\n",
      "smoothing, exponential, 288–290\n",
      "double exponential smoothing, 299–313\n",
      "forecast setup, 290–296\n",
      "Holt’s Trend-Corrected Exponential \n",
      "Smoothing, 299–313\n",
      "Multiplicative Holt-Winters Smoothing, \n",
      "313–333\n",
      "trends, 296–299\n",
      "Solver, 20–26\n",
      "absolute references, 110\n",
      "clustering, 34–35\n",
      "results, 49\n",
      "JuiceLand problem, 124–126\n",
      "linear regression, training the model, \n",
      "218–220\n",
      "OpenSolver, 26–27\n",
      "sorting, 13–16\n",
      "spaces, 88–91\n",
      "spherical k-means, 372–373\n",
      "spreadsheets\n",
      "arrays, formulas, 19–20\n",
      "charts, inserting, 8–9\n",
      "copying\n",
      "data, 4–5\n",
      "formulas, 4–5\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Index408\n",
      "ﬁ lters, 13–16\n",
      "formatting\n",
      "cells, 5–7\n",
      "conditional, 6–7\n",
      "Freeze Panes, 3\n",
      "Freeze Top Row, 3\n",
      "Holt’s Trend-Corrected Exponential \n",
      "Smoothing, 300–306\n",
      "navigating, Control button, 2–3\n",
      "Paste Special option, 7–8\n",
      "PivotTables, 16–19\n",
      "sorting, 13–16\n",
      "Tukey fences, 338–340\n",
      "limitations, 340–341\n",
      "SSCP (sum of squares and cross \n",
      "products) matrix, 227–228\n",
      "standard deviation\n",
      "CDF, 147–148\n",
      "scenarios from, 148–150\n",
      "constraints, 151–153\n",
      "standard normal distribution, 343–344\n",
      "standardizing data, 40\n",
      "statistics, 221\n",
      "coeffi  cient standard error, 226–227\n",
      "coeffi  cient tests, 226–230\n",
      "F test, 223–225\n",
      "logistic regression, 245\n",
      "matrix inversion, 226\n",
      "matrix multiplication, 226\n",
      "prediction standard error, 226\n",
      "residual sum of squares, 222\n",
      "R-squared, 222–223\n",
      "t distribution, 230\n",
      "t test, 226–230\n",
      "total sum of squares, 222\n",
      "stop words, 91\n",
      "SUBSTITUTE command, 87–88\n",
      "sum of squared error, 215\n",
      "SUMIF function, 116\n",
      "SUMPRODUCT formula, 19–20\n",
      "SUMPRODUCT function, 109\n",
      "supervised machine learning, 30\n",
      "switches, 133\n",
      "SwordForecasting.xlsm, 286\n",
      "symmetry in network graphs, 158\n",
      "T\n",
      "t distribution, 230\n",
      "t test, 226–230\n",
      "TDIST function, 297\n",
      "time series data, forecasting and, 286–287\n",
      "tokens, conditional probability, 92–93\n",
      "total sum of squares, 222\n",
      "training data\n",
      "decision stumps, 260–263\n",
      "oversampling, 210\n",
      "random sample, 258–260\n",
      "trends, forecasting, exponential \n",
      "smoothing, 296–299\n",
      "triple exponential smoothing, 313–333\n",
      "true negative rate, 235–236\n",
      "true positive rate/recall/sensitivity, 237\n",
      "Tukey fences, 337–338\n",
      "limitations, 340–341\n",
      "lower inner fence, 338\n",
      "spreadsheets, 338–340\n",
      "upper inner fence, 338\n",
      "U\n",
      "undirected network graphs, 158\n",
      "uniform distribution, 146\n",
      "unsupervised machine learning, 30, 336\n",
      "upper inner fence (Tukey fences), 338\n",
      "V\n",
      "validation set, 231–233\n",
      "values\n",
      "locating, with formulas, 10–11\n",
      "missing, 253–254\n",
      "variables\n",
      "coeffi  cient, 214\n",
      "dependent, 208\n",
      "dummy variables, 210–212\n",
      "independent, 208\n",
      "multiplying, 137–144\n",
      "vector math, R, 364–367\n",
      "VLOOKUP formulas, 12\n",
      "VLOOKUP function, 116\n",
      "Voronoi diagram, 32\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "Index 409\n",
      "W–Z\n",
      "weak learners, 254–255\n",
      "weighted errors, 272\n",
      "reweighting, 277–278\n",
      "WineKMC.xlsx, 36\n",
      "WineNetwork.xlsx, building graph, \n",
      "170–172\n",
      "workbooks\n",
      "Ensemble.xlsm, 252\n",
      "Mandrill.xlsx, 87\n",
      "OrangeJuiceBlending.xlsx, 118\n",
      "Pregnancy Duration.xlsx, 336\n",
      "SwordForecasting.xlsm, 286\n",
      "WineKMC.xlsx, 36\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\n",
    "    \"D:\\Library\\Sliit\\Data Smart Using Data Science to Transform Information into Insight (John W. Foreman) ().pdf\",\n",
    "    mode=\"single\",\n",
    "    pages_delimiter=\"\\n-------THIS IS A CUSTOM END OF PAGE-------\\n\",\n",
    ")\n",
    "docs = loader.load()\n",
    "print(docs[0].page_content[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8da911b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
