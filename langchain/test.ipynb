{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9174579f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a light-hearted joke:  \n",
      "\n",
      "**\"Why don't skeletons fight each other? They don't have the guts!\"**  \n",
      "\n",
      "ðŸ˜„ Let me know if you want another one!"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "import textwrap\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"qwen3:1.7b\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    )\n",
    "\n",
    "response = model.stream(\"tell me a joke.\")\n",
    "# response.content\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e1519f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     11\u001b[39m model = ChatOpenAI(\n\u001b[32m     12\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mfunctiongemma:270m\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m     base_url=\u001b[33m\"\u001b[39m\u001b[33mhttp://localhost:11434/v1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m     tools=[get_weather]\n\u001b[32m     15\u001b[39m     )\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# response = model.invoke(\"Why do parrots talk?\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m response = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhat is the weather in sf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m response.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:399\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    398\u001b[39m             \u001b[38;5;28mself\u001b[39m.generate_prompt(\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m                 [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m],\n\u001b[32m    400\u001b[39m                 stop=stop,\n\u001b[32m    401\u001b[39m                 callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    402\u001b[39m                 tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    403\u001b[39m                 metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    404\u001b[39m                 run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    405\u001b[39m                 run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    406\u001b[39m                 **kwargs,\n\u001b[32m    407\u001b[39m             ).generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:382\u001b[39m, in \u001b[36mBaseChatModel._convert_input\u001b[39m\u001b[34m(self, model_input)\u001b[39m\n\u001b[32m    377\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages=convert_to_messages(model_input))\n\u001b[32m    378\u001b[39m msg = (\n\u001b[32m    379\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    380\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    381\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "import textwrap\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"none\"\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"functiongemma:270m\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    tools=[get_weather]\n",
    "    )\n",
    "\n",
    "# response = model.invoke(\"Why do parrots talk?\")\n",
    "response = model.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20aaec6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: \n",
      "Tool Calls: [{'name': 'get_weather', 'args': {'city': 'sf'}, 'id': 'call_v74wjrqd', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 1. Setup environment\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"ollama\" # Can be anything for local\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "# 2. Initialize the model\n",
    "model = ChatOpenAI(\n",
    "    model=\"functiongemma:270m\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# 3. Bind the tools to the model\n",
    "# This tells the model what functions it is allowed to \"call\"\n",
    "model_with_tools = model.bind_tools([get_weather])\n",
    "\n",
    "# 4. Invoke the model\n",
    "response = model_with_tools.invoke(\"what is the weather in sf\")\n",
    "\n",
    "# 5. Inspect the tool calls\n",
    "print(f\"Content: {response.content}\")\n",
    "print(f\"Tool Calls: {response.tool_calls}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1aada86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3639: UserWarning: WARNING! system_prompt is not default parameter.\n",
      "                system_prompt was transferred to model_kwargs.\n",
      "                Please confirm that system_prompt is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "c:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3639: UserWarning: WARNING! tools is not default parameter.\n",
      "                tools was transferred to model_kwargs.\n",
      "                Please confirm that tools is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "c:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3639: UserWarning: WARNING! context_schema is not default parameter.\n",
      "                context_schema was transferred to model_kwargs.\n",
      "                Please confirm that context_schema is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "c:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3639: UserWarning: WARNING! response_format is not default parameter.\n",
      "                response_format was transferred to model_kwargs.\n",
      "                Please confirm that response_format is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "c:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3639: UserWarning: WARNING! checkpointer is not default parameter.\n",
      "                checkpointer was transferred to model_kwargs.\n",
      "                Please confirm that checkpointer is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Run agent\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# `thread_id` is a unique identifier for a given conversation.\u001b[39;00m\n\u001b[32m     65\u001b[39m config = {\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m}}\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m response = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhat is the weather outside?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[38;5;28mprint\u001b[39m(response[\u001b[33m'\u001b[39m\u001b[33mstructured_response\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# ResponseFormat(\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m#     punny_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m#     weather_conditions=\"It's always sunny in Florida!\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     79\u001b[39m \n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Note that we can continue the conversation using the same `thread_id`.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:399\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    398\u001b[39m             \u001b[38;5;28mself\u001b[39m.generate_prompt(\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m                 [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m],\n\u001b[32m    400\u001b[39m                 stop=stop,\n\u001b[32m    401\u001b[39m                 callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    402\u001b[39m                 tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    403\u001b[39m                 metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    404\u001b[39m                 run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    405\u001b[39m                 run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    406\u001b[39m                 **kwargs,\n\u001b[32m    407\u001b[39m             ).generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pkmpp\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:382\u001b[39m, in \u001b[36mBaseChatModel._convert_input\u001b[39m\u001b[34m(self, model_input)\u001b[39m\n\u001b[32m    377\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages=convert_to_messages(model_input))\n\u001b[32m    378\u001b[39m msg = (\n\u001b[32m    379\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    380\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    381\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "\n",
    "# Define system prompt\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n",
    "\n",
    "You have access to two tools:\n",
    "\n",
    "- get_weather_for_location: use this to get the weather for a specific location\n",
    "- get_user_location: use this to get the user's location\n",
    "\n",
    "If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\"\n",
    "\n",
    "# Define context schema\n",
    "@dataclass\n",
    "class Context:\n",
    "    \"\"\"Custom runtime context schema.\"\"\"\n",
    "    user_id: str\n",
    "\n",
    "# Define tools\n",
    "@tool\n",
    "def get_weather_for_location(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "@tool\n",
    "def get_user_location(runtime: ToolRuntime[Context]) -> str:\n",
    "    \"\"\"Retrieve user information based on user ID.\"\"\"\n",
    "    user_id = runtime.context.user_id\n",
    "    return \"Florida\" if user_id == \"1\" else \"SF\"\n",
    "\n",
    "# Define response format\n",
    "@dataclass\n",
    "class ResponseFormat:\n",
    "    \"\"\"Response schema for the agent.\"\"\"\n",
    "    # A punny response (always required)\n",
    "    punny_response: str\n",
    "    # Any interesting information about the weather if available\n",
    "    weather_conditions: str | None = None\n",
    "\n",
    "# Set up memory\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "# Create agent\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"ollama\"\n",
    "agent = ChatOpenAI(\n",
    "    model=\"functiongemma:270m\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    tools=[get_user_location, get_weather_for_location],\n",
    "    context_schema=Context,\n",
    "    response_format=ToolStrategy(ResponseFormat),\n",
    "    checkpointer=checkpointer\n",
    ")\n",
    "\n",
    "# Run agent\n",
    "# `thread_id` is a unique identifier for a given conversation.\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n",
    "    config=config,\n",
    "    context=Context(user_id=\"1\")\n",
    ")\n",
    "\n",
    "print(response['structured_response'])\n",
    "# ResponseFormat(\n",
    "#     punny_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n",
    "#     weather_conditions=\"It's always sunny in Florida!\"\n",
    "# )\n",
    "\n",
    "\n",
    "# Note that we can continue the conversation using the same `thread_id`.\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n",
    "    config=config,\n",
    "    context=Context(user_id=\"1\")\n",
    ")\n",
    "\n",
    "print(response['structured_response'])\n",
    "# ResponseFormat(\n",
    "#     punny_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n",
    "#     weather_conditions=None\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
