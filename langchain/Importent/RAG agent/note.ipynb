{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa7c1400",
   "metadata": {},
   "source": [
    "# RAG(retrieval augmented generation) Agent with langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e2727f",
   "metadata": {},
   "source": [
    "## Preview\n",
    "In this guide we’ll build an app that answers questions about the website’s content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a2e1dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain langchain-text-splitters langchain-community bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e6f539",
   "metadata": {},
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99af17d7",
   "metadata": {},
   "source": [
    "### Chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0cdfba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    # model=\"functiongemma:270m\", \n",
    "    # model=\"qwen3:1.7b\", \n",
    "    # model=\"qwen3:0.6b-fp16\", \n",
    "    # model=\"qwen3:1.7b-q8_0\", \n",
    "    # model=\"qwen3:1.7b-fp16\", \n",
    "    model=\"qwen3:4b\", \n",
    "\n",
    "    # model=\"phi4-mini\", \n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c3b163",
   "metadata": {},
   "source": [
    "### Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19ce90b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"qwen3-embedding:0.6b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f778b",
   "metadata": {},
   "source": [
    "### Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22c6b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f720c633",
   "metadata": {},
   "source": [
    "## 01. indexing\n",
    "\n",
    "> load -> split -> store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab356dd1",
   "metadata": {},
   "source": [
    "### Loading documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1594f273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43047\n"
     ]
    }
   ],
   "source": [
    "import bs4  # BeautifulSoup: parse website in to text\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52ad0d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510cd33f",
   "metadata": {},
   "source": [
    "### Splitting documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3edf68b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 63 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200,\n",
    "    add_start_index = True, # track index in original document\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0bf3a0",
   "metadata": {},
   "source": [
    "### Storing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d6b9f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4fc7d8ae-8bf7-41ad-a439-279e52d3ed79', '18676a05-8766-4dcc-a1db-da59b5bed4c3', '152f2120-486c-4cb6-b9eb-f52315720bdb']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2f5460",
   "metadata": {},
   "source": [
    "## 2. Retrieval and generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca3998f",
   "metadata": {},
   "source": [
    "### RAG agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b2e8b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieved_docs = vector_store.similarity_search(\"llm\" , k=2)\n",
    "# serialized = \"\\n\\n\".join(\n",
    "#         (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "#         for doc in retrieved_docs\n",
    "#     )\n",
    "\n",
    "# retrieved_docs , serialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "30dc0d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query , k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "242ca073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a blog post. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "agent = create_agent(model, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c5a80424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the standard method for Task Decomposition?\n",
      "\n",
      "Once you get the answer, look up common extensions of that method.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (call_riiza4us)\n",
      " Call ID: call_riiza4us\n",
      "  Args:\n",
      "    query: What is the standard method for Task Decomposition?\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17734}\n",
      "Content: The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The standard method for Task Decomposition is **Chain of Thought (CoT)**, which transforms complex tasks into smaller, manageable steps by instructing models to \"think step by step\" (Wei et al., 2022). \n",
      "\n",
      "A common extension of this method is **Tree of Thoughts (ToT)** (Yao et al., 2023), which explores multiple reasoning pathways at each step, creating a tree structure that can be navigated via BFS (breadth-first search) or DFS (depth-first search). This allows for more comprehensive problem-solving by considering diverse possibilities during decomposition.\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
    "    \"Once you get the answer, look up common extensions of that method.\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bd8581",
   "metadata": {},
   "source": [
    "### RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e1e6acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state message.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    retrieved_docs = vector_store.similarity_search(last_query)\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    system_message = (\n",
    "        \"you are a helpful assistant. Use the following context in your response:\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "\n",
    "    return system_message\n",
    "\n",
    "agent = create_agent(model, tools=[], middleware=[prompt_with_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8c5c959d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is task decomposition?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Based on the context provided, task decomposition refers to the process of breaking down complex tasks into smaller, more manageable steps. This concept is particularly important in AI workflows where complicated tasks require structured planning.\n",
      "\n",
      "The context specifically describes two approaches within this framework:\n",
      "1. **Chain of Thought (CoT)** - A technique where models are instructed to \"think step by step\" to decompose hard tasks into smaller, simpler steps. This transforms big tasks into multiple manageable components and provides visibility into the model's reasoning process.\n",
      "\n",
      "2. **Tree of Thoughts (ToT)** - An extension of CoT that explores multiple reasoning paths at each step. It first decomposes problems into multiple thought steps, generating multiple possibilities per step to create a tree structure. The search can proceed via BFS (breadth-first search) or DFS (depth-first search), with states evaluated through classification or majority voting.\n",
      "\n",
      "Task decomposition is thus a strategic planning technique that enables agents to handle complex requests systematically by first breaking them into smaller units before executing.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is task decomposition?\"\n",
    "\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]}, stream_mode=\"values\"\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8189f29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933da77",
   "metadata": {},
   "source": [
    "### Returning source documents\n",
    "The above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\n",
    "\n",
    "01. Adding a key to the state to store the retrieved documents\n",
    "02. Adding a new node via a pre-model hook to populate that key (as well as inject the context).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "585712a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain_core.documents import Document\n",
    "from langchain.agents.middleware import AgentMiddleware, AgentState\n",
    "\n",
    "\n",
    "class State(AgentState):\n",
    "    context: list[Document]\n",
    "\n",
    "\n",
    "class RetrieveDocumentsMiddleware(AgentMiddleware[State]):\n",
    "    state_schema = State\n",
    "\n",
    "    def before_model(self, state: AgentState) -> dict[str, Any] | None:\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        retrieved_docs = vector_store.similarity_search(last_message.text)\n",
    "\n",
    "        docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "        augmented_message_content = (\n",
    "            f\"{last_message.text}\\n\\n\"\n",
    "            \"Use the following context to answer the query:\\n\"\n",
    "            f\"{docs_content}\"\n",
    "        )\n",
    "        return {\n",
    "            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\n",
    "            \"context\": retrieved_docs,\n",
    "        }\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model,\n",
    "    tools=[],\n",
    "    middleware=[RetrieveDocumentsMiddleware()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "91660fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is task decomposition?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is task decomposition?\n",
      "\n",
      "Use the following context to answer the query:\n",
      "The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\n",
      "\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\n",
      "Instruction:\n",
      "\n",
      "Given the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\n",
      "\n",
      "(3) Task execution: Expert models execute on the specific tasks and log results.\n",
      "Instruction:\n",
      "\n",
      "Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\n",
      "\n",
      "\n",
      "Planning & Reacting: translate the reflections and the environment information into actions\n",
      "\n",
      "Planning is essentially in order to optimize believability at the moment vs in time.\n",
      "Prompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\n",
      "Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\n",
      "Environment information is present in a tree structure.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The generative agent architecture. (Image source: Park et al. 2023)\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Task decomposition is the process of breaking down a complex user request into smaller, more manageable subtasks to facilitate efficient execution by an AI agent. Based on the provided context, this is primarily achieved through techniques like **Chain of Thought (CoT)** and **Tree of Thoughts (ToT)**:\n",
      "\n",
      "1. **Chain of Thought (CoT)**: Instructs the model to \"think step by step\" to decompose complex tasks into simpler, sequential steps. This allows the system to utilize test-time computation to handle intricate problems by transforming them into smaller, interpretable components (as described by Wei et al., 2022).\n",
      "\n",
      "2. **Tree of Thoughts (ToT)**: Extends CoT by exploring multiple reasoning paths at each step, generating a tree structure of potential solutions. This enables the system to handle uncertainty in complex tasks through breadth-first or depth-first search strategies (Yao et al., 2023).\n",
      "\n",
      "This decomposition is critical for AI planning, as it helps the agent parse user inputs into well-structured tasks with defined dependencies (e.g., using the `dep` field to track resource reliance from prior tasks). By reducing complexity, it ensures the system can execute user requests systematically while maintaining logical flow and resource efficiency.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is task decomposition?\"\n",
    "\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]}, stream_mode=\"values\"\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd50738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
